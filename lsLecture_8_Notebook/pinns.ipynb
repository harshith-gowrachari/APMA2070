{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a069bff",
   "metadata": {},
   "source": [
    "## Submodule-2.1 : Dynamical Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0792af43",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "#### 1. [PINN for Burger's Equation in TensorFlow](#PINN-for-Burger's-Equation-in-TensorFlow)\n",
    "#### 2. [PINN for Burger's Equation in JAX](#PINN-for-Burger's-Equation-in-JAX)\n",
    "#### 3. [PINN for a Boundary Layer Problem](#PINN-for-a-Boundary-Layer-Problem)\n",
    "#### 4. [Neural Network with Hard Constraints](#Neural-Network-with-Hard-Constraints)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd3f65f",
   "metadata": {},
   "source": [
    "## PINN-for-Burger's-Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "860b77dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, 'Utilities/')\n",
    "import os\n",
    "\n",
    "from scipy.interpolate import griddata\n",
    "from pyDOE import lhs\n",
    "from plotting import newfig, savefig\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import scipy.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c77d7858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 0 and loss is: 0.2566310167312622\n",
      "Iteration is: 1 and loss is: 0.23526228964328766\n",
      "Iteration is: 2 and loss is: 0.22125136852264404\n",
      "Iteration is: 3 and loss is: 0.2152591347694397\n",
      "Iteration is: 4 and loss is: 0.21643051505088806\n",
      "Iteration is: 5 and loss is: 0.22051557898521423\n",
      "Iteration is: 6 and loss is: 0.22254502773284912\n",
      "Iteration is: 7 and loss is: 0.22123509645462036\n",
      "Iteration is: 8 and loss is: 0.2179287075996399\n",
      "Iteration is: 9 and loss is: 0.21430082619190216\n",
      "Iteration is: 10 and loss is: 0.21143503487110138\n",
      "Iteration is: 11 and loss is: 0.2097298800945282\n",
      "Iteration is: 12 and loss is: 0.20907731354236603\n",
      "Iteration is: 13 and loss is: 0.20907141268253326\n",
      "Iteration is: 14 and loss is: 0.20919497311115265\n",
      "Iteration is: 15 and loss is: 0.2089935839176178\n",
      "Iteration is: 16 and loss is: 0.20821210741996765\n",
      "Iteration is: 17 and loss is: 0.2068447321653366\n",
      "Iteration is: 18 and loss is: 0.20509067177772522\n",
      "Iteration is: 19 and loss is: 0.2032514214515686\n",
      "Iteration is: 20 and loss is: 0.20160870254039764\n",
      "Iteration is: 21 and loss is: 0.20031368732452393\n",
      "Iteration is: 22 and loss is: 0.1993217170238495\n",
      "Iteration is: 23 and loss is: 0.19841761887073517\n",
      "Iteration is: 24 and loss is: 0.1973397135734558\n",
      "Iteration is: 25 and loss is: 0.19593293964862823\n",
      "Iteration is: 26 and loss is: 0.19422461092472076\n",
      "Iteration is: 27 and loss is: 0.19237802922725677\n",
      "Iteration is: 28 and loss is: 0.19056735932826996\n",
      "Iteration is: 29 and loss is: 0.1888543963432312\n",
      "Iteration is: 30 and loss is: 0.18714410066604614\n",
      "Iteration is: 31 and loss is: 0.1852571964263916\n",
      "Iteration is: 32 and loss is: 0.18307088315486908\n",
      "Iteration is: 33 and loss is: 0.18061403930187225\n",
      "Iteration is: 34 and loss is: 0.17804372310638428\n",
      "Iteration is: 35 and loss is: 0.17552711069583893\n",
      "Iteration is: 36 and loss is: 0.17311783134937286\n",
      "Iteration is: 37 and loss is: 0.17072910070419312\n",
      "Iteration is: 38 and loss is: 0.16824722290039062\n",
      "Iteration is: 39 and loss is: 0.16568174958229065\n",
      "Iteration is: 40 and loss is: 0.16318590939044952\n",
      "Iteration is: 41 and loss is: 0.16091670095920563\n",
      "Iteration is: 42 and loss is: 0.1588827222585678\n",
      "Iteration is: 43 and loss is: 0.1569649577140808\n",
      "Iteration is: 44 and loss is: 0.15510302782058716\n",
      "Iteration is: 45 and loss is: 0.15339994430541992\n",
      "Iteration is: 46 and loss is: 0.15198446810245514\n",
      "Iteration is: 47 and loss is: 0.1508234590291977\n",
      "Iteration is: 48 and loss is: 0.14979110658168793\n",
      "Iteration is: 49 and loss is: 0.14889764785766602\n",
      "Iteration is: 50 and loss is: 0.1482483148574829\n",
      "Iteration is: 51 and loss is: 0.14777912199497223\n",
      "Iteration is: 52 and loss is: 0.14729447662830353\n",
      "Iteration is: 53 and loss is: 0.1467658281326294\n",
      "Iteration is: 54 and loss is: 0.14626574516296387\n",
      "Iteration is: 55 and loss is: 0.14570298790931702\n",
      "Iteration is: 56 and loss is: 0.14497317373752594\n",
      "Iteration is: 57 and loss is: 0.14414356648921967\n",
      "Iteration is: 58 and loss is: 0.14321483671665192\n",
      "Iteration is: 59 and loss is: 0.1420833319425583\n",
      "Iteration is: 60 and loss is: 0.14080394804477692\n",
      "Iteration is: 61 and loss is: 0.1394791603088379\n",
      "Iteration is: 62 and loss is: 0.1380748599767685\n",
      "Iteration is: 63 and loss is: 0.13660776615142822\n",
      "Iteration is: 64 and loss is: 0.13515442609786987\n",
      "Iteration is: 65 and loss is: 0.13368332386016846\n",
      "Iteration is: 66 and loss is: 0.1321907341480255\n",
      "Iteration is: 67 and loss is: 0.1307348757982254\n",
      "Iteration is: 68 and loss is: 0.1292707622051239\n",
      "Iteration is: 69 and loss is: 0.1277618408203125\n",
      "Iteration is: 70 and loss is: 0.12623974680900574\n",
      "Iteration is: 71 and loss is: 0.1246735155582428\n",
      "Iteration is: 72 and loss is: 0.12307177484035492\n",
      "Iteration is: 73 and loss is: 0.12147055566310883\n",
      "Iteration is: 74 and loss is: 0.11983572691679001\n",
      "Iteration is: 75 and loss is: 0.11820924282073975\n",
      "Iteration is: 76 and loss is: 0.11662177741527557\n",
      "Iteration is: 77 and loss is: 0.11508794873952866\n",
      "Iteration is: 78 and loss is: 0.1136520504951477\n",
      "Iteration is: 79 and loss is: 0.11229774355888367\n",
      "Iteration is: 80 and loss is: 0.11108487844467163\n",
      "Iteration is: 81 and loss is: 0.11000023782253265\n",
      "Iteration is: 82 and loss is: 0.10906646400690079\n",
      "Iteration is: 83 and loss is: 0.10825975239276886\n",
      "Iteration is: 84 and loss is: 0.10760527849197388\n",
      "Iteration is: 85 and loss is: 0.10705451667308807\n",
      "Iteration is: 86 and loss is: 0.10659933090209961\n",
      "Iteration is: 87 and loss is: 0.10619945824146271\n",
      "Iteration is: 88 and loss is: 0.10579840838909149\n",
      "Iteration is: 89 and loss is: 0.10538260638713837\n",
      "Iteration is: 90 and loss is: 0.10491597652435303\n",
      "Iteration is: 91 and loss is: 0.1043953150510788\n",
      "Iteration is: 92 and loss is: 0.10384710878133774\n",
      "Iteration is: 93 and loss is: 0.10327647626399994\n",
      "Iteration is: 94 and loss is: 0.10270283371210098\n",
      "Iteration is: 95 and loss is: 0.10214076936244965\n",
      "Iteration is: 96 and loss is: 0.10159572958946228\n",
      "Iteration is: 97 and loss is: 0.10106892138719559\n",
      "Iteration is: 98 and loss is: 0.10055893659591675\n",
      "Iteration is: 99 and loss is: 0.10007699579000473\n",
      "Iteration is: 100 and loss is: 0.09962654858827591\n",
      "Iteration is: 101 and loss is: 0.09921139478683472\n",
      "Iteration is: 102 and loss is: 0.09882955998182297\n",
      "Iteration is: 103 and loss is: 0.09847600758075714\n",
      "Iteration is: 104 and loss is: 0.09814418852329254\n",
      "Iteration is: 105 and loss is: 0.09782343357801437\n",
      "Iteration is: 106 and loss is: 0.09750746190547943\n",
      "Iteration is: 107 and loss is: 0.09718938171863556\n",
      "Iteration is: 108 and loss is: 0.09687039256095886\n",
      "Iteration is: 109 and loss is: 0.09655176848173141\n",
      "Iteration is: 110 and loss is: 0.09624254703521729\n",
      "Iteration is: 111 and loss is: 0.09595838934183121\n",
      "Iteration is: 112 and loss is: 0.09575261175632477\n",
      "Iteration is: 113 and loss is: 0.09573458135128021\n",
      "Iteration is: 114 and loss is: 0.09586568176746368\n",
      "Iteration is: 115 and loss is: 0.09547023475170135\n",
      "Iteration is: 116 and loss is: 0.09469334781169891\n",
      "Iteration is: 117 and loss is: 0.09468528628349304\n",
      "Iteration is: 118 and loss is: 0.09484755247831345\n",
      "Iteration is: 119 and loss is: 0.09431066364049911\n",
      "Iteration is: 120 and loss is: 0.09402507543563843\n",
      "Iteration is: 121 and loss is: 0.09421437978744507\n",
      "Iteration is: 122 and loss is: 0.09390582889318466\n",
      "Iteration is: 123 and loss is: 0.09352030605077744\n",
      "Iteration is: 124 and loss is: 0.09361597895622253\n",
      "Iteration is: 125 and loss is: 0.09347134828567505\n",
      "Iteration is: 126 and loss is: 0.09307795763015747\n",
      "Iteration is: 127 and loss is: 0.09304910153150558\n",
      "Iteration is: 128 and loss is: 0.0930066704750061\n",
      "Iteration is: 129 and loss is: 0.09267517924308777\n",
      "Iteration is: 130 and loss is: 0.09253612160682678\n",
      "Iteration is: 131 and loss is: 0.09253039956092834\n",
      "Iteration is: 132 and loss is: 0.09229648113250732\n",
      "Iteration is: 133 and loss is: 0.09207763522863388\n",
      "Iteration is: 134 and loss is: 0.09204176813364029\n",
      "Iteration is: 135 and loss is: 0.09191572666168213\n",
      "Iteration is: 136 and loss is: 0.091681107878685\n",
      "Iteration is: 137 and loss is: 0.09156237542629242\n",
      "Iteration is: 138 and loss is: 0.09150366485118866\n",
      "Iteration is: 139 and loss is: 0.09134268015623093\n",
      "Iteration is: 140 and loss is: 0.09115336835384369\n",
      "Iteration is: 141 and loss is: 0.0910535678267479\n",
      "Iteration is: 142 and loss is: 0.09097860753536224\n",
      "Iteration is: 143 and loss is: 0.09083791077136993\n",
      "Iteration is: 144 and loss is: 0.09067252278327942\n",
      "Iteration is: 145 and loss is: 0.09055700898170471\n",
      "Iteration is: 146 and loss is: 0.09047887474298477\n",
      "Iteration is: 147 and loss is: 0.0903811976313591\n",
      "Iteration is: 148 and loss is: 0.09024763107299805\n",
      "Iteration is: 149 and loss is: 0.09010949730873108\n",
      "Iteration is: 150 and loss is: 0.08999595046043396\n",
      "Iteration is: 151 and loss is: 0.08990645408630371\n",
      "Iteration is: 152 and loss is: 0.08982306718826294\n",
      "Iteration is: 153 and loss is: 0.08973076194524765\n",
      "Iteration is: 154 and loss is: 0.08962595462799072\n",
      "Iteration is: 155 and loss is: 0.08951251208782196\n",
      "Iteration is: 156 and loss is: 0.08939749747514725\n",
      "Iteration is: 157 and loss is: 0.08928579092025757\n",
      "Iteration is: 158 and loss is: 0.08917940407991409\n",
      "Iteration is: 159 and loss is: 0.08907785266637802\n",
      "Iteration is: 160 and loss is: 0.0889798104763031\n",
      "Iteration is: 161 and loss is: 0.0888841301202774\n",
      "Iteration is: 162 and loss is: 0.08879027515649796\n",
      "Iteration is: 163 and loss is: 0.08869864791631699\n",
      "Iteration is: 164 and loss is: 0.0886114239692688\n",
      "Iteration is: 165 and loss is: 0.08853551745414734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 166 and loss is: 0.08849170804023743\n",
      "Iteration is: 167 and loss is: 0.08854237198829651\n",
      "Iteration is: 168 and loss is: 0.08885208517313004\n",
      "Iteration is: 169 and loss is: 0.08965422213077545\n",
      "Iteration is: 170 and loss is: 0.09042058140039444\n",
      "Iteration is: 171 and loss is: 0.08939284086227417\n",
      "Iteration is: 172 and loss is: 0.08793023228645325\n",
      "Iteration is: 173 and loss is: 0.08872589468955994\n",
      "Iteration is: 174 and loss is: 0.08907507359981537\n",
      "Iteration is: 175 and loss is: 0.08781430125236511\n",
      "Iteration is: 176 and loss is: 0.08811157941818237\n",
      "Iteration is: 177 and loss is: 0.08850400894880295\n",
      "Iteration is: 178 and loss is: 0.0875641256570816\n",
      "Iteration is: 179 and loss is: 0.08781102299690247\n",
      "Iteration is: 180 and loss is: 0.08800071477890015\n",
      "Iteration is: 181 and loss is: 0.0873023122549057\n",
      "Iteration is: 182 and loss is: 0.08757937699556351\n",
      "Iteration is: 183 and loss is: 0.08755649626255035\n",
      "Iteration is: 184 and loss is: 0.08707951754331589\n",
      "Iteration is: 185 and loss is: 0.08734317868947983\n",
      "Iteration is: 186 and loss is: 0.08717349171638489\n",
      "Iteration is: 187 and loss is: 0.0868878960609436\n",
      "Iteration is: 188 and loss is: 0.08708421140909195\n",
      "Iteration is: 189 and loss is: 0.08685559034347534\n",
      "Iteration is: 190 and loss is: 0.08670113980770111\n",
      "Iteration is: 191 and loss is: 0.08681567013263702\n",
      "Iteration is: 192 and loss is: 0.08658843487501144\n",
      "Iteration is: 193 and loss is: 0.08650179952383041\n",
      "Iteration is: 194 and loss is: 0.08655168861150742\n",
      "Iteration is: 195 and loss is: 0.08634991198778152\n",
      "Iteration is: 196 and loss is: 0.08628974854946136\n",
      "Iteration is: 197 and loss is: 0.08629967272281647\n",
      "Iteration is: 198 and loss is: 0.0861251950263977\n",
      "Iteration is: 199 and loss is: 0.08606842905282974\n",
      "Iteration is: 200 and loss is: 0.0860569030046463\n",
      "Iteration is: 201 and loss is: 0.08590586483478546\n",
      "Iteration is: 202 and loss is: 0.0858415886759758\n",
      "Iteration is: 203 and loss is: 0.08581949770450592\n",
      "Iteration is: 204 and loss is: 0.0856892466545105\n",
      "Iteration is: 205 and loss is: 0.08561181277036667\n",
      "Iteration is: 206 and loss is: 0.08558273315429688\n",
      "Iteration is: 207 and loss is: 0.0854729413986206\n",
      "Iteration is: 208 and loss is: 0.08538128435611725\n",
      "Iteration is: 209 and loss is: 0.0853428766131401\n",
      "Iteration is: 210 and loss is: 0.08525463938713074\n",
      "Iteration is: 211 and loss is: 0.08515264838933945\n",
      "Iteration is: 212 and loss is: 0.08509797602891922\n",
      "Iteration is: 213 and loss is: 0.08502938598394394\n",
      "Iteration is: 214 and loss is: 0.08492833375930786\n",
      "Iteration is: 215 and loss is: 0.08485107123851776\n",
      "Iteration is: 216 and loss is: 0.08479035645723343\n",
      "Iteration is: 217 and loss is: 0.08470433950424194\n",
      "Iteration is: 218 and loss is: 0.08461067825555801\n",
      "Iteration is: 219 and loss is: 0.08453819155693054\n",
      "Iteration is: 220 and loss is: 0.08446759730577469\n",
      "Iteration is: 221 and loss is: 0.0843786746263504\n",
      "Iteration is: 222 and loss is: 0.08428819477558136\n",
      "Iteration is: 223 and loss is: 0.08421142399311066\n",
      "Iteration is: 224 and loss is: 0.0841357558965683\n",
      "Iteration is: 225 and loss is: 0.08404873311519623\n",
      "Iteration is: 226 and loss is: 0.08395792543888092\n",
      "Iteration is: 227 and loss is: 0.0838732197880745\n",
      "Iteration is: 228 and loss is: 0.08379287272691727\n",
      "Iteration is: 229 and loss is: 0.0837097242474556\n",
      "Iteration is: 230 and loss is: 0.08362092822790146\n",
      "Iteration is: 231 and loss is: 0.08352982997894287\n",
      "Iteration is: 232 and loss is: 0.08344033360481262\n",
      "Iteration is: 233 and loss is: 0.08335379511117935\n",
      "Iteration is: 234 and loss is: 0.0832681879401207\n",
      "Iteration is: 235 and loss is: 0.0831805169582367\n",
      "Iteration is: 236 and loss is: 0.08309012651443481\n",
      "Iteration is: 237 and loss is: 0.08299759030342102\n",
      "Iteration is: 238 and loss is: 0.08290426433086395\n",
      "Iteration is: 239 and loss is: 0.08281060308218002\n",
      "Iteration is: 240 and loss is: 0.08271683007478714\n",
      "Iteration is: 241 and loss is: 0.08262293040752411\n",
      "Iteration is: 242 and loss is: 0.08252884447574615\n",
      "Iteration is: 243 and loss is: 0.08243439346551895\n",
      "Iteration is: 244 and loss is: 0.08233950287103653\n",
      "Iteration is: 245 and loss is: 0.08224447816610336\n",
      "Iteration is: 246 and loss is: 0.0821501836180687\n",
      "Iteration is: 247 and loss is: 0.08205921202898026\n",
      "Iteration is: 248 and loss is: 0.08197829872369766\n",
      "Iteration is: 249 and loss is: 0.08192898333072662\n",
      "Iteration is: 250 and loss is: 0.08197011053562164\n",
      "Iteration is: 251 and loss is: 0.0822441503405571\n",
      "Iteration is: 252 and loss is: 0.08284892141819\n",
      "Iteration is: 253 and loss is: 0.08345439285039902\n",
      "Iteration is: 254 and loss is: 0.08320774883031845\n",
      "Iteration is: 255 and loss is: 0.08239935338497162\n",
      "Iteration is: 256 and loss is: 0.08154825866222382\n",
      "Iteration is: 257 and loss is: 0.08153662085533142\n",
      "Iteration is: 258 and loss is: 0.082148976624012\n",
      "Iteration is: 259 and loss is: 0.08178222924470901\n",
      "Iteration is: 260 and loss is: 0.08092909306287766\n",
      "Iteration is: 261 and loss is: 0.08116964995861053\n",
      "Iteration is: 262 and loss is: 0.08149708807468414\n",
      "Iteration is: 263 and loss is: 0.08092804253101349\n",
      "Iteration is: 264 and loss is: 0.08062925934791565\n",
      "Iteration is: 265 and loss is: 0.08088144659996033\n",
      "Iteration is: 266 and loss is: 0.08075930178165436\n",
      "Iteration is: 267 and loss is: 0.08044267445802689\n",
      "Iteration is: 268 and loss is: 0.08039551973342896\n",
      "Iteration is: 269 and loss is: 0.08035885542631149\n",
      "Iteration is: 270 and loss is: 0.08025592565536499\n",
      "Iteration is: 271 and loss is: 0.08013535290956497\n",
      "Iteration is: 272 and loss is: 0.07995366305112839\n",
      "Iteration is: 273 and loss is: 0.07991158962249756\n",
      "Iteration is: 274 and loss is: 0.0799204558134079\n",
      "Iteration is: 275 and loss is: 0.07969944179058075\n",
      "Iteration is: 276 and loss is: 0.07952730357646942\n",
      "Iteration is: 277 and loss is: 0.07957515120506287\n",
      "Iteration is: 278 and loss is: 0.07949404418468475\n",
      "Iteration is: 279 and loss is: 0.07926984131336212\n",
      "Iteration is: 280 and loss is: 0.07919001579284668\n",
      "Iteration is: 281 and loss is: 0.07917331159114838\n",
      "Iteration is: 282 and loss is: 0.07904992997646332\n",
      "Iteration is: 283 and loss is: 0.07891858369112015\n",
      "Iteration is: 284 and loss is: 0.07883703708648682\n",
      "Iteration is: 285 and loss is: 0.07873257249593735\n",
      "Iteration is: 286 and loss is: 0.07863013446331024\n",
      "Iteration is: 287 and loss is: 0.07856142520904541\n",
      "Iteration is: 288 and loss is: 0.07846124470233917\n",
      "Iteration is: 289 and loss is: 0.07831980288028717\n",
      "Iteration is: 290 and loss is: 0.07821021974086761\n",
      "Iteration is: 291 and loss is: 0.0781463161110878\n",
      "Iteration is: 292 and loss is: 0.07806460559368134\n",
      "Iteration is: 293 and loss is: 0.07794099301099777\n",
      "Iteration is: 294 and loss is: 0.07781799137592316\n",
      "Iteration is: 295 and loss is: 0.07771804928779602\n",
      "Iteration is: 296 and loss is: 0.0776207372546196\n",
      "Iteration is: 297 and loss is: 0.07750985771417618\n",
      "Iteration is: 298 and loss is: 0.07739656418561935\n",
      "Iteration is: 299 and loss is: 0.07729610800743103\n",
      "Iteration is: 300 and loss is: 0.07720421254634857\n",
      "Iteration is: 301 and loss is: 0.07710729539394379\n",
      "Iteration is: 302 and loss is: 0.07699920237064362\n",
      "Iteration is: 303 and loss is: 0.07688798010349274\n",
      "Iteration is: 304 and loss is: 0.07678460329771042\n",
      "Iteration is: 305 and loss is: 0.07669824361801147\n",
      "Iteration is: 306 and loss is: 0.07663579285144806\n",
      "Iteration is: 307 and loss is: 0.0766138806939125\n",
      "Iteration is: 308 and loss is: 0.07666787505149841\n",
      "Iteration is: 309 and loss is: 0.07686599344015121\n",
      "Iteration is: 310 and loss is: 0.07727210968732834\n",
      "Iteration is: 311 and loss is: 0.07782086730003357\n",
      "Iteration is: 312 and loss is: 0.0779864639043808\n",
      "Iteration is: 313 and loss is: 0.0771905705332756\n",
      "Iteration is: 314 and loss is: 0.07591137290000916\n",
      "Iteration is: 315 and loss is: 0.07553013414144516\n",
      "Iteration is: 316 and loss is: 0.07610711455345154\n",
      "Iteration is: 317 and loss is: 0.07639524340629578\n",
      "Iteration is: 318 and loss is: 0.07579401135444641\n",
      "Iteration is: 319 and loss is: 0.0751257985830307\n",
      "Iteration is: 320 and loss is: 0.07515519112348557\n",
      "Iteration is: 321 and loss is: 0.07538216561079025\n",
      "Iteration is: 322 and loss is: 0.0751533955335617\n",
      "Iteration is: 323 and loss is: 0.07476035505533218\n",
      "Iteration is: 324 and loss is: 0.07463916391134262\n",
      "Iteration is: 325 and loss is: 0.07460704445838928\n",
      "Iteration is: 326 and loss is: 0.07439588755369186\n",
      "Iteration is: 327 and loss is: 0.07420871406793594\n",
      "Iteration is: 328 and loss is: 0.07418902218341827\n",
      "Iteration is: 329 and loss is: 0.07405747473239899\n",
      "Iteration is: 330 and loss is: 0.07374625653028488\n",
      "Iteration is: 331 and loss is: 0.07353872805833817\n",
      "Iteration is: 332 and loss is: 0.07355590909719467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 333 and loss is: 0.07353556156158447\n",
      "Iteration is: 334 and loss is: 0.07327693700790405\n",
      "Iteration is: 335 and loss is: 0.07300188392400742\n",
      "Iteration is: 336 and loss is: 0.07288768887519836\n",
      "Iteration is: 337 and loss is: 0.07283227145671844\n",
      "Iteration is: 338 and loss is: 0.0726868212223053\n",
      "Iteration is: 339 and loss is: 0.07248789817094803\n",
      "Iteration is: 340 and loss is: 0.07235917448997498\n",
      "Iteration is: 341 and loss is: 0.07228666543960571\n",
      "Iteration is: 342 and loss is: 0.07217172533273697\n",
      "Iteration is: 343 and loss is: 0.07198157161474228\n",
      "Iteration is: 344 and loss is: 0.07178449630737305\n",
      "Iteration is: 345 and loss is: 0.0716402605175972\n",
      "Iteration is: 346 and loss is: 0.07153845578432083\n",
      "Iteration is: 347 and loss is: 0.07142512500286102\n",
      "Iteration is: 348 and loss is: 0.07129192352294922\n",
      "Iteration is: 349 and loss is: 0.07115800678730011\n",
      "Iteration is: 350 and loss is: 0.07109259068965912\n",
      "Iteration is: 351 and loss is: 0.07114940136671066\n",
      "Iteration is: 352 and loss is: 0.07141409069299698\n",
      "Iteration is: 353 and loss is: 0.07197362929582596\n",
      "Iteration is: 354 and loss is: 0.07282401621341705\n",
      "Iteration is: 355 and loss is: 0.07330702990293503\n",
      "Iteration is: 356 and loss is: 0.0724935233592987\n",
      "Iteration is: 357 and loss is: 0.07057856023311615\n",
      "Iteration is: 358 and loss is: 0.06985016912221909\n",
      "Iteration is: 359 and loss is: 0.07075806707143784\n",
      "Iteration is: 360 and loss is: 0.07128404825925827\n",
      "Iteration is: 361 and loss is: 0.07031948864459991\n",
      "Iteration is: 362 and loss is: 0.06933503597974777\n",
      "Iteration is: 363 and loss is: 0.06968750804662704\n",
      "Iteration is: 364 and loss is: 0.07020766288042068\n",
      "Iteration is: 365 and loss is: 0.06961728632450104\n",
      "Iteration is: 366 and loss is: 0.06888816505670547\n",
      "Iteration is: 367 and loss is: 0.06906759738922119\n",
      "Iteration is: 368 and loss is: 0.06935884058475494\n",
      "Iteration is: 369 and loss is: 0.06894326210021973\n",
      "Iteration is: 370 and loss is: 0.06841760873794556\n",
      "Iteration is: 371 and loss is: 0.06851290911436081\n",
      "Iteration is: 372 and loss is: 0.06871345639228821\n",
      "Iteration is: 373 and loss is: 0.06838694959878922\n",
      "Iteration is: 374 and loss is: 0.06798490881919861\n",
      "Iteration is: 375 and loss is: 0.06799504160881042\n",
      "Iteration is: 376 and loss is: 0.06810170412063599\n",
      "Iteration is: 377 and loss is: 0.06789717823266983\n",
      "Iteration is: 378 and loss is: 0.06758193671703339\n",
      "Iteration is: 379 and loss is: 0.06752237677574158\n",
      "Iteration is: 380 and loss is: 0.06758216768503189\n",
      "Iteration is: 381 and loss is: 0.06745540350675583\n",
      "Iteration is: 382 and loss is: 0.06720462441444397\n",
      "Iteration is: 383 and loss is: 0.06707803159952164\n",
      "Iteration is: 384 and loss is: 0.06708426028490067\n",
      "Iteration is: 385 and loss is: 0.0670289620757103\n",
      "Iteration is: 386 and loss is: 0.06685007363557816\n",
      "Iteration is: 387 and loss is: 0.06669104844331741\n",
      "Iteration is: 388 and loss is: 0.06663763523101807\n",
      "Iteration is: 389 and loss is: 0.06660652160644531\n",
      "Iteration is: 390 and loss is: 0.06650416553020477\n",
      "Iteration is: 391 and loss is: 0.06635317206382751\n",
      "Iteration is: 392 and loss is: 0.06624337285757065\n",
      "Iteration is: 393 and loss is: 0.06619121134281158\n",
      "Iteration is: 394 and loss is: 0.06613785773515701\n",
      "Iteration is: 395 and loss is: 0.06604110449552536\n",
      "Iteration is: 396 and loss is: 0.06592531502246857\n",
      "Iteration is: 397 and loss is: 0.06583934277296066\n",
      "Iteration is: 398 and loss is: 0.06579719483852386\n",
      "Iteration is: 399 and loss is: 0.06577330082654953\n",
      "Iteration is: 400 and loss is: 0.0657595694065094\n",
      "Iteration is: 401 and loss is: 0.06576162576675415\n",
      "Iteration is: 402 and loss is: 0.06582631915807724\n",
      "Iteration is: 403 and loss is: 0.06593423336744308\n",
      "Iteration is: 404 and loss is: 0.06598994135856628\n",
      "Iteration is: 405 and loss is: 0.06586778163909912\n",
      "Iteration is: 406 and loss is: 0.0655103400349617\n",
      "Iteration is: 407 and loss is: 0.06512255221605301\n",
      "Iteration is: 408 and loss is: 0.06492868065834045\n",
      "Iteration is: 409 and loss is: 0.06496976315975189\n",
      "Iteration is: 410 and loss is: 0.0650811418890953\n",
      "Iteration is: 411 and loss is: 0.06506265699863434\n",
      "Iteration is: 412 and loss is: 0.06486992537975311\n",
      "Iteration is: 413 and loss is: 0.06462159752845764\n",
      "Iteration is: 414 and loss is: 0.06448341906070709\n",
      "Iteration is: 415 and loss is: 0.0644795149564743\n",
      "Iteration is: 416 and loss is: 0.06450065225362778\n",
      "Iteration is: 417 and loss is: 0.06444013118743896\n",
      "Iteration is: 418 and loss is: 0.06429116427898407\n",
      "Iteration is: 419 and loss is: 0.06414057314395905\n",
      "Iteration is: 420 and loss is: 0.06405556201934814\n",
      "Iteration is: 421 and loss is: 0.06402619183063507\n",
      "Iteration is: 422 and loss is: 0.06399408727884293\n",
      "Iteration is: 423 and loss is: 0.06391759216785431\n",
      "Iteration is: 424 and loss is: 0.06380747258663177\n",
      "Iteration is: 425 and loss is: 0.06370148807764053\n",
      "Iteration is: 426 and loss is: 0.06362558901309967\n",
      "Iteration is: 427 and loss is: 0.06357355415821075\n",
      "Iteration is: 428 and loss is: 0.06352118402719498\n",
      "Iteration is: 429 and loss is: 0.06345102190971375\n",
      "Iteration is: 430 and loss is: 0.06336405873298645\n",
      "Iteration is: 431 and loss is: 0.06327494233846664\n",
      "Iteration is: 432 and loss is: 0.06319684535264969\n",
      "Iteration is: 433 and loss is: 0.06313200294971466\n",
      "Iteration is: 434 and loss is: 0.06307215243577957\n",
      "Iteration is: 435 and loss is: 0.06300707161426544\n",
      "Iteration is: 436 and loss is: 0.06293261051177979\n",
      "Iteration is: 437 and loss is: 0.0628523975610733\n",
      "Iteration is: 438 and loss is: 0.06277363002300262\n",
      "Iteration is: 439 and loss is: 0.0627012848854065\n",
      "Iteration is: 440 and loss is: 0.06263495981693268\n",
      "Iteration is: 441 and loss is: 0.06257016956806183\n",
      "Iteration is: 442 and loss is: 0.06250220537185669\n",
      "Iteration is: 443 and loss is: 0.062429316341876984\n",
      "Iteration is: 444 and loss is: 0.062353260815143585\n",
      "Iteration is: 445 and loss is: 0.06227744743227959\n",
      "Iteration is: 446 and loss is: 0.06220458447933197\n",
      "Iteration is: 447 and loss is: 0.062134869396686554\n",
      "Iteration is: 448 and loss is: 0.06206660717725754\n",
      "Iteration is: 449 and loss is: 0.06199745833873749\n",
      "Iteration is: 450 and loss is: 0.06192595884203911\n",
      "Iteration is: 451 and loss is: 0.06185213476419449\n",
      "Iteration is: 452 and loss is: 0.06177709251642227\n",
      "Iteration is: 453 and loss is: 0.061702243983745575\n",
      "Iteration is: 454 and loss is: 0.06162851303815842\n",
      "Iteration is: 455 and loss is: 0.06155587360262871\n",
      "Iteration is: 456 and loss is: 0.06148374825716019\n",
      "Iteration is: 457 and loss is: 0.06141126900911331\n",
      "Iteration is: 458 and loss is: 0.06133772432804108\n",
      "Iteration is: 459 and loss is: 0.06126292422413826\n",
      "Iteration is: 460 and loss is: 0.06118706613779068\n",
      "Iteration is: 461 and loss is: 0.06111054867506027\n",
      "Iteration is: 462 and loss is: 0.06103380769491196\n",
      "Iteration is: 463 and loss is: 0.06095714122056961\n",
      "Iteration is: 464 and loss is: 0.06088066101074219\n",
      "Iteration is: 465 and loss is: 0.060804229229688644\n",
      "Iteration is: 466 and loss is: 0.06072767823934555\n",
      "Iteration is: 467 and loss is: 0.06065085530281067\n",
      "Iteration is: 468 and loss is: 0.06057356297969818\n",
      "Iteration is: 469 and loss is: 0.060495734214782715\n",
      "Iteration is: 470 and loss is: 0.0604173019528389\n",
      "Iteration is: 471 and loss is: 0.06033836305141449\n",
      "Iteration is: 472 and loss is: 0.060258910059928894\n",
      "Iteration is: 473 and loss is: 0.06017903983592987\n",
      "Iteration is: 474 and loss is: 0.06009876728057861\n",
      "Iteration is: 475 and loss is: 0.06001817435026169\n",
      "Iteration is: 476 and loss is: 0.05993732064962387\n",
      "Iteration is: 477 and loss is: 0.05985628813505173\n",
      "Iteration is: 478 and loss is: 0.059775225818157196\n",
      "Iteration is: 479 and loss is: 0.05969448387622833\n",
      "Iteration is: 480 and loss is: 0.059614576399326324\n",
      "Iteration is: 481 and loss is: 0.05953683704137802\n",
      "Iteration is: 482 and loss is: 0.059463612735271454\n",
      "Iteration is: 483 and loss is: 0.059400804340839386\n",
      "Iteration is: 484 and loss is: 0.05935835838317871\n",
      "Iteration is: 485 and loss is: 0.05936338007450104\n",
      "Iteration is: 486 and loss is: 0.05944858863949776\n",
      "Iteration is: 487 and loss is: 0.05970952659845352\n",
      "Iteration is: 488 and loss is: 0.06008705869317055\n",
      "Iteration is: 489 and loss is: 0.06057795137166977\n",
      "Iteration is: 490 and loss is: 0.06026216968894005\n",
      "Iteration is: 491 and loss is: 0.059454306960105896\n",
      "Iteration is: 492 and loss is: 0.05872611328959465\n",
      "Iteration is: 493 and loss is: 0.058909133076667786\n",
      "Iteration is: 494 and loss is: 0.05932880565524101\n",
      "Iteration is: 495 and loss is: 0.059030093252658844\n",
      "Iteration is: 496 and loss is: 0.058396145701408386\n",
      "Iteration is: 497 and loss is: 0.05826695263385773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 498 and loss is: 0.05858601629734039\n",
      "Iteration is: 499 and loss is: 0.05856701731681824\n",
      "Iteration is: 500 and loss is: 0.05808525159955025\n",
      "Iteration is: 501 and loss is: 0.05785463750362396\n",
      "Iteration is: 502 and loss is: 0.05802217870950699\n",
      "Iteration is: 503 and loss is: 0.05799572914838791\n",
      "Iteration is: 504 and loss is: 0.05766826122999191\n",
      "Iteration is: 505 and loss is: 0.05749168619513512\n",
      "Iteration is: 506 and loss is: 0.05757303535938263\n",
      "Iteration is: 507 and loss is: 0.05754704028367996\n",
      "Iteration is: 508 and loss is: 0.05728299170732498\n",
      "Iteration is: 509 and loss is: 0.057105787098407745\n",
      "Iteration is: 510 and loss is: 0.05711670219898224\n",
      "Iteration is: 511 and loss is: 0.05708037316799164\n",
      "Iteration is: 512 and loss is: 0.05689644068479538\n",
      "Iteration is: 513 and loss is: 0.056727953255176544\n",
      "Iteration is: 514 and loss is: 0.056684527546167374\n",
      "Iteration is: 515 and loss is: 0.05664270743727684\n",
      "Iteration is: 516 and loss is: 0.05649663880467415\n",
      "Iteration is: 517 and loss is: 0.05633072555065155\n",
      "Iteration is: 518 and loss is: 0.05624273791909218\n",
      "Iteration is: 519 and loss is: 0.05619272589683533\n",
      "Iteration is: 520 and loss is: 0.056092262268066406\n",
      "Iteration is: 521 and loss is: 0.055942170321941376\n",
      "Iteration is: 522 and loss is: 0.055811718106269836\n",
      "Iteration is: 523 and loss is: 0.055727191269397736\n",
      "Iteration is: 524 and loss is: 0.055647656321525574\n",
      "Iteration is: 525 and loss is: 0.05553482100367546\n",
      "Iteration is: 526 and loss is: 0.05539686232805252\n",
      "Iteration is: 527 and loss is: 0.05527007579803467\n",
      "Iteration is: 528 and loss is: 0.05516856163740158\n",
      "Iteration is: 529 and loss is: 0.055075280368328094\n",
      "Iteration is: 530 and loss is: 0.054968856275081635\n",
      "Iteration is: 531 and loss is: 0.05484363064169884\n",
      "Iteration is: 532 and loss is: 0.054712265729904175\n",
      "Iteration is: 533 and loss is: 0.05458688363432884\n",
      "Iteration is: 534 and loss is: 0.054471179842948914\n",
      "Iteration is: 535 and loss is: 0.05435971915721893\n",
      "Iteration is: 536 and loss is: 0.05424545332789421\n",
      "Iteration is: 537 and loss is: 0.05412483215332031\n",
      "Iteration is: 538 and loss is: 0.053998544812202454\n",
      "Iteration is: 539 and loss is: 0.05386999249458313\n",
      "Iteration is: 540 and loss is: 0.05374175310134888\n",
      "Iteration is: 541 and loss is: 0.053615741431713104\n",
      "Iteration is: 542 and loss is: 0.05349154770374298\n",
      "Iteration is: 543 and loss is: 0.053369488567113876\n",
      "Iteration is: 544 and loss is: 0.053249381482601166\n",
      "Iteration is: 545 and loss is: 0.0531354583799839\n",
      "Iteration is: 546 and loss is: 0.05303354561328888\n",
      "Iteration is: 547 and loss is: 0.05296880751848221\n",
      "Iteration is: 548 and loss is: 0.05297893285751343\n",
      "Iteration is: 549 and loss is: 0.053214725106954575\n",
      "Iteration is: 550 and loss is: 0.053824812173843384\n",
      "Iteration is: 551 and loss is: 0.05539567023515701\n",
      "Iteration is: 552 and loss is: 0.056484635919332504\n",
      "Iteration is: 553 and loss is: 0.05630975589156151\n",
      "Iteration is: 554 and loss is: 0.05274857580661774\n",
      "Iteration is: 555 and loss is: 0.05236068367958069\n",
      "Iteration is: 556 and loss is: 0.054500311613082886\n",
      "Iteration is: 557 and loss is: 0.05324835702776909\n",
      "Iteration is: 558 and loss is: 0.05146269127726555\n",
      "Iteration is: 559 and loss is: 0.05232081562280655\n",
      "Iteration is: 560 and loss is: 0.052996326237916946\n",
      "Iteration is: 561 and loss is: 0.05176134407520294\n",
      "Iteration is: 562 and loss is: 0.0509280264377594\n",
      "Iteration is: 563 and loss is: 0.051790617406368256\n",
      "Iteration is: 564 and loss is: 0.051872316747903824\n",
      "Iteration is: 565 and loss is: 0.050641708076000214\n",
      "Iteration is: 566 and loss is: 0.05064588412642479\n",
      "Iteration is: 567 and loss is: 0.05126483738422394\n",
      "Iteration is: 568 and loss is: 0.05064362660050392\n",
      "Iteration is: 569 and loss is: 0.049936629831790924\n",
      "Iteration is: 570 and loss is: 0.05017395317554474\n",
      "Iteration is: 571 and loss is: 0.05039716884493828\n",
      "Iteration is: 572 and loss is: 0.04995964467525482\n",
      "Iteration is: 573 and loss is: 0.04939322918653488\n",
      "Iteration is: 574 and loss is: 0.04938611015677452\n",
      "Iteration is: 575 and loss is: 0.049578435719013214\n",
      "Iteration is: 576 and loss is: 0.04936512932181358\n",
      "Iteration is: 577 and loss is: 0.04893903806805611\n",
      "Iteration is: 578 and loss is: 0.04868499934673309\n",
      "Iteration is: 579 and loss is: 0.048661183565855026\n",
      "Iteration is: 580 and loss is: 0.04863812029361725\n",
      "Iteration is: 581 and loss is: 0.04846804216504097\n",
      "Iteration is: 582 and loss is: 0.048288896679878235\n",
      "Iteration is: 583 and loss is: 0.048151880502700806\n",
      "Iteration is: 584 and loss is: 0.04801362752914429\n",
      "Iteration is: 585 and loss is: 0.04779718071222305\n",
      "Iteration is: 586 and loss is: 0.047530509531497955\n",
      "Iteration is: 587 and loss is: 0.04731648415327072\n",
      "Iteration is: 588 and loss is: 0.04723300412297249\n",
      "Iteration is: 589 and loss is: 0.04729339852929115\n",
      "Iteration is: 590 and loss is: 0.047657981514930725\n",
      "Iteration is: 591 and loss is: 0.04873126745223999\n",
      "Iteration is: 592 and loss is: 0.05192141979932785\n",
      "Iteration is: 593 and loss is: 0.05634530633687973\n",
      "Iteration is: 594 and loss is: 0.056287072598934174\n",
      "Iteration is: 595 and loss is: 0.04821214824914932\n",
      "Iteration is: 596 and loss is: 0.04857547581195831\n",
      "Iteration is: 597 and loss is: 0.05258091539144516\n",
      "Iteration is: 598 and loss is: 0.047228582203388214\n",
      "Iteration is: 599 and loss is: 0.04846956580877304\n",
      "Iteration is: 600 and loss is: 0.05069877207279205\n",
      "Iteration is: 601 and loss is: 0.045968323945999146\n",
      "Iteration is: 602 and loss is: 0.048564281314611435\n",
      "Iteration is: 603 and loss is: 0.049447037279605865\n",
      "Iteration is: 604 and loss is: 0.04526714235544205\n",
      "Iteration is: 605 and loss is: 0.048215486109256744\n",
      "Iteration is: 606 and loss is: 0.04795239865779877\n",
      "Iteration is: 607 and loss is: 0.04493759199976921\n",
      "Iteration is: 608 and loss is: 0.0478871688246727\n",
      "Iteration is: 609 and loss is: 0.04672282561659813\n",
      "Iteration is: 610 and loss is: 0.0446307547390461\n",
      "Iteration is: 611 and loss is: 0.04735752195119858\n",
      "Iteration is: 612 and loss is: 0.046675339341163635\n",
      "Iteration is: 613 and loss is: 0.04389301687479019\n",
      "Iteration is: 614 and loss is: 0.046139560639858246\n",
      "Iteration is: 615 and loss is: 0.04620324820280075\n",
      "Iteration is: 616 and loss is: 0.04342891275882721\n",
      "Iteration is: 617 and loss is: 0.04498758912086487\n",
      "Iteration is: 618 and loss is: 0.0457419827580452\n",
      "Iteration is: 619 and loss is: 0.04355195164680481\n",
      "Iteration is: 620 and loss is: 0.04362228512763977\n",
      "Iteration is: 621 and loss is: 0.044337883591651917\n",
      "Iteration is: 622 and loss is: 0.043532416224479675\n",
      "Iteration is: 623 and loss is: 0.043001238256692886\n",
      "Iteration is: 624 and loss is: 0.04286123439669609\n",
      "Iteration is: 625 and loss is: 0.04282601177692413\n",
      "Iteration is: 626 and loss is: 0.043352074921131134\n",
      "Iteration is: 627 and loss is: 0.04287860542535782\n",
      "Iteration is: 628 and loss is: 0.041665688157081604\n",
      "Iteration is: 629 and loss is: 0.04166806861758232\n",
      "Iteration is: 630 and loss is: 0.041805513203144073\n",
      "Iteration is: 631 and loss is: 0.04159921407699585\n",
      "Iteration is: 632 and loss is: 0.04189237207174301\n",
      "Iteration is: 633 and loss is: 0.042086828500032425\n",
      "Iteration is: 634 and loss is: 0.041802071034908295\n",
      "Iteration is: 635 and loss is: 0.04199953377246857\n",
      "Iteration is: 636 and loss is: 0.04254550486803055\n",
      "Iteration is: 637 and loss is: 0.04265190660953522\n",
      "Iteration is: 638 and loss is: 0.043146342039108276\n",
      "Iteration is: 639 and loss is: 0.04379895702004433\n",
      "Iteration is: 640 and loss is: 0.04288078099489212\n",
      "Iteration is: 641 and loss is: 0.04163166508078575\n",
      "Iteration is: 642 and loss is: 0.04034169390797615\n",
      "Iteration is: 643 and loss is: 0.03928874060511589\n",
      "Iteration is: 644 and loss is: 0.03933463990688324\n",
      "Iteration is: 645 and loss is: 0.0400950089097023\n",
      "Iteration is: 646 and loss is: 0.04080949351191521\n",
      "Iteration is: 647 and loss is: 0.041641611605882645\n",
      "Iteration is: 648 and loss is: 0.042639099061489105\n",
      "Iteration is: 649 and loss is: 0.04246459901332855\n",
      "Iteration is: 650 and loss is: 0.041635069996118546\n",
      "Iteration is: 651 and loss is: 0.04000663757324219\n",
      "Iteration is: 652 and loss is: 0.03837567940354347\n",
      "Iteration is: 653 and loss is: 0.03801160678267479\n",
      "Iteration is: 654 and loss is: 0.038751956075429916\n",
      "Iteration is: 655 and loss is: 0.03979383036494255\n",
      "Iteration is: 656 and loss is: 0.040928054600954056\n",
      "Iteration is: 657 and loss is: 0.04228514805436134\n",
      "Iteration is: 658 and loss is: 0.04256559908390045\n",
      "Iteration is: 659 and loss is: 0.04203908145427704\n",
      "Iteration is: 660 and loss is: 0.03992268443107605\n",
      "Iteration is: 661 and loss is: 0.03756076097488403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 662 and loss is: 0.036982957273721695\n",
      "Iteration is: 663 and loss is: 0.03801746293902397\n",
      "Iteration is: 664 and loss is: 0.039404094219207764\n",
      "Iteration is: 665 and loss is: 0.03982669115066528\n",
      "Iteration is: 666 and loss is: 0.039337340742349625\n",
      "Iteration is: 667 and loss is: 0.03772089630365372\n",
      "Iteration is: 668 and loss is: 0.03650568053126335\n",
      "Iteration is: 669 and loss is: 0.03612232208251953\n",
      "Iteration is: 670 and loss is: 0.03659557178616524\n",
      "Iteration is: 671 and loss is: 0.03752657398581505\n",
      "Iteration is: 672 and loss is: 0.03834052383899689\n",
      "Iteration is: 673 and loss is: 0.03908434137701988\n",
      "Iteration is: 674 and loss is: 0.039297059178352356\n",
      "Iteration is: 675 and loss is: 0.03931186720728874\n",
      "Iteration is: 676 and loss is: 0.03836287185549736\n",
      "Iteration is: 677 and loss is: 0.03710995987057686\n",
      "Iteration is: 678 and loss is: 0.035751186311244965\n",
      "Iteration is: 679 and loss is: 0.03504167124629021\n",
      "Iteration is: 680 and loss is: 0.03505531698465347\n",
      "Iteration is: 681 and loss is: 0.03558671846985817\n",
      "Iteration is: 682 and loss is: 0.036491770297288895\n",
      "Iteration is: 683 and loss is: 0.037465453147888184\n",
      "Iteration is: 684 and loss is: 0.038785044103860855\n",
      "Iteration is: 685 and loss is: 0.03978310152888298\n",
      "Iteration is: 686 and loss is: 0.040298931300640106\n",
      "Iteration is: 687 and loss is: 0.03879794478416443\n",
      "Iteration is: 688 and loss is: 0.03638853132724762\n",
      "Iteration is: 689 and loss is: 0.03434506803750992\n",
      "Iteration is: 690 and loss is: 0.03442937508225441\n",
      "Iteration is: 691 and loss is: 0.03594528138637543\n",
      "Iteration is: 692 and loss is: 0.03690264746546745\n",
      "Iteration is: 693 and loss is: 0.036629486829042435\n",
      "Iteration is: 694 and loss is: 0.035100698471069336\n",
      "Iteration is: 695 and loss is: 0.03382246941328049\n",
      "Iteration is: 696 and loss is: 0.03363904729485512\n",
      "Iteration is: 697 and loss is: 0.03434063494205475\n",
      "Iteration is: 698 and loss is: 0.035167396068573\n",
      "Iteration is: 699 and loss is: 0.035388678312301636\n",
      "Iteration is: 700 and loss is: 0.035065218806266785\n",
      "Iteration is: 701 and loss is: 0.03427281230688095\n",
      "Iteration is: 702 and loss is: 0.033513493835926056\n",
      "Iteration is: 703 and loss is: 0.03300212696194649\n",
      "Iteration is: 704 and loss is: 0.03282596543431282\n",
      "Iteration is: 705 and loss is: 0.032919980585575104\n",
      "Iteration is: 706 and loss is: 0.033243704587221146\n",
      "Iteration is: 707 and loss is: 0.03389984741806984\n",
      "Iteration is: 708 and loss is: 0.03514464944601059\n",
      "Iteration is: 709 and loss is: 0.037781812250614166\n",
      "Iteration is: 710 and loss is: 0.04231686890125275\n",
      "Iteration is: 711 and loss is: 0.048655763268470764\n",
      "Iteration is: 712 and loss is: 0.04790527746081352\n",
      "Iteration is: 713 and loss is: 0.04095597565174103\n",
      "Iteration is: 714 and loss is: 0.03259487450122833\n",
      "Iteration is: 715 and loss is: 0.03761149197816849\n",
      "Iteration is: 716 and loss is: 0.041951242834329605\n",
      "Iteration is: 717 and loss is: 0.033982936292886734\n",
      "Iteration is: 718 and loss is: 0.03411225229501724\n",
      "Iteration is: 719 and loss is: 0.03922227770090103\n",
      "Iteration is: 720 and loss is: 0.03440903127193451\n",
      "Iteration is: 721 and loss is: 0.033019404858350754\n",
      "Iteration is: 722 and loss is: 0.03680272027850151\n",
      "Iteration is: 723 and loss is: 0.03361654654145241\n",
      "Iteration is: 724 and loss is: 0.03279183804988861\n",
      "Iteration is: 725 and loss is: 0.03536110371351242\n",
      "Iteration is: 726 and loss is: 0.032651547342538834\n",
      "Iteration is: 727 and loss is: 0.0326421782374382\n",
      "Iteration is: 728 and loss is: 0.03435954451560974\n",
      "Iteration is: 729 and loss is: 0.03212079405784607\n",
      "Iteration is: 730 and loss is: 0.03225153684616089\n",
      "Iteration is: 731 and loss is: 0.0335206463932991\n",
      "Iteration is: 732 and loss is: 0.03190828114748001\n",
      "Iteration is: 733 and loss is: 0.03158949315547943\n",
      "Iteration is: 734 and loss is: 0.03279448300600052\n",
      "Iteration is: 735 and loss is: 0.03211316838860512\n",
      "Iteration is: 736 and loss is: 0.031074313446879387\n",
      "Iteration is: 737 and loss is: 0.03168032318353653\n",
      "Iteration is: 738 and loss is: 0.032164037227630615\n",
      "Iteration is: 739 and loss is: 0.031365927308797836\n",
      "Iteration is: 740 and loss is: 0.030793631449341774\n",
      "Iteration is: 741 and loss is: 0.031243013218045235\n",
      "Iteration is: 742 and loss is: 0.031523365527391434\n",
      "Iteration is: 743 and loss is: 0.030994191765785217\n",
      "Iteration is: 744 and loss is: 0.03051769733428955\n",
      "Iteration is: 745 and loss is: 0.030709397047758102\n",
      "Iteration is: 746 and loss is: 0.031004291027784348\n",
      "Iteration is: 747 and loss is: 0.030820459127426147\n",
      "Iteration is: 748 and loss is: 0.03038117289543152\n",
      "Iteration is: 749 and loss is: 0.030165135860443115\n",
      "Iteration is: 750 and loss is: 0.0302786435931921\n",
      "Iteration is: 751 and loss is: 0.030459467321634293\n",
      "Iteration is: 752 and loss is: 0.03044924885034561\n",
      "Iteration is: 753 and loss is: 0.03022768162190914\n",
      "Iteration is: 754 and loss is: 0.029942018911242485\n",
      "Iteration is: 755 and loss is: 0.02976152114570141\n",
      "Iteration is: 756 and loss is: 0.02973766066133976\n",
      "Iteration is: 757 and loss is: 0.029803566634655\n",
      "Iteration is: 758 and loss is: 0.029867999255657196\n",
      "Iteration is: 759 and loss is: 0.029877467080950737\n",
      "Iteration is: 760 and loss is: 0.029831863939762115\n",
      "Iteration is: 761 and loss is: 0.0297459177672863\n",
      "Iteration is: 762 and loss is: 0.029646750539541245\n",
      "Iteration is: 763 and loss is: 0.029540378600358963\n",
      "Iteration is: 764 and loss is: 0.02944331243634224\n",
      "Iteration is: 765 and loss is: 0.029359064996242523\n",
      "Iteration is: 766 and loss is: 0.029299922287464142\n",
      "Iteration is: 767 and loss is: 0.029277630150318146\n",
      "Iteration is: 768 and loss is: 0.029325295239686966\n",
      "Iteration is: 769 and loss is: 0.029502052813768387\n",
      "Iteration is: 770 and loss is: 0.02997135929763317\n",
      "Iteration is: 771 and loss is: 0.030991191044449806\n",
      "Iteration is: 772 and loss is: 0.03302066773176193\n",
      "Iteration is: 773 and loss is: 0.0359136164188385\n",
      "Iteration is: 774 and loss is: 0.03673883527517319\n",
      "Iteration is: 775 and loss is: 0.03547820448875427\n",
      "Iteration is: 776 and loss is: 0.031223230063915253\n",
      "Iteration is: 777 and loss is: 0.0287344828248024\n",
      "Iteration is: 778 and loss is: 0.029585354030132294\n",
      "Iteration is: 779 and loss is: 0.03165512531995773\n",
      "Iteration is: 780 and loss is: 0.03195767104625702\n",
      "Iteration is: 781 and loss is: 0.02969694882631302\n",
      "Iteration is: 782 and loss is: 0.028414025902748108\n",
      "Iteration is: 783 and loss is: 0.029316943138837814\n",
      "Iteration is: 784 and loss is: 0.03072676621377468\n",
      "Iteration is: 785 and loss is: 0.03091627173125744\n",
      "Iteration is: 786 and loss is: 0.02988685853779316\n",
      "Iteration is: 787 and loss is: 0.028501372784376144\n",
      "Iteration is: 788 and loss is: 0.028193673118948936\n",
      "Iteration is: 789 and loss is: 0.02897123619914055\n",
      "Iteration is: 790 and loss is: 0.02952907420694828\n",
      "Iteration is: 791 and loss is: 0.029142826795578003\n",
      "Iteration is: 792 and loss is: 0.028297744691371918\n",
      "Iteration is: 793 and loss is: 0.027796465903520584\n",
      "Iteration is: 794 and loss is: 0.027855386957526207\n",
      "Iteration is: 795 and loss is: 0.02833038941025734\n",
      "Iteration is: 796 and loss is: 0.02907664328813553\n",
      "Iteration is: 797 and loss is: 0.029896683990955353\n",
      "Iteration is: 798 and loss is: 0.030837997794151306\n",
      "Iteration is: 799 and loss is: 0.030728723853826523\n",
      "Iteration is: 800 and loss is: 0.02893083542585373\n",
      "Iteration is: 801 and loss is: 0.027660531923174858\n",
      "Iteration is: 802 and loss is: 0.028146937489509583\n",
      "Iteration is: 803 and loss is: 0.02913660928606987\n",
      "Iteration is: 804 and loss is: 0.029138652607798576\n",
      "Iteration is: 805 and loss is: 0.028155256062746048\n",
      "Iteration is: 806 and loss is: 0.027341006323695183\n",
      "Iteration is: 807 and loss is: 0.027196727693080902\n",
      "Iteration is: 808 and loss is: 0.02768118679523468\n",
      "Iteration is: 809 and loss is: 0.02872334234416485\n",
      "Iteration is: 810 and loss is: 0.030815567821264267\n",
      "Iteration is: 811 and loss is: 0.0337321013212204\n",
      "Iteration is: 812 and loss is: 0.03566785901784897\n",
      "Iteration is: 813 and loss is: 0.031641606241464615\n",
      "Iteration is: 814 and loss is: 0.029816683381795883\n",
      "Iteration is: 815 and loss is: 0.0279475599527359\n",
      "Iteration is: 816 and loss is: 0.029165834188461304\n",
      "Iteration is: 817 and loss is: 0.02962394990026951\n",
      "Iteration is: 818 and loss is: 0.029044492170214653\n",
      "Iteration is: 819 and loss is: 0.02946297638118267\n",
      "Iteration is: 820 and loss is: 0.02966339699923992\n",
      "Iteration is: 821 and loss is: 0.03175756335258484\n",
      "Iteration is: 822 and loss is: 0.036534134298563004\n",
      "Iteration is: 823 and loss is: 0.05161583796143532\n",
      "Iteration is: 824 and loss is: 0.05550350993871689\n",
      "Iteration is: 825 and loss is: 0.043876126408576965\n",
      "Iteration is: 826 and loss is: 0.027593208476901054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 827 and loss is: 0.041748255491256714\n",
      "Iteration is: 828 and loss is: 0.03518024459481239\n",
      "Iteration is: 829 and loss is: 0.03106030449271202\n",
      "Iteration is: 830 and loss is: 0.04434199631214142\n",
      "Iteration is: 831 and loss is: 0.028522856533527374\n",
      "Iteration is: 832 and loss is: 0.03780278563499451\n",
      "Iteration is: 833 and loss is: 0.0310650747269392\n",
      "Iteration is: 834 and loss is: 0.03286650404334068\n",
      "Iteration is: 835 and loss is: 0.033318378031253815\n",
      "Iteration is: 836 and loss is: 0.02988486923277378\n",
      "Iteration is: 837 and loss is: 0.03359883278608322\n",
      "Iteration is: 838 and loss is: 0.028123287484049797\n",
      "Iteration is: 839 and loss is: 0.032835815101861954\n",
      "Iteration is: 840 and loss is: 0.02772882580757141\n",
      "Iteration is: 841 and loss is: 0.03178437799215317\n",
      "Iteration is: 842 and loss is: 0.027768347412347794\n",
      "Iteration is: 843 and loss is: 0.030399540439248085\n",
      "Iteration is: 844 and loss is: 0.02807747572660446\n",
      "Iteration is: 845 and loss is: 0.029202040284872055\n",
      "Iteration is: 846 and loss is: 0.02846737951040268\n",
      "Iteration is: 847 and loss is: 0.028186336159706116\n",
      "Iteration is: 848 and loss is: 0.028550773859024048\n",
      "Iteration is: 849 and loss is: 0.027457289397716522\n",
      "Iteration is: 850 and loss is: 0.028456367552280426\n",
      "Iteration is: 851 and loss is: 0.0272214338183403\n",
      "Iteration is: 852 and loss is: 0.027868250384926796\n",
      "Iteration is: 853 and loss is: 0.027357496321201324\n",
      "Iteration is: 854 and loss is: 0.02711533010005951\n",
      "Iteration is: 855 and loss is: 0.027039729058742523\n",
      "Iteration is: 856 and loss is: 0.027329763397574425\n",
      "Iteration is: 857 and loss is: 0.026408934965729713\n",
      "Iteration is: 858 and loss is: 0.02703949809074402\n",
      "Iteration is: 859 and loss is: 0.026779435575008392\n",
      "Iteration is: 860 and loss is: 0.026464268565177917\n",
      "Iteration is: 861 and loss is: 0.026525666937232018\n",
      "Iteration is: 862 and loss is: 0.026539567857980728\n",
      "Iteration is: 863 and loss is: 0.026423215866088867\n",
      "Iteration is: 864 and loss is: 0.026064660400152206\n",
      "Iteration is: 865 and loss is: 0.026401236653327942\n",
      "Iteration is: 866 and loss is: 0.025999054312705994\n",
      "Iteration is: 867 and loss is: 0.026003872975707054\n",
      "Iteration is: 868 and loss is: 0.025963494554162025\n",
      "Iteration is: 869 and loss is: 0.02586652711033821\n",
      "Iteration is: 870 and loss is: 0.02586190029978752\n",
      "Iteration is: 871 and loss is: 0.025752373039722443\n",
      "Iteration is: 872 and loss is: 0.02569591999053955\n",
      "Iteration is: 873 and loss is: 0.025582648813724518\n",
      "Iteration is: 874 and loss is: 0.025612276047468185\n",
      "Iteration is: 875 and loss is: 0.025451038032770157\n",
      "Iteration is: 876 and loss is: 0.025486521422863007\n",
      "Iteration is: 877 and loss is: 0.025412749499082565\n",
      "Iteration is: 878 and loss is: 0.02528909593820572\n",
      "Iteration is: 879 and loss is: 0.025313884019851685\n",
      "Iteration is: 880 and loss is: 0.02521006390452385\n",
      "Iteration is: 881 and loss is: 0.025189027190208435\n",
      "Iteration is: 882 and loss is: 0.02513807825744152\n",
      "Iteration is: 883 and loss is: 0.025078997015953064\n",
      "Iteration is: 884 and loss is: 0.025015540421009064\n",
      "Iteration is: 885 and loss is: 0.024980856105685234\n",
      "Iteration is: 886 and loss is: 0.024948446080088615\n",
      "Iteration is: 887 and loss is: 0.024865705519914627\n",
      "Iteration is: 888 and loss is: 0.024851156398653984\n",
      "Iteration is: 889 and loss is: 0.02478831075131893\n",
      "Iteration is: 890 and loss is: 0.024729501456022263\n",
      "Iteration is: 891 and loss is: 0.0247047059237957\n",
      "Iteration is: 892 and loss is: 0.02464885637164116\n",
      "Iteration is: 893 and loss is: 0.024601858109235764\n",
      "Iteration is: 894 and loss is: 0.024549148976802826\n",
      "Iteration is: 895 and loss is: 0.02451271563768387\n",
      "Iteration is: 896 and loss is: 0.02446250058710575\n",
      "Iteration is: 897 and loss is: 0.024410318583250046\n",
      "Iteration is: 898 and loss is: 0.0243785809725523\n",
      "Iteration is: 899 and loss is: 0.024322398006916046\n",
      "Iteration is: 900 and loss is: 0.02427472546696663\n",
      "Iteration is: 901 and loss is: 0.02423393540084362\n",
      "Iteration is: 902 and loss is: 0.024188049137592316\n",
      "Iteration is: 903 and loss is: 0.024142354726791382\n",
      "Iteration is: 904 and loss is: 0.0240908432751894\n",
      "Iteration is: 905 and loss is: 0.02405078336596489\n",
      "Iteration is: 906 and loss is: 0.02400658093392849\n",
      "Iteration is: 907 and loss is: 0.02395613119006157\n",
      "Iteration is: 908 and loss is: 0.023913148790597916\n",
      "Iteration is: 909 and loss is: 0.023867640644311905\n",
      "Iteration is: 910 and loss is: 0.023822829127311707\n",
      "Iteration is: 911 and loss is: 0.023776520043611526\n",
      "Iteration is: 912 and loss is: 0.023729629814624786\n",
      "Iteration is: 913 and loss is: 0.023687342181801796\n",
      "Iteration is: 914 and loss is: 0.023640990257263184\n",
      "Iteration is: 915 and loss is: 0.023593844845891\n",
      "Iteration is: 916 and loss is: 0.023548927158117294\n",
      "Iteration is: 917 and loss is: 0.023503242060542107\n",
      "Iteration is: 918 and loss is: 0.02345883473753929\n",
      "Iteration is: 919 and loss is: 0.02341238409280777\n",
      "Iteration is: 920 and loss is: 0.023365098983049393\n",
      "Iteration is: 921 and loss is: 0.02332008257508278\n",
      "Iteration is: 922 and loss is: 0.023274313658475876\n",
      "Iteration is: 923 and loss is: 0.02322852425277233\n",
      "Iteration is: 924 and loss is: 0.02318231761455536\n",
      "Iteration is: 925 and loss is: 0.023135067895054817\n",
      "Iteration is: 926 and loss is: 0.023089058697223663\n",
      "Iteration is: 927 and loss is: 0.023043029010295868\n",
      "Iteration is: 928 and loss is: 0.022996503859758377\n",
      "Iteration is: 929 and loss is: 0.022950146347284317\n",
      "Iteration is: 930 and loss is: 0.022902939468622208\n",
      "Iteration is: 931 and loss is: 0.02285580337047577\n",
      "Iteration is: 932 and loss is: 0.02280907705426216\n",
      "Iteration is: 933 and loss is: 0.022761967033147812\n",
      "Iteration is: 934 and loss is: 0.022714952006936073\n",
      "Iteration is: 935 and loss is: 0.022667722776532173\n",
      "Iteration is: 936 and loss is: 0.02261996455490589\n",
      "Iteration is: 937 and loss is: 0.022572215646505356\n",
      "Iteration is: 938 and loss is: 0.02252422645688057\n",
      "Iteration is: 939 and loss is: 0.022475892677903175\n",
      "Iteration is: 940 and loss is: 0.02242755889892578\n",
      "Iteration is: 941 and loss is: 0.022378938272595406\n",
      "Iteration is: 942 and loss is: 0.022329961881041527\n",
      "Iteration is: 943 and loss is: 0.022280804812908173\n",
      "Iteration is: 944 and loss is: 0.022231128066778183\n",
      "Iteration is: 945 and loss is: 0.022181013599038124\n",
      "Iteration is: 946 and loss is: 0.022130485624074936\n",
      "Iteration is: 947 and loss is: 0.022079404443502426\n",
      "Iteration is: 948 and loss is: 0.022027844563126564\n",
      "Iteration is: 949 and loss is: 0.02197597548365593\n",
      "Iteration is: 950 and loss is: 0.02192402444779873\n",
      "Iteration is: 951 and loss is: 0.021872924640774727\n",
      "Iteration is: 952 and loss is: 0.021824998781085014\n",
      "Iteration is: 953 and loss is: 0.021786220371723175\n",
      "Iteration is: 954 and loss is: 0.021772034466266632\n",
      "Iteration is: 955 and loss is: 0.021829567849636078\n",
      "Iteration is: 956 and loss is: 0.0220806784927845\n",
      "Iteration is: 957 and loss is: 0.022950217127799988\n",
      "Iteration is: 958 and loss is: 0.025355003774166107\n",
      "Iteration is: 959 and loss is: 0.03253249451518059\n",
      "Iteration is: 960 and loss is: 0.04570463299751282\n",
      "Iteration is: 961 and loss is: 0.06268241256475449\n",
      "Iteration is: 962 and loss is: 0.06292526423931122\n",
      "Iteration is: 963 and loss is: 0.04112044349312782\n",
      "Iteration is: 964 and loss is: 0.022976722568273544\n",
      "Iteration is: 965 and loss is: 0.03629890829324722\n",
      "Iteration is: 966 and loss is: 0.057803861796855927\n",
      "Iteration is: 967 and loss is: 0.04743126407265663\n",
      "Iteration is: 968 and loss is: 0.025068268179893494\n",
      "Iteration is: 969 and loss is: 0.032380878925323486\n",
      "Iteration is: 970 and loss is: 0.03371071815490723\n",
      "Iteration is: 971 and loss is: 0.023603294044733047\n",
      "Iteration is: 972 and loss is: 0.03131553530693054\n",
      "Iteration is: 973 and loss is: 0.023511916399002075\n",
      "Iteration is: 974 and loss is: 0.028899546712636948\n",
      "Iteration is: 975 and loss is: 0.029820725321769714\n",
      "Iteration is: 976 and loss is: 0.023832006379961967\n",
      "Iteration is: 977 and loss is: 0.024773912504315376\n",
      "Iteration is: 978 and loss is: 0.028804393485188484\n",
      "Iteration is: 979 and loss is: 0.02473977766931057\n",
      "Iteration is: 980 and loss is: 0.02328210510313511\n",
      "Iteration is: 981 and loss is: 0.026551974937319756\n",
      "Iteration is: 982 and loss is: 0.024273527786135674\n",
      "Iteration is: 983 and loss is: 0.022918568924069405\n",
      "Iteration is: 984 and loss is: 0.02532322332262993\n",
      "Iteration is: 985 and loss is: 0.024171417579054832\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 986 and loss is: 0.02252628654241562\n",
      "Iteration is: 987 and loss is: 0.023946400731801987\n",
      "Iteration is: 988 and loss is: 0.02374759316444397\n",
      "Iteration is: 989 and loss is: 0.0222806204110384\n",
      "Iteration is: 990 and loss is: 0.022926412522792816\n",
      "Iteration is: 991 and loss is: 0.023348184302449226\n",
      "Iteration is: 992 and loss is: 0.02209397777915001\n",
      "Iteration is: 993 and loss is: 0.02203996479511261\n",
      "Iteration is: 994 and loss is: 0.022742226719856262\n",
      "Iteration is: 995 and loss is: 0.022214971482753754\n",
      "Iteration is: 996 and loss is: 0.021595831960439682\n",
      "Iteration is: 997 and loss is: 0.022065164521336555\n",
      "Iteration is: 998 and loss is: 0.022136345505714417\n",
      "Iteration is: 999 and loss is: 0.021501289680600166\n",
      "Iteration is: 1000 and loss is: 0.02147984690964222\n",
      "Iteration is: 1001 and loss is: 0.021788708865642548\n",
      "Iteration is: 1002 and loss is: 0.021543055772781372\n",
      "Iteration is: 1003 and loss is: 0.021200956776738167\n",
      "Iteration is: 1004 and loss is: 0.021349558606743813\n",
      "Iteration is: 1005 and loss is: 0.021470721811056137\n",
      "Iteration is: 1006 and loss is: 0.0212327279150486\n",
      "Iteration is: 1007 and loss is: 0.020998213440179825\n",
      "Iteration is: 1008 and loss is: 0.021094201132655144\n",
      "Iteration is: 1009 and loss is: 0.02117845043540001\n",
      "Iteration is: 1010 and loss is: 0.02099945768713951\n",
      "Iteration is: 1011 and loss is: 0.020828990265727043\n",
      "Iteration is: 1012 and loss is: 0.020846758037805557\n",
      "Iteration is: 1013 and loss is: 0.02091033384203911\n",
      "Iteration is: 1014 and loss is: 0.020835990086197853\n",
      "Iteration is: 1015 and loss is: 0.020700491964817047\n",
      "Iteration is: 1016 and loss is: 0.020616741850972176\n",
      "Iteration is: 1017 and loss is: 0.020613688975572586\n",
      "Iteration is: 1018 and loss is: 0.020626934245228767\n",
      "Iteration is: 1019 and loss is: 0.020596299320459366\n",
      "Iteration is: 1020 and loss is: 0.020502332597970963\n",
      "Iteration is: 1021 and loss is: 0.02040463127195835\n",
      "Iteration is: 1022 and loss is: 0.020370911806821823\n",
      "Iteration is: 1023 and loss is: 0.020371604710817337\n",
      "Iteration is: 1024 and loss is: 0.02036355622112751\n",
      "Iteration is: 1025 and loss is: 0.020324479788541794\n",
      "Iteration is: 1026 and loss is: 0.02026519738137722\n",
      "Iteration is: 1027 and loss is: 0.020199596881866455\n",
      "Iteration is: 1028 and loss is: 0.020148415118455887\n",
      "Iteration is: 1029 and loss is: 0.020119890570640564\n",
      "Iteration is: 1030 and loss is: 0.020097211003303528\n",
      "Iteration is: 1031 and loss is: 0.020079581066966057\n",
      "Iteration is: 1032 and loss is: 0.020061299204826355\n",
      "Iteration is: 1033 and loss is: 0.02003411203622818\n",
      "Iteration is: 1034 and loss is: 0.019999604672193527\n",
      "Iteration is: 1035 and loss is: 0.019964534789323807\n",
      "Iteration is: 1036 and loss is: 0.019928179681301117\n",
      "Iteration is: 1037 and loss is: 0.019888393580913544\n",
      "Iteration is: 1038 and loss is: 0.019851911813020706\n",
      "Iteration is: 1039 and loss is: 0.01981920376420021\n",
      "Iteration is: 1040 and loss is: 0.019787922501564026\n",
      "Iteration is: 1041 and loss is: 0.01976136490702629\n",
      "Iteration is: 1042 and loss is: 0.01973995566368103\n",
      "Iteration is: 1043 and loss is: 0.019727367907762527\n",
      "Iteration is: 1044 and loss is: 0.019730467349290848\n",
      "Iteration is: 1045 and loss is: 0.01976771280169487\n",
      "Iteration is: 1046 and loss is: 0.01986907795071602\n",
      "Iteration is: 1047 and loss is: 0.020118054002523422\n",
      "Iteration is: 1048 and loss is: 0.020643867552280426\n",
      "Iteration is: 1049 and loss is: 0.021837474778294563\n",
      "Iteration is: 1050 and loss is: 0.02407553233206272\n",
      "Iteration is: 1051 and loss is: 0.028880257159471512\n",
      "Iteration is: 1052 and loss is: 0.03471597284078598\n",
      "Iteration is: 1053 and loss is: 0.04129716753959656\n",
      "Iteration is: 1054 and loss is: 0.03750571236014366\n",
      "Iteration is: 1055 and loss is: 0.02657342702150345\n",
      "Iteration is: 1056 and loss is: 0.019953247159719467\n",
      "Iteration is: 1057 and loss is: 0.023189354687929153\n",
      "Iteration is: 1058 and loss is: 0.02878328040242195\n",
      "Iteration is: 1059 and loss is: 0.027181893587112427\n",
      "Iteration is: 1060 and loss is: 0.020974978804588318\n",
      "Iteration is: 1061 and loss is: 0.021017251536250114\n",
      "Iteration is: 1062 and loss is: 0.02500240132212639\n",
      "Iteration is: 1063 and loss is: 0.023307129740715027\n",
      "Iteration is: 1064 and loss is: 0.019739020615816116\n",
      "Iteration is: 1065 and loss is: 0.02158864587545395\n",
      "Iteration is: 1066 and loss is: 0.02351004257798195\n",
      "Iteration is: 1067 and loss is: 0.021326135843992233\n",
      "Iteration is: 1068 and loss is: 0.019410910084843636\n",
      "Iteration is: 1069 and loss is: 0.020681265741586685\n",
      "Iteration is: 1070 and loss is: 0.021946195513010025\n",
      "Iteration is: 1071 and loss is: 0.02078085020184517\n",
      "Iteration is: 1072 and loss is: 0.01935718208551407\n",
      "Iteration is: 1073 and loss is: 0.020136598497629166\n",
      "Iteration is: 1074 and loss is: 0.02100563421845436\n",
      "Iteration is: 1075 and loss is: 0.020011726766824722\n",
      "Iteration is: 1076 and loss is: 0.019122350960969925\n",
      "Iteration is: 1077 and loss is: 0.019640648737549782\n",
      "Iteration is: 1078 and loss is: 0.020243627950549126\n",
      "Iteration is: 1079 and loss is: 0.019736558198928833\n",
      "Iteration is: 1080 and loss is: 0.01902364194393158\n",
      "Iteration is: 1081 and loss is: 0.019232003018260002\n",
      "Iteration is: 1082 and loss is: 0.019695566967129707\n",
      "Iteration is: 1083 and loss is: 0.019398245960474014\n",
      "Iteration is: 1084 and loss is: 0.018897345289587975\n",
      "Iteration is: 1085 and loss is: 0.018958738073706627\n",
      "Iteration is: 1086 and loss is: 0.01927522011101246\n",
      "Iteration is: 1087 and loss is: 0.019217662513256073\n",
      "Iteration is: 1088 and loss is: 0.018844228237867355\n",
      "Iteration is: 1089 and loss is: 0.01870107091963291\n",
      "Iteration is: 1090 and loss is: 0.018869508057832718\n",
      "Iteration is: 1091 and loss is: 0.018965214490890503\n",
      "Iteration is: 1092 and loss is: 0.01880788430571556\n",
      "Iteration is: 1093 and loss is: 0.0185895673930645\n",
      "Iteration is: 1094 and loss is: 0.01854989305138588\n",
      "Iteration is: 1095 and loss is: 0.018643274903297424\n",
      "Iteration is: 1096 and loss is: 0.01868579164147377\n",
      "Iteration is: 1097 and loss is: 0.018596218898892403\n",
      "Iteration is: 1098 and loss is: 0.018444858491420746\n",
      "Iteration is: 1099 and loss is: 0.018353711813688278\n",
      "Iteration is: 1100 and loss is: 0.01835617423057556\n",
      "Iteration is: 1101 and loss is: 0.018393943086266518\n",
      "Iteration is: 1102 and loss is: 0.018386706709861755\n",
      "Iteration is: 1103 and loss is: 0.018309777602553368\n",
      "Iteration is: 1104 and loss is: 0.01820528507232666\n",
      "Iteration is: 1105 and loss is: 0.018125269562005997\n",
      "Iteration is: 1106 and loss is: 0.01809074357151985\n",
      "Iteration is: 1107 and loss is: 0.018083032220602036\n",
      "Iteration is: 1108 and loss is: 0.018073774874210358\n",
      "Iteration is: 1109 and loss is: 0.018042221665382385\n",
      "Iteration is: 1110 and loss is: 0.017983440309762955\n",
      "Iteration is: 1111 and loss is: 0.017909720540046692\n",
      "Iteration is: 1112 and loss is: 0.01783532276749611\n",
      "Iteration is: 1113 and loss is: 0.017771728336811066\n",
      "Iteration is: 1114 and loss is: 0.017719760537147522\n",
      "Iteration is: 1115 and loss is: 0.017677826806902885\n",
      "Iteration is: 1116 and loss is: 0.017641976475715637\n",
      "Iteration is: 1117 and loss is: 0.017606593668460846\n",
      "Iteration is: 1118 and loss is: 0.017568495124578476\n",
      "Iteration is: 1119 and loss is: 0.01752592995762825\n",
      "Iteration is: 1120 and loss is: 0.01748131960630417\n",
      "Iteration is: 1121 and loss is: 0.01743360608816147\n",
      "Iteration is: 1122 and loss is: 0.017385758459568024\n",
      "Iteration is: 1123 and loss is: 0.017338216304779053\n",
      "Iteration is: 1124 and loss is: 0.01729554682970047\n",
      "Iteration is: 1125 and loss is: 0.017259225249290466\n",
      "Iteration is: 1126 and loss is: 0.01724083535373211\n",
      "Iteration is: 1127 and loss is: 0.017254196107387543\n",
      "Iteration is: 1128 and loss is: 0.01734269969165325\n",
      "Iteration is: 1129 and loss is: 0.01758616417646408\n",
      "Iteration is: 1130 and loss is: 0.018205149099230766\n",
      "Iteration is: 1131 and loss is: 0.019685763865709305\n",
      "Iteration is: 1132 and loss is: 0.023366786539554596\n",
      "Iteration is: 1133 and loss is: 0.031628165394067764\n",
      "Iteration is: 1134 and loss is: 0.04897359758615494\n",
      "Iteration is: 1135 and loss is: 0.0690993219614029\n",
      "Iteration is: 1136 and loss is: 0.06982799619436264\n",
      "Iteration is: 1137 and loss is: 0.028961844742298126\n",
      "Iteration is: 1138 and loss is: 0.022672485560178757\n",
      "Iteration is: 1139 and loss is: 0.0432390496134758\n",
      "Iteration is: 1140 and loss is: 0.021025577560067177\n",
      "Iteration is: 1141 and loss is: 0.02948322519659996\n",
      "Iteration is: 1142 and loss is: 0.030694911256432533\n",
      "Iteration is: 1143 and loss is: 0.020189471542835236\n",
      "Iteration is: 1144 and loss is: 0.03074513003230095\n",
      "Iteration is: 1145 and loss is: 0.0186585932970047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 1146 and loss is: 0.028187964111566544\n",
      "Iteration is: 1147 and loss is: 0.018565818667411804\n",
      "Iteration is: 1148 and loss is: 0.025979310274124146\n",
      "Iteration is: 1149 and loss is: 0.019147049635648727\n",
      "Iteration is: 1150 and loss is: 0.02328328788280487\n",
      "Iteration is: 1151 and loss is: 0.018782468512654305\n",
      "Iteration is: 1152 and loss is: 0.022121354937553406\n",
      "Iteration is: 1153 and loss is: 0.019085410982370377\n",
      "Iteration is: 1154 and loss is: 0.020870165899395943\n",
      "Iteration is: 1155 and loss is: 0.019241273403167725\n",
      "Iteration is: 1156 and loss is: 0.019502690061926842\n",
      "Iteration is: 1157 and loss is: 0.018955053761601448\n",
      "Iteration is: 1158 and loss is: 0.018868958577513695\n",
      "Iteration is: 1159 and loss is: 0.018665820360183716\n",
      "Iteration is: 1160 and loss is: 0.018236543983221054\n",
      "Iteration is: 1161 and loss is: 0.018496420234441757\n",
      "Iteration is: 1162 and loss is: 0.017616504803299904\n",
      "Iteration is: 1163 and loss is: 0.01827421970665455\n",
      "Iteration is: 1164 and loss is: 0.017143191769719124\n",
      "Iteration is: 1165 and loss is: 0.01798928715288639\n",
      "Iteration is: 1166 and loss is: 0.01699698530137539\n",
      "Iteration is: 1167 and loss is: 0.017693495377898216\n",
      "Iteration is: 1168 and loss is: 0.017030103132128716\n",
      "Iteration is: 1169 and loss is: 0.01732836849987507\n",
      "Iteration is: 1170 and loss is: 0.017079003155231476\n",
      "Iteration is: 1171 and loss is: 0.016947660595178604\n",
      "Iteration is: 1172 and loss is: 0.017081227153539658\n",
      "Iteration is: 1173 and loss is: 0.016707444563508034\n",
      "Iteration is: 1174 and loss is: 0.017005641013383865\n",
      "Iteration is: 1175 and loss is: 0.01658429205417633\n",
      "Iteration is: 1176 and loss is: 0.01680789142847061\n",
      "Iteration is: 1177 and loss is: 0.016556421294808388\n",
      "Iteration is: 1178 and loss is: 0.016571372747421265\n",
      "Iteration is: 1179 and loss is: 0.01652170717716217\n",
      "Iteration is: 1180 and loss is: 0.01637224107980728\n",
      "Iteration is: 1181 and loss is: 0.016423901543021202\n",
      "Iteration is: 1182 and loss is: 0.016270214691758156\n",
      "Iteration is: 1183 and loss is: 0.01623581349849701\n",
      "Iteration is: 1184 and loss is: 0.01619531959295273\n",
      "Iteration is: 1185 and loss is: 0.016024168580770493\n",
      "Iteration is: 1186 and loss is: 0.01608678139746189\n",
      "Iteration is: 1187 and loss is: 0.015905095264315605\n",
      "Iteration is: 1188 and loss is: 0.015939027070999146\n",
      "Iteration is: 1189 and loss is: 0.015887035056948662\n",
      "Iteration is: 1190 and loss is: 0.015807000920176506\n",
      "Iteration is: 1191 and loss is: 0.015824176371097565\n",
      "Iteration is: 1192 and loss is: 0.01573425717651844\n",
      "Iteration is: 1193 and loss is: 0.015691213309764862\n",
      "Iteration is: 1194 and loss is: 0.01566406898200512\n",
      "Iteration is: 1195 and loss is: 0.015595081262290478\n",
      "Iteration is: 1196 and loss is: 0.015555541962385178\n",
      "Iteration is: 1197 and loss is: 0.015547914430499077\n",
      "Iteration is: 1198 and loss is: 0.015470579266548157\n",
      "Iteration is: 1199 and loss is: 0.01546783372759819\n",
      "Iteration is: 1200 and loss is: 0.015422728843986988\n",
      "Iteration is: 1201 and loss is: 0.015377238392829895\n",
      "Iteration is: 1202 and loss is: 0.015360206365585327\n",
      "Iteration is: 1203 and loss is: 0.015312118455767632\n",
      "Iteration is: 1204 and loss is: 0.015280049294233322\n",
      "Iteration is: 1205 and loss is: 0.015252579003572464\n",
      "Iteration is: 1206 and loss is: 0.015215083956718445\n",
      "Iteration is: 1207 and loss is: 0.015178920701146126\n",
      "Iteration is: 1208 and loss is: 0.015160181559622288\n",
      "Iteration is: 1209 and loss is: 0.015116878785192966\n",
      "Iteration is: 1210 and loss is: 0.015090543776750565\n",
      "Iteration is: 1211 and loss is: 0.0150647247210145\n",
      "Iteration is: 1212 and loss is: 0.015026304870843887\n",
      "Iteration is: 1213 and loss is: 0.015000304207205772\n",
      "Iteration is: 1214 and loss is: 0.014970803633332253\n",
      "Iteration is: 1215 and loss is: 0.014938853681087494\n",
      "Iteration is: 1216 and loss is: 0.014908731915056705\n",
      "Iteration is: 1217 and loss is: 0.014884162694215775\n",
      "Iteration is: 1218 and loss is: 0.014851685613393784\n",
      "Iteration is: 1219 and loss is: 0.014824625104665756\n",
      "Iteration is: 1220 and loss is: 0.014800249598920345\n",
      "Iteration is: 1221 and loss is: 0.01476987823843956\n",
      "Iteration is: 1222 and loss is: 0.014743984676897526\n",
      "Iteration is: 1223 and loss is: 0.01471813302487135\n",
      "Iteration is: 1224 and loss is: 0.014691082760691643\n",
      "Iteration is: 1225 and loss is: 0.014663022011518478\n",
      "Iteration is: 1226 and loss is: 0.014638298191130161\n",
      "Iteration is: 1227 and loss is: 0.014611990191042423\n",
      "Iteration is: 1228 and loss is: 0.014584271237254143\n",
      "Iteration is: 1229 and loss is: 0.014559894800186157\n",
      "Iteration is: 1230 and loss is: 0.014534207992255688\n",
      "Iteration is: 1231 and loss is: 0.014508180320262909\n",
      "Iteration is: 1232 and loss is: 0.014482710510492325\n",
      "Iteration is: 1233 and loss is: 0.014458262361586094\n",
      "Iteration is: 1234 and loss is: 0.014432888478040695\n",
      "Iteration is: 1235 and loss is: 0.014407085254788399\n",
      "Iteration is: 1236 and loss is: 0.014382961206138134\n",
      "Iteration is: 1237 and loss is: 0.014358073472976685\n",
      "Iteration is: 1238 and loss is: 0.014333073981106281\n",
      "Iteration is: 1239 and loss is: 0.01430853083729744\n",
      "Iteration is: 1240 and loss is: 0.014284439384937286\n",
      "Iteration is: 1241 and loss is: 0.014260202646255493\n",
      "Iteration is: 1242 and loss is: 0.014235585927963257\n",
      "Iteration is: 1243 and loss is: 0.014211850240826607\n",
      "Iteration is: 1244 and loss is: 0.01418804656714201\n",
      "Iteration is: 1245 and loss is: 0.014164047315716743\n",
      "Iteration is: 1246 and loss is: 0.014140235260128975\n",
      "Iteration is: 1247 and loss is: 0.01411667000502348\n",
      "Iteration is: 1248 and loss is: 0.014093316160142422\n",
      "Iteration is: 1249 and loss is: 0.014069711789488792\n",
      "Iteration is: 1250 and loss is: 0.0140463188290596\n",
      "Iteration is: 1251 and loss is: 0.01402318011969328\n",
      "Iteration is: 1252 and loss is: 0.014000067487359047\n",
      "Iteration is: 1253 and loss is: 0.01397700048983097\n",
      "Iteration is: 1254 and loss is: 0.013953914865851402\n",
      "Iteration is: 1255 and loss is: 0.013931110501289368\n",
      "Iteration is: 1256 and loss is: 0.013908376917243004\n",
      "Iteration is: 1257 and loss is: 0.01388560701161623\n",
      "Iteration is: 1258 and loss is: 0.01386295072734356\n",
      "Iteration is: 1259 and loss is: 0.013840380124747753\n",
      "Iteration is: 1260 and loss is: 0.01381797157227993\n",
      "Iteration is: 1261 and loss is: 0.013795575127005577\n",
      "Iteration is: 1262 and loss is: 0.01377321407198906\n",
      "Iteration is: 1263 and loss is: 0.013750983402132988\n",
      "Iteration is: 1264 and loss is: 0.013728859834372997\n",
      "Iteration is: 1265 and loss is: 0.013706794008612633\n",
      "Iteration is: 1266 and loss is: 0.013684781268239021\n",
      "Iteration is: 1267 and loss is: 0.013662803918123245\n",
      "Iteration is: 1268 and loss is: 0.01364095602184534\n",
      "Iteration is: 1269 and loss is: 0.01361919566988945\n",
      "Iteration is: 1270 and loss is: 0.01359749399125576\n",
      "Iteration is: 1271 and loss is: 0.013575838878750801\n",
      "Iteration is: 1272 and loss is: 0.013554238714277744\n",
      "Iteration is: 1273 and loss is: 0.013532737269997597\n",
      "Iteration is: 1274 and loss is: 0.013511322438716888\n",
      "Iteration is: 1275 and loss is: 0.013489944860339165\n",
      "Iteration is: 1276 and loss is: 0.013468652032315731\n",
      "Iteration is: 1277 and loss is: 0.013447406701743603\n",
      "Iteration is: 1278 and loss is: 0.01342623122036457\n",
      "Iteration is: 1279 and loss is: 0.013405133038759232\n",
      "Iteration is: 1280 and loss is: 0.013384103775024414\n",
      "Iteration is: 1281 and loss is: 0.013363141566514969\n",
      "Iteration is: 1282 and loss is: 0.013342219404876232\n",
      "Iteration is: 1283 and loss is: 0.013321379199624062\n",
      "Iteration is: 1284 and loss is: 0.013300600461661816\n",
      "Iteration is: 1285 and loss is: 0.013279873877763748\n",
      "Iteration is: 1286 and loss is: 0.013259219005703926\n",
      "Iteration is: 1287 and loss is: 0.013238633051514626\n",
      "Iteration is: 1288 and loss is: 0.013218093663454056\n",
      "Iteration is: 1289 and loss is: 0.013197626918554306\n",
      "Iteration is: 1290 and loss is: 0.013177214190363884\n",
      "Iteration is: 1291 and loss is: 0.013156866654753685\n",
      "Iteration is: 1292 and loss is: 0.013136574998497963\n",
      "Iteration is: 1293 and loss is: 0.013116342946887016\n",
      "Iteration is: 1294 and loss is: 0.013096170499920845\n",
      "Iteration is: 1295 and loss is: 0.013076066970825195\n",
      "Iteration is: 1296 and loss is: 0.01305600069463253\n",
      "Iteration is: 1297 and loss is: 0.013036017306149006\n",
      "Iteration is: 1298 and loss is: 0.013016071170568466\n",
      "Iteration is: 1299 and loss is: 0.012996193021535873\n",
      "Iteration is: 1300 and loss is: 0.012976380065083504\n",
      "Iteration is: 1301 and loss is: 0.012956605292856693\n",
      "Iteration is: 1302 and loss is: 0.012936904095113277\n",
      "Iteration is: 1303 and loss is: 0.012917248532176018\n",
      "Iteration is: 1304 and loss is: 0.012897659093141556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 1305 and loss is: 0.012878114357590675\n",
      "Iteration is: 1306 and loss is: 0.01285862922668457\n",
      "Iteration is: 1307 and loss is: 0.012839196249842644\n",
      "Iteration is: 1308 and loss is: 0.01281982846558094\n",
      "Iteration is: 1309 and loss is: 0.012800516560673714\n",
      "Iteration is: 1310 and loss is: 0.012781249359250069\n",
      "Iteration is: 1311 and loss is: 0.012762049213051796\n",
      "Iteration is: 1312 and loss is: 0.012742887251079082\n",
      "Iteration is: 1313 and loss is: 0.012723797932267189\n",
      "Iteration is: 1314 and loss is: 0.012704752385616302\n",
      "Iteration is: 1315 and loss is: 0.012685775756835938\n",
      "Iteration is: 1316 and loss is: 0.012666834518313408\n",
      "Iteration is: 1317 and loss is: 0.012647971510887146\n",
      "Iteration is: 1318 and loss is: 0.012629145756363869\n",
      "Iteration is: 1319 and loss is: 0.012610409408807755\n",
      "Iteration is: 1320 and loss is: 0.01259174756705761\n",
      "Iteration is: 1321 and loss is: 0.012573201209306717\n",
      "Iteration is: 1322 and loss is: 0.012554817833006382\n",
      "Iteration is: 1323 and loss is: 0.012536752037703991\n",
      "Iteration is: 1324 and loss is: 0.012519219890236855\n",
      "Iteration is: 1325 and loss is: 0.012502828612923622\n",
      "Iteration is: 1326 and loss is: 0.01248888112604618\n",
      "Iteration is: 1327 and loss is: 0.012480208650231361\n",
      "Iteration is: 1328 and loss is: 0.012483599595725536\n",
      "Iteration is: 1329 and loss is: 0.012514421716332436\n",
      "Iteration is: 1330 and loss is: 0.012610998004674911\n",
      "Iteration is: 1331 and loss is: 0.012859806418418884\n",
      "Iteration is: 1332 and loss is: 0.013489112257957458\n",
      "Iteration is: 1333 and loss is: 0.014962740242481232\n",
      "Iteration is: 1334 and loss is: 0.01850472204387188\n",
      "Iteration is: 1335 and loss is: 0.025371914729475975\n",
      "Iteration is: 1336 and loss is: 0.03703158721327782\n",
      "Iteration is: 1337 and loss is: 0.04517050087451935\n",
      "Iteration is: 1338 and loss is: 0.03936699405312538\n",
      "Iteration is: 1339 and loss is: 0.020460475236177444\n",
      "Iteration is: 1340 and loss is: 0.01318403147161007\n",
      "Iteration is: 1341 and loss is: 0.02359761670231819\n",
      "Iteration is: 1342 and loss is: 0.025765832513570786\n",
      "Iteration is: 1343 and loss is: 0.015413275919854641\n",
      "Iteration is: 1344 and loss is: 0.014818847179412842\n",
      "Iteration is: 1345 and loss is: 0.02124277874827385\n",
      "Iteration is: 1346 and loss is: 0.01640053279697895\n",
      "Iteration is: 1347 and loss is: 0.013208596035838127\n",
      "Iteration is: 1348 and loss is: 0.018417734652757645\n",
      "Iteration is: 1349 and loss is: 0.015856657177209854\n",
      "Iteration is: 1350 and loss is: 0.012786011211574078\n",
      "Iteration is: 1351 and loss is: 0.01655479706823826\n",
      "Iteration is: 1352 and loss is: 0.01497676596045494\n",
      "Iteration is: 1353 and loss is: 0.012685555964708328\n",
      "Iteration is: 1354 and loss is: 0.015510816127061844\n",
      "Iteration is: 1355 and loss is: 0.014192482456564903\n",
      "Iteration is: 1356 and loss is: 0.012640539556741714\n",
      "Iteration is: 1357 and loss is: 0.01473261322826147\n",
      "Iteration is: 1358 and loss is: 0.013497903943061829\n",
      "Iteration is: 1359 and loss is: 0.012610174715518951\n",
      "Iteration is: 1360 and loss is: 0.014110060408711433\n",
      "Iteration is: 1361 and loss is: 0.01300425361841917\n",
      "Iteration is: 1362 and loss is: 0.012574632652103901\n",
      "Iteration is: 1363 and loss is: 0.013605834916234016\n",
      "Iteration is: 1364 and loss is: 0.012680558487772942\n",
      "Iteration is: 1365 and loss is: 0.012523949146270752\n",
      "Iteration is: 1366 and loss is: 0.013209108263254166\n",
      "Iteration is: 1367 and loss is: 0.012440724298357964\n",
      "Iteration is: 1368 and loss is: 0.01243736781179905\n",
      "Iteration is: 1369 and loss is: 0.012896819040179253\n",
      "Iteration is: 1370 and loss is: 0.012292113155126572\n",
      "Iteration is: 1371 and loss is: 0.01232363935559988\n",
      "Iteration is: 1372 and loss is: 0.012646438553929329\n",
      "Iteration is: 1373 and loss is: 0.01219902466982603\n",
      "Iteration is: 1374 and loss is: 0.01221439428627491\n",
      "Iteration is: 1375 and loss is: 0.012456059455871582\n",
      "Iteration is: 1376 and loss is: 0.012128368020057678\n",
      "Iteration is: 1377 and loss is: 0.01210025604814291\n",
      "Iteration is: 1378 and loss is: 0.012292108498513699\n",
      "Iteration is: 1379 and loss is: 0.012067947536706924\n",
      "Iteration is: 1380 and loss is: 0.01200105156749487\n",
      "Iteration is: 1381 and loss is: 0.012151079252362251\n",
      "Iteration is: 1382 and loss is: 0.012010915204882622\n",
      "Iteration is: 1383 and loss is: 0.011915886774659157\n",
      "Iteration is: 1384 and loss is: 0.012028146535158157\n",
      "Iteration is: 1385 and loss is: 0.011957730166614056\n",
      "Iteration is: 1386 and loss is: 0.011848405003547668\n",
      "Iteration is: 1387 and loss is: 0.011917158029973507\n",
      "Iteration is: 1388 and loss is: 0.011898057535290718\n",
      "Iteration is: 1389 and loss is: 0.011793272569775581\n",
      "Iteration is: 1390 and loss is: 0.011815888807177544\n",
      "Iteration is: 1391 and loss is: 0.011830156669020653\n",
      "Iteration is: 1392 and loss is: 0.011748291552066803\n",
      "Iteration is: 1393 and loss is: 0.011729701422154903\n",
      "Iteration is: 1394 and loss is: 0.011755712330341339\n",
      "Iteration is: 1395 and loss is: 0.011707734316587448\n",
      "Iteration is: 1396 and loss is: 0.011662881821393967\n",
      "Iteration is: 1397 and loss is: 0.011677924543619156\n",
      "Iteration is: 1398 and loss is: 0.011662077158689499\n",
      "Iteration is: 1399 and loss is: 0.011612912639975548\n",
      "Iteration is: 1400 and loss is: 0.011603833176195621\n",
      "Iteration is: 1401 and loss is: 0.011605484411120415\n",
      "Iteration is: 1402 and loss is: 0.011571208015084267\n",
      "Iteration is: 1403 and loss is: 0.011542148888111115\n",
      "Iteration is: 1404 and loss is: 0.011541090905666351\n",
      "Iteration is: 1405 and loss is: 0.011526554822921753\n",
      "Iteration is: 1406 and loss is: 0.011494269594550133\n",
      "Iteration is: 1407 and loss is: 0.011478818953037262\n",
      "Iteration is: 1408 and loss is: 0.011473231017589569\n",
      "Iteration is: 1409 and loss is: 0.011451929807662964\n",
      "Iteration is: 1410 and loss is: 0.011426584795117378\n",
      "Iteration is: 1411 and loss is: 0.011415230110287666\n",
      "Iteration is: 1412 and loss is: 0.011404717341065407\n",
      "Iteration is: 1413 and loss is: 0.011383110657334328\n",
      "Iteration is: 1414 and loss is: 0.011362750083208084\n",
      "Iteration is: 1415 and loss is: 0.011351339519023895\n",
      "Iteration is: 1416 and loss is: 0.011338330805301666\n",
      "Iteration is: 1417 and loss is: 0.011318391188979149\n",
      "Iteration is: 1418 and loss is: 0.011300377547740936\n",
      "Iteration is: 1419 and loss is: 0.011287994682788849\n",
      "Iteration is: 1420 and loss is: 0.01127416081726551\n",
      "Iteration is: 1421 and loss is: 0.011255991645157337\n",
      "Iteration is: 1422 and loss is: 0.011238910257816315\n",
      "Iteration is: 1423 and loss is: 0.011225625872612\n",
      "Iteration is: 1424 and loss is: 0.011211838573217392\n",
      "Iteration is: 1425 and loss is: 0.011195133440196514\n",
      "Iteration is: 1426 and loss is: 0.011178595945239067\n",
      "Iteration is: 1427 and loss is: 0.011164536699652672\n",
      "Iteration is: 1428 and loss is: 0.011150886304676533\n",
      "Iteration is: 1429 and loss is: 0.011135408654808998\n",
      "Iteration is: 1430 and loss is: 0.011119344271719456\n",
      "Iteration is: 1431 and loss is: 0.011104685254395008\n",
      "Iteration is: 1432 and loss is: 0.011090949177742004\n",
      "Iteration is: 1433 and loss is: 0.01107645221054554\n",
      "Iteration is: 1434 and loss is: 0.0110610481351614\n",
      "Iteration is: 1435 and loss is: 0.011046061292290688\n",
      "Iteration is: 1436 and loss is: 0.011032006703317165\n",
      "Iteration is: 1437 and loss is: 0.011018045246601105\n",
      "Iteration is: 1438 and loss is: 0.011003455147147179\n",
      "Iteration is: 1439 and loss is: 0.010988588444888592\n",
      "Iteration is: 1440 and loss is: 0.010974177159368992\n",
      "Iteration is: 1441 and loss is: 0.010960248298943043\n",
      "Iteration is: 1442 and loss is: 0.010946275666356087\n",
      "Iteration is: 1443 and loss is: 0.010931918397545815\n",
      "Iteration is: 1444 and loss is: 0.010917479172348976\n",
      "Iteration is: 1445 and loss is: 0.010903337970376015\n",
      "Iteration is: 1446 and loss is: 0.010889489203691483\n",
      "Iteration is: 1447 and loss is: 0.010875625535845757\n",
      "Iteration is: 1448 and loss is: 0.010861577466130257\n",
      "Iteration is: 1449 and loss is: 0.010847462341189384\n",
      "Iteration is: 1450 and loss is: 0.010833483189344406\n",
      "Iteration is: 1451 and loss is: 0.010819727554917336\n",
      "Iteration is: 1452 and loss is: 0.010806010104715824\n",
      "Iteration is: 1453 and loss is: 0.01079223956912756\n",
      "Iteration is: 1454 and loss is: 0.010778398253023624\n",
      "Iteration is: 1455 and loss is: 0.010764604434370995\n",
      "Iteration is: 1456 and loss is: 0.010750909335911274\n",
      "Iteration is: 1457 and loss is: 0.010737340897321701\n",
      "Iteration is: 1458 and loss is: 0.010723792016506195\n",
      "Iteration is: 1459 and loss is: 0.010710208676755428\n",
      "Iteration is: 1460 and loss is: 0.010696617886424065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 1461 and loss is: 0.010683063417673111\n",
      "Iteration is: 1462 and loss is: 0.010669589042663574\n",
      "Iteration is: 1463 and loss is: 0.010656170547008514\n",
      "Iteration is: 1464 and loss is: 0.010642796754837036\n",
      "Iteration is: 1465 and loss is: 0.010629428550601006\n",
      "Iteration is: 1466 and loss is: 0.010616086423397064\n",
      "Iteration is: 1467 and loss is: 0.010602733120322227\n",
      "Iteration is: 1468 and loss is: 0.010589445941150188\n",
      "Iteration is: 1469 and loss is: 0.010576187632977962\n",
      "Iteration is: 1470 and loss is: 0.010562987998127937\n",
      "Iteration is: 1471 and loss is: 0.010549809783697128\n",
      "Iteration is: 1472 and loss is: 0.01053665578365326\n",
      "Iteration is: 1473 and loss is: 0.010523530654609203\n",
      "Iteration is: 1474 and loss is: 0.01051042415201664\n",
      "Iteration is: 1475 and loss is: 0.01049734279513359\n",
      "Iteration is: 1476 and loss is: 0.010484294965863228\n",
      "Iteration is: 1477 and loss is: 0.010471288114786148\n",
      "Iteration is: 1478 and loss is: 0.010458304546773434\n",
      "Iteration is: 1479 and loss is: 0.010445360094308853\n",
      "Iteration is: 1480 and loss is: 0.010432441718876362\n",
      "Iteration is: 1481 and loss is: 0.01041954942047596\n",
      "Iteration is: 1482 and loss is: 0.010406684130430222\n",
      "Iteration is: 1483 and loss is: 0.010393837466835976\n",
      "Iteration is: 1484 and loss is: 0.010381020605564117\n",
      "Iteration is: 1485 and loss is: 0.010368229821324348\n",
      "Iteration is: 1486 and loss is: 0.01035546138882637\n",
      "Iteration is: 1487 and loss is: 0.010342740453779697\n",
      "Iteration is: 1488 and loss is: 0.010330034419894218\n",
      "Iteration is: 1489 and loss is: 0.010317348875105381\n",
      "Iteration is: 1490 and loss is: 0.010304697789251804\n",
      "Iteration is: 1491 and loss is: 0.01029207557439804\n",
      "Iteration is: 1492 and loss is: 0.01027948409318924\n",
      "Iteration is: 1493 and loss is: 0.010266919620335102\n",
      "Iteration is: 1494 and loss is: 0.010254365392029285\n",
      "Iteration is: 1495 and loss is: 0.010241849347949028\n",
      "Iteration is: 1496 and loss is: 0.01022935751825571\n",
      "Iteration is: 1497 and loss is: 0.010216879658401012\n",
      "Iteration is: 1498 and loss is: 0.01020444743335247\n",
      "Iteration is: 1499 and loss is: 0.010192029178142548\n",
      "Iteration is: 1500 and loss is: 0.010179653763771057\n",
      "Iteration is: 1501 and loss is: 0.010167298838496208\n",
      "Iteration is: 1502 and loss is: 0.01015498861670494\n",
      "Iteration is: 1503 and loss is: 0.010142707265913486\n",
      "Iteration is: 1504 and loss is: 0.010130498558282852\n",
      "Iteration is: 1505 and loss is: 0.010118354111909866\n",
      "Iteration is: 1506 and loss is: 0.010106348432600498\n",
      "Iteration is: 1507 and loss is: 0.010094559751451015\n",
      "Iteration is: 1508 and loss is: 0.010083137080073357\n",
      "Iteration is: 1509 and loss is: 0.010072408244013786\n",
      "Iteration is: 1510 and loss is: 0.0100629897788167\n",
      "Iteration is: 1511 and loss is: 0.010056143626570702\n",
      "Iteration is: 1512 and loss is: 0.01005440391600132\n",
      "Iteration is: 1513 and loss is: 0.010063085705041885\n",
      "Iteration is: 1514 and loss is: 0.01009335182607174\n",
      "Iteration is: 1515 and loss is: 0.0101691959425807\n",
      "Iteration is: 1516 and loss is: 0.010342102497816086\n",
      "Iteration is: 1517 and loss is: 0.01072576642036438\n",
      "Iteration is: 1518 and loss is: 0.011562854051589966\n",
      "Iteration is: 1519 and loss is: 0.013385210186243057\n",
      "Iteration is: 1520 and loss is: 0.017194269225001335\n",
      "Iteration is: 1521 and loss is: 0.024818584322929382\n",
      "Iteration is: 1522 and loss is: 0.03702959790825844\n",
      "Iteration is: 1523 and loss is: 0.05196961760520935\n",
      "Iteration is: 1524 and loss is: 0.050652991980314255\n",
      "Iteration is: 1525 and loss is: 0.029883230105042458\n",
      "Iteration is: 1526 and loss is: 0.01066347025334835\n",
      "Iteration is: 1527 and loss is: 0.02258153446018696\n",
      "Iteration is: 1528 and loss is: 0.03093121200799942\n",
      "Iteration is: 1529 and loss is: 0.014884315431118011\n",
      "Iteration is: 1530 and loss is: 0.014402318745851517\n",
      "Iteration is: 1531 and loss is: 0.024618886411190033\n",
      "Iteration is: 1532 and loss is: 0.01424011867493391\n",
      "Iteration is: 1533 and loss is: 0.013350707478821278\n",
      "Iteration is: 1534 and loss is: 0.019679995253682137\n",
      "Iteration is: 1535 and loss is: 0.011920366436243057\n",
      "Iteration is: 1536 and loss is: 0.013609369285404682\n",
      "Iteration is: 1537 and loss is: 0.016890209168195724\n",
      "Iteration is: 1538 and loss is: 0.01087898574769497\n",
      "Iteration is: 1539 and loss is: 0.013369224965572357\n",
      "Iteration is: 1540 and loss is: 0.014125596731901169\n",
      "Iteration is: 1541 and loss is: 0.010403212159872055\n",
      "Iteration is: 1542 and loss is: 0.013545619323849678\n",
      "Iteration is: 1543 and loss is: 0.01216964516788721\n",
      "Iteration is: 1544 and loss is: 0.010587010532617569\n",
      "Iteration is: 1545 and loss is: 0.012999975122511387\n",
      "Iteration is: 1546 and loss is: 0.010898446664214134\n",
      "Iteration is: 1547 and loss is: 0.010912373661994934\n",
      "Iteration is: 1548 and loss is: 0.012137284502387047\n",
      "Iteration is: 1549 and loss is: 0.010297797620296478\n",
      "Iteration is: 1550 and loss is: 0.011129498481750488\n",
      "Iteration is: 1551 and loss is: 0.01126173697412014\n",
      "Iteration is: 1552 and loss is: 0.010119049809873104\n",
      "Iteration is: 1553 and loss is: 0.011112538166344166\n",
      "Iteration is: 1554 and loss is: 0.010568375699222088\n",
      "Iteration is: 1555 and loss is: 0.010193856433033943\n",
      "Iteration is: 1556 and loss is: 0.010883105918765068\n",
      "Iteration is: 1557 and loss is: 0.01017715223133564\n",
      "Iteration is: 1558 and loss is: 0.010287517681717873\n",
      "Iteration is: 1559 and loss is: 0.010582450777292252\n",
      "Iteration is: 1560 and loss is: 0.009996399283409119\n",
      "Iteration is: 1561 and loss is: 0.010308582335710526\n",
      "Iteration is: 1562 and loss is: 0.01029248721897602\n",
      "Iteration is: 1563 and loss is: 0.009931402280926704\n",
      "Iteration is: 1564 and loss is: 0.01024986058473587\n",
      "Iteration is: 1565 and loss is: 0.010073615238070488\n",
      "Iteration is: 1566 and loss is: 0.009902111254632473\n",
      "Iteration is: 1567 and loss is: 0.010134989395737648\n",
      "Iteration is: 1568 and loss is: 0.009928883984684944\n",
      "Iteration is: 1569 and loss is: 0.00987916998565197\n",
      "Iteration is: 1570 and loss is: 0.010019751265645027\n",
      "Iteration is: 1571 and loss is: 0.009833699092268944\n",
      "Iteration is: 1572 and loss is: 0.00983988307416439\n",
      "Iteration is: 1573 and loss is: 0.00991690531373024\n",
      "Iteration is: 1574 and loss is: 0.009770678356289864\n",
      "Iteration is: 1575 and loss is: 0.00979480892419815\n",
      "Iteration is: 1576 and loss is: 0.009829478338360786\n",
      "Iteration is: 1577 and loss is: 0.009716501459479332\n",
      "Iteration is: 1578 and loss is: 0.009744205512106419\n",
      "Iteration is: 1579 and loss is: 0.009759364649653435\n",
      "Iteration is: 1580 and loss is: 0.00967253278940916\n",
      "Iteration is: 1581 and loss is: 0.009692972525954247\n",
      "Iteration is: 1582 and loss is: 0.009698064997792244\n",
      "Iteration is: 1583 and loss is: 0.009631771594285965\n",
      "Iteration is: 1584 and loss is: 0.009644891135394573\n",
      "Iteration is: 1585 and loss is: 0.009645812213420868\n",
      "Iteration is: 1586 and loss is: 0.009593160822987556\n",
      "Iteration is: 1587 and loss is: 0.00959739089012146\n",
      "Iteration is: 1588 and loss is: 0.0095980204641819\n",
      "Iteration is: 1589 and loss is: 0.009556878358125687\n",
      "Iteration is: 1590 and loss is: 0.009553196839988232\n",
      "Iteration is: 1591 and loss is: 0.009554087184369564\n",
      "Iteration is: 1592 and loss is: 0.009521832689642906\n",
      "Iteration is: 1593 and loss is: 0.00951177068054676\n",
      "Iteration is: 1594 and loss is: 0.00951312854886055\n",
      "Iteration is: 1595 and loss is: 0.009488619863986969\n",
      "Iteration is: 1596 and loss is: 0.009473633021116257\n",
      "Iteration is: 1597 and loss is: 0.009473682381212711\n",
      "Iteration is: 1598 and loss is: 0.009455934166908264\n",
      "Iteration is: 1599 and loss is: 0.009438294917345047\n",
      "Iteration is: 1600 and loss is: 0.009435437619686127\n",
      "Iteration is: 1601 and loss is: 0.009423380717635155\n",
      "Iteration is: 1602 and loss is: 0.009405355900526047\n",
      "Iteration is: 1603 and loss is: 0.009398454800248146\n",
      "Iteration is: 1604 and loss is: 0.009390275925397873\n",
      "Iteration is: 1605 and loss is: 0.009374142624437809\n",
      "Iteration is: 1606 and loss is: 0.009363526478409767\n",
      "Iteration is: 1607 and loss is: 0.009356723167002201\n",
      "Iteration is: 1608 and loss is: 0.00934365764260292\n",
      "Iteration is: 1609 and loss is: 0.009330794215202332\n",
      "Iteration is: 1610 and loss is: 0.009323183447122574\n",
      "Iteration is: 1611 and loss is: 0.009313142858445644\n",
      "Iteration is: 1612 and loss is: 0.00930003635585308\n",
      "Iteration is: 1613 and loss is: 0.009290404617786407\n",
      "Iteration is: 1614 and loss is: 0.009281930513679981\n",
      "Iteration is: 1615 and loss is: 0.009270312264561653\n",
      "Iteration is: 1616 and loss is: 0.009259135462343693\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 1617 and loss is: 0.009250389412045479\n",
      "Iteration is: 1618 and loss is: 0.00924051832407713\n",
      "Iteration is: 1619 and loss is: 0.00922921672463417\n",
      "Iteration is: 1620 and loss is: 0.009219340980052948\n",
      "Iteration is: 1621 and loss is: 0.009210318326950073\n",
      "Iteration is: 1622 and loss is: 0.009199971333146095\n",
      "Iteration is: 1623 and loss is: 0.009189346805214882\n",
      "Iteration is: 1624 and loss is: 0.009179973974823952\n",
      "Iteration is: 1625 and loss is: 0.00917057879269123\n",
      "Iteration is: 1626 and loss is: 0.009160250425338745\n",
      "Iteration is: 1627 and loss is: 0.009150192141532898\n",
      "Iteration is: 1628 and loss is: 0.009140883572399616\n",
      "Iteration is: 1629 and loss is: 0.009131306782364845\n",
      "Iteration is: 1630 and loss is: 0.009121185168623924\n",
      "Iteration is: 1631 and loss is: 0.009111439809203148\n",
      "Iteration is: 1632 and loss is: 0.00910209584981203\n",
      "Iteration is: 1633 and loss is: 0.009092489257454872\n",
      "Iteration is: 1634 and loss is: 0.009082596749067307\n",
      "Iteration is: 1635 and loss is: 0.009073019027709961\n",
      "Iteration is: 1636 and loss is: 0.009063663892447948\n",
      "Iteration is: 1637 and loss is: 0.009054099209606647\n",
      "Iteration is: 1638 and loss is: 0.009044392965734005\n",
      "Iteration is: 1639 and loss is: 0.009034896269440651\n",
      "Iteration is: 1640 and loss is: 0.009025556966662407\n",
      "Iteration is: 1641 and loss is: 0.009016088210046291\n",
      "Iteration is: 1642 and loss is: 0.00900651328265667\n",
      "Iteration is: 1643 and loss is: 0.008997065015137196\n",
      "Iteration is: 1644 and loss is: 0.008987745270133018\n",
      "Iteration is: 1645 and loss is: 0.008978350088000298\n",
      "Iteration is: 1646 and loss is: 0.008968893438577652\n",
      "Iteration is: 1647 and loss is: 0.00895950011909008\n",
      "Iteration is: 1648 and loss is: 0.008950202725827694\n",
      "Iteration is: 1649 and loss is: 0.008940888568758965\n",
      "Iteration is: 1650 and loss is: 0.008931521326303482\n",
      "Iteration is: 1651 and loss is: 0.008922183886170387\n",
      "Iteration is: 1652 and loss is: 0.008912917226552963\n",
      "Iteration is: 1653 and loss is: 0.00890367291867733\n",
      "Iteration is: 1654 and loss is: 0.008894383907318115\n",
      "Iteration is: 1655 and loss is: 0.008885100483894348\n",
      "Iteration is: 1656 and loss is: 0.008875859901309013\n",
      "Iteration is: 1657 and loss is: 0.008866675198078156\n",
      "Iteration is: 1658 and loss is: 0.008857468143105507\n",
      "Iteration is: 1659 and loss is: 0.008848242461681366\n",
      "Iteration is: 1660 and loss is: 0.008839050307869911\n",
      "Iteration is: 1661 and loss is: 0.008829887956380844\n",
      "Iteration is: 1662 and loss is: 0.00882074423134327\n",
      "Iteration is: 1663 and loss is: 0.008811596781015396\n",
      "Iteration is: 1664 and loss is: 0.008802447468042374\n",
      "Iteration is: 1665 and loss is: 0.00879332423210144\n",
      "Iteration is: 1666 and loss is: 0.008784219622612\n",
      "Iteration is: 1667 and loss is: 0.008775126188993454\n",
      "Iteration is: 1668 and loss is: 0.008766040205955505\n",
      "Iteration is: 1669 and loss is: 0.0087569709867239\n",
      "Iteration is: 1670 and loss is: 0.008747903630137444\n",
      "Iteration is: 1671 and loss is: 0.008738860487937927\n",
      "Iteration is: 1672 and loss is: 0.008729824796319008\n",
      "Iteration is: 1673 and loss is: 0.008720794692635536\n",
      "Iteration is: 1674 and loss is: 0.00871177576482296\n",
      "Iteration is: 1675 and loss is: 0.008702775463461876\n",
      "Iteration is: 1676 and loss is: 0.008693782612681389\n",
      "Iteration is: 1677 and loss is: 0.00868480559438467\n",
      "Iteration is: 1678 and loss is: 0.008675837889313698\n",
      "Iteration is: 1679 and loss is: 0.008666880428791046\n",
      "Iteration is: 1680 and loss is: 0.008657919242978096\n",
      "Iteration is: 1681 and loss is: 0.008648999035358429\n",
      "Iteration is: 1682 and loss is: 0.008640073239803314\n",
      "Iteration is: 1683 and loss is: 0.008631153032183647\n",
      "Iteration is: 1684 and loss is: 0.008622251451015472\n",
      "Iteration is: 1685 and loss is: 0.008613361045718193\n",
      "Iteration is: 1686 and loss is: 0.008604477159678936\n",
      "Iteration is: 1687 and loss is: 0.008595606312155724\n",
      "Iteration is: 1688 and loss is: 0.008586755022406578\n",
      "Iteration is: 1689 and loss is: 0.008577904663980007\n",
      "Iteration is: 1690 and loss is: 0.008569059893488884\n",
      "Iteration is: 1691 and loss is: 0.00856024120002985\n",
      "Iteration is: 1692 and loss is: 0.008551415055990219\n",
      "Iteration is: 1693 and loss is: 0.00854259729385376\n",
      "Iteration is: 1694 and loss is: 0.008533811196684837\n",
      "Iteration is: 1695 and loss is: 0.008525020442903042\n",
      "Iteration is: 1696 and loss is: 0.008516242727637291\n",
      "Iteration is: 1697 and loss is: 0.008507486432790756\n",
      "Iteration is: 1698 and loss is: 0.008498722687363625\n",
      "Iteration is: 1699 and loss is: 0.008489975705742836\n",
      "Iteration is: 1700 and loss is: 0.008481252007186413\n",
      "Iteration is: 1701 and loss is: 0.008472521789371967\n",
      "Iteration is: 1702 and loss is: 0.008463803678750992\n",
      "Iteration is: 1703 and loss is: 0.008455099537968636\n",
      "Iteration is: 1704 and loss is: 0.008446408435702324\n",
      "Iteration is: 1705 and loss is: 0.008437717333436012\n",
      "Iteration is: 1706 and loss is: 0.008429044857621193\n",
      "Iteration is: 1707 and loss is: 0.00842038169503212\n",
      "Iteration is: 1708 and loss is: 0.008411726914346218\n",
      "Iteration is: 1709 and loss is: 0.00840308703482151\n",
      "Iteration is: 1710 and loss is: 0.00839444249868393\n",
      "Iteration is: 1711 and loss is: 0.008385831490159035\n",
      "Iteration is: 1712 and loss is: 0.008377202786505222\n",
      "Iteration is: 1713 and loss is: 0.008368605747818947\n",
      "Iteration is: 1714 and loss is: 0.008360009640455246\n",
      "Iteration is: 1715 and loss is: 0.008351420983672142\n",
      "Iteration is: 1716 and loss is: 0.008342844434082508\n",
      "Iteration is: 1717 and loss is: 0.008334282785654068\n",
      "Iteration is: 1718 and loss is: 0.008325731381773949\n",
      "Iteration is: 1719 and loss is: 0.008317176252603531\n",
      "Iteration is: 1720 and loss is: 0.008308635093271732\n",
      "Iteration is: 1721 and loss is: 0.008300114423036575\n",
      "Iteration is: 1722 and loss is: 0.008291592821478844\n",
      "Iteration is: 1723 and loss is: 0.008283080533146858\n",
      "Iteration is: 1724 and loss is: 0.008274581283330917\n",
      "Iteration is: 1725 and loss is: 0.008266087621450424\n",
      "Iteration is: 1726 and loss is: 0.008257623761892319\n",
      "Iteration is: 1727 and loss is: 0.008249148726463318\n",
      "Iteration is: 1728 and loss is: 0.008240679278969765\n",
      "Iteration is: 1729 and loss is: 0.008232221007347107\n",
      "Iteration is: 1730 and loss is: 0.00822378508746624\n",
      "Iteration is: 1731 and loss is: 0.008215358480811119\n",
      "Iteration is: 1732 and loss is: 0.00820692628622055\n",
      "Iteration is: 1733 and loss is: 0.008198515512049198\n",
      "Iteration is: 1734 and loss is: 0.00819010753184557\n",
      "Iteration is: 1735 and loss is: 0.008181714452803135\n",
      "Iteration is: 1736 and loss is: 0.008173324167728424\n",
      "Iteration is: 1737 and loss is: 0.008164948783814907\n",
      "Iteration is: 1738 and loss is: 0.008156582713127136\n",
      "Iteration is: 1739 and loss is: 0.008148225024342537\n",
      "Iteration is: 1740 and loss is: 0.008139878511428833\n",
      "Iteration is: 1741 and loss is: 0.008131531998515129\n",
      "Iteration is: 1742 and loss is: 0.008123200386762619\n",
      "Iteration is: 1743 and loss is: 0.008114872500300407\n",
      "Iteration is: 1744 and loss is: 0.008106566965579987\n",
      "Iteration is: 1745 and loss is: 0.008098255842924118\n",
      "Iteration is: 1746 and loss is: 0.008089957758784294\n",
      "Iteration is: 1747 and loss is: 0.008081680163741112\n",
      "Iteration is: 1748 and loss is: 0.008073401637375355\n",
      "Iteration is: 1749 and loss is: 0.00806515570729971\n",
      "Iteration is: 1750 and loss is: 0.008056912571191788\n",
      "Iteration is: 1751 and loss is: 0.008048688061535358\n",
      "Iteration is: 1752 and loss is: 0.008040512911975384\n",
      "Iteration is: 1753 and loss is: 0.008032387122511864\n",
      "Iteration is: 1754 and loss is: 0.008024387061595917\n",
      "Iteration is: 1755 and loss is: 0.008016617968678474\n",
      "Iteration is: 1756 and loss is: 0.008009291253983974\n",
      "Iteration is: 1757 and loss is: 0.008002901449799538\n",
      "Iteration is: 1758 and loss is: 0.007998442277312279\n",
      "Iteration is: 1759 and loss is: 0.007998105138540268\n",
      "Iteration is: 1760 and loss is: 0.008006680756807327\n",
      "Iteration is: 1761 and loss is: 0.008034761995077133\n",
      "Iteration is: 1762 and loss is: 0.008106078952550888\n",
      "Iteration is: 1763 and loss is: 0.008274562656879425\n",
      "Iteration is: 1764 and loss is: 0.00866234302520752\n",
      "Iteration is: 1765 and loss is: 0.009549545124173164\n",
      "Iteration is: 1766 and loss is: 0.011549029499292374\n",
      "Iteration is: 1767 and loss is: 0.01598074845969677\n",
      "Iteration is: 1768 and loss is: 0.024933485314249992\n",
      "Iteration is: 1769 and loss is: 0.04128038138151169\n",
      "Iteration is: 1770 and loss is: 0.05781835317611694\n",
      "Iteration is: 1771 and loss is: 0.06483345478773117\n",
      "Iteration is: 1772 and loss is: 0.03626515343785286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 1773 and loss is: 0.009862768463790417\n",
      "Iteration is: 1774 and loss is: 0.021276148036122322\n",
      "Iteration is: 1775 and loss is: 0.02931167557835579\n",
      "Iteration is: 1776 and loss is: 0.012456409633159637\n",
      "Iteration is: 1777 and loss is: 0.015683669596910477\n",
      "Iteration is: 1778 and loss is: 0.023490427061915398\n",
      "Iteration is: 1779 and loss is: 0.01151550654321909\n",
      "Iteration is: 1780 and loss is: 0.013933592475950718\n",
      "Iteration is: 1781 and loss is: 0.01420450396835804\n",
      "Iteration is: 1782 and loss is: 0.010506853461265564\n",
      "Iteration is: 1783 and loss is: 0.014135883189737797\n",
      "Iteration is: 1784 and loss is: 0.009764001704752445\n",
      "Iteration is: 1785 and loss is: 0.01269582286477089\n",
      "Iteration is: 1786 and loss is: 0.010284125804901123\n",
      "Iteration is: 1787 and loss is: 0.010455261915922165\n",
      "Iteration is: 1788 and loss is: 0.01154722087085247\n",
      "Iteration is: 1789 and loss is: 0.009056207723915577\n",
      "Iteration is: 1790 and loss is: 0.01121934037655592\n",
      "Iteration is: 1791 and loss is: 0.009081581607460976\n",
      "Iteration is: 1792 and loss is: 0.010372819378972054\n",
      "Iteration is: 1793 and loss is: 0.009295480325818062\n",
      "Iteration is: 1794 and loss is: 0.009616095572710037\n",
      "Iteration is: 1795 and loss is: 0.009563460946083069\n",
      "Iteration is: 1796 and loss is: 0.00895347073674202\n",
      "Iteration is: 1797 and loss is: 0.00948209036141634\n",
      "Iteration is: 1798 and loss is: 0.008782031945884228\n",
      "Iteration is: 1799 and loss is: 0.009196080267429352\n",
      "Iteration is: 1800 and loss is: 0.008735299110412598\n",
      "Iteration is: 1801 and loss is: 0.009010201320052147\n",
      "Iteration is: 1802 and loss is: 0.008713798597455025\n",
      "Iteration is: 1803 and loss is: 0.008721668273210526\n",
      "Iteration is: 1804 and loss is: 0.008780770003795624\n",
      "Iteration is: 1805 and loss is: 0.00848919153213501\n",
      "Iteration is: 1806 and loss is: 0.008764876052737236\n",
      "Iteration is: 1807 and loss is: 0.008393630385398865\n",
      "Iteration is: 1808 and loss is: 0.008656999096274376\n",
      "Iteration is: 1809 and loss is: 0.008404332213103771\n",
      "Iteration is: 1810 and loss is: 0.008421201258897781\n",
      "Iteration is: 1811 and loss is: 0.008491059765219688\n",
      "Iteration is: 1812 and loss is: 0.008243700489401817\n",
      "Iteration is: 1813 and loss is: 0.008454717695713043\n",
      "Iteration is: 1814 and loss is: 0.008232353255152702\n",
      "Iteration is: 1815 and loss is: 0.008319307118654251\n",
      "Iteration is: 1816 and loss is: 0.008273720741271973\n",
      "Iteration is: 1817 and loss is: 0.008187517523765564\n",
      "Iteration is: 1818 and loss is: 0.008287354372441769\n",
      "Iteration is: 1819 and loss is: 0.008130943402647972\n",
      "Iteration is: 1820 and loss is: 0.008211453445255756\n",
      "Iteration is: 1821 and loss is: 0.008146822452545166\n",
      "Iteration is: 1822 and loss is: 0.008111940696835518\n",
      "Iteration is: 1823 and loss is: 0.008144814521074295\n",
      "Iteration is: 1824 and loss is: 0.008060885593295097\n",
      "Iteration is: 1825 and loss is: 0.008107271045446396\n",
      "Iteration is: 1826 and loss is: 0.008047379553318024\n",
      "Iteration is: 1827 and loss is: 0.00804511085152626\n",
      "Iteration is: 1828 and loss is: 0.008047779090702534\n",
      "Iteration is: 1829 and loss is: 0.007993762381374836\n",
      "Iteration is: 1830 and loss is: 0.008022556081414223\n",
      "Iteration is: 1831 and loss is: 0.007978231646120548\n",
      "Iteration is: 1832 and loss is: 0.007975496351718903\n",
      "Iteration is: 1833 and loss is: 0.007970230653882027\n",
      "Iteration is: 1834 and loss is: 0.007937008515000343\n",
      "Iteration is: 1835 and loss is: 0.00794934667646885\n",
      "Iteration is: 1836 and loss is: 0.007917637005448341\n",
      "Iteration is: 1837 and loss is: 0.007914085872471333\n",
      "Iteration is: 1838 and loss is: 0.007907280698418617\n",
      "Iteration is: 1839 and loss is: 0.007881651632487774\n",
      "Iteration is: 1840 and loss is: 0.007885495200753212\n",
      "Iteration is: 1841 and loss is: 0.007864223793148994\n",
      "Iteration is: 1842 and loss is: 0.007855519652366638\n",
      "Iteration is: 1843 and loss is: 0.007849127054214478\n",
      "Iteration is: 1844 and loss is: 0.007830793038010597\n",
      "Iteration is: 1845 and loss is: 0.007827986031770706\n",
      "Iteration is: 1846 and loss is: 0.00781254842877388\n",
      "Iteration is: 1847 and loss is: 0.007803372107446194\n",
      "Iteration is: 1848 and loss is: 0.007796433288604021\n",
      "Iteration is: 1849 and loss is: 0.007781079970300198\n",
      "Iteration is: 1850 and loss is: 0.007776197977364063\n",
      "Iteration is: 1851 and loss is: 0.0077641811221838\n",
      "Iteration is: 1852 and loss is: 0.007753504440188408\n",
      "Iteration is: 1853 and loss is: 0.007747318129986525\n",
      "Iteration is: 1854 and loss is: 0.0077344076707959175\n",
      "Iteration is: 1855 and loss is: 0.007727125659584999\n",
      "Iteration is: 1856 and loss is: 0.007717892527580261\n",
      "Iteration is: 1857 and loss is: 0.007706974633038044\n",
      "Iteration is: 1858 and loss is: 0.007700107526034117\n",
      "Iteration is: 1859 and loss is: 0.0076893954537808895\n",
      "Iteration is: 1860 and loss is: 0.007680757436901331\n",
      "Iteration is: 1861 and loss is: 0.007672728970646858\n",
      "Iteration is: 1862 and loss is: 0.007662356831133366\n",
      "Iteration is: 1863 and loss is: 0.007654721383005381\n",
      "Iteration is: 1864 and loss is: 0.007645602338016033\n",
      "Iteration is: 1865 and loss is: 0.007636360824108124\n",
      "Iteration is: 1866 and loss is: 0.0076286266557872295\n",
      "Iteration is: 1867 and loss is: 0.007619213778525591\n",
      "Iteration is: 1868 and loss is: 0.007610843516886234\n",
      "Iteration is: 1869 and loss is: 0.0076026227325201035\n",
      "Iteration is: 1870 and loss is: 0.0075935302302241325\n",
      "Iteration is: 1871 and loss is: 0.007585560902953148\n",
      "Iteration is: 1872 and loss is: 0.00757699366658926\n",
      "Iteration is: 1873 and loss is: 0.007568358909338713\n",
      "Iteration is: 1874 and loss is: 0.007560390047729015\n",
      "Iteration is: 1875 and loss is: 0.007551748305559158\n",
      "Iteration is: 1876 and loss is: 0.00754348561167717\n",
      "Iteration is: 1877 and loss is: 0.007535411044955254\n",
      "Iteration is: 1878 and loss is: 0.007526889443397522\n",
      "Iteration is: 1879 and loss is: 0.0075188325718045235\n",
      "Iteration is: 1880 and loss is: 0.007510674186050892\n",
      "Iteration is: 1881 and loss is: 0.00750234117731452\n",
      "Iteration is: 1882 and loss is: 0.007494378834962845\n",
      "Iteration is: 1883 and loss is: 0.007486185058951378\n",
      "Iteration is: 1884 and loss is: 0.007478028070181608\n",
      "Iteration is: 1885 and loss is: 0.007470077369362116\n",
      "Iteration is: 1886 and loss is: 0.007461925502866507\n",
      "Iteration is: 1887 and loss is: 0.0074539026245474815\n",
      "Iteration is: 1888 and loss is: 0.00744595704600215\n",
      "Iteration is: 1889 and loss is: 0.007437882944941521\n",
      "Iteration is: 1890 and loss is: 0.007429958321154118\n",
      "Iteration is: 1891 and loss is: 0.007422023918479681\n",
      "Iteration is: 1892 and loss is: 0.007414024323225021\n",
      "Iteration is: 1893 and loss is: 0.007406169548630714\n",
      "Iteration is: 1894 and loss is: 0.007398255635052919\n",
      "Iteration is: 1895 and loss is: 0.007390343118458986\n",
      "Iteration is: 1896 and loss is: 0.007382513489574194\n",
      "Iteration is: 1897 and loss is: 0.007374649401754141\n",
      "Iteration is: 1898 and loss is: 0.007366791367530823\n",
      "Iteration is: 1899 and loss is: 0.007359004113823175\n",
      "Iteration is: 1900 and loss is: 0.007351178675889969\n",
      "Iteration is: 1901 and loss is: 0.007343388628214598\n",
      "Iteration is: 1902 and loss is: 0.007335633970797062\n",
      "Iteration is: 1903 and loss is: 0.0073278555646538734\n",
      "Iteration is: 1904 and loss is: 0.007320113480091095\n",
      "Iteration is: 1905 and loss is: 0.0073123956099152565\n",
      "Iteration is: 1906 and loss is: 0.007304658181965351\n",
      "Iteration is: 1907 and loss is: 0.007296956144273281\n",
      "Iteration is: 1908 and loss is: 0.007289270404726267\n",
      "Iteration is: 1909 and loss is: 0.007281583733856678\n",
      "Iteration is: 1910 and loss is: 0.007273922208696604\n",
      "Iteration is: 1911 and loss is: 0.0072662727907299995\n",
      "Iteration is: 1912 and loss is: 0.007258624769747257\n",
      "Iteration is: 1913 and loss is: 0.007251002825796604\n",
      "Iteration is: 1914 and loss is: 0.007243389263749123\n",
      "Iteration is: 1915 and loss is: 0.007235768251121044\n",
      "Iteration is: 1916 and loss is: 0.007228182628750801\n",
      "Iteration is: 1917 and loss is: 0.007220601662993431\n",
      "Iteration is: 1918 and loss is: 0.007213026285171509\n",
      "Iteration is: 1919 and loss is: 0.007205469999462366\n",
      "Iteration is: 1920 and loss is: 0.007197926752269268\n",
      "Iteration is: 1921 and loss is: 0.0071903918869793415\n",
      "Iteration is: 1922 and loss is: 0.007182861678302288\n",
      "Iteration is: 1923 and loss is: 0.0071753570809960365\n",
      "Iteration is: 1924 and loss is: 0.00716784643009305\n",
      "Iteration is: 1925 and loss is: 0.007160354405641556\n",
      "Iteration is: 1926 and loss is: 0.007152884267270565\n",
      "Iteration is: 1927 and loss is: 0.00714541133493185\n",
      "Iteration is: 1928 and loss is: 0.00713795330375433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 1929 and loss is: 0.0071305036544799805\n",
      "Iteration is: 1930 and loss is: 0.007123067043721676\n",
      "Iteration is: 1931 and loss is: 0.007115636952221394\n",
      "Iteration is: 1932 and loss is: 0.007108217105269432\n",
      "Iteration is: 1933 and loss is: 0.007100808434188366\n",
      "Iteration is: 1934 and loss is: 0.007093420252203941\n",
      "Iteration is: 1935 and loss is: 0.007086025085300207\n",
      "Iteration is: 1936 and loss is: 0.007078651338815689\n",
      "Iteration is: 1937 and loss is: 0.0070712873712182045\n",
      "Iteration is: 1938 and loss is: 0.007063941098749638\n",
      "Iteration is: 1939 and loss is: 0.007056581787765026\n",
      "Iteration is: 1940 and loss is: 0.007049251347780228\n",
      "Iteration is: 1941 and loss is: 0.007041924633085728\n",
      "Iteration is: 1942 and loss is: 0.007034604903310537\n",
      "Iteration is: 1943 and loss is: 0.007027290295809507\n",
      "Iteration is: 1944 and loss is: 0.007020008750259876\n",
      "Iteration is: 1945 and loss is: 0.007012710906565189\n",
      "Iteration is: 1946 and loss is: 0.007005438208580017\n",
      "Iteration is: 1947 and loss is: 0.006998162716627121\n",
      "Iteration is: 1948 and loss is: 0.006990905851125717\n",
      "Iteration is: 1949 and loss is: 0.0069836582988500595\n",
      "Iteration is: 1950 and loss is: 0.006976414937525988\n",
      "Iteration is: 1951 and loss is: 0.006969194859266281\n",
      "Iteration is: 1952 and loss is: 0.006961965002119541\n",
      "Iteration is: 1953 and loss is: 0.0069547598250210285\n",
      "Iteration is: 1954 and loss is: 0.006947559304535389\n",
      "Iteration is: 1955 and loss is: 0.006940361112356186\n",
      "Iteration is: 1956 and loss is: 0.00693318247795105\n",
      "Iteration is: 1957 and loss is: 0.006926000118255615\n",
      "Iteration is: 1958 and loss is: 0.006918838247656822\n",
      "Iteration is: 1959 and loss is: 0.0069116875529289246\n",
      "Iteration is: 1960 and loss is: 0.00690454849973321\n",
      "Iteration is: 1961 and loss is: 0.006897408980876207\n",
      "Iteration is: 1962 and loss is: 0.006890278775244951\n",
      "Iteration is: 1963 and loss is: 0.006883165333420038\n",
      "Iteration is: 1964 and loss is: 0.0068760523572564125\n",
      "Iteration is: 1965 and loss is: 0.006868948228657246\n",
      "Iteration is: 1966 and loss is: 0.006861863657832146\n",
      "Iteration is: 1967 and loss is: 0.006854788400232792\n",
      "Iteration is: 1968 and loss is: 0.0068477145396173\n",
      "Iteration is: 1969 and loss is: 0.006840649992227554\n",
      "Iteration is: 1970 and loss is: 0.0068335989490151405\n",
      "Iteration is: 1971 and loss is: 0.006826551631093025\n",
      "Iteration is: 1972 and loss is: 0.006819519214332104\n",
      "Iteration is: 1973 and loss is: 0.006812495645135641\n",
      "Iteration is: 1974 and loss is: 0.0068054767325520515\n",
      "Iteration is: 1975 and loss is: 0.006798466667532921\n",
      "Iteration is: 1976 and loss is: 0.0067914691753685474\n",
      "Iteration is: 1977 and loss is: 0.006784485187381506\n",
      "Iteration is: 1978 and loss is: 0.006777504459023476\n",
      "Iteration is: 1979 and loss is: 0.006770530715584755\n",
      "Iteration is: 1980 and loss is: 0.00676356814801693\n",
      "Iteration is: 1981 and loss is: 0.006756610237061977\n",
      "Iteration is: 1982 and loss is: 0.006749675143510103\n",
      "Iteration is: 1983 and loss is: 0.006742740049958229\n",
      "Iteration is: 1984 and loss is: 0.006735808681696653\n",
      "Iteration is: 1985 and loss is: 0.006728890351951122\n",
      "Iteration is: 1986 and loss is: 0.0067219920456409454\n",
      "Iteration is: 1987 and loss is: 0.006715094670653343\n",
      "Iteration is: 1988 and loss is: 0.006708194967359304\n",
      "Iteration is: 1989 and loss is: 0.006701318547129631\n",
      "Iteration is: 1990 and loss is: 0.006694452837109566\n",
      "Iteration is: 1991 and loss is: 0.006687582936137915\n",
      "Iteration is: 1992 and loss is: 0.006680730730295181\n",
      "Iteration is: 1993 and loss is: 0.006673887372016907\n",
      "Iteration is: 1994 and loss is: 0.006667057052254677\n",
      "Iteration is: 1995 and loss is: 0.006660222541540861\n",
      "Iteration is: 1996 and loss is: 0.006653408985584974\n",
      "Iteration is: 1997 and loss is: 0.006646600551903248\n",
      "Iteration is: 1998 and loss is: 0.00663980096578598\n",
      "Iteration is: 1999 and loss is: 0.006633023265749216\n",
      "Iteration is: 2000 and loss is: 0.00662623904645443\n",
      "Iteration is: 2001 and loss is: 0.006619460880756378\n",
      "Iteration is: 2002 and loss is: 0.006612699944525957\n",
      "Iteration is: 2003 and loss is: 0.0066059548407793045\n",
      "Iteration is: 2004 and loss is: 0.006599195301532745\n",
      "Iteration is: 2005 and loss is: 0.006592466961592436\n",
      "Iteration is: 2006 and loss is: 0.006585745606571436\n",
      "Iteration is: 2007 and loss is: 0.006579028442502022\n",
      "Iteration is: 2008 and loss is: 0.006572319660335779\n",
      "Iteration is: 2009 and loss is: 0.00656561553478241\n",
      "Iteration is: 2010 and loss is: 0.0065589239820837975\n",
      "Iteration is: 2011 and loss is: 0.006552249193191528\n",
      "Iteration is: 2012 and loss is: 0.006545572075992823\n",
      "Iteration is: 2013 and loss is: 0.006538907997310162\n",
      "Iteration is: 2014 and loss is: 0.006532258819788694\n",
      "Iteration is: 2015 and loss is: 0.006525609642267227\n",
      "Iteration is: 2016 and loss is: 0.006518982350826263\n",
      "Iteration is: 2017 and loss is: 0.006512351334095001\n",
      "Iteration is: 2018 and loss is: 0.006505722180008888\n",
      "Iteration is: 2019 and loss is: 0.006499122828245163\n",
      "Iteration is: 2020 and loss is: 0.00649252301082015\n",
      "Iteration is: 2021 and loss is: 0.006485932972282171\n",
      "Iteration is: 2022 and loss is: 0.006479352712631226\n",
      "Iteration is: 2023 and loss is: 0.006472783163189888\n",
      "Iteration is: 2024 and loss is: 0.006466221995651722\n",
      "Iteration is: 2025 and loss is: 0.006459666416049004\n",
      "Iteration is: 2026 and loss is: 0.0064531187526881695\n",
      "Iteration is: 2027 and loss is: 0.006446587387472391\n",
      "Iteration is: 2028 and loss is: 0.0064400564879179\n",
      "Iteration is: 2029 and loss is: 0.006433537229895592\n",
      "Iteration is: 2030 and loss is: 0.006427024491131306\n",
      "Iteration is: 2031 and loss is: 0.006420533638447523\n",
      "Iteration is: 2032 and loss is: 0.006414043717086315\n",
      "Iteration is: 2033 and loss is: 0.006407557521015406\n",
      "Iteration is: 2034 and loss is: 0.006401081569492817\n",
      "Iteration is: 2035 and loss is: 0.006394629366695881\n",
      "Iteration is: 2036 and loss is: 0.006388175766915083\n",
      "Iteration is: 2037 and loss is: 0.006381734274327755\n",
      "Iteration is: 2038 and loss is: 0.0063752951100468636\n",
      "Iteration is: 2039 and loss is: 0.006368868984282017\n",
      "Iteration is: 2040 and loss is: 0.006362450309097767\n",
      "Iteration is: 2041 and loss is: 0.00635604839771986\n",
      "Iteration is: 2042 and loss is: 0.006349649280309677\n",
      "Iteration is: 2043 and loss is: 0.006343252956867218\n",
      "Iteration is: 2044 and loss is: 0.006336879450827837\n",
      "Iteration is: 2045 and loss is: 0.00633050873875618\n",
      "Iteration is: 2046 and loss is: 0.006324147339910269\n",
      "Iteration is: 2047 and loss is: 0.006317793391644955\n",
      "Iteration is: 2048 and loss is: 0.006311449687927961\n",
      "Iteration is: 2049 and loss is: 0.006305116228759289\n",
      "Iteration is: 2050 and loss is: 0.006298788823187351\n",
      "Iteration is: 2051 and loss is: 0.006292473524808884\n",
      "Iteration is: 2052 and loss is: 0.006286179181188345\n",
      "Iteration is: 2053 and loss is: 0.006279881112277508\n",
      "Iteration is: 2054 and loss is: 0.006273593753576279\n",
      "Iteration is: 2055 and loss is: 0.006267320364713669\n",
      "Iteration is: 2056 and loss is: 0.006261041387915611\n",
      "Iteration is: 2057 and loss is: 0.006254782900214195\n",
      "Iteration is: 2058 and loss is: 0.006248527206480503\n",
      "Iteration is: 2059 and loss is: 0.0062422966584563255\n",
      "Iteration is: 2060 and loss is: 0.006236061919480562\n",
      "Iteration is: 2061 and loss is: 0.006229836959391832\n",
      "Iteration is: 2062 and loss is: 0.006223623640835285\n",
      "Iteration is: 2063 and loss is: 0.006217421032488346\n",
      "Iteration is: 2064 and loss is: 0.006211225874722004\n",
      "Iteration is: 2065 and loss is: 0.006205041892826557\n",
      "Iteration is: 2066 and loss is: 0.006198862101882696\n",
      "Iteration is: 2067 and loss is: 0.006192698143422604\n",
      "Iteration is: 2068 and loss is: 0.0061865439638495445\n",
      "Iteration is: 2069 and loss is: 0.006180395372211933\n",
      "Iteration is: 2070 and loss is: 0.006174249108880758\n",
      "Iteration is: 2071 and loss is: 0.0061681270599365234\n",
      "Iteration is: 2072 and loss is: 0.006162007339298725\n",
      "Iteration is: 2073 and loss is: 0.006155898794531822\n",
      "Iteration is: 2074 and loss is: 0.006149796303361654\n",
      "Iteration is: 2075 and loss is: 0.006143702194094658\n",
      "Iteration is: 2076 and loss is: 0.006137621123343706\n",
      "Iteration is: 2077 and loss is: 0.006131554022431374\n",
      "Iteration is: 2078 and loss is: 0.006125489249825478\n",
      "Iteration is: 2079 and loss is: 0.006119428668171167\n",
      "Iteration is: 2080 and loss is: 0.0061133848503232\n",
      "Iteration is: 2081 and loss is: 0.00610735360532999\n",
      "Iteration is: 2082 and loss is: 0.0061013298109173775\n",
      "Iteration is: 2083 and loss is: 0.006095312535762787\n",
      "Iteration is: 2084 and loss is: 0.006089300848543644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 2085 and loss is: 0.0060833049938082695\n",
      "Iteration is: 2086 and loss is: 0.006077321711927652\n",
      "Iteration is: 2087 and loss is: 0.006071347743272781\n",
      "Iteration is: 2088 and loss is: 0.006065388210117817\n",
      "Iteration is: 2089 and loss is: 0.006059444509446621\n",
      "Iteration is: 2090 and loss is: 0.006053511518985033\n",
      "Iteration is: 2091 and loss is: 0.006047625560313463\n",
      "Iteration is: 2092 and loss is: 0.0060417791828513145\n",
      "Iteration is: 2093 and loss is: 0.006036014296114445\n",
      "Iteration is: 2094 and loss is: 0.006030404008924961\n",
      "Iteration is: 2095 and loss is: 0.006025085225701332\n",
      "Iteration is: 2096 and loss is: 0.00602031871676445\n",
      "Iteration is: 2097 and loss is: 0.0060166665352880955\n",
      "Iteration is: 2098 and loss is: 0.0060151973739266396\n",
      "Iteration is: 2099 and loss is: 0.006018319632858038\n",
      "Iteration is: 2100 and loss is: 0.006030532997101545\n",
      "Iteration is: 2101 and loss is: 0.006062609143555164\n",
      "Iteration is: 2102 and loss is: 0.006133484188467264\n",
      "Iteration is: 2103 and loss is: 0.006293128244578838\n",
      "Iteration is: 2104 and loss is: 0.0066122799180448055\n",
      "Iteration is: 2105 and loss is: 0.007310015149414539\n",
      "Iteration is: 2106 and loss is: 0.008494718000292778\n",
      "Iteration is: 2107 and loss is: 0.010721464641392231\n",
      "Iteration is: 2108 and loss is: 0.012674560770392418\n",
      "Iteration is: 2109 and loss is: 0.013816902413964272\n",
      "Iteration is: 2110 and loss is: 0.011287553235888481\n",
      "Iteration is: 2111 and loss is: 0.008029932156205177\n",
      "Iteration is: 2112 and loss is: 0.006672337185591459\n",
      "Iteration is: 2113 and loss is: 0.007505498826503754\n",
      "Iteration is: 2114 and loss is: 0.008664351888000965\n",
      "Iteration is: 2115 and loss is: 0.008299579843878746\n",
      "Iteration is: 2116 and loss is: 0.007359322160482407\n",
      "Iteration is: 2117 and loss is: 0.006763899698853493\n",
      "Iteration is: 2118 and loss is: 0.006754505448043346\n",
      "Iteration is: 2119 and loss is: 0.007015869952738285\n",
      "Iteration is: 2120 and loss is: 0.007422031369060278\n",
      "Iteration is: 2121 and loss is: 0.008021736517548561\n",
      "Iteration is: 2122 and loss is: 0.007958078756928444\n",
      "Iteration is: 2123 and loss is: 0.007178996689617634\n",
      "Iteration is: 2124 and loss is: 0.006239472422748804\n",
      "Iteration is: 2125 and loss is: 0.006004275754094124\n",
      "Iteration is: 2126 and loss is: 0.006428825668990612\n",
      "Iteration is: 2127 and loss is: 0.006841311231255531\n",
      "Iteration is: 2128 and loss is: 0.006879637949168682\n",
      "Iteration is: 2129 and loss is: 0.006679548881947994\n",
      "Iteration is: 2130 and loss is: 0.006532493978738785\n",
      "Iteration is: 2131 and loss is: 0.0062805842608213425\n",
      "Iteration is: 2132 and loss is: 0.0059906067326664925\n",
      "Iteration is: 2133 and loss is: 0.005861901678144932\n",
      "Iteration is: 2134 and loss is: 0.005990819539874792\n",
      "Iteration is: 2135 and loss is: 0.006187859922647476\n",
      "Iteration is: 2136 and loss is: 0.006235226988792419\n",
      "Iteration is: 2137 and loss is: 0.006183263845741749\n",
      "Iteration is: 2138 and loss is: 0.006105827167630196\n",
      "Iteration is: 2139 and loss is: 0.006026474758982658\n",
      "Iteration is: 2140 and loss is: 0.005909559316933155\n",
      "Iteration is: 2141 and loss is: 0.005824402906000614\n",
      "Iteration is: 2142 and loss is: 0.005843622609972954\n",
      "Iteration is: 2143 and loss is: 0.005935714580118656\n",
      "Iteration is: 2144 and loss is: 0.0060146162286400795\n",
      "Iteration is: 2145 and loss is: 0.0060448735021054745\n",
      "Iteration is: 2146 and loss is: 0.006075168028473854\n",
      "Iteration is: 2147 and loss is: 0.006093329284340143\n",
      "Iteration is: 2148 and loss is: 0.006082498002797365\n",
      "Iteration is: 2149 and loss is: 0.005999947898089886\n",
      "Iteration is: 2150 and loss is: 0.005899839103221893\n",
      "Iteration is: 2151 and loss is: 0.0058205146342515945\n",
      "Iteration is: 2152 and loss is: 0.0057799480855464935\n",
      "Iteration is: 2153 and loss is: 0.0057650841772556305\n",
      "Iteration is: 2154 and loss is: 0.005772807169705629\n",
      "Iteration is: 2155 and loss is: 0.005809048190712929\n",
      "Iteration is: 2156 and loss is: 0.005866819992661476\n",
      "Iteration is: 2157 and loss is: 0.005935221910476685\n",
      "Iteration is: 2158 and loss is: 0.005997286643832922\n",
      "Iteration is: 2159 and loss is: 0.006069713272154331\n",
      "Iteration is: 2160 and loss is: 0.006119430065155029\n",
      "Iteration is: 2161 and loss is: 0.006144261918962002\n",
      "Iteration is: 2162 and loss is: 0.0060842642560601234\n",
      "Iteration is: 2163 and loss is: 0.005978035274893045\n",
      "Iteration is: 2164 and loss is: 0.005846354179084301\n",
      "Iteration is: 2165 and loss is: 0.005746102891862392\n",
      "Iteration is: 2166 and loss is: 0.00569300539791584\n",
      "Iteration is: 2167 and loss is: 0.005685054697096348\n",
      "Iteration is: 2168 and loss is: 0.0057134609669446945\n",
      "Iteration is: 2169 and loss is: 0.005773525685071945\n",
      "Iteration is: 2170 and loss is: 0.005870945751667023\n",
      "Iteration is: 2171 and loss is: 0.006005464121699333\n",
      "Iteration is: 2172 and loss is: 0.006207244470715523\n",
      "Iteration is: 2173 and loss is: 0.006427805870771408\n",
      "Iteration is: 2174 and loss is: 0.006674186326563358\n",
      "Iteration is: 2175 and loss is: 0.0067514944821596146\n",
      "Iteration is: 2176 and loss is: 0.006671556271612644\n",
      "Iteration is: 2177 and loss is: 0.006348846945911646\n",
      "Iteration is: 2178 and loss is: 0.005988326855003834\n",
      "Iteration is: 2179 and loss is: 0.005716837011277676\n",
      "Iteration is: 2180 and loss is: 0.005624036304652691\n",
      "Iteration is: 2181 and loss is: 0.005693531595170498\n",
      "Iteration is: 2182 and loss is: 0.00589344184845686\n",
      "Iteration is: 2183 and loss is: 0.00625031441450119\n",
      "Iteration is: 2184 and loss is: 0.006799590773880482\n",
      "Iteration is: 2185 and loss is: 0.007780128158628941\n",
      "Iteration is: 2186 and loss is: 0.008777103386819363\n",
      "Iteration is: 2187 and loss is: 0.009519930928945541\n",
      "Iteration is: 2188 and loss is: 0.00869158748537302\n",
      "Iteration is: 2189 and loss is: 0.007050642278045416\n",
      "Iteration is: 2190 and loss is: 0.005937986541539431\n",
      "Iteration is: 2191 and loss is: 0.006075583398342133\n",
      "Iteration is: 2192 and loss is: 0.00682434719055891\n",
      "Iteration is: 2193 and loss is: 0.007154685445129871\n",
      "Iteration is: 2194 and loss is: 0.0069243209436535835\n",
      "Iteration is: 2195 and loss is: 0.0063485270366072655\n",
      "Iteration is: 2196 and loss is: 0.00584740936756134\n",
      "Iteration is: 2197 and loss is: 0.005674602463841438\n",
      "Iteration is: 2198 and loss is: 0.005924949422478676\n",
      "Iteration is: 2199 and loss is: 0.006491190753877163\n",
      "Iteration is: 2200 and loss is: 0.007047577761113644\n",
      "Iteration is: 2201 and loss is: 0.007491110824048519\n",
      "Iteration is: 2202 and loss is: 0.007384633645415306\n",
      "Iteration is: 2203 and loss is: 0.006831764243543148\n",
      "Iteration is: 2204 and loss is: 0.0060230884701013565\n",
      "Iteration is: 2205 and loss is: 0.005646726582199335\n",
      "Iteration is: 2206 and loss is: 0.005907818675041199\n",
      "Iteration is: 2207 and loss is: 0.0064086271449923515\n",
      "Iteration is: 2208 and loss is: 0.006728813052177429\n",
      "Iteration is: 2209 and loss is: 0.006666935048997402\n",
      "Iteration is: 2210 and loss is: 0.00646666344255209\n",
      "Iteration is: 2211 and loss is: 0.006098488345742226\n",
      "Iteration is: 2212 and loss is: 0.005774290766566992\n",
      "Iteration is: 2213 and loss is: 0.005554880481213331\n",
      "Iteration is: 2214 and loss is: 0.005507592111825943\n",
      "Iteration is: 2215 and loss is: 0.005600005388259888\n",
      "Iteration is: 2216 and loss is: 0.005725676193833351\n",
      "Iteration is: 2217 and loss is: 0.005795882549136877\n",
      "Iteration is: 2218 and loss is: 0.005763988941907883\n",
      "Iteration is: 2219 and loss is: 0.005673952400684357\n",
      "Iteration is: 2220 and loss is: 0.005565090104937553\n",
      "Iteration is: 2221 and loss is: 0.0054862285032868385\n",
      "Iteration is: 2222 and loss is: 0.005449219606816769\n",
      "Iteration is: 2223 and loss is: 0.005448999814689159\n",
      "Iteration is: 2224 and loss is: 0.005472315009683371\n",
      "Iteration is: 2225 and loss is: 0.005507968831807375\n",
      "Iteration is: 2226 and loss is: 0.0055483607575297356\n",
      "Iteration is: 2227 and loss is: 0.005579335615038872\n",
      "Iteration is: 2228 and loss is: 0.00559635367244482\n",
      "Iteration is: 2229 and loss is: 0.00558495381847024\n",
      "Iteration is: 2230 and loss is: 0.005557475611567497\n",
      "Iteration is: 2231 and loss is: 0.005515444092452526\n",
      "Iteration is: 2232 and loss is: 0.00547514483332634\n",
      "Iteration is: 2233 and loss is: 0.005439505912363529\n",
      "Iteration is: 2234 and loss is: 0.005412447266280651\n",
      "Iteration is: 2235 and loss is: 0.0053932080045342445\n",
      "Iteration is: 2236 and loss is: 0.00538110826164484\n",
      "Iteration is: 2237 and loss is: 0.005374342203140259\n",
      "Iteration is: 2238 and loss is: 0.0053709219209849834\n",
      "Iteration is: 2239 and loss is: 0.005369380582123995\n",
      "Iteration is: 2240 and loss is: 0.005369381047785282\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 2241 and loss is: 0.0053711943328380585\n",
      "Iteration is: 2242 and loss is: 0.005375044420361519\n",
      "Iteration is: 2243 and loss is: 0.005381456576287746\n",
      "Iteration is: 2244 and loss is: 0.005390167236328125\n",
      "Iteration is: 2245 and loss is: 0.005403799470514059\n",
      "Iteration is: 2246 and loss is: 0.005423687398433685\n",
      "Iteration is: 2247 and loss is: 0.005458025727421045\n",
      "Iteration is: 2248 and loss is: 0.005509282927960157\n",
      "Iteration is: 2249 and loss is: 0.005596125032752752\n",
      "Iteration is: 2250 and loss is: 0.005717472638934851\n",
      "Iteration is: 2251 and loss is: 0.005913028959184885\n",
      "Iteration is: 2252 and loss is: 0.006147860549390316\n",
      "Iteration is: 2253 and loss is: 0.0064810034818947315\n",
      "Iteration is: 2254 and loss is: 0.006752222776412964\n",
      "Iteration is: 2255 and loss is: 0.0070051997900009155\n",
      "Iteration is: 2256 and loss is: 0.006948388181626797\n",
      "Iteration is: 2257 and loss is: 0.00668759411200881\n",
      "Iteration is: 2258 and loss is: 0.006181162316352129\n",
      "Iteration is: 2259 and loss is: 0.005698869004845619\n",
      "Iteration is: 2260 and loss is: 0.005384428426623344\n",
      "Iteration is: 2261 and loss is: 0.005307120271027088\n",
      "Iteration is: 2262 and loss is: 0.005433918908238411\n",
      "Iteration is: 2263 and loss is: 0.005728473886847496\n",
      "Iteration is: 2264 and loss is: 0.0062344688922166824\n",
      "Iteration is: 2265 and loss is: 0.006942946929484606\n",
      "Iteration is: 2266 and loss is: 0.008057779632508755\n",
      "Iteration is: 2267 and loss is: 0.008865591138601303\n",
      "Iteration is: 2268 and loss is: 0.00896401610225439\n",
      "Iteration is: 2269 and loss is: 0.007698850706219673\n",
      "Iteration is: 2270 and loss is: 0.006197269074618816\n",
      "Iteration is: 2271 and loss is: 0.005570773035287857\n",
      "Iteration is: 2272 and loss is: 0.005905600264668465\n",
      "Iteration is: 2273 and loss is: 0.006530382204800844\n",
      "Iteration is: 2274 and loss is: 0.006799930706620216\n",
      "Iteration is: 2275 and loss is: 0.00675338227301836\n",
      "Iteration is: 2276 and loss is: 0.006373696029186249\n",
      "Iteration is: 2277 and loss is: 0.0058808159083127975\n",
      "Iteration is: 2278 and loss is: 0.005424940958619118\n",
      "Iteration is: 2279 and loss is: 0.005244915373623371\n",
      "Iteration is: 2280 and loss is: 0.005396603606641293\n",
      "Iteration is: 2281 and loss is: 0.005703398026525974\n",
      "Iteration is: 2282 and loss is: 0.005935912020504475\n",
      "Iteration is: 2283 and loss is: 0.0059433141723275185\n",
      "Iteration is: 2284 and loss is: 0.005807745270431042\n",
      "Iteration is: 2285 and loss is: 0.005598207004368305\n",
      "Iteration is: 2286 and loss is: 0.005430031567811966\n",
      "Iteration is: 2287 and loss is: 0.005305563565343618\n",
      "Iteration is: 2288 and loss is: 0.0052392808720469475\n",
      "Iteration is: 2289 and loss is: 0.005240717902779579\n",
      "Iteration is: 2290 and loss is: 0.005320606287568808\n",
      "Iteration is: 2291 and loss is: 0.005471404641866684\n",
      "Iteration is: 2292 and loss is: 0.005636243149638176\n",
      "Iteration is: 2293 and loss is: 0.0058041904121637344\n",
      "Iteration is: 2294 and loss is: 0.00587501609697938\n",
      "Iteration is: 2295 and loss is: 0.005873345769941807\n",
      "Iteration is: 2296 and loss is: 0.005717082880437374\n",
      "Iteration is: 2297 and loss is: 0.005496636964380741\n",
      "Iteration is: 2298 and loss is: 0.005286457482725382\n",
      "Iteration is: 2299 and loss is: 0.00517676118761301\n",
      "Iteration is: 2300 and loss is: 0.005187144037336111\n",
      "Iteration is: 2301 and loss is: 0.005299626849591732\n",
      "Iteration is: 2302 and loss is: 0.005494860000908375\n",
      "Iteration is: 2303 and loss is: 0.005747182294726372\n",
      "Iteration is: 2304 and loss is: 0.006103087682276964\n",
      "Iteration is: 2305 and loss is: 0.00642898865044117\n",
      "Iteration is: 2306 and loss is: 0.006700438912957907\n",
      "Iteration is: 2307 and loss is: 0.006545152515172958\n",
      "Iteration is: 2308 and loss is: 0.006043912842869759\n",
      "Iteration is: 2309 and loss is: 0.00543205114081502\n",
      "Iteration is: 2310 and loss is: 0.005149648524820805\n",
      "Iteration is: 2311 and loss is: 0.0052918968722224236\n",
      "Iteration is: 2312 and loss is: 0.005665854550898075\n",
      "Iteration is: 2313 and loss is: 0.006080045364797115\n",
      "Iteration is: 2314 and loss is: 0.006388559937477112\n",
      "Iteration is: 2315 and loss is: 0.006706790067255497\n",
      "Iteration is: 2316 and loss is: 0.006716136354953051\n",
      "Iteration is: 2317 and loss is: 0.006475052796304226\n",
      "Iteration is: 2318 and loss is: 0.005872633773833513\n",
      "Iteration is: 2319 and loss is: 0.005316190421581268\n",
      "Iteration is: 2320 and loss is: 0.005147742107510567\n",
      "Iteration is: 2321 and loss is: 0.005367681849747896\n",
      "Iteration is: 2322 and loss is: 0.005698655731976032\n",
      "Iteration is: 2323 and loss is: 0.0058879731222987175\n",
      "Iteration is: 2324 and loss is: 0.005924823693931103\n",
      "Iteration is: 2325 and loss is: 0.0057797497138381\n",
      "Iteration is: 2326 and loss is: 0.00557544082403183\n",
      "Iteration is: 2327 and loss is: 0.005329233128577471\n",
      "Iteration is: 2328 and loss is: 0.00513949291780591\n",
      "Iteration is: 2329 and loss is: 0.005052325315773487\n",
      "Iteration is: 2330 and loss is: 0.0050709443166852\n",
      "Iteration is: 2331 and loss is: 0.005150143057107925\n",
      "Iteration is: 2332 and loss is: 0.005235404707491398\n",
      "Iteration is: 2333 and loss is: 0.005302367731928825\n",
      "Iteration is: 2334 and loss is: 0.005336260423064232\n",
      "Iteration is: 2335 and loss is: 0.005357788875699043\n",
      "Iteration is: 2336 and loss is: 0.005343245342373848\n",
      "Iteration is: 2337 and loss is: 0.005317830480635166\n",
      "Iteration is: 2338 and loss is: 0.005261548329144716\n",
      "Iteration is: 2339 and loss is: 0.005201744846999645\n",
      "Iteration is: 2340 and loss is: 0.005133673548698425\n",
      "Iteration is: 2341 and loss is: 0.005074383690953255\n",
      "Iteration is: 2342 and loss is: 0.005027206614613533\n",
      "Iteration is: 2343 and loss is: 0.004997532814741135\n",
      "Iteration is: 2344 and loss is: 0.00498419813811779\n",
      "Iteration is: 2345 and loss is: 0.004983725491911173\n",
      "Iteration is: 2346 and loss is: 0.004993150942027569\n",
      "Iteration is: 2347 and loss is: 0.005012114532291889\n",
      "Iteration is: 2348 and loss is: 0.005043666809797287\n",
      "Iteration is: 2349 and loss is: 0.0050889877602458\n",
      "Iteration is: 2350 and loss is: 0.005154536571353674\n",
      "Iteration is: 2351 and loss is: 0.005233420990407467\n",
      "Iteration is: 2352 and loss is: 0.005335401743650436\n",
      "Iteration is: 2353 and loss is: 0.005431413650512695\n",
      "Iteration is: 2354 and loss is: 0.005541926249861717\n",
      "Iteration is: 2355 and loss is: 0.005614158697426319\n",
      "Iteration is: 2356 and loss is: 0.005695284344255924\n",
      "Iteration is: 2357 and loss is: 0.005717022344470024\n",
      "Iteration is: 2358 and loss is: 0.005743382498621941\n",
      "Iteration is: 2359 and loss is: 0.005696824751794338\n",
      "Iteration is: 2360 and loss is: 0.005638204514980316\n",
      "Iteration is: 2361 and loss is: 0.005513171665370464\n",
      "Iteration is: 2362 and loss is: 0.0053799692541360855\n",
      "Iteration is: 2363 and loss is: 0.00522985402494669\n",
      "Iteration is: 2364 and loss is: 0.00510437274351716\n",
      "Iteration is: 2365 and loss is: 0.005007236730307341\n",
      "Iteration is: 2366 and loss is: 0.0049457848072052\n",
      "Iteration is: 2367 and loss is: 0.004913119599223137\n",
      "Iteration is: 2368 and loss is: 0.004900812637060881\n",
      "Iteration is: 2369 and loss is: 0.004901489242911339\n",
      "Iteration is: 2370 and loss is: 0.004911464639008045\n",
      "Iteration is: 2371 and loss is: 0.004931069910526276\n",
      "Iteration is: 2372 and loss is: 0.004962765611708164\n",
      "Iteration is: 2373 and loss is: 0.005012777633965015\n",
      "Iteration is: 2374 and loss is: 0.005083704367280006\n",
      "Iteration is: 2375 and loss is: 0.005193263292312622\n",
      "Iteration is: 2376 and loss is: 0.00534116942435503\n",
      "Iteration is: 2377 and loss is: 0.005580521654337645\n",
      "Iteration is: 2378 and loss is: 0.0058865053579211235\n",
      "Iteration is: 2379 and loss is: 0.006368180736899376\n",
      "Iteration is: 2380 and loss is: 0.0068415189161896706\n",
      "Iteration is: 2381 and loss is: 0.007366270292550325\n",
      "Iteration is: 2382 and loss is: 0.007449259515851736\n",
      "Iteration is: 2383 and loss is: 0.007145262323319912\n",
      "Iteration is: 2384 and loss is: 0.006413409486413002\n",
      "Iteration is: 2385 and loss is: 0.00570047739893198\n",
      "Iteration is: 2386 and loss is: 0.005279515869915485\n",
      "Iteration is: 2387 and loss is: 0.005200642626732588\n",
      "Iteration is: 2388 and loss is: 0.005366669036448002\n",
      "Iteration is: 2389 and loss is: 0.005727333482354879\n",
      "Iteration is: 2390 and loss is: 0.006408680230379105\n",
      "Iteration is: 2391 and loss is: 0.007211803924292326\n",
      "Iteration is: 2392 and loss is: 0.00813480094075203\n",
      "Iteration is: 2393 and loss is: 0.008204616606235504\n",
      "Iteration is: 2394 and loss is: 0.00733236875385046\n",
      "Iteration is: 2395 and loss is: 0.006057002115994692\n",
      "Iteration is: 2396 and loss is: 0.005266966298222542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 2397 and loss is: 0.005270342342555523\n",
      "Iteration is: 2398 and loss is: 0.005717664025723934\n",
      "Iteration is: 2399 and loss is: 0.006236501038074493\n",
      "Iteration is: 2400 and loss is: 0.006489149294793606\n",
      "Iteration is: 2401 and loss is: 0.006477175280451775\n",
      "Iteration is: 2402 and loss is: 0.005984190851449966\n",
      "Iteration is: 2403 and loss is: 0.005337595008313656\n",
      "Iteration is: 2404 and loss is: 0.0048843733966350555\n",
      "Iteration is: 2405 and loss is: 0.004878103733062744\n",
      "Iteration is: 2406 and loss is: 0.005192268639802933\n",
      "Iteration is: 2407 and loss is: 0.005478973500430584\n",
      "Iteration is: 2408 and loss is: 0.005514242686331272\n",
      "Iteration is: 2409 and loss is: 0.005296303424984217\n",
      "Iteration is: 2410 and loss is: 0.005032727960497141\n",
      "Iteration is: 2411 and loss is: 0.004862140864133835\n",
      "Iteration is: 2412 and loss is: 0.004832080565392971\n",
      "Iteration is: 2413 and loss is: 0.00488888518884778\n",
      "Iteration is: 2414 and loss is: 0.004968285094946623\n",
      "Iteration is: 2415 and loss is: 0.005044534802436829\n",
      "Iteration is: 2416 and loss is: 0.005096681881695986\n",
      "Iteration is: 2417 and loss is: 0.005126409698277712\n",
      "Iteration is: 2418 and loss is: 0.005087161436676979\n",
      "Iteration is: 2419 and loss is: 0.004997984040528536\n",
      "Iteration is: 2420 and loss is: 0.004872402176260948\n",
      "Iteration is: 2421 and loss is: 0.004776962101459503\n",
      "Iteration is: 2422 and loss is: 0.004748640116304159\n",
      "Iteration is: 2423 and loss is: 0.004789126571267843\n",
      "Iteration is: 2424 and loss is: 0.004870055243372917\n",
      "Iteration is: 2425 and loss is: 0.004957218188792467\n",
      "Iteration is: 2426 and loss is: 0.00504944147542119\n",
      "Iteration is: 2427 and loss is: 0.005122798960655928\n",
      "Iteration is: 2428 and loss is: 0.005191558972001076\n",
      "Iteration is: 2429 and loss is: 0.005189117975533009\n",
      "Iteration is: 2430 and loss is: 0.005121483933180571\n",
      "Iteration is: 2431 and loss is: 0.004980818368494511\n",
      "Iteration is: 2432 and loss is: 0.0048381732776761055\n",
      "Iteration is: 2433 and loss is: 0.004740553442388773\n",
      "Iteration is: 2434 and loss is: 0.004710547626018524\n",
      "Iteration is: 2435 and loss is: 0.004740553442388773\n",
      "Iteration is: 2436 and loss is: 0.00481572886928916\n",
      "Iteration is: 2437 and loss is: 0.00493133720010519\n",
      "Iteration is: 2438 and loss is: 0.005076441913843155\n",
      "Iteration is: 2439 and loss is: 0.0052650910802185535\n",
      "Iteration is: 2440 and loss is: 0.005421965382993221\n",
      "Iteration is: 2441 and loss is: 0.005524429026991129\n",
      "Iteration is: 2442 and loss is: 0.005440307781100273\n",
      "Iteration is: 2443 and loss is: 0.005233214236795902\n",
      "Iteration is: 2444 and loss is: 0.004955190699547529\n",
      "Iteration is: 2445 and loss is: 0.004750211723148823\n",
      "Iteration is: 2446 and loss is: 0.004674212075769901\n",
      "Iteration is: 2447 and loss is: 0.004721860401332378\n",
      "Iteration is: 2448 and loss is: 0.0048602428287267685\n",
      "Iteration is: 2449 and loss is: 0.00507004652172327\n",
      "Iteration is: 2450 and loss is: 0.005394275300204754\n",
      "Iteration is: 2451 and loss is: 0.005778355523943901\n",
      "Iteration is: 2452 and loss is: 0.006243828684091568\n",
      "Iteration is: 2453 and loss is: 0.006399017293006182\n",
      "Iteration is: 2454 and loss is: 0.006166451144963503\n",
      "Iteration is: 2455 and loss is: 0.005531502887606621\n",
      "Iteration is: 2456 and loss is: 0.004966976121068001\n",
      "Iteration is: 2457 and loss is: 0.004776977933943272\n",
      "Iteration is: 2458 and loss is: 0.004951551556587219\n",
      "Iteration is: 2459 and loss is: 0.005303598940372467\n",
      "Iteration is: 2460 and loss is: 0.0056907907128334045\n",
      "Iteration is: 2461 and loss is: 0.006203924305737019\n",
      "Iteration is: 2462 and loss is: 0.006593450903892517\n",
      "Iteration is: 2463 and loss is: 0.0068811578676104546\n",
      "Iteration is: 2464 and loss is: 0.006525850854814053\n",
      "Iteration is: 2465 and loss is: 0.0057463208213448524\n",
      "Iteration is: 2466 and loss is: 0.005060582421720028\n",
      "Iteration is: 2467 and loss is: 0.004889717325568199\n",
      "Iteration is: 2468 and loss is: 0.005123867653310299\n",
      "Iteration is: 2469 and loss is: 0.005391959100961685\n",
      "Iteration is: 2470 and loss is: 0.005540642887353897\n",
      "Iteration is: 2471 and loss is: 0.005503159947693348\n",
      "Iteration is: 2472 and loss is: 0.005376073997467756\n",
      "Iteration is: 2473 and loss is: 0.0051262350752949715\n",
      "Iteration is: 2474 and loss is: 0.0048555852845311165\n",
      "Iteration is: 2475 and loss is: 0.00466274656355381\n",
      "Iteration is: 2476 and loss is: 0.004624227061867714\n",
      "Iteration is: 2477 and loss is: 0.0047195665538311005\n",
      "Iteration is: 2478 and loss is: 0.004857748746871948\n",
      "Iteration is: 2479 and loss is: 0.004952998831868172\n",
      "Iteration is: 2480 and loss is: 0.004954140167683363\n",
      "Iteration is: 2481 and loss is: 0.00488461647182703\n",
      "Iteration is: 2482 and loss is: 0.004778234288096428\n",
      "Iteration is: 2483 and loss is: 0.004692922346293926\n",
      "Iteration is: 2484 and loss is: 0.0046447329223155975\n",
      "Iteration is: 2485 and loss is: 0.004629164002835751\n",
      "Iteration is: 2486 and loss is: 0.004620841238647699\n",
      "Iteration is: 2487 and loss is: 0.004605882801115513\n",
      "Iteration is: 2488 and loss is: 0.004587233066558838\n",
      "Iteration is: 2489 and loss is: 0.004578618332743645\n",
      "Iteration is: 2490 and loss is: 0.004588315263390541\n",
      "Iteration is: 2491 and loss is: 0.004611587151885033\n",
      "Iteration is: 2492 and loss is: 0.004638819023966789\n",
      "Iteration is: 2493 and loss is: 0.004658647812902927\n",
      "Iteration is: 2494 and loss is: 0.004675442352890968\n",
      "Iteration is: 2495 and loss is: 0.00468890555202961\n",
      "Iteration is: 2496 and loss is: 0.004714827053248882\n",
      "Iteration is: 2497 and loss is: 0.004748327191919088\n",
      "Iteration is: 2498 and loss is: 0.004799254238605499\n",
      "Iteration is: 2499 and loss is: 0.004846541676670313\n",
      "Iteration is: 2500 and loss is: 0.004898861050605774\n",
      "Iteration is: 2501 and loss is: 0.00493073184043169\n",
      "Iteration is: 2502 and loss is: 0.004967167973518372\n",
      "Iteration is: 2503 and loss is: 0.004983210936188698\n",
      "Iteration is: 2504 and loss is: 0.005014271475374699\n",
      "Iteration is: 2505 and loss is: 0.005021767690777779\n",
      "Iteration is: 2506 and loss is: 0.0050397226586937904\n",
      "Iteration is: 2507 and loss is: 0.005016650538891554\n",
      "Iteration is: 2508 and loss is: 0.004987274296581745\n",
      "Iteration is: 2509 and loss is: 0.004913176875561476\n",
      "Iteration is: 2510 and loss is: 0.004835658706724644\n",
      "Iteration is: 2511 and loss is: 0.004744860343635082\n",
      "Iteration is: 2512 and loss is: 0.004670364316552877\n",
      "Iteration is: 2513 and loss is: 0.004608850926160812\n",
      "Iteration is: 2514 and loss is: 0.00456579215824604\n",
      "Iteration is: 2515 and loss is: 0.004534803330898285\n",
      "Iteration is: 2516 and loss is: 0.004513263236731291\n",
      "Iteration is: 2517 and loss is: 0.00449768640100956\n",
      "Iteration is: 2518 and loss is: 0.004486856982111931\n",
      "Iteration is: 2519 and loss is: 0.004479650408029556\n",
      "Iteration is: 2520 and loss is: 0.004475271329283714\n",
      "Iteration is: 2521 and loss is: 0.004472511820495129\n",
      "Iteration is: 2522 and loss is: 0.00447038421407342\n",
      "Iteration is: 2523 and loss is: 0.004468186292797327\n",
      "Iteration is: 2524 and loss is: 0.00446614995598793\n",
      "Iteration is: 2525 and loss is: 0.004464970901608467\n",
      "Iteration is: 2526 and loss is: 0.004466060549020767\n",
      "Iteration is: 2527 and loss is: 0.004471223801374435\n",
      "Iteration is: 2528 and loss is: 0.004484306089580059\n",
      "Iteration is: 2529 and loss is: 0.004510887432843447\n",
      "Iteration is: 2530 and loss is: 0.004564187489449978\n",
      "Iteration is: 2531 and loss is: 0.004660791251808405\n",
      "Iteration is: 2532 and loss is: 0.004846365191042423\n",
      "Iteration is: 2533 and loss is: 0.005157641135156155\n",
      "Iteration is: 2534 and loss is: 0.0057337176986038685\n",
      "Iteration is: 2535 and loss is: 0.006540242116898298\n",
      "Iteration is: 2536 and loss is: 0.007769860327243805\n",
      "Iteration is: 2537 and loss is: 0.008716607466340065\n",
      "Iteration is: 2538 and loss is: 0.009198164567351341\n",
      "Iteration is: 2539 and loss is: 0.008564085699617863\n",
      "Iteration is: 2540 and loss is: 0.007438330445438623\n",
      "Iteration is: 2541 and loss is: 0.0065508754923939705\n",
      "Iteration is: 2542 and loss is: 0.0061627281829714775\n",
      "Iteration is: 2543 and loss is: 0.00597341125831008\n",
      "Iteration is: 2544 and loss is: 0.005947135388851166\n",
      "Iteration is: 2545 and loss is: 0.006736636161804199\n",
      "Iteration is: 2546 and loss is: 0.008064662106335163\n",
      "Iteration is: 2547 and loss is: 0.009432755410671234\n",
      "Iteration is: 2548 and loss is: 0.008781850337982178\n",
      "Iteration is: 2549 and loss is: 0.0067220027558505535\n",
      "Iteration is: 2550 and loss is: 0.005118055269122124\n",
      "Iteration is: 2551 and loss is: 0.0048581864684820175\n",
      "Iteration is: 2552 and loss is: 0.005607446655631065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 2553 and loss is: 0.006265588104724884\n",
      "Iteration is: 2554 and loss is: 0.006114101968705654\n",
      "Iteration is: 2555 and loss is: 0.005219817161560059\n",
      "Iteration is: 2556 and loss is: 0.004522986710071564\n",
      "Iteration is: 2557 and loss is: 0.004627893678843975\n",
      "Iteration is: 2558 and loss is: 0.005202042870223522\n",
      "Iteration is: 2559 and loss is: 0.00551803782582283\n",
      "Iteration is: 2560 and loss is: 0.005250077694654465\n",
      "Iteration is: 2561 and loss is: 0.004795405548065901\n",
      "Iteration is: 2562 and loss is: 0.00459181796759367\n",
      "Iteration is: 2563 and loss is: 0.0047141630202531815\n",
      "Iteration is: 2564 and loss is: 0.0048747314140200615\n",
      "Iteration is: 2565 and loss is: 0.004869965836405754\n",
      "Iteration is: 2566 and loss is: 0.004784977063536644\n",
      "Iteration is: 2567 and loss is: 0.0047078561037778854\n",
      "Iteration is: 2568 and loss is: 0.004624510183930397\n",
      "Iteration is: 2569 and loss is: 0.004484315402805805\n",
      "Iteration is: 2570 and loss is: 0.004413506481796503\n",
      "Iteration is: 2571 and loss is: 0.004496416542679071\n",
      "Iteration is: 2572 and loss is: 0.0046258652582764626\n",
      "Iteration is: 2573 and loss is: 0.004648892674595118\n",
      "Iteration is: 2574 and loss is: 0.004528289195150137\n",
      "Iteration is: 2575 and loss is: 0.004409184679389\n",
      "Iteration is: 2576 and loss is: 0.004388806410133839\n",
      "Iteration is: 2577 and loss is: 0.004434425383806229\n",
      "Iteration is: 2578 and loss is: 0.0044710091315209866\n",
      "Iteration is: 2579 and loss is: 0.004482032265514135\n",
      "Iteration is: 2580 and loss is: 0.004504573531448841\n",
      "Iteration is: 2581 and loss is: 0.004529835656285286\n",
      "Iteration is: 2582 and loss is: 0.0045225778594613075\n",
      "Iteration is: 2583 and loss is: 0.00445942860096693\n",
      "Iteration is: 2584 and loss is: 0.004385076928883791\n",
      "Iteration is: 2585 and loss is: 0.004346235189586878\n",
      "Iteration is: 2586 and loss is: 0.004351942799985409\n",
      "Iteration is: 2587 and loss is: 0.004373025149106979\n",
      "Iteration is: 2588 and loss is: 0.004387080669403076\n",
      "Iteration is: 2589 and loss is: 0.004398040007799864\n",
      "Iteration is: 2590 and loss is: 0.00441052857786417\n",
      "Iteration is: 2591 and loss is: 0.004420023411512375\n",
      "Iteration is: 2592 and loss is: 0.004407909698784351\n",
      "Iteration is: 2593 and loss is: 0.004378944635391235\n",
      "Iteration is: 2594 and loss is: 0.004346069879829884\n",
      "Iteration is: 2595 and loss is: 0.0043244268745183945\n",
      "Iteration is: 2596 and loss is: 0.004314258228987455\n",
      "Iteration is: 2597 and loss is: 0.004309413023293018\n",
      "Iteration is: 2598 and loss is: 0.004307042807340622\n",
      "Iteration is: 2599 and loss is: 0.004310406744480133\n",
      "Iteration is: 2600 and loss is: 0.00432098563760519\n",
      "Iteration is: 2601 and loss is: 0.004333708435297012\n",
      "Iteration is: 2602 and loss is: 0.004342969041317701\n",
      "Iteration is: 2603 and loss is: 0.004345136694610119\n",
      "Iteration is: 2604 and loss is: 0.0043446677736938\n",
      "Iteration is: 2605 and loss is: 0.004342140629887581\n",
      "Iteration is: 2606 and loss is: 0.004338481463491917\n",
      "Iteration is: 2607 and loss is: 0.004329150542616844\n",
      "Iteration is: 2608 and loss is: 0.004316244274377823\n",
      "Iteration is: 2609 and loss is: 0.004301606677472591\n",
      "Iteration is: 2610 and loss is: 0.004290059208869934\n",
      "Iteration is: 2611 and loss is: 0.00428190128877759\n",
      "Iteration is: 2612 and loss is: 0.004276076331734657\n",
      "Iteration is: 2613 and loss is: 0.004270522389560938\n",
      "Iteration is: 2614 and loss is: 0.004265193827450275\n",
      "Iteration is: 2615 and loss is: 0.004260829649865627\n",
      "Iteration is: 2616 and loss is: 0.004258076660335064\n",
      "Iteration is: 2617 and loss is: 0.004256410989910364\n",
      "Iteration is: 2618 and loss is: 0.004255050793290138\n",
      "Iteration is: 2619 and loss is: 0.004253458697348833\n",
      "Iteration is: 2620 and loss is: 0.004252376034855843\n",
      "Iteration is: 2621 and loss is: 0.004252701532095671\n",
      "Iteration is: 2622 and loss is: 0.004255746956914663\n",
      "Iteration is: 2623 and loss is: 0.004262153059244156\n",
      "Iteration is: 2624 and loss is: 0.004274146631360054\n",
      "Iteration is: 2625 and loss is: 0.004293958656489849\n",
      "Iteration is: 2626 and loss is: 0.004328773356974125\n",
      "Iteration is: 2627 and loss is: 0.004382987506687641\n",
      "Iteration is: 2628 and loss is: 0.004474159330129623\n",
      "Iteration is: 2629 and loss is: 0.0046036699786782265\n",
      "Iteration is: 2630 and loss is: 0.004814573097974062\n",
      "Iteration is: 2631 and loss is: 0.0050790575332939625\n",
      "Iteration is: 2632 and loss is: 0.005477236583828926\n",
      "Iteration is: 2633 and loss is: 0.005839756224304438\n",
      "Iteration is: 2634 and loss is: 0.006217087619006634\n",
      "Iteration is: 2635 and loss is: 0.006220878101885319\n",
      "Iteration is: 2636 and loss is: 0.005936590954661369\n",
      "Iteration is: 2637 and loss is: 0.005314498208463192\n",
      "Iteration is: 2638 and loss is: 0.004713964182883501\n",
      "Iteration is: 2639 and loss is: 0.0043532648123800755\n",
      "Iteration is: 2640 and loss is: 0.004309888929128647\n",
      "Iteration is: 2641 and loss is: 0.004516403190791607\n",
      "Iteration is: 2642 and loss is: 0.004887616727501154\n",
      "Iteration is: 2643 and loss is: 0.005421075504273176\n",
      "Iteration is: 2644 and loss is: 0.0059313103556632996\n",
      "Iteration is: 2645 and loss is: 0.00641399621963501\n",
      "Iteration is: 2646 and loss is: 0.006341777741909027\n",
      "Iteration is: 2647 and loss is: 0.005759742576628923\n",
      "Iteration is: 2648 and loss is: 0.004912595264613628\n",
      "Iteration is: 2649 and loss is: 0.004360533785074949\n",
      "Iteration is: 2650 and loss is: 0.004338383208960295\n",
      "Iteration is: 2651 and loss is: 0.0046851010993123055\n",
      "Iteration is: 2652 and loss is: 0.005135406740009785\n",
      "Iteration is: 2653 and loss is: 0.005447676870971918\n",
      "Iteration is: 2654 and loss is: 0.005626743659377098\n",
      "Iteration is: 2655 and loss is: 0.005447068251669407\n",
      "Iteration is: 2656 and loss is: 0.005050682462751865\n",
      "Iteration is: 2657 and loss is: 0.00454717455431819\n",
      "Iteration is: 2658 and loss is: 0.0042353239841759205\n",
      "Iteration is: 2659 and loss is: 0.004250070545822382\n",
      "Iteration is: 2660 and loss is: 0.0044831810519099236\n",
      "Iteration is: 2661 and loss is: 0.004712841473519802\n",
      "Iteration is: 2662 and loss is: 0.004774740431457758\n",
      "Iteration is: 2663 and loss is: 0.004683292470872402\n",
      "Iteration is: 2664 and loss is: 0.004487465135753155\n",
      "Iteration is: 2665 and loss is: 0.004305330570787191\n",
      "Iteration is: 2666 and loss is: 0.004188785795122385\n",
      "Iteration is: 2667 and loss is: 0.004160952754318714\n",
      "Iteration is: 2668 and loss is: 0.0041997614316642284\n",
      "Iteration is: 2669 and loss is: 0.004265883471816778\n",
      "Iteration is: 2670 and loss is: 0.004329345189034939\n",
      "Iteration is: 2671 and loss is: 0.004366366658359766\n",
      "Iteration is: 2672 and loss is: 0.004377985373139381\n",
      "Iteration is: 2673 and loss is: 0.0043500326573848724\n",
      "Iteration is: 2674 and loss is: 0.0043037328869104385\n",
      "Iteration is: 2675 and loss is: 0.004243900533765554\n",
      "Iteration is: 2676 and loss is: 0.004193267319351435\n",
      "Iteration is: 2677 and loss is: 0.00415533222258091\n",
      "Iteration is: 2678 and loss is: 0.004132590256631374\n",
      "Iteration is: 2679 and loss is: 0.004122018348425627\n",
      "Iteration is: 2680 and loss is: 0.004120589233934879\n",
      "Iteration is: 2681 and loss is: 0.0041246856562793255\n",
      "Iteration is: 2682 and loss is: 0.0041312407702207565\n",
      "Iteration is: 2683 and loss is: 0.004139941651374102\n",
      "Iteration is: 2684 and loss is: 0.00415155291557312\n",
      "Iteration is: 2685 and loss is: 0.004168502986431122\n",
      "Iteration is: 2686 and loss is: 0.004189430270344019\n",
      "Iteration is: 2687 and loss is: 0.004217913839966059\n",
      "Iteration is: 2688 and loss is: 0.004252349957823753\n",
      "Iteration is: 2689 and loss is: 0.004303439985960722\n",
      "Iteration is: 2690 and loss is: 0.0043646590784192085\n",
      "Iteration is: 2691 and loss is: 0.004453708417713642\n",
      "Iteration is: 2692 and loss is: 0.004546572919934988\n",
      "Iteration is: 2693 and loss is: 0.004672125447541475\n",
      "Iteration is: 2694 and loss is: 0.004771327134221792\n",
      "Iteration is: 2695 and loss is: 0.004882704932242632\n",
      "Iteration is: 2696 and loss is: 0.004906150512397289\n",
      "Iteration is: 2697 and loss is: 0.004891539458185434\n",
      "Iteration is: 2698 and loss is: 0.00476042041555047\n",
      "Iteration is: 2699 and loss is: 0.004592926241457462\n",
      "Iteration is: 2700 and loss is: 0.004394937306642532\n",
      "Iteration is: 2701 and loss is: 0.004234739113599062\n",
      "Iteration is: 2702 and loss is: 0.004128217697143555\n",
      "Iteration is: 2703 and loss is: 0.004080208484083414\n",
      "Iteration is: 2704 and loss is: 0.004080199170857668\n",
      "Iteration is: 2705 and loss is: 0.004118151031434536\n",
      "Iteration is: 2706 and loss is: 0.004191617481410503\n",
      "Iteration is: 2707 and loss is: 0.004298723302781582\n",
      "Iteration is: 2708 and loss is: 0.004452145658433437\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 2709 and loss is: 0.004625062458217144\n",
      "Iteration is: 2710 and loss is: 0.004829733166843653\n",
      "Iteration is: 2711 and loss is: 0.004971977323293686\n",
      "Iteration is: 2712 and loss is: 0.005075796972960234\n",
      "Iteration is: 2713 and loss is: 0.005026784725487232\n",
      "Iteration is: 2714 and loss is: 0.004906231537461281\n",
      "Iteration is: 2715 and loss is: 0.004680754616856575\n",
      "Iteration is: 2716 and loss is: 0.004458234645426273\n",
      "Iteration is: 2717 and loss is: 0.004259956069290638\n",
      "Iteration is: 2718 and loss is: 0.004127786494791508\n",
      "Iteration is: 2719 and loss is: 0.00405875314027071\n",
      "Iteration is: 2720 and loss is: 0.004042265936732292\n",
      "Iteration is: 2721 and loss is: 0.004066172521561384\n",
      "Iteration is: 2722 and loss is: 0.004123700316995382\n",
      "Iteration is: 2723 and loss is: 0.004217530135065317\n",
      "Iteration is: 2724 and loss is: 0.004343447275459766\n",
      "Iteration is: 2725 and loss is: 0.004516516346484423\n",
      "Iteration is: 2726 and loss is: 0.004698026925325394\n",
      "Iteration is: 2727 and loss is: 0.00491139804944396\n",
      "Iteration is: 2728 and loss is: 0.005053197033703327\n",
      "Iteration is: 2729 and loss is: 0.005169598385691643\n",
      "Iteration is: 2730 and loss is: 0.005127451382577419\n",
      "Iteration is: 2731 and loss is: 0.0050169313326478004\n",
      "Iteration is: 2732 and loss is: 0.004777331370860338\n",
      "Iteration is: 2733 and loss is: 0.004526613280177116\n",
      "Iteration is: 2734 and loss is: 0.00428804662078619\n",
      "Iteration is: 2735 and loss is: 0.004122577607631683\n",
      "Iteration is: 2736 and loss is: 0.004035968333482742\n",
      "Iteration is: 2737 and loss is: 0.004019913729280233\n",
      "Iteration is: 2738 and loss is: 0.0040590111166238785\n",
      "Iteration is: 2739 and loss is: 0.004142062738537788\n",
      "Iteration is: 2740 and loss is: 0.004271195735782385\n",
      "Iteration is: 2741 and loss is: 0.004432107787579298\n",
      "Iteration is: 2742 and loss is: 0.004639028571546078\n",
      "Iteration is: 2743 and loss is: 0.0048185354098677635\n",
      "Iteration is: 2744 and loss is: 0.004978042561560869\n",
      "Iteration is: 2745 and loss is: 0.004993635695427656\n",
      "Iteration is: 2746 and loss is: 0.004917806014418602\n",
      "Iteration is: 2747 and loss is: 0.004707504995167255\n",
      "Iteration is: 2748 and loss is: 0.004475563298910856\n",
      "Iteration is: 2749 and loss is: 0.004253664519637823\n",
      "Iteration is: 2750 and loss is: 0.004098026547580957\n",
      "Iteration is: 2751 and loss is: 0.0040105655789375305\n",
      "Iteration is: 2752 and loss is: 0.003982167690992355\n",
      "Iteration is: 2753 and loss is: 0.003999476321041584\n",
      "Iteration is: 2754 and loss is: 0.004054193384945393\n",
      "Iteration is: 2755 and loss is: 0.004146460443735123\n",
      "Iteration is: 2756 and loss is: 0.004268554970622063\n",
      "Iteration is: 2757 and loss is: 0.0044255563989281654\n",
      "Iteration is: 2758 and loss is: 0.004573263693600893\n",
      "Iteration is: 2759 and loss is: 0.00472007691860199\n",
      "Iteration is: 2760 and loss is: 0.004786286037415266\n",
      "Iteration is: 2761 and loss is: 0.004815568681806326\n",
      "Iteration is: 2762 and loss is: 0.004741464741528034\n",
      "Iteration is: 2763 and loss is: 0.004642751067876816\n",
      "Iteration is: 2764 and loss is: 0.004489314742386341\n",
      "Iteration is: 2765 and loss is: 0.0043486556969583035\n",
      "Iteration is: 2766 and loss is: 0.004211242310702801\n",
      "Iteration is: 2767 and loss is: 0.004106226377189159\n",
      "Iteration is: 2768 and loss is: 0.00402731541544199\n",
      "Iteration is: 2769 and loss is: 0.003976765088737011\n",
      "Iteration is: 2770 and loss is: 0.003947854042053223\n",
      "Iteration is: 2771 and loss is: 0.003935432061553001\n",
      "Iteration is: 2772 and loss is: 0.003934729378670454\n",
      "Iteration is: 2773 and loss is: 0.003942577634006739\n",
      "Iteration is: 2774 and loss is: 0.003957771696150303\n",
      "Iteration is: 2775 and loss is: 0.003980572335422039\n",
      "Iteration is: 2776 and loss is: 0.004014971200376749\n",
      "Iteration is: 2777 and loss is: 0.004064163193106651\n",
      "Iteration is: 2778 and loss is: 0.0041427090764045715\n",
      "Iteration is: 2779 and loss is: 0.004254629369825125\n",
      "Iteration is: 2780 and loss is: 0.00443868525326252\n",
      "Iteration is: 2781 and loss is: 0.004679013509303331\n",
      "Iteration is: 2782 and loss is: 0.005047548562288284\n",
      "Iteration is: 2783 and loss is: 0.005406025797128677\n",
      "Iteration is: 2784 and loss is: 0.00579830352216959\n",
      "Iteration is: 2785 and loss is: 0.005867424886673689\n",
      "Iteration is: 2786 and loss is: 0.00568055547773838\n",
      "Iteration is: 2787 and loss is: 0.005168174859136343\n",
      "Iteration is: 2788 and loss is: 0.004666867200285196\n",
      "Iteration is: 2789 and loss is: 0.004380356054753065\n",
      "Iteration is: 2790 and loss is: 0.004369217902421951\n",
      "Iteration is: 2791 and loss is: 0.004513326101005077\n",
      "Iteration is: 2792 and loss is: 0.004680112469941378\n",
      "Iteration is: 2793 and loss is: 0.004890889395028353\n",
      "Iteration is: 2794 and loss is: 0.005077283829450607\n",
      "Iteration is: 2795 and loss is: 0.005379167385399342\n",
      "Iteration is: 2796 and loss is: 0.005484045948833227\n",
      "Iteration is: 2797 and loss is: 0.00540397223085165\n",
      "Iteration is: 2798 and loss is: 0.005164192058146\n",
      "Iteration is: 2799 and loss is: 0.004975443705916405\n",
      "Iteration is: 2800 and loss is: 0.0048493267968297005\n",
      "Iteration is: 2801 and loss is: 0.004689080640673637\n",
      "Iteration is: 2802 and loss is: 0.004468834958970547\n",
      "Iteration is: 2803 and loss is: 0.004349522292613983\n",
      "Iteration is: 2804 and loss is: 0.004548851866275072\n",
      "Iteration is: 2805 and loss is: 0.004978358279913664\n",
      "Iteration is: 2806 and loss is: 0.0054532005451619625\n",
      "Iteration is: 2807 and loss is: 0.005556159652769566\n",
      "Iteration is: 2808 and loss is: 0.0052175866439938545\n",
      "Iteration is: 2809 and loss is: 0.004588976968079805\n",
      "Iteration is: 2810 and loss is: 0.0040603987872600555\n",
      "Iteration is: 2811 and loss is: 0.0038919891230762005\n",
      "Iteration is: 2812 and loss is: 0.004084951709955931\n",
      "Iteration is: 2813 and loss is: 0.004437450785189867\n",
      "Iteration is: 2814 and loss is: 0.004720860160887241\n",
      "Iteration is: 2815 and loss is: 0.004888368304818869\n",
      "Iteration is: 2816 and loss is: 0.004882124252617359\n",
      "Iteration is: 2817 and loss is: 0.004833178129047155\n",
      "Iteration is: 2818 and loss is: 0.004609155002981424\n",
      "Iteration is: 2819 and loss is: 0.004291308578103781\n",
      "Iteration is: 2820 and loss is: 0.004004615359008312\n",
      "Iteration is: 2821 and loss is: 0.0039510480128228664\n",
      "Iteration is: 2822 and loss is: 0.004120539408177137\n",
      "Iteration is: 2823 and loss is: 0.004338064696639776\n",
      "Iteration is: 2824 and loss is: 0.004473771434277296\n",
      "Iteration is: 2825 and loss is: 0.0044517433270812035\n",
      "Iteration is: 2826 and loss is: 0.004355110228061676\n",
      "Iteration is: 2827 and loss is: 0.004169424530118704\n",
      "Iteration is: 2828 and loss is: 0.003992263227701187\n",
      "Iteration is: 2829 and loss is: 0.003872885601595044\n",
      "Iteration is: 2830 and loss is: 0.0038500556256622076\n",
      "Iteration is: 2831 and loss is: 0.0039023871067911386\n",
      "Iteration is: 2832 and loss is: 0.003971568308770657\n",
      "Iteration is: 2833 and loss is: 0.004015255719423294\n",
      "Iteration is: 2834 and loss is: 0.004014967940747738\n",
      "Iteration is: 2835 and loss is: 0.003991405013948679\n",
      "Iteration is: 2836 and loss is: 0.003951251972466707\n",
      "Iteration is: 2837 and loss is: 0.003910099156200886\n",
      "Iteration is: 2838 and loss is: 0.003869956359267235\n",
      "Iteration is: 2839 and loss is: 0.00383803085424006\n",
      "Iteration is: 2840 and loss is: 0.003818084951490164\n",
      "Iteration is: 2841 and loss is: 0.0038119845557957888\n",
      "Iteration is: 2842 and loss is: 0.0038176123052835464\n",
      "Iteration is: 2843 and loss is: 0.0038295919075608253\n",
      "Iteration is: 2844 and loss is: 0.0038422076031565666\n",
      "Iteration is: 2845 and loss is: 0.0038519296795129776\n",
      "Iteration is: 2846 and loss is: 0.003860294120386243\n",
      "Iteration is: 2847 and loss is: 0.003868925618007779\n",
      "Iteration is: 2848 and loss is: 0.003882684977725148\n",
      "Iteration is: 2849 and loss is: 0.003899691393598914\n",
      "Iteration is: 2850 and loss is: 0.003923713695257902\n",
      "Iteration is: 2851 and loss is: 0.003946966957300901\n",
      "Iteration is: 2852 and loss is: 0.003974295221269131\n",
      "Iteration is: 2853 and loss is: 0.00399399921298027\n",
      "Iteration is: 2854 and loss is: 0.004017323721200228\n",
      "Iteration is: 2855 and loss is: 0.004033067263662815\n",
      "Iteration is: 2856 and loss is: 0.00405872380360961\n",
      "Iteration is: 2857 and loss is: 0.004080498591065407\n",
      "Iteration is: 2858 and loss is: 0.004118630196899176\n",
      "Iteration is: 2859 and loss is: 0.004152017179876566\n",
      "Iteration is: 2860 and loss is: 0.004203515592962503\n",
      "Iteration is: 2861 and loss is: 0.004239048343151808\n",
      "Iteration is: 2862 and loss is: 0.004282953217625618\n",
      "Iteration is: 2863 and loss is: 0.0042884862050414085\n",
      "Iteration is: 2864 and loss is: 0.004285390023142099\n",
      "Iteration is: 2865 and loss is: 0.0042338441126048565\n",
      "Iteration is: 2866 and loss is: 0.004172788001596928\n",
      "Iteration is: 2867 and loss is: 0.004085594788193703\n",
      "Iteration is: 2868 and loss is: 0.004006149247288704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 2869 and loss is: 0.003930442035198212\n",
      "Iteration is: 2870 and loss is: 0.003872523084282875\n",
      "Iteration is: 2871 and loss is: 0.0038278321735560894\n",
      "Iteration is: 2872 and loss is: 0.003796965815126896\n",
      "Iteration is: 2873 and loss is: 0.0037757025565952063\n",
      "Iteration is: 2874 and loss is: 0.003762112697586417\n",
      "Iteration is: 2875 and loss is: 0.0037536732852458954\n",
      "Iteration is: 2876 and loss is: 0.0037486357614398003\n",
      "Iteration is: 2877 and loss is: 0.003745295573025942\n",
      "Iteration is: 2878 and loss is: 0.003742517437785864\n",
      "Iteration is: 2879 and loss is: 0.0037396815605461597\n",
      "Iteration is: 2880 and loss is: 0.0037366803735494614\n",
      "Iteration is: 2881 and loss is: 0.003733731573447585\n",
      "Iteration is: 2882 and loss is: 0.0037310714833438396\n",
      "Iteration is: 2883 and loss is: 0.00372878834605217\n",
      "Iteration is: 2884 and loss is: 0.0037268614396452904\n",
      "Iteration is: 2885 and loss is: 0.0037251506000757217\n",
      "Iteration is: 2886 and loss is: 0.003723530098795891\n",
      "Iteration is: 2887 and loss is: 0.003721880726516247\n",
      "Iteration is: 2888 and loss is: 0.003720125649124384\n",
      "Iteration is: 2889 and loss is: 0.0037183004897087812\n",
      "Iteration is: 2890 and loss is: 0.003716553095728159\n",
      "Iteration is: 2891 and loss is: 0.0037151353899389505\n",
      "Iteration is: 2892 and loss is: 0.003714502789080143\n",
      "Iteration is: 2893 and loss is: 0.0037154206074774265\n",
      "Iteration is: 2894 and loss is: 0.0037193363532423973\n",
      "Iteration is: 2895 and loss is: 0.0037294647190719843\n",
      "Iteration is: 2896 and loss is: 0.0037518797907978296\n",
      "Iteration is: 2897 and loss is: 0.003800946054980159\n",
      "Iteration is: 2898 and loss is: 0.003900832263752818\n",
      "Iteration is: 2899 and loss is: 0.004114439710974693\n",
      "Iteration is: 2900 and loss is: 0.004518609959632158\n",
      "Iteration is: 2901 and loss is: 0.005347598809748888\n",
      "Iteration is: 2902 and loss is: 0.006617145147174597\n",
      "Iteration is: 2903 and loss is: 0.008647442795336246\n",
      "Iteration is: 2904 and loss is: 0.01032239105552435\n",
      "Iteration is: 2905 and loss is: 0.01148255355656147\n",
      "Iteration is: 2906 and loss is: 0.011998001486063004\n",
      "Iteration is: 2907 and loss is: 0.011544865556061268\n",
      "Iteration is: 2908 and loss is: 0.00918890442699194\n",
      "Iteration is: 2909 and loss is: 0.0061529288068413734\n",
      "Iteration is: 2910 and loss is: 0.003968235105276108\n",
      "Iteration is: 2911 and loss is: 0.004520081914961338\n",
      "Iteration is: 2912 and loss is: 0.006442730315029621\n",
      "Iteration is: 2913 and loss is: 0.006910581607371569\n",
      "Iteration is: 2914 and loss is: 0.005524564068764448\n",
      "Iteration is: 2915 and loss is: 0.004073450807482004\n",
      "Iteration is: 2916 and loss is: 0.004187494050711393\n",
      "Iteration is: 2917 and loss is: 0.005101965274661779\n",
      "Iteration is: 2918 and loss is: 0.005242901388555765\n",
      "Iteration is: 2919 and loss is: 0.004643081687390804\n",
      "Iteration is: 2920 and loss is: 0.004332922399044037\n",
      "Iteration is: 2921 and loss is: 0.004552407190203667\n",
      "Iteration is: 2922 and loss is: 0.004434795118868351\n",
      "Iteration is: 2923 and loss is: 0.003961790818721056\n",
      "Iteration is: 2924 and loss is: 0.003903078380972147\n",
      "Iteration is: 2925 and loss is: 0.004329369403421879\n",
      "Iteration is: 2926 and loss is: 0.004488755948841572\n",
      "Iteration is: 2927 and loss is: 0.004047737922519445\n",
      "Iteration is: 2928 and loss is: 0.0037114243023097515\n",
      "Iteration is: 2929 and loss is: 0.003873129840940237\n",
      "Iteration is: 2930 and loss is: 0.004112073220312595\n",
      "Iteration is: 2931 and loss is: 0.004039010964334011\n",
      "Iteration is: 2932 and loss is: 0.0038695083931088448\n",
      "Iteration is: 2933 and loss is: 0.00389258936047554\n",
      "Iteration is: 2934 and loss is: 0.003919338807463646\n",
      "Iteration is: 2935 and loss is: 0.003793037496507168\n",
      "Iteration is: 2936 and loss is: 0.003696499392390251\n",
      "Iteration is: 2937 and loss is: 0.003786413697525859\n",
      "Iteration is: 2938 and loss is: 0.003893123473972082\n",
      "Iteration is: 2939 and loss is: 0.003830809611827135\n",
      "Iteration is: 2940 and loss is: 0.0037078610621392727\n",
      "Iteration is: 2941 and loss is: 0.0036903792060911655\n",
      "Iteration is: 2942 and loss is: 0.0037402906455099583\n",
      "Iteration is: 2943 and loss is: 0.0037386207841336727\n",
      "Iteration is: 2944 and loss is: 0.0037008756771683693\n",
      "Iteration is: 2945 and loss is: 0.003712438978254795\n",
      "Iteration is: 2946 and loss is: 0.003748036688193679\n",
      "Iteration is: 2947 and loss is: 0.0037300563417375088\n",
      "Iteration is: 2948 and loss is: 0.0036693706642836332\n",
      "Iteration is: 2949 and loss is: 0.0036446689628064632\n",
      "Iteration is: 2950 and loss is: 0.003668090794235468\n",
      "Iteration is: 2951 and loss is: 0.003683534450829029\n",
      "Iteration is: 2952 and loss is: 0.0036696535535156727\n",
      "Iteration is: 2953 and loss is: 0.0036589456722140312\n",
      "Iteration is: 2954 and loss is: 0.0036680721677839756\n",
      "Iteration is: 2955 and loss is: 0.0036695341113954782\n",
      "Iteration is: 2956 and loss is: 0.0036481707356870174\n",
      "Iteration is: 2957 and loss is: 0.003626267658546567\n",
      "Iteration is: 2958 and loss is: 0.003626463934779167\n",
      "Iteration is: 2959 and loss is: 0.0036371061578392982\n",
      "Iteration is: 2960 and loss is: 0.003637222573161125\n",
      "Iteration is: 2961 and loss is: 0.0036290260031819344\n",
      "Iteration is: 2962 and loss is: 0.003626604098826647\n",
      "Iteration is: 2963 and loss is: 0.0036304094828665257\n",
      "Iteration is: 2964 and loss is: 0.0036280632484704256\n",
      "Iteration is: 2965 and loss is: 0.003617360955104232\n",
      "Iteration is: 2966 and loss is: 0.003608201164752245\n",
      "Iteration is: 2967 and loss is: 0.0036068707704544067\n",
      "Iteration is: 2968 and loss is: 0.0036078987177461386\n",
      "Iteration is: 2969 and loss is: 0.003604925936087966\n",
      "Iteration is: 2970 and loss is: 0.0036000553518533707\n",
      "Iteration is: 2971 and loss is: 0.0035987808369100094\n",
      "Iteration is: 2972 and loss is: 0.003600550116971135\n",
      "Iteration is: 2973 and loss is: 0.0036004693247377872\n",
      "Iteration is: 2974 and loss is: 0.0035969135351479053\n",
      "Iteration is: 2975 and loss is: 0.003593035973608494\n",
      "Iteration is: 2976 and loss is: 0.0035914236214011908\n",
      "Iteration is: 2977 and loss is: 0.0035905158147215843\n",
      "Iteration is: 2978 and loss is: 0.0035879528149962425\n",
      "Iteration is: 2979 and loss is: 0.00358395092189312\n",
      "Iteration is: 2980 and loss is: 0.003580770455300808\n",
      "Iteration is: 2981 and loss is: 0.0035791455302387476\n",
      "Iteration is: 2982 and loss is: 0.003577659372240305\n",
      "Iteration is: 2983 and loss is: 0.0035751324612647295\n",
      "Iteration is: 2984 and loss is: 0.003572156885638833\n",
      "Iteration is: 2985 and loss is: 0.003569898661226034\n",
      "Iteration is: 2986 and loss is: 0.0035684830509126186\n",
      "Iteration is: 2987 and loss is: 0.003566958010196686\n",
      "Iteration is: 2988 and loss is: 0.0035647740587592125\n",
      "Iteration is: 2989 and loss is: 0.0035623833537101746\n",
      "Iteration is: 2990 and loss is: 0.00356046250090003\n",
      "Iteration is: 2991 and loss is: 0.0035589663311839104\n",
      "Iteration is: 2992 and loss is: 0.0035573644563555717\n",
      "Iteration is: 2993 and loss is: 0.0035553579218685627\n",
      "Iteration is: 2994 and loss is: 0.0035532433539628983\n",
      "Iteration is: 2995 and loss is: 0.003551395144313574\n",
      "Iteration is: 2996 and loss is: 0.0035498025827109814\n",
      "Iteration is: 2997 and loss is: 0.003548151347786188\n",
      "Iteration is: 2998 and loss is: 0.0035462721716612577\n",
      "Iteration is: 2999 and loss is: 0.0035443168599158525\n",
      "Iteration is: 3000 and loss is: 0.003542517777532339\n",
      "Iteration is: 3001 and loss is: 0.0035408944822847843\n",
      "Iteration is: 3002 and loss is: 0.0035392860881984234\n",
      "Iteration is: 3003 and loss is: 0.0035376138985157013\n",
      "Iteration is: 3004 and loss is: 0.003535953350365162\n",
      "Iteration is: 3005 and loss is: 0.003534497693181038\n",
      "Iteration is: 3006 and loss is: 0.0035334029234945774\n",
      "Iteration is: 3007 and loss is: 0.0035327905789017677\n",
      "Iteration is: 3008 and loss is: 0.0035329358652234077\n",
      "Iteration is: 3009 and loss is: 0.0035344725474715233\n",
      "Iteration is: 3010 and loss is: 0.0035387524403631687\n",
      "Iteration is: 3011 and loss is: 0.003547866363078356\n",
      "Iteration is: 3012 and loss is: 0.003566368715837598\n",
      "Iteration is: 3013 and loss is: 0.00360085372813046\n",
      "Iteration is: 3014 and loss is: 0.00366814061999321\n",
      "Iteration is: 3015 and loss is: 0.0037875964771956205\n",
      "Iteration is: 3016 and loss is: 0.004018939100205898\n",
      "Iteration is: 3017 and loss is: 0.004391973838210106\n",
      "Iteration is: 3018 and loss is: 0.005065512843430042\n",
      "Iteration is: 3019 and loss is: 0.0058758151717484\n",
      "Iteration is: 3020 and loss is: 0.006898549385368824\n",
      "Iteration is: 3021 and loss is: 0.007271063979715109\n",
      "Iteration is: 3022 and loss is: 0.007012238260358572\n",
      "Iteration is: 3023 and loss is: 0.006084741093218327\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 3024 and loss is: 0.004992798436433077\n",
      "Iteration is: 3025 and loss is: 0.004119149874895811\n",
      "Iteration is: 3026 and loss is: 0.003745156340301037\n",
      "Iteration is: 3027 and loss is: 0.003988170996308327\n",
      "Iteration is: 3028 and loss is: 0.0047617340460419655\n",
      "Iteration is: 3029 and loss is: 0.005801530554890633\n",
      "Iteration is: 3030 and loss is: 0.006296950858086348\n",
      "Iteration is: 3031 and loss is: 0.006090833805501461\n",
      "Iteration is: 3032 and loss is: 0.005029876716434956\n",
      "Iteration is: 3033 and loss is: 0.003939650021493435\n",
      "Iteration is: 3034 and loss is: 0.0035841474309563637\n",
      "Iteration is: 3035 and loss is: 0.004031605087220669\n",
      "Iteration is: 3036 and loss is: 0.004702639766037464\n",
      "Iteration is: 3037 and loss is: 0.00495231943204999\n",
      "Iteration is: 3038 and loss is: 0.0047264788299798965\n",
      "Iteration is: 3039 and loss is: 0.004135253839194775\n",
      "Iteration is: 3040 and loss is: 0.0036576332058757544\n",
      "Iteration is: 3041 and loss is: 0.003520757192745805\n",
      "Iteration is: 3042 and loss is: 0.003721528686583042\n",
      "Iteration is: 3043 and loss is: 0.0039985873736441135\n",
      "Iteration is: 3044 and loss is: 0.004059316124767065\n",
      "Iteration is: 3045 and loss is: 0.003879756201058626\n",
      "Iteration is: 3046 and loss is: 0.00361685105599463\n",
      "Iteration is: 3047 and loss is: 0.003499251091852784\n",
      "Iteration is: 3048 and loss is: 0.0035779355093836784\n",
      "Iteration is: 3049 and loss is: 0.0037416936829686165\n",
      "Iteration is: 3050 and loss is: 0.003863070160150528\n",
      "Iteration is: 3051 and loss is: 0.0038576677907258272\n",
      "Iteration is: 3052 and loss is: 0.003765809116885066\n",
      "Iteration is: 3053 and loss is: 0.003619531635195017\n",
      "Iteration is: 3054 and loss is: 0.0035085638519376516\n",
      "Iteration is: 3055 and loss is: 0.0034794392995536327\n",
      "Iteration is: 3056 and loss is: 0.003524966537952423\n",
      "Iteration is: 3057 and loss is: 0.0035933838225901127\n",
      "Iteration is: 3058 and loss is: 0.003629071870818734\n",
      "Iteration is: 3059 and loss is: 0.0036203693598508835\n",
      "Iteration is: 3060 and loss is: 0.0035737319849431515\n",
      "Iteration is: 3061 and loss is: 0.0035211537033319473\n",
      "Iteration is: 3062 and loss is: 0.0034790453501045704\n",
      "Iteration is: 3063 and loss is: 0.0034583276137709618\n",
      "Iteration is: 3064 and loss is: 0.003458971157670021\n",
      "Iteration is: 3065 and loss is: 0.0034744935110211372\n",
      "Iteration is: 3066 and loss is: 0.003495396114885807\n",
      "Iteration is: 3067 and loss is: 0.003510682610794902\n",
      "Iteration is: 3068 and loss is: 0.0035159308463335037\n",
      "Iteration is: 3069 and loss is: 0.0035086041316390038\n",
      "Iteration is: 3070 and loss is: 0.0034947756212204695\n",
      "Iteration is: 3071 and loss is: 0.0034770090132951736\n",
      "Iteration is: 3072 and loss is: 0.003460506908595562\n",
      "Iteration is: 3073 and loss is: 0.0034470665268599987\n",
      "Iteration is: 3074 and loss is: 0.0034382613375782967\n",
      "Iteration is: 3075 and loss is: 0.0034336408134549856\n",
      "Iteration is: 3076 and loss is: 0.003432241268455982\n",
      "Iteration is: 3077 and loss is: 0.0034331735223531723\n",
      "Iteration is: 3078 and loss is: 0.0034357034601271152\n",
      "Iteration is: 3079 and loss is: 0.0034391148947179317\n",
      "Iteration is: 3080 and loss is: 0.0034423754550516605\n",
      "Iteration is: 3081 and loss is: 0.003445390146225691\n",
      "Iteration is: 3082 and loss is: 0.003447852097451687\n",
      "Iteration is: 3083 and loss is: 0.0034506593365222216\n",
      "Iteration is: 3084 and loss is: 0.003453380661085248\n",
      "Iteration is: 3085 and loss is: 0.003457307815551758\n",
      "Iteration is: 3086 and loss is: 0.0034617162309587\n",
      "Iteration is: 3087 and loss is: 0.0034683961421251297\n",
      "Iteration is: 3088 and loss is: 0.0034758043475449085\n",
      "Iteration is: 3089 and loss is: 0.003486541099846363\n",
      "Iteration is: 3090 and loss is: 0.0034983623772859573\n",
      "Iteration is: 3091 and loss is: 0.003515932708978653\n",
      "Iteration is: 3092 and loss is: 0.0035357256419956684\n",
      "Iteration is: 3093 and loss is: 0.0035656020045280457\n",
      "Iteration is: 3094 and loss is: 0.0035990397445857525\n",
      "Iteration is: 3095 and loss is: 0.0036488636396825314\n",
      "Iteration is: 3096 and loss is: 0.0037006745114922523\n",
      "Iteration is: 3097 and loss is: 0.003773664589971304\n",
      "Iteration is: 3098 and loss is: 0.003837158204987645\n",
      "Iteration is: 3099 and loss is: 0.003917239606380463\n",
      "Iteration is: 3100 and loss is: 0.003959778696298599\n",
      "Iteration is: 3101 and loss is: 0.0039968788623809814\n",
      "Iteration is: 3102 and loss is: 0.003965683281421661\n",
      "Iteration is: 3103 and loss is: 0.003910460975021124\n",
      "Iteration is: 3104 and loss is: 0.0037995856255292892\n",
      "Iteration is: 3105 and loss is: 0.00368487648665905\n",
      "Iteration is: 3106 and loss is: 0.003568572923541069\n",
      "Iteration is: 3107 and loss is: 0.003479556180536747\n",
      "Iteration is: 3108 and loss is: 0.003419299144297838\n",
      "Iteration is: 3109 and loss is: 0.0033878558315336704\n",
      "Iteration is: 3110 and loss is: 0.003378801979124546\n",
      "Iteration is: 3111 and loss is: 0.0033860313706099987\n",
      "Iteration is: 3112 and loss is: 0.0034059169702231884\n",
      "Iteration is: 3113 and loss is: 0.003437486942857504\n",
      "Iteration is: 3114 and loss is: 0.003484508488327265\n",
      "Iteration is: 3115 and loss is: 0.003547264728695154\n",
      "Iteration is: 3116 and loss is: 0.003638272173702717\n",
      "Iteration is: 3117 and loss is: 0.0037480187602341175\n",
      "Iteration is: 3118 and loss is: 0.0039057601243257523\n",
      "Iteration is: 3119 and loss is: 0.004070118069648743\n",
      "Iteration is: 3120 and loss is: 0.004290827549993992\n",
      "Iteration is: 3121 and loss is: 0.004449015483260155\n",
      "Iteration is: 3122 and loss is: 0.004592917859554291\n",
      "Iteration is: 3123 and loss is: 0.004545192699879408\n",
      "Iteration is: 3124 and loss is: 0.0043845754116773605\n",
      "Iteration is: 3125 and loss is: 0.004077673424035311\n",
      "Iteration is: 3126 and loss is: 0.003768922993913293\n",
      "Iteration is: 3127 and loss is: 0.003529534675180912\n",
      "Iteration is: 3128 and loss is: 0.003410800825804472\n",
      "Iteration is: 3129 and loss is: 0.0034079658798873425\n",
      "Iteration is: 3130 and loss is: 0.003499331884086132\n",
      "Iteration is: 3131 and loss is: 0.0036668735556304455\n",
      "Iteration is: 3132 and loss is: 0.0038765748031437397\n",
      "Iteration is: 3133 and loss is: 0.004136331379413605\n",
      "Iteration is: 3134 and loss is: 0.004342354834079742\n",
      "Iteration is: 3135 and loss is: 0.00449446402490139\n",
      "Iteration is: 3136 and loss is: 0.004431264009326696\n",
      "Iteration is: 3137 and loss is: 0.004214729182422161\n",
      "Iteration is: 3138 and loss is: 0.0038708157371729612\n",
      "Iteration is: 3139 and loss is: 0.003565030638128519\n",
      "Iteration is: 3140 and loss is: 0.003382990136742592\n",
      "Iteration is: 3141 and loss is: 0.0033531831577420235\n",
      "Iteration is: 3142 and loss is: 0.003444426693022251\n",
      "Iteration is: 3143 and loss is: 0.003613443113863468\n",
      "Iteration is: 3144 and loss is: 0.003843524493277073\n",
      "Iteration is: 3145 and loss is: 0.004070773255079985\n",
      "Iteration is: 3146 and loss is: 0.004287536256015301\n",
      "Iteration is: 3147 and loss is: 0.004320337437093258\n",
      "Iteration is: 3148 and loss is: 0.00417462270706892\n",
      "Iteration is: 3149 and loss is: 0.0038557203952223063\n",
      "Iteration is: 3150 and loss is: 0.0035489890724420547\n",
      "Iteration is: 3151 and loss is: 0.00337869580835104\n",
      "Iteration is: 3152 and loss is: 0.0033792261965572834\n",
      "Iteration is: 3153 and loss is: 0.003504264634102583\n",
      "Iteration is: 3154 and loss is: 0.0036980509757995605\n",
      "Iteration is: 3155 and loss is: 0.003954190760850906\n",
      "Iteration is: 3156 and loss is: 0.004198525100946426\n",
      "Iteration is: 3157 and loss is: 0.0044494434259831905\n",
      "Iteration is: 3158 and loss is: 0.00448365556076169\n",
      "Iteration is: 3159 and loss is: 0.0043008653447031975\n",
      "Iteration is: 3160 and loss is: 0.003916365560144186\n",
      "Iteration is: 3161 and loss is: 0.0035610543563961983\n",
      "Iteration is: 3162 and loss is: 0.00339052127674222\n",
      "Iteration is: 3163 and loss is: 0.0034243534319102764\n",
      "Iteration is: 3164 and loss is: 0.003584232646971941\n",
      "Iteration is: 3165 and loss is: 0.0037805107422173023\n",
      "Iteration is: 3166 and loss is: 0.003990513272583485\n",
      "Iteration is: 3167 and loss is: 0.004113758914172649\n",
      "Iteration is: 3168 and loss is: 0.004160367883741856\n",
      "Iteration is: 3169 and loss is: 0.004021822940558195\n",
      "Iteration is: 3170 and loss is: 0.003772254567593336\n",
      "Iteration is: 3171 and loss is: 0.0035093221813440323\n",
      "Iteration is: 3172 and loss is: 0.003355692606419325\n",
      "Iteration is: 3173 and loss is: 0.0033466978929936886\n",
      "Iteration is: 3174 and loss is: 0.0034410508815199137\n",
      "Iteration is: 3175 and loss is: 0.003575364127755165\n",
      "Iteration is: 3176 and loss is: 0.0036895787343382835\n",
      "Iteration is: 3177 and loss is: 0.003773896489292383\n",
      "Iteration is: 3178 and loss is: 0.003781489096581936\n",
      "Iteration is: 3179 and loss is: 0.003737875958904624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 3180 and loss is: 0.0036226268857717514\n",
      "Iteration is: 3181 and loss is: 0.0034821052104234695\n",
      "Iteration is: 3182 and loss is: 0.003355355467647314\n",
      "Iteration is: 3183 and loss is: 0.003288386855274439\n",
      "Iteration is: 3184 and loss is: 0.003292909823358059\n",
      "Iteration is: 3185 and loss is: 0.0033499698620289564\n",
      "Iteration is: 3186 and loss is: 0.003428533673286438\n",
      "Iteration is: 3187 and loss is: 0.003501919098198414\n",
      "Iteration is: 3188 and loss is: 0.003569282591342926\n",
      "Iteration is: 3189 and loss is: 0.003611921798437834\n",
      "Iteration is: 3190 and loss is: 0.0036459160037338734\n",
      "Iteration is: 3191 and loss is: 0.003632213920354843\n",
      "Iteration is: 3192 and loss is: 0.0035813061986118555\n",
      "Iteration is: 3193 and loss is: 0.003480072133243084\n",
      "Iteration is: 3194 and loss is: 0.003372643142938614\n",
      "Iteration is: 3195 and loss is: 0.0032904886174947023\n",
      "Iteration is: 3196 and loss is: 0.0032573300413787365\n",
      "Iteration is: 3197 and loss is: 0.0032706749625504017\n",
      "Iteration is: 3198 and loss is: 0.00331673352047801\n",
      "Iteration is: 3199 and loss is: 0.0033854683861136436\n",
      "Iteration is: 3200 and loss is: 0.003469622228294611\n",
      "Iteration is: 3201 and loss is: 0.003582633100450039\n",
      "Iteration is: 3202 and loss is: 0.0036963378079235554\n",
      "Iteration is: 3203 and loss is: 0.003812752664089203\n",
      "Iteration is: 3204 and loss is: 0.003848657011985779\n",
      "Iteration is: 3205 and loss is: 0.0038136751390993595\n",
      "Iteration is: 3206 and loss is: 0.0036820187233388424\n",
      "Iteration is: 3207 and loss is: 0.0035284573677927256\n",
      "Iteration is: 3208 and loss is: 0.003391595557332039\n",
      "Iteration is: 3209 and loss is: 0.0033064652234315872\n",
      "Iteration is: 3210 and loss is: 0.0032707066275179386\n",
      "Iteration is: 3211 and loss is: 0.0032728961668908596\n",
      "Iteration is: 3212 and loss is: 0.0033057471737265587\n",
      "Iteration is: 3213 and loss is: 0.0033681406639516354\n",
      "Iteration is: 3214 and loss is: 0.0034681858960539103\n",
      "Iteration is: 3215 and loss is: 0.003588601015508175\n",
      "Iteration is: 3216 and loss is: 0.003726215334609151\n",
      "Iteration is: 3217 and loss is: 0.003820154583081603\n",
      "Iteration is: 3218 and loss is: 0.00387357035651803\n",
      "Iteration is: 3219 and loss is: 0.00384024647064507\n",
      "Iteration is: 3220 and loss is: 0.0037675313651561737\n",
      "Iteration is: 3221 and loss is: 0.00365133723244071\n",
      "Iteration is: 3222 and loss is: 0.003539933357387781\n",
      "Iteration is: 3223 and loss is: 0.0034331558272242546\n",
      "Iteration is: 3224 and loss is: 0.0033494955860078335\n",
      "Iteration is: 3225 and loss is: 0.003285822691395879\n",
      "Iteration is: 3226 and loss is: 0.003244883380830288\n",
      "Iteration is: 3227 and loss is: 0.003222827333956957\n",
      "Iteration is: 3228 and loss is: 0.003215488512068987\n",
      "Iteration is: 3229 and loss is: 0.0032171658240258694\n",
      "Iteration is: 3230 and loss is: 0.0032229823991656303\n",
      "Iteration is: 3231 and loss is: 0.0032300460152328014\n",
      "Iteration is: 3232 and loss is: 0.003237636759877205\n",
      "Iteration is: 3233 and loss is: 0.003247773740440607\n",
      "Iteration is: 3234 and loss is: 0.003263590857386589\n",
      "Iteration is: 3235 and loss is: 0.0032925745472311974\n",
      "Iteration is: 3236 and loss is: 0.003340756753459573\n",
      "Iteration is: 3237 and loss is: 0.003426147624850273\n",
      "Iteration is: 3238 and loss is: 0.0035503676626831293\n",
      "Iteration is: 3239 and loss is: 0.003747326321899891\n",
      "Iteration is: 3240 and loss is: 0.003973240964114666\n",
      "Iteration is: 3241 and loss is: 0.004267622251063585\n",
      "Iteration is: 3242 and loss is: 0.004466387443244457\n",
      "Iteration is: 3243 and loss is: 0.004606248810887337\n",
      "Iteration is: 3244 and loss is: 0.004503780044615269\n",
      "Iteration is: 3245 and loss is: 0.004292912781238556\n",
      "Iteration is: 3246 and loss is: 0.004015578888356686\n",
      "Iteration is: 3247 and loss is: 0.003818650497123599\n",
      "Iteration is: 3248 and loss is: 0.003720145672559738\n",
      "Iteration is: 3249 and loss is: 0.0036922632716596127\n",
      "Iteration is: 3250 and loss is: 0.0036538485437631607\n",
      "Iteration is: 3251 and loss is: 0.0035864878445863724\n",
      "Iteration is: 3252 and loss is: 0.0035331477411091328\n",
      "Iteration is: 3253 and loss is: 0.0035377037711441517\n",
      "Iteration is: 3254 and loss is: 0.003660476766526699\n",
      "Iteration is: 3255 and loss is: 0.0038394476287066936\n",
      "Iteration is: 3256 and loss is: 0.004041862674057484\n",
      "Iteration is: 3257 and loss is: 0.0041642035357654095\n",
      "Iteration is: 3258 and loss is: 0.004197993315756321\n",
      "Iteration is: 3259 and loss is: 0.004093550611287355\n",
      "Iteration is: 3260 and loss is: 0.0039016390219330788\n",
      "Iteration is: 3261 and loss is: 0.003640210721641779\n",
      "Iteration is: 3262 and loss is: 0.0034130041021853685\n",
      "Iteration is: 3263 and loss is: 0.0032739629969000816\n",
      "Iteration is: 3264 and loss is: 0.0032475292682647705\n",
      "Iteration is: 3265 and loss is: 0.0032906909473240376\n",
      "Iteration is: 3266 and loss is: 0.0033422335982322693\n",
      "Iteration is: 3267 and loss is: 0.0033521968871355057\n",
      "Iteration is: 3268 and loss is: 0.003311329986900091\n",
      "Iteration is: 3269 and loss is: 0.003246246837079525\n",
      "Iteration is: 3270 and loss is: 0.00319422478787601\n",
      "Iteration is: 3271 and loss is: 0.0031832396052777767\n",
      "Iteration is: 3272 and loss is: 0.0032166410237550735\n",
      "Iteration is: 3273 and loss is: 0.0032805942464619875\n",
      "Iteration is: 3274 and loss is: 0.0033504413440823555\n",
      "Iteration is: 3275 and loss is: 0.0034265820868313313\n",
      "Iteration is: 3276 and loss is: 0.0034972974099218845\n",
      "Iteration is: 3277 and loss is: 0.0035977798979729414\n",
      "Iteration is: 3278 and loss is: 0.003690636483952403\n",
      "Iteration is: 3279 and loss is: 0.003803074359893799\n",
      "Iteration is: 3280 and loss is: 0.0038573388010263443\n",
      "Iteration is: 3281 and loss is: 0.003883357858285308\n",
      "Iteration is: 3282 and loss is: 0.0038302261382341385\n",
      "Iteration is: 3283 and loss is: 0.0037422662135213614\n",
      "Iteration is: 3284 and loss is: 0.0036057003308087587\n",
      "Iteration is: 3285 and loss is: 0.0034610251896083355\n",
      "Iteration is: 3286 and loss is: 0.003318759147077799\n",
      "Iteration is: 3287 and loss is: 0.00321022467687726\n",
      "Iteration is: 3288 and loss is: 0.0031475205905735493\n",
      "Iteration is: 3289 and loss is: 0.003131910227239132\n",
      "Iteration is: 3290 and loss is: 0.0031497259624302387\n",
      "Iteration is: 3291 and loss is: 0.003181680105626583\n",
      "Iteration is: 3292 and loss is: 0.003213035874068737\n",
      "Iteration is: 3293 and loss is: 0.003237357595935464\n",
      "Iteration is: 3294 and loss is: 0.003261691890656948\n",
      "Iteration is: 3295 and loss is: 0.0032946430146694183\n",
      "Iteration is: 3296 and loss is: 0.0033568362705409527\n",
      "Iteration is: 3297 and loss is: 0.0034488695673644543\n",
      "Iteration is: 3298 and loss is: 0.003599107963964343\n",
      "Iteration is: 3299 and loss is: 0.0037648517172783613\n",
      "Iteration is: 3300 and loss is: 0.0039762756787240505\n",
      "Iteration is: 3301 and loss is: 0.004097754135727882\n",
      "Iteration is: 3302 and loss is: 0.004157310351729393\n",
      "Iteration is: 3303 and loss is: 0.004032074008136988\n",
      "Iteration is: 3304 and loss is: 0.003835659474134445\n",
      "Iteration is: 3305 and loss is: 0.0036169555969536304\n",
      "Iteration is: 3306 and loss is: 0.0034680841490626335\n",
      "Iteration is: 3307 and loss is: 0.0033876283559948206\n",
      "Iteration is: 3308 and loss is: 0.0033462136052548885\n",
      "Iteration is: 3309 and loss is: 0.0033086994662880898\n",
      "Iteration is: 3310 and loss is: 0.0032821432687342167\n",
      "Iteration is: 3311 and loss is: 0.0033045492600649595\n",
      "Iteration is: 3312 and loss is: 0.0033912493381649256\n",
      "Iteration is: 3313 and loss is: 0.0035546941217035055\n",
      "Iteration is: 3314 and loss is: 0.0037350826896727085\n",
      "Iteration is: 3315 and loss is: 0.003909670747816563\n",
      "Iteration is: 3316 and loss is: 0.004000156186521053\n",
      "Iteration is: 3317 and loss is: 0.004022155422717333\n",
      "Iteration is: 3318 and loss is: 0.003938265144824982\n",
      "Iteration is: 3319 and loss is: 0.0038153803907334805\n",
      "Iteration is: 3320 and loss is: 0.003649946302175522\n",
      "Iteration is: 3321 and loss is: 0.0035118204541504383\n",
      "Iteration is: 3322 and loss is: 0.0033895401284098625\n",
      "Iteration is: 3323 and loss is: 0.003304616315290332\n",
      "Iteration is: 3324 and loss is: 0.0032312655821442604\n",
      "Iteration is: 3325 and loss is: 0.0031735014636069536\n",
      "Iteration is: 3326 and loss is: 0.00312912929803133\n",
      "Iteration is: 3327 and loss is: 0.003107963828369975\n",
      "Iteration is: 3328 and loss is: 0.0031137592159211636\n",
      "Iteration is: 3329 and loss is: 0.0031438181176781654\n",
      "Iteration is: 3330 and loss is: 0.003193565644323826\n",
      "Iteration is: 3331 and loss is: 0.0032549723982810974\n",
      "Iteration is: 3332 and loss is: 0.0033346456475555897\n",
      "Iteration is: 3333 and loss is: 0.0034150597639381886\n",
      "Iteration is: 3334 and loss is: 0.0035170321352779865\n",
      "Iteration is: 3335 and loss is: 0.0035922997631132603\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 3336 and loss is: 0.0036622758489102125\n",
      "Iteration is: 3337 and loss is: 0.0036593882832676172\n",
      "Iteration is: 3338 and loss is: 0.003622001502662897\n",
      "Iteration is: 3339 and loss is: 0.003529318142682314\n",
      "Iteration is: 3340 and loss is: 0.0034324522130191326\n",
      "Iteration is: 3341 and loss is: 0.00333552248775959\n",
      "Iteration is: 3342 and loss is: 0.0032547065056860447\n",
      "Iteration is: 3343 and loss is: 0.0031828992068767548\n",
      "Iteration is: 3344 and loss is: 0.003123179078102112\n",
      "Iteration is: 3345 and loss is: 0.0030795910861343145\n",
      "Iteration is: 3346 and loss is: 0.0030603278428316116\n",
      "Iteration is: 3347 and loss is: 0.0030686515383422375\n",
      "Iteration is: 3348 and loss is: 0.003100304864346981\n",
      "Iteration is: 3349 and loss is: 0.00314676808193326\n",
      "Iteration is: 3350 and loss is: 0.0031969621777534485\n",
      "Iteration is: 3351 and loss is: 0.0032489062286913395\n",
      "Iteration is: 3352 and loss is: 0.00329934642650187\n",
      "Iteration is: 3353 and loss is: 0.003366537857800722\n",
      "Iteration is: 3354 and loss is: 0.0034519927576184273\n",
      "Iteration is: 3355 and loss is: 0.0035973666235804558\n",
      "Iteration is: 3356 and loss is: 0.0037736818194389343\n",
      "Iteration is: 3357 and loss is: 0.004026075359433889\n",
      "Iteration is: 3358 and loss is: 0.004207574296742678\n",
      "Iteration is: 3359 and loss is: 0.004329080693423748\n",
      "Iteration is: 3360 and loss is: 0.004209462553262711\n",
      "Iteration is: 3361 and loss is: 0.0039647719822824\n",
      "Iteration is: 3362 and loss is: 0.0036634602583944798\n",
      "Iteration is: 3363 and loss is: 0.003452977631241083\n",
      "Iteration is: 3364 and loss is: 0.0033610216341912746\n",
      "Iteration is: 3365 and loss is: 0.003350336104631424\n",
      "Iteration is: 3366 and loss is: 0.0033513077069073915\n",
      "Iteration is: 3367 and loss is: 0.00334120518527925\n",
      "Iteration is: 3368 and loss is: 0.0033577391877770424\n",
      "Iteration is: 3369 and loss is: 0.0034138564951717854\n",
      "Iteration is: 3370 and loss is: 0.0035437096375972033\n",
      "Iteration is: 3371 and loss is: 0.0036799972876906395\n",
      "Iteration is: 3372 and loss is: 0.0038015383761376143\n",
      "Iteration is: 3373 and loss is: 0.0038449177518486977\n",
      "Iteration is: 3374 and loss is: 0.0038157901726663113\n",
      "Iteration is: 3375 and loss is: 0.003688451834022999\n",
      "Iteration is: 3376 and loss is: 0.0035093813203275204\n",
      "Iteration is: 3377 and loss is: 0.0033066461328417063\n",
      "Iteration is: 3378 and loss is: 0.003154777456074953\n",
      "Iteration is: 3379 and loss is: 0.0030842209234833717\n",
      "Iteration is: 3380 and loss is: 0.0030923867598176003\n",
      "Iteration is: 3381 and loss is: 0.0031342115253210068\n",
      "Iteration is: 3382 and loss is: 0.003163677640259266\n",
      "Iteration is: 3383 and loss is: 0.0031528256367892027\n",
      "Iteration is: 3384 and loss is: 0.0031072485726326704\n",
      "Iteration is: 3385 and loss is: 0.003054082626476884\n",
      "Iteration is: 3386 and loss is: 0.0030211012344807386\n",
      "Iteration is: 3387 and loss is: 0.003023189725354314\n",
      "Iteration is: 3388 and loss is: 0.003056800691410899\n",
      "Iteration is: 3389 and loss is: 0.003109301906079054\n",
      "Iteration is: 3390 and loss is: 0.0031636704225093126\n",
      "Iteration is: 3391 and loss is: 0.0032241991721093655\n",
      "Iteration is: 3392 and loss is: 0.0032834059093147516\n",
      "Iteration is: 3393 and loss is: 0.0033680149354040623\n",
      "Iteration is: 3394 and loss is: 0.0034495543222874403\n",
      "Iteration is: 3395 and loss is: 0.003551389090716839\n",
      "Iteration is: 3396 and loss is: 0.003619337687268853\n",
      "Iteration is: 3397 and loss is: 0.0036805085837841034\n",
      "Iteration is: 3398 and loss is: 0.0036872136406600475\n",
      "Iteration is: 3399 and loss is: 0.0036713159643113613\n",
      "Iteration is: 3400 and loss is: 0.003599327988922596\n",
      "Iteration is: 3401 and loss is: 0.0035057147033512592\n",
      "Iteration is: 3402 and loss is: 0.0033806192222982645\n",
      "Iteration is: 3403 and loss is: 0.003261435776948929\n",
      "Iteration is: 3404 and loss is: 0.0031547099351882935\n",
      "Iteration is: 3405 and loss is: 0.003079540329053998\n",
      "Iteration is: 3406 and loss is: 0.003032361390069127\n",
      "Iteration is: 3407 and loss is: 0.003009309060871601\n",
      "Iteration is: 3408 and loss is: 0.0029995671939104795\n",
      "Iteration is: 3409 and loss is: 0.0029961448162794113\n",
      "Iteration is: 3410 and loss is: 0.002995277289301157\n",
      "Iteration is: 3411 and loss is: 0.0029972644988447428\n",
      "Iteration is: 3412 and loss is: 0.003006148152053356\n",
      "Iteration is: 3413 and loss is: 0.0030267275869846344\n",
      "Iteration is: 3414 and loss is: 0.003066279925405979\n",
      "Iteration is: 3415 and loss is: 0.003126370720565319\n",
      "Iteration is: 3416 and loss is: 0.0032182796858251095\n",
      "Iteration is: 3417 and loss is: 0.0033279189374297857\n",
      "Iteration is: 3418 and loss is: 0.003474532626569271\n",
      "Iteration is: 3419 and loss is: 0.003606968093663454\n",
      "Iteration is: 3420 and loss is: 0.003754956182092428\n",
      "Iteration is: 3421 and loss is: 0.0038315043784677982\n",
      "Iteration is: 3422 and loss is: 0.0038841040804982185\n",
      "Iteration is: 3423 and loss is: 0.0038508898578584194\n",
      "Iteration is: 3424 and loss is: 0.0037947825621813536\n",
      "Iteration is: 3425 and loss is: 0.003699414199218154\n",
      "Iteration is: 3426 and loss is: 0.0035997466184198856\n",
      "Iteration is: 3427 and loss is: 0.0034725223667919636\n",
      "Iteration is: 3428 and loss is: 0.0033327601850032806\n",
      "Iteration is: 3429 and loss is: 0.003179331077262759\n",
      "Iteration is: 3430 and loss is: 0.003048004349693656\n",
      "Iteration is: 3431 and loss is: 0.0029634996317327023\n",
      "Iteration is: 3432 and loss is: 0.0029392624273896217\n",
      "Iteration is: 3433 and loss is: 0.002964908257126808\n",
      "Iteration is: 3434 and loss is: 0.0030151712708175182\n",
      "Iteration is: 3435 and loss is: 0.0030646808445453644\n",
      "Iteration is: 3436 and loss is: 0.0030960682779550552\n",
      "Iteration is: 3437 and loss is: 0.0031120884232223034\n",
      "Iteration is: 3438 and loss is: 0.0031230454333126545\n",
      "Iteration is: 3439 and loss is: 0.003157618921250105\n",
      "Iteration is: 3440 and loss is: 0.0032323263585567474\n",
      "Iteration is: 3441 and loss is: 0.0033829649910330772\n",
      "Iteration is: 3442 and loss is: 0.003581020748242736\n",
      "Iteration is: 3443 and loss is: 0.003852629102766514\n",
      "Iteration is: 3444 and loss is: 0.004038375802338123\n",
      "Iteration is: 3445 and loss is: 0.0041482094675302505\n",
      "Iteration is: 3446 and loss is: 0.004013776313513517\n",
      "Iteration is: 3447 and loss is: 0.0037799032870680094\n",
      "Iteration is: 3448 and loss is: 0.0035308983642607927\n",
      "Iteration is: 3449 and loss is: 0.003392074489966035\n",
      "Iteration is: 3450 and loss is: 0.0033533372916281223\n",
      "Iteration is: 3451 and loss is: 0.003349465783685446\n",
      "Iteration is: 3452 and loss is: 0.0033002039417624474\n",
      "Iteration is: 3453 and loss is: 0.0032108249142766\n",
      "Iteration is: 3454 and loss is: 0.0031504621729254723\n",
      "Iteration is: 3455 and loss is: 0.003175168763846159\n",
      "Iteration is: 3456 and loss is: 0.0033193971030414104\n",
      "Iteration is: 3457 and loss is: 0.0035201343707740307\n",
      "Iteration is: 3458 and loss is: 0.003727975767105818\n",
      "Iteration is: 3459 and loss is: 0.0038528419099748135\n",
      "Iteration is: 3460 and loss is: 0.003888101549819112\n",
      "Iteration is: 3461 and loss is: 0.0037968051619827747\n",
      "Iteration is: 3462 and loss is: 0.0036575952544808388\n",
      "Iteration is: 3463 and loss is: 0.003491532988846302\n",
      "Iteration is: 3464 and loss is: 0.003391680307686329\n",
      "Iteration is: 3465 and loss is: 0.003325483063235879\n",
      "Iteration is: 3466 and loss is: 0.0032882194500416517\n",
      "Iteration is: 3467 and loss is: 0.0032021370716392994\n",
      "Iteration is: 3468 and loss is: 0.0030865389853715897\n",
      "Iteration is: 3469 and loss is: 0.002972089685499668\n",
      "Iteration is: 3470 and loss is: 0.002919444115832448\n",
      "Iteration is: 3471 and loss is: 0.0029456503689289093\n",
      "Iteration is: 3472 and loss is: 0.0030253182630985975\n",
      "Iteration is: 3473 and loss is: 0.0031221448443830013\n",
      "Iteration is: 3474 and loss is: 0.0031970757991075516\n",
      "Iteration is: 3475 and loss is: 0.0032573482021689415\n",
      "Iteration is: 3476 and loss is: 0.0032724691554903984\n",
      "Iteration is: 3477 and loss is: 0.0032757180742919445\n",
      "Iteration is: 3478 and loss is: 0.0032399401534348726\n",
      "Iteration is: 3479 and loss is: 0.003194329794496298\n",
      "Iteration is: 3480 and loss is: 0.003135284874588251\n",
      "Iteration is: 3481 and loss is: 0.0030810744501650333\n",
      "Iteration is: 3482 and loss is: 0.003029904793947935\n",
      "Iteration is: 3483 and loss is: 0.0029822909273207188\n",
      "Iteration is: 3484 and loss is: 0.0029360055923461914\n",
      "Iteration is: 3485 and loss is: 0.0028988756239414215\n",
      "Iteration is: 3486 and loss is: 0.002880220767110586\n",
      "Iteration is: 3487 and loss is: 0.0028854738920927048\n",
      "Iteration is: 3488 and loss is: 0.002911340445280075\n",
      "Iteration is: 3489 and loss is: 0.0029474939219653606\n",
      "Iteration is: 3490 and loss is: 0.0029837852343916893\n",
      "Iteration is: 3491 and loss is: 0.003011654131114483\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 3492 and loss is: 0.0030328796710819006\n",
      "Iteration is: 3493 and loss is: 0.0030491759534925222\n",
      "Iteration is: 3494 and loss is: 0.0030746974516659975\n",
      "Iteration is: 3495 and loss is: 0.003111906349658966\n",
      "Iteration is: 3496 and loss is: 0.003182332729920745\n",
      "Iteration is: 3497 and loss is: 0.0032721045427024364\n",
      "Iteration is: 3498 and loss is: 0.003408296499401331\n",
      "Iteration is: 3499 and loss is: 0.0035308287478983402\n",
      "Iteration is: 3500 and loss is: 0.00366064696572721\n",
      "Iteration is: 3501 and loss is: 0.0036949184723198414\n",
      "Iteration is: 3502 and loss is: 0.0036779295187443495\n",
      "Iteration is: 3503 and loss is: 0.0035787455271929502\n",
      "Iteration is: 3504 and loss is: 0.0034734003711491823\n",
      "Iteration is: 3505 and loss is: 0.003373953280970454\n",
      "Iteration is: 3506 and loss is: 0.0032942011021077633\n",
      "Iteration is: 3507 and loss is: 0.0032015994656831026\n",
      "Iteration is: 3508 and loss is: 0.003093261504545808\n",
      "Iteration is: 3509 and loss is: 0.0029747358057647943\n",
      "Iteration is: 3510 and loss is: 0.0028832508251070976\n",
      "Iteration is: 3511 and loss is: 0.0028454838320612907\n",
      "Iteration is: 3512 and loss is: 0.002866173628717661\n",
      "Iteration is: 3513 and loss is: 0.0029264765325933695\n",
      "Iteration is: 3514 and loss is: 0.0029971038457006216\n",
      "Iteration is: 3515 and loss is: 0.0030577690340578556\n",
      "Iteration is: 3516 and loss is: 0.0030969535000622272\n",
      "Iteration is: 3517 and loss is: 0.0031305188313126564\n",
      "Iteration is: 3518 and loss is: 0.0031733615323901176\n",
      "Iteration is: 3519 and loss is: 0.003270937129855156\n",
      "Iteration is: 3520 and loss is: 0.003423127345740795\n",
      "Iteration is: 3521 and loss is: 0.0036748643033206463\n",
      "Iteration is: 3522 and loss is: 0.003902993630617857\n",
      "Iteration is: 3523 and loss is: 0.0041016521863639355\n",
      "Iteration is: 3524 and loss is: 0.004051007330417633\n",
      "Iteration is: 3525 and loss is: 0.003851768560707569\n",
      "Iteration is: 3526 and loss is: 0.00356882531195879\n",
      "Iteration is: 3527 and loss is: 0.0033720878418534994\n",
      "Iteration is: 3528 and loss is: 0.003296525916084647\n",
      "Iteration is: 3529 and loss is: 0.003286578692495823\n",
      "Iteration is: 3530 and loss is: 0.003238821169361472\n",
      "Iteration is: 3531 and loss is: 0.003139020875096321\n",
      "Iteration is: 3532 and loss is: 0.0030487370677292347\n",
      "Iteration is: 3533 and loss is: 0.003041693940758705\n",
      "Iteration is: 3534 and loss is: 0.0031633295584470034\n",
      "Iteration is: 3535 and loss is: 0.0033555517438799143\n",
      "Iteration is: 3536 and loss is: 0.003555197734385729\n",
      "Iteration is: 3537 and loss is: 0.003669616300612688\n",
      "Iteration is: 3538 and loss is: 0.003684747964143753\n",
      "Iteration is: 3539 and loss is: 0.003577536903321743\n",
      "Iteration is: 3540 and loss is: 0.003429113421589136\n",
      "Iteration is: 3541 and loss is: 0.003279377706348896\n",
      "Iteration is: 3542 and loss is: 0.0032116230577230453\n",
      "Iteration is: 3543 and loss is: 0.003188264323398471\n",
      "Iteration is: 3544 and loss is: 0.003186195157468319\n",
      "Iteration is: 3545 and loss is: 0.0031179175712168217\n",
      "Iteration is: 3546 and loss is: 0.0030013318173587322\n",
      "Iteration is: 3547 and loss is: 0.002874333644285798\n",
      "Iteration is: 3548 and loss is: 0.0028101964853703976\n",
      "Iteration is: 3549 and loss is: 0.0028310499619692564\n",
      "Iteration is: 3550 and loss is: 0.0029064700938761234\n",
      "Iteration is: 3551 and loss is: 0.0029899580404162407\n",
      "Iteration is: 3552 and loss is: 0.0030405502766370773\n",
      "Iteration is: 3553 and loss is: 0.0030650165863335133\n",
      "Iteration is: 3554 and loss is: 0.003055016277357936\n",
      "Iteration is: 3555 and loss is: 0.003044977318495512\n",
      "Iteration is: 3556 and loss is: 0.0030281413346529007\n",
      "Iteration is: 3557 and loss is: 0.0030167079530656338\n",
      "Iteration is: 3558 and loss is: 0.002998501993715763\n",
      "Iteration is: 3559 and loss is: 0.0029725288040935993\n",
      "Iteration is: 3560 and loss is: 0.002932874718680978\n",
      "Iteration is: 3561 and loss is: 0.002885612193495035\n",
      "Iteration is: 3562 and loss is: 0.00283807422965765\n",
      "Iteration is: 3563 and loss is: 0.0028027426451444626\n",
      "Iteration is: 3564 and loss is: 0.0027859057299792767\n",
      "Iteration is: 3565 and loss is: 0.002786022610962391\n",
      "Iteration is: 3566 and loss is: 0.00279447715729475\n",
      "Iteration is: 3567 and loss is: 0.002801650669425726\n",
      "Iteration is: 3568 and loss is: 0.002801865339279175\n",
      "Iteration is: 3569 and loss is: 0.00279492000117898\n",
      "Iteration is: 3570 and loss is: 0.0027854610234498978\n",
      "Iteration is: 3571 and loss is: 0.002779074478894472\n",
      "Iteration is: 3572 and loss is: 0.002779699396342039\n",
      "Iteration is: 3573 and loss is: 0.0027879285626113415\n",
      "Iteration is: 3574 and loss is: 0.0028029130771756172\n",
      "Iteration is: 3575 and loss is: 0.002822806127369404\n",
      "Iteration is: 3576 and loss is: 0.002850891789421439\n",
      "Iteration is: 3577 and loss is: 0.0028881067410111427\n",
      "Iteration is: 3578 and loss is: 0.0029471092857420444\n",
      "Iteration is: 3579 and loss is: 0.003026828868314624\n",
      "Iteration is: 3580 and loss is: 0.0031499769538640976\n",
      "Iteration is: 3581 and loss is: 0.0033015403896570206\n",
      "Iteration is: 3582 and loss is: 0.003518484765663743\n",
      "Iteration is: 3583 and loss is: 0.0037527428939938545\n",
      "Iteration is: 3584 and loss is: 0.0040541114285588264\n",
      "Iteration is: 3585 and loss is: 0.004318568855524063\n",
      "Iteration is: 3586 and loss is: 0.004594136960804462\n",
      "Iteration is: 3587 and loss is: 0.004743232857435942\n",
      "Iteration is: 3588 and loss is: 0.004802468698471785\n",
      "Iteration is: 3589 and loss is: 0.004657397512346506\n",
      "Iteration is: 3590 and loss is: 0.004367549903690815\n",
      "Iteration is: 3591 and loss is: 0.0039039705879986286\n",
      "Iteration is: 3592 and loss is: 0.0034172944724559784\n",
      "Iteration is: 3593 and loss is: 0.003003472462296486\n",
      "Iteration is: 3594 and loss is: 0.0027802421245723963\n",
      "Iteration is: 3595 and loss is: 0.002766628749668598\n",
      "Iteration is: 3596 and loss is: 0.002903697080910206\n",
      "Iteration is: 3597 and loss is: 0.003094503190368414\n",
      "Iteration is: 3598 and loss is: 0.0032463204115629196\n",
      "Iteration is: 3599 and loss is: 0.0033354866318404675\n",
      "Iteration is: 3600 and loss is: 0.0033429344184696674\n",
      "Iteration is: 3601 and loss is: 0.0033273007720708847\n",
      "Iteration is: 3602 and loss is: 0.003261010628193617\n",
      "Iteration is: 3603 and loss is: 0.003183226566761732\n",
      "Iteration is: 3604 and loss is: 0.0030510136857628822\n",
      "Iteration is: 3605 and loss is: 0.0029145271982997656\n",
      "Iteration is: 3606 and loss is: 0.002798852976411581\n",
      "Iteration is: 3607 and loss is: 0.002749102655798197\n",
      "Iteration is: 3608 and loss is: 0.0027724625542759895\n",
      "Iteration is: 3609 and loss is: 0.0028451664838939905\n",
      "Iteration is: 3610 and loss is: 0.0029328723903745413\n",
      "Iteration is: 3611 and loss is: 0.0029928316362202168\n",
      "Iteration is: 3612 and loss is: 0.003019121242687106\n",
      "Iteration is: 3613 and loss is: 0.002985146827995777\n",
      "Iteration is: 3614 and loss is: 0.0029211407527327538\n",
      "Iteration is: 3615 and loss is: 0.0028388318605720997\n",
      "Iteration is: 3616 and loss is: 0.002774057211354375\n",
      "Iteration is: 3617 and loss is: 0.002740331459790468\n",
      "Iteration is: 3618 and loss is: 0.002736966358497739\n",
      "Iteration is: 3619 and loss is: 0.0027520284056663513\n",
      "Iteration is: 3620 and loss is: 0.002774536842480302\n",
      "Iteration is: 3621 and loss is: 0.0028017424046993256\n",
      "Iteration is: 3622 and loss is: 0.002831532619893551\n",
      "Iteration is: 3623 and loss is: 0.002869797870516777\n",
      "Iteration is: 3624 and loss is: 0.0029051760211586952\n",
      "Iteration is: 3625 and loss is: 0.002936730859801173\n",
      "Iteration is: 3626 and loss is: 0.002945942571386695\n",
      "Iteration is: 3627 and loss is: 0.0029361832421272993\n",
      "Iteration is: 3628 and loss is: 0.002902757842093706\n",
      "Iteration is: 3629 and loss is: 0.002859284169971943\n",
      "Iteration is: 3630 and loss is: 0.0028110132552683353\n",
      "Iteration is: 3631 and loss is: 0.002768447156995535\n",
      "Iteration is: 3632 and loss is: 0.0027344380505383015\n",
      "Iteration is: 3633 and loss is: 0.0027115466073155403\n",
      "Iteration is: 3634 and loss is: 0.0026989346370100975\n",
      "Iteration is: 3635 and loss is: 0.0026947511360049248\n",
      "Iteration is: 3636 and loss is: 0.0026962049305438995\n",
      "Iteration is: 3637 and loss is: 0.0027005623560398817\n",
      "Iteration is: 3638 and loss is: 0.0027059894055128098\n",
      "Iteration is: 3639 and loss is: 0.002711770823225379\n",
      "Iteration is: 3640 and loss is: 0.0027187257073819637\n",
      "Iteration is: 3641 and loss is: 0.002728072227910161\n",
      "Iteration is: 3642 and loss is: 0.002742587123066187\n",
      "Iteration is: 3643 and loss is: 0.002763585653156042\n",
      "Iteration is: 3644 and loss is: 0.0027966941706836224\n",
      "Iteration is: 3645 and loss is: 0.0028415806591510773\n",
      "Iteration is: 3646 and loss is: 0.0029092971235513687\n",
      "Iteration is: 3647 and loss is: 0.0029910013545304537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 3648 and loss is: 0.0031042112968862057\n",
      "Iteration is: 3649 and loss is: 0.003217788878828287\n",
      "Iteration is: 3650 and loss is: 0.0033570746891200542\n",
      "Iteration is: 3651 and loss is: 0.0034647793509066105\n",
      "Iteration is: 3652 and loss is: 0.0035781031474471092\n",
      "Iteration is: 3653 and loss is: 0.003638552501797676\n",
      "Iteration is: 3654 and loss is: 0.003691457211971283\n",
      "Iteration is: 3655 and loss is: 0.003697681240737438\n",
      "Iteration is: 3656 and loss is: 0.0036893822252750397\n",
      "Iteration is: 3657 and loss is: 0.003627186641097069\n",
      "Iteration is: 3658 and loss is: 0.0035302345640957355\n",
      "Iteration is: 3659 and loss is: 0.0033679455518722534\n",
      "Iteration is: 3660 and loss is: 0.0031827047932893038\n",
      "Iteration is: 3661 and loss is: 0.0029892809689044952\n",
      "Iteration is: 3662 and loss is: 0.002837474923580885\n",
      "Iteration is: 3663 and loss is: 0.00274362089112401\n",
      "Iteration is: 3664 and loss is: 0.002711195033043623\n",
      "Iteration is: 3665 and loss is: 0.0027194726280868053\n",
      "Iteration is: 3666 and loss is: 0.0027438560500741005\n",
      "Iteration is: 3667 and loss is: 0.0027623334899544716\n",
      "Iteration is: 3668 and loss is: 0.00276361545547843\n",
      "Iteration is: 3669 and loss is: 0.002750895917415619\n",
      "Iteration is: 3670 and loss is: 0.002733487170189619\n",
      "Iteration is: 3671 and loss is: 0.0027271676808595657\n",
      "Iteration is: 3672 and loss is: 0.0027416436932981014\n",
      "Iteration is: 3673 and loss is: 0.0027835555374622345\n",
      "Iteration is: 3674 and loss is: 0.002843138761818409\n",
      "Iteration is: 3675 and loss is: 0.002919105114415288\n",
      "Iteration is: 3676 and loss is: 0.0029803826473653316\n",
      "Iteration is: 3677 and loss is: 0.0030358205549418926\n",
      "Iteration is: 3678 and loss is: 0.0030475538223981857\n",
      "Iteration is: 3679 and loss is: 0.003046204335987568\n",
      "Iteration is: 3680 and loss is: 0.0030099372379481792\n",
      "Iteration is: 3681 and loss is: 0.0029767677187919617\n",
      "Iteration is: 3682 and loss is: 0.002940443344414234\n",
      "Iteration is: 3683 and loss is: 0.0029187100008130074\n",
      "Iteration is: 3684 and loss is: 0.002901118714362383\n",
      "Iteration is: 3685 and loss is: 0.002885819412767887\n",
      "Iteration is: 3686 and loss is: 0.0028602415695786476\n",
      "Iteration is: 3687 and loss is: 0.0028263521380722523\n",
      "Iteration is: 3688 and loss is: 0.002783779986202717\n",
      "Iteration is: 3689 and loss is: 0.002743931021541357\n",
      "Iteration is: 3690 and loss is: 0.0027130418457090855\n",
      "Iteration is: 3691 and loss is: 0.002698346506804228\n",
      "Iteration is: 3692 and loss is: 0.0026989446487277746\n",
      "Iteration is: 3693 and loss is: 0.0027139494195580482\n",
      "Iteration is: 3694 and loss is: 0.002735298126935959\n",
      "Iteration is: 3695 and loss is: 0.0027633951976895332\n",
      "Iteration is: 3696 and loss is: 0.0027894717641174793\n",
      "Iteration is: 3697 and loss is: 0.002822964685037732\n",
      "Iteration is: 3698 and loss is: 0.0028551346622407436\n",
      "Iteration is: 3699 and loss is: 0.0029039657674729824\n",
      "Iteration is: 3700 and loss is: 0.002955489791929722\n",
      "Iteration is: 3701 and loss is: 0.0030315339099615812\n",
      "Iteration is: 3702 and loss is: 0.003110155463218689\n",
      "Iteration is: 3703 and loss is: 0.0032146843150258064\n",
      "Iteration is: 3704 and loss is: 0.003319656476378441\n",
      "Iteration is: 3705 and loss is: 0.0034486635122448206\n",
      "Iteration is: 3706 and loss is: 0.0035714181140065193\n",
      "Iteration is: 3707 and loss is: 0.0037080068141222\n",
      "Iteration is: 3708 and loss is: 0.0038097789511084557\n",
      "Iteration is: 3709 and loss is: 0.0038986760191619396\n",
      "Iteration is: 3710 and loss is: 0.003908080514520407\n",
      "Iteration is: 3711 and loss is: 0.0038808726239949465\n",
      "Iteration is: 3712 and loss is: 0.0037588919512927532\n",
      "Iteration is: 3713 and loss is: 0.003605687990784645\n",
      "Iteration is: 3714 and loss is: 0.0033891452476382256\n",
      "Iteration is: 3715 and loss is: 0.0031714546494185925\n",
      "Iteration is: 3716 and loss is: 0.002950575202703476\n",
      "Iteration is: 3717 and loss is: 0.0027764583937823772\n",
      "Iteration is: 3718 and loss is: 0.0026612216606736183\n",
      "Iteration is: 3719 and loss is: 0.0026184488087892532\n",
      "Iteration is: 3720 and loss is: 0.002637847326695919\n",
      "Iteration is: 3721 and loss is: 0.0026987637393176556\n",
      "Iteration is: 3722 and loss is: 0.0027802882250398397\n",
      "Iteration is: 3723 and loss is: 0.0028618848882615566\n",
      "Iteration is: 3724 and loss is: 0.0029410822317004204\n",
      "Iteration is: 3725 and loss is: 0.0029907526914030313\n",
      "Iteration is: 3726 and loss is: 0.0030195496510714293\n",
      "Iteration is: 3727 and loss is: 0.0029901869129389524\n",
      "Iteration is: 3728 and loss is: 0.0029270602390170097\n",
      "Iteration is: 3729 and loss is: 0.0028249092865735292\n",
      "Iteration is: 3730 and loss is: 0.0027246330864727497\n",
      "Iteration is: 3731 and loss is: 0.002643865067511797\n",
      "Iteration is: 3732 and loss is: 0.00259916833601892\n",
      "Iteration is: 3733 and loss is: 0.002589293522760272\n",
      "Iteration is: 3734 and loss is: 0.0026052913162857294\n",
      "Iteration is: 3735 and loss is: 0.0026365951634943485\n",
      "Iteration is: 3736 and loss is: 0.0026730233803391457\n",
      "Iteration is: 3737 and loss is: 0.0027107384521514177\n",
      "Iteration is: 3738 and loss is: 0.0027391710318624973\n",
      "Iteration is: 3739 and loss is: 0.00276041473262012\n",
      "Iteration is: 3740 and loss is: 0.0027648387476801872\n",
      "Iteration is: 3741 and loss is: 0.0027607909869402647\n",
      "Iteration is: 3742 and loss is: 0.0027457058895379305\n",
      "Iteration is: 3743 and loss is: 0.002729082014411688\n",
      "Iteration is: 3744 and loss is: 0.002709838328883052\n",
      "Iteration is: 3745 and loss is: 0.002692020032554865\n",
      "Iteration is: 3746 and loss is: 0.00267310393974185\n",
      "Iteration is: 3747 and loss is: 0.002654828131198883\n",
      "Iteration is: 3748 and loss is: 0.002636335790157318\n",
      "Iteration is: 3749 and loss is: 0.0026196176186203957\n",
      "Iteration is: 3750 and loss is: 0.002604856388643384\n",
      "Iteration is: 3751 and loss is: 0.002593031618744135\n",
      "Iteration is: 3752 and loss is: 0.002583814086392522\n",
      "Iteration is: 3753 and loss is: 0.0025772317312657833\n",
      "Iteration is: 3754 and loss is: 0.0025726899039000273\n",
      "Iteration is: 3755 and loss is: 0.002569991396740079\n",
      "Iteration is: 3756 and loss is: 0.0025684679858386517\n",
      "Iteration is: 3757 and loss is: 0.0025679469108581543\n",
      "Iteration is: 3758 and loss is: 0.0025679110549390316\n",
      "Iteration is: 3759 and loss is: 0.002568596974015236\n",
      "Iteration is: 3760 and loss is: 0.0025698463432490826\n",
      "Iteration is: 3761 and loss is: 0.00257238931953907\n",
      "Iteration is: 3762 and loss is: 0.0025762917939573526\n",
      "Iteration is: 3763 and loss is: 0.002583034336566925\n",
      "Iteration is: 3764 and loss is: 0.002593067940324545\n",
      "Iteration is: 3765 and loss is: 0.0026096354704350233\n",
      "Iteration is: 3766 and loss is: 0.002634104574099183\n",
      "Iteration is: 3767 and loss is: 0.0026740594767034054\n",
      "Iteration is: 3768 and loss is: 0.0027323083486407995\n",
      "Iteration is: 3769 and loss is: 0.0028266888111829758\n",
      "Iteration is: 3770 and loss is: 0.0029592246282845736\n",
      "Iteration is: 3771 and loss is: 0.003168264636769891\n",
      "Iteration is: 3772 and loss is: 0.003438920248299837\n",
      "Iteration is: 3773 and loss is: 0.003837547730654478\n",
      "Iteration is: 3774 and loss is: 0.004296594765037298\n",
      "Iteration is: 3775 and loss is: 0.004899495281279087\n",
      "Iteration is: 3776 and loss is: 0.00553971016779542\n",
      "Iteration is: 3777 and loss is: 0.006253219675272703\n",
      "Iteration is: 3778 and loss is: 0.006825854070484638\n",
      "Iteration is: 3779 and loss is: 0.007153262384235859\n",
      "Iteration is: 3780 and loss is: 0.006646323949098587\n",
      "Iteration is: 3781 and loss is: 0.005527494475245476\n",
      "Iteration is: 3782 and loss is: 0.003929342608898878\n",
      "Iteration is: 3783 and loss is: 0.0027876566164195538\n",
      "Iteration is: 3784 and loss is: 0.002596154110506177\n",
      "Iteration is: 3785 and loss is: 0.003213073592633009\n",
      "Iteration is: 3786 and loss is: 0.003985255025327206\n",
      "Iteration is: 3787 and loss is: 0.004198798444122076\n",
      "Iteration is: 3788 and loss is: 0.003786190878599882\n",
      "Iteration is: 3789 and loss is: 0.003068090882152319\n",
      "Iteration is: 3790 and loss is: 0.0026540346443653107\n",
      "Iteration is: 3791 and loss is: 0.0027322187088429928\n",
      "Iteration is: 3792 and loss is: 0.0030526542104780674\n",
      "Iteration is: 3793 and loss is: 0.0032289032824337482\n",
      "Iteration is: 3794 and loss is: 0.0030822944827377796\n",
      "Iteration is: 3795 and loss is: 0.0028364320751279593\n",
      "Iteration is: 3796 and loss is: 0.0027286093682050705\n",
      "Iteration is: 3797 and loss is: 0.0028213439509272575\n",
      "Iteration is: 3798 and loss is: 0.002931911963969469\n",
      "Iteration is: 3799 and loss is: 0.002892537275329232\n",
      "Iteration is: 3800 and loss is: 0.0027216030284762383\n",
      "Iteration is: 3801 and loss is: 0.002573505975306034\n",
      "Iteration is: 3802 and loss is: 0.002579348161816597\n",
      "Iteration is: 3803 and loss is: 0.002701279241591692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 3804 and loss is: 0.0027929237112402916\n",
      "Iteration is: 3805 and loss is: 0.0027533266693353653\n",
      "Iteration is: 3806 and loss is: 0.00262448750436306\n",
      "Iteration is: 3807 and loss is: 0.0025269105099141598\n",
      "Iteration is: 3808 and loss is: 0.002528982236981392\n",
      "Iteration is: 3809 and loss is: 0.0025939950719475746\n",
      "Iteration is: 3810 and loss is: 0.002639384474605322\n",
      "Iteration is: 3811 and loss is: 0.0026209417264908552\n",
      "Iteration is: 3812 and loss is: 0.002566725481301546\n",
      "Iteration is: 3813 and loss is: 0.002532115438953042\n",
      "Iteration is: 3814 and loss is: 0.002539583947509527\n",
      "Iteration is: 3815 and loss is: 0.0025649764575064182\n",
      "Iteration is: 3816 and loss is: 0.002569496165961027\n",
      "Iteration is: 3817 and loss is: 0.002544086193665862\n",
      "Iteration is: 3818 and loss is: 0.002510783728212118\n",
      "Iteration is: 3819 and loss is: 0.002497671637684107\n",
      "Iteration is: 3820 and loss is: 0.002510288031771779\n",
      "Iteration is: 3821 and loss is: 0.0025308013428002596\n",
      "Iteration is: 3822 and loss is: 0.0025380635634064674\n",
      "Iteration is: 3823 and loss is: 0.0025254525244235992\n",
      "Iteration is: 3824 and loss is: 0.0025044996291399\n",
      "Iteration is: 3825 and loss is: 0.0024906250182539225\n",
      "Iteration is: 3826 and loss is: 0.002490237820893526\n",
      "Iteration is: 3827 and loss is: 0.0024974369443953037\n",
      "Iteration is: 3828 and loss is: 0.0025019929744303226\n",
      "Iteration is: 3829 and loss is: 0.0024988562799990177\n",
      "Iteration is: 3830 and loss is: 0.0024907104671001434\n",
      "Iteration is: 3831 and loss is: 0.0024841339327394962\n",
      "Iteration is: 3832 and loss is: 0.0024830701295286417\n",
      "Iteration is: 3833 and loss is: 0.0024862061254680157\n",
      "Iteration is: 3834 and loss is: 0.0024888324551284313\n",
      "Iteration is: 3835 and loss is: 0.0024873861111700535\n",
      "Iteration is: 3836 and loss is: 0.0024819006212055683\n",
      "Iteration is: 3837 and loss is: 0.0024753818288445473\n",
      "Iteration is: 3838 and loss is: 0.0024709913413971663\n",
      "Iteration is: 3839 and loss is: 0.0024698670022189617\n",
      "Iteration is: 3840 and loss is: 0.0024707382544875145\n",
      "Iteration is: 3841 and loss is: 0.002471347339451313\n",
      "Iteration is: 3842 and loss is: 0.002470211125910282\n",
      "Iteration is: 3843 and loss is: 0.0024674858432263136\n",
      "Iteration is: 3844 and loss is: 0.0024644373916089535\n",
      "Iteration is: 3845 and loss is: 0.0024623521603643894\n",
      "Iteration is: 3846 and loss is: 0.0024616727605462074\n",
      "Iteration is: 3847 and loss is: 0.0024618618190288544\n",
      "Iteration is: 3848 and loss is: 0.0024619721807539463\n",
      "Iteration is: 3849 and loss is: 0.0024613067507743835\n",
      "Iteration is: 3850 and loss is: 0.0024598229210823774\n",
      "Iteration is: 3851 and loss is: 0.0024580080062150955\n",
      "Iteration is: 3852 and loss is: 0.0024565004277974367\n",
      "Iteration is: 3853 and loss is: 0.0024556401185691357\n",
      "Iteration is: 3854 and loss is: 0.002455473877489567\n",
      "Iteration is: 3855 and loss is: 0.00245568435639143\n",
      "Iteration is: 3856 and loss is: 0.0024560389574617147\n",
      "Iteration is: 3857 and loss is: 0.0024563977494835854\n",
      "Iteration is: 3858 and loss is: 0.002457090886309743\n",
      "Iteration is: 3859 and loss is: 0.00245843268930912\n",
      "Iteration is: 3860 and loss is: 0.002461234573274851\n",
      "Iteration is: 3861 and loss is: 0.002465892117470503\n",
      "Iteration is: 3862 and loss is: 0.0024736705236136913\n",
      "Iteration is: 3863 and loss is: 0.002485028002411127\n",
      "Iteration is: 3864 and loss is: 0.002502777613699436\n",
      "Iteration is: 3865 and loss is: 0.002527836011722684\n",
      "Iteration is: 3866 and loss is: 0.0025672223418951035\n",
      "Iteration is: 3867 and loss is: 0.00262144161388278\n",
      "Iteration is: 3868 and loss is: 0.00270663108676672\n",
      "Iteration is: 3869 and loss is: 0.0028156619518995285\n",
      "Iteration is: 3870 and loss is: 0.0029798115137964487\n",
      "Iteration is: 3871 and loss is: 0.0031570582650601864\n",
      "Iteration is: 3872 and loss is: 0.0033911613281816244\n",
      "Iteration is: 3873 and loss is: 0.0035620226990431547\n",
      "Iteration is: 3874 and loss is: 0.00371996546164155\n",
      "Iteration is: 3875 and loss is: 0.0037245438434183598\n",
      "Iteration is: 3876 and loss is: 0.00365828862413764\n",
      "Iteration is: 3877 and loss is: 0.0034978780895471573\n",
      "Iteration is: 3878 and loss is: 0.003315065521746874\n",
      "Iteration is: 3879 and loss is: 0.00312252645380795\n",
      "Iteration is: 3880 and loss is: 0.002935880795121193\n",
      "Iteration is: 3881 and loss is: 0.0027516402769833803\n",
      "Iteration is: 3882 and loss is: 0.002596744569018483\n",
      "Iteration is: 3883 and loss is: 0.0024986835196614265\n",
      "Iteration is: 3884 and loss is: 0.0024847807362675667\n",
      "Iteration is: 3885 and loss is: 0.002555335406213999\n",
      "Iteration is: 3886 and loss is: 0.0026779891923069954\n",
      "Iteration is: 3887 and loss is: 0.002815856132656336\n",
      "Iteration is: 3888 and loss is: 0.0029204958118498325\n",
      "Iteration is: 3889 and loss is: 0.002979849698022008\n",
      "Iteration is: 3890 and loss is: 0.0029739257879555225\n",
      "Iteration is: 3891 and loss is: 0.002934938296675682\n",
      "Iteration is: 3892 and loss is: 0.0028613123577088118\n",
      "Iteration is: 3893 and loss is: 0.002793781226500869\n",
      "Iteration is: 3894 and loss is: 0.0027160989120602608\n",
      "Iteration is: 3895 and loss is: 0.002647166606038809\n",
      "Iteration is: 3896 and loss is: 0.0025704572908580303\n",
      "Iteration is: 3897 and loss is: 0.0025047757662832737\n",
      "Iteration is: 3898 and loss is: 0.0024577854201197624\n",
      "Iteration is: 3899 and loss is: 0.002441156655550003\n",
      "Iteration is: 3900 and loss is: 0.0024545895867049694\n",
      "Iteration is: 3901 and loss is: 0.002488587284460664\n",
      "Iteration is: 3902 and loss is: 0.0025313743390142918\n",
      "Iteration is: 3903 and loss is: 0.00256905285641551\n",
      "Iteration is: 3904 and loss is: 0.002600177191197872\n",
      "Iteration is: 3905 and loss is: 0.0026104766875505447\n",
      "Iteration is: 3906 and loss is: 0.0026091698091477156\n",
      "Iteration is: 3907 and loss is: 0.002583907451480627\n",
      "Iteration is: 3908 and loss is: 0.0025497465394437313\n",
      "Iteration is: 3909 and loss is: 0.0025055892765522003\n",
      "Iteration is: 3910 and loss is: 0.002465893980115652\n",
      "Iteration is: 3911 and loss is: 0.002434380818158388\n",
      "Iteration is: 3912 and loss is: 0.0024149203673005104\n",
      "Iteration is: 3913 and loss is: 0.0024056797847151756\n",
      "Iteration is: 3914 and loss is: 0.0024032294750213623\n",
      "Iteration is: 3915 and loss is: 0.002404044847935438\n",
      "Iteration is: 3916 and loss is: 0.002405767096206546\n",
      "Iteration is: 3917 and loss is: 0.002407312858849764\n",
      "Iteration is: 3918 and loss is: 0.0024081245064735413\n",
      "Iteration is: 3919 and loss is: 0.002408880041912198\n",
      "Iteration is: 3920 and loss is: 0.002409715671092272\n",
      "Iteration is: 3921 and loss is: 0.0024117897264659405\n",
      "Iteration is: 3922 and loss is: 0.0024152970872819424\n",
      "Iteration is: 3923 and loss is: 0.0024204878136515617\n",
      "Iteration is: 3924 and loss is: 0.002426775870844722\n",
      "Iteration is: 3925 and loss is: 0.0024335714988410473\n",
      "Iteration is: 3926 and loss is: 0.0024402854032814503\n",
      "Iteration is: 3927 and loss is: 0.002447080332785845\n",
      "Iteration is: 3928 and loss is: 0.0024544023908674717\n",
      "Iteration is: 3929 and loss is: 0.002463518176227808\n",
      "Iteration is: 3930 and loss is: 0.002475668676197529\n",
      "Iteration is: 3931 and loss is: 0.0024926059413701296\n",
      "Iteration is: 3932 and loss is: 0.0025160659570246935\n",
      "Iteration is: 3933 and loss is: 0.0025486177764832973\n",
      "Iteration is: 3934 and loss is: 0.0025929720140993595\n",
      "Iteration is: 3935 and loss is: 0.002653983887284994\n",
      "Iteration is: 3936 and loss is: 0.0027366257272660732\n",
      "Iteration is: 3937 and loss is: 0.002850180258974433\n",
      "Iteration is: 3938 and loss is: 0.0030037357937544584\n",
      "Iteration is: 3939 and loss is: 0.0032120519317686558\n",
      "Iteration is: 3940 and loss is: 0.0034889234229922295\n",
      "Iteration is: 3941 and loss is: 0.0038471906445920467\n",
      "Iteration is: 3942 and loss is: 0.004289231728762388\n",
      "Iteration is: 3943 and loss is: 0.004795603454113007\n",
      "Iteration is: 3944 and loss is: 0.005288015119731426\n",
      "Iteration is: 3945 and loss is: 0.005681569688022137\n",
      "Iteration is: 3946 and loss is: 0.0057516638189554214\n",
      "Iteration is: 3947 and loss is: 0.005445769056677818\n",
      "Iteration is: 3948 and loss is: 0.0046680080704391\n",
      "Iteration is: 3949 and loss is: 0.0036789739970117807\n",
      "Iteration is: 3950 and loss is: 0.002807739656418562\n",
      "Iteration is: 3951 and loss is: 0.002395390998572111\n",
      "Iteration is: 3952 and loss is: 0.0025148140266537666\n",
      "Iteration is: 3953 and loss is: 0.0029533165507018566\n",
      "Iteration is: 3954 and loss is: 0.0033535254187881947\n",
      "Iteration is: 3955 and loss is: 0.003436347935348749\n",
      "Iteration is: 3956 and loss is: 0.0031834300607442856\n",
      "Iteration is: 3957 and loss is: 0.0027567208744585514\n",
      "Iteration is: 3958 and loss is: 0.00244120298884809\n",
      "Iteration is: 3959 and loss is: 0.0023799939081072807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 3960 and loss is: 0.002536305459216237\n",
      "Iteration is: 3961 and loss is: 0.002739986404776573\n",
      "Iteration is: 3962 and loss is: 0.002820519730448723\n",
      "Iteration is: 3963 and loss is: 0.0027361558750271797\n",
      "Iteration is: 3964 and loss is: 0.0025540252681821585\n",
      "Iteration is: 3965 and loss is: 0.002411074936389923\n",
      "Iteration is: 3966 and loss is: 0.002379856538027525\n",
      "Iteration is: 3967 and loss is: 0.0024451788049191236\n",
      "Iteration is: 3968 and loss is: 0.002528248354792595\n",
      "Iteration is: 3969 and loss is: 0.002553537953644991\n",
      "Iteration is: 3970 and loss is: 0.0025064428336918354\n",
      "Iteration is: 3971 and loss is: 0.002424485282972455\n",
      "Iteration is: 3972 and loss is: 0.0023659714497625828\n",
      "Iteration is: 3973 and loss is: 0.0023597748950123787\n",
      "Iteration is: 3974 and loss is: 0.0023942883126437664\n",
      "Iteration is: 3975 and loss is: 0.002432742156088352\n",
      "Iteration is: 3976 and loss is: 0.0024418968241661787\n",
      "Iteration is: 3977 and loss is: 0.0024162502959370613\n",
      "Iteration is: 3978 and loss is: 0.0023736811708658934\n",
      "Iteration is: 3979 and loss is: 0.0023415102623403072\n",
      "Iteration is: 3980 and loss is: 0.002334860386326909\n",
      "Iteration is: 3981 and loss is: 0.002350252354517579\n",
      "Iteration is: 3982 and loss is: 0.002371438778936863\n",
      "Iteration is: 3983 and loss is: 0.002381614875048399\n",
      "Iteration is: 3984 and loss is: 0.0023745200596749783\n",
      "Iteration is: 3985 and loss is: 0.002354524563997984\n",
      "Iteration is: 3986 and loss is: 0.0023331979755312204\n",
      "Iteration is: 3987 and loss is: 0.0023203114978969097\n",
      "Iteration is: 3988 and loss is: 0.002319386461749673\n",
      "Iteration is: 3989 and loss is: 0.0023271464742720127\n",
      "Iteration is: 3990 and loss is: 0.002336683915928006\n",
      "Iteration is: 3991 and loss is: 0.0023418792989104986\n",
      "Iteration is: 3992 and loss is: 0.0023398585617542267\n",
      "Iteration is: 3993 and loss is: 0.002332029165700078\n",
      "Iteration is: 3994 and loss is: 0.0023218016140162945\n",
      "Iteration is: 3995 and loss is: 0.0023130811750888824\n",
      "Iteration is: 3996 and loss is: 0.002308237599208951\n",
      "Iteration is: 3997 and loss is: 0.002307542599737644\n",
      "Iteration is: 3998 and loss is: 0.0023096315562725067\n",
      "Iteration is: 3999 and loss is: 0.002312422264367342\n",
      "Iteration is: 4000 and loss is: 0.0023141333367675543\n",
      "Iteration is: 4001 and loss is: 0.0023137880489230156\n",
      "Iteration is: 4002 and loss is: 0.0023114096838980913\n",
      "Iteration is: 4003 and loss is: 0.002307635499164462\n",
      "Iteration is: 4004 and loss is: 0.0023034382611513138\n",
      "Iteration is: 4005 and loss is: 0.0022996319457888603\n",
      "Iteration is: 4006 and loss is: 0.002296741120517254\n",
      "Iteration is: 4007 and loss is: 0.002294899895787239\n",
      "Iteration is: 4008 and loss is: 0.0022939445916563272\n",
      "Iteration is: 4009 and loss is: 0.0022935629822313786\n",
      "Iteration is: 4010 and loss is: 0.0022933960426598787\n",
      "Iteration is: 4011 and loss is: 0.00229317182675004\n",
      "Iteration is: 4012 and loss is: 0.002292692894116044\n",
      "Iteration is: 4013 and loss is: 0.002291902434080839\n",
      "Iteration is: 4014 and loss is: 0.0022907881066203117\n",
      "Iteration is: 4015 and loss is: 0.0022894153371453285\n",
      "Iteration is: 4016 and loss is: 0.002287843730300665\n",
      "Iteration is: 4017 and loss is: 0.0022861589677631855\n",
      "Iteration is: 4018 and loss is: 0.002284399699419737\n",
      "Iteration is: 4019 and loss is: 0.0022826334461569786\n",
      "Iteration is: 4020 and loss is: 0.002280890243127942\n",
      "Iteration is: 4021 and loss is: 0.0022792008239775896\n",
      "Iteration is: 4022 and loss is: 0.0022775931283831596\n",
      "Iteration is: 4023 and loss is: 0.0022760864812880754\n",
      "Iteration is: 4024 and loss is: 0.002274709288030863\n",
      "Iteration is: 4025 and loss is: 0.0022735106758773327\n",
      "Iteration is: 4026 and loss is: 0.0022725712042301893\n",
      "Iteration is: 4027 and loss is: 0.0022719944827258587\n",
      "Iteration is: 4028 and loss is: 0.002271973993629217\n",
      "Iteration is: 4029 and loss is: 0.0022728319745510817\n",
      "Iteration is: 4030 and loss is: 0.0022751109208911657\n",
      "Iteration is: 4031 and loss is: 0.0022796769626438618\n",
      "Iteration is: 4032 and loss is: 0.0022881454788148403\n",
      "Iteration is: 4033 and loss is: 0.002303085522726178\n",
      "Iteration is: 4034 and loss is: 0.002329450100660324\n",
      "Iteration is: 4035 and loss is: 0.002375085838139057\n",
      "Iteration is: 4036 and loss is: 0.002455766312777996\n",
      "Iteration is: 4037 and loss is: 0.0025951170828193426\n",
      "Iteration is: 4038 and loss is: 0.002842481015250087\n",
      "Iteration is: 4039 and loss is: 0.0032597421668469906\n",
      "Iteration is: 4040 and loss is: 0.003974966239184141\n",
      "Iteration is: 4041 and loss is: 0.005064611788839102\n",
      "Iteration is: 4042 and loss is: 0.006640321109443903\n",
      "Iteration is: 4043 and loss is: 0.008473582565784454\n",
      "Iteration is: 4044 and loss is: 0.009992669336497784\n",
      "Iteration is: 4045 and loss is: 0.011053172871470451\n",
      "Iteration is: 4046 and loss is: 0.010578086599707603\n",
      "Iteration is: 4047 and loss is: 0.008495798334479332\n",
      "Iteration is: 4048 and loss is: 0.005537053570151329\n",
      "Iteration is: 4049 and loss is: 0.003457888960838318\n",
      "Iteration is: 4050 and loss is: 0.0034964163787662983\n",
      "Iteration is: 4051 and loss is: 0.005093721207231283\n",
      "Iteration is: 4052 and loss is: 0.005803355015814304\n",
      "Iteration is: 4053 and loss is: 0.004586152732372284\n",
      "Iteration is: 4054 and loss is: 0.003038731636479497\n",
      "Iteration is: 4055 and loss is: 0.0029167442116886377\n",
      "Iteration is: 4056 and loss is: 0.003799484111368656\n",
      "Iteration is: 4057 and loss is: 0.0040324353612959385\n",
      "Iteration is: 4058 and loss is: 0.0032085473649203777\n",
      "Iteration is: 4059 and loss is: 0.0025528003461658955\n",
      "Iteration is: 4060 and loss is: 0.002931741066277027\n",
      "Iteration is: 4061 and loss is: 0.003455002326518297\n",
      "Iteration is: 4062 and loss is: 0.0031819059513509274\n",
      "Iteration is: 4063 and loss is: 0.0025173043832182884\n",
      "Iteration is: 4064 and loss is: 0.0025644854176789522\n",
      "Iteration is: 4065 and loss is: 0.0029864462558180094\n",
      "Iteration is: 4066 and loss is: 0.0029372242279350758\n",
      "Iteration is: 4067 and loss is: 0.002478598617017269\n",
      "Iteration is: 4068 and loss is: 0.0023868500720709562\n",
      "Iteration is: 4069 and loss is: 0.002711773384362459\n",
      "Iteration is: 4070 and loss is: 0.0027951966039836407\n",
      "Iteration is: 4071 and loss is: 0.0024828987661749125\n",
      "Iteration is: 4072 and loss is: 0.002278390573337674\n",
      "Iteration is: 4073 and loss is: 0.0024718279018998146\n",
      "Iteration is: 4074 and loss is: 0.0026295012794435024\n",
      "Iteration is: 4075 and loss is: 0.0024801339022815228\n",
      "Iteration is: 4076 and loss is: 0.0022742568980902433\n",
      "Iteration is: 4077 and loss is: 0.002335008466616273\n",
      "Iteration is: 4078 and loss is: 0.002487731631845236\n",
      "Iteration is: 4079 and loss is: 0.002442943397909403\n",
      "Iteration is: 4080 and loss is: 0.0022929618135094643\n",
      "Iteration is: 4081 and loss is: 0.0022705099545419216\n",
      "Iteration is: 4082 and loss is: 0.0023754690773785114\n",
      "Iteration is: 4083 and loss is: 0.0023914691992104053\n",
      "Iteration is: 4084 and loss is: 0.0023038324434310198\n",
      "Iteration is: 4085 and loss is: 0.00225011445581913\n",
      "Iteration is: 4086 and loss is: 0.0023010186851024628\n",
      "Iteration is: 4087 and loss is: 0.002338724210858345\n",
      "Iteration is: 4088 and loss is: 0.002299027983099222\n",
      "Iteration is: 4089 and loss is: 0.0022476559970527887\n",
      "Iteration is: 4090 and loss is: 0.002256513573229313\n",
      "Iteration is: 4091 and loss is: 0.0022935108281672\n",
      "Iteration is: 4092 and loss is: 0.002286207629367709\n",
      "Iteration is: 4093 and loss is: 0.0022501195780932903\n",
      "Iteration is: 4094 and loss is: 0.0022342565935105085\n",
      "Iteration is: 4095 and loss is: 0.0022560390643775463\n",
      "Iteration is: 4096 and loss is: 0.0022665453143417835\n",
      "Iteration is: 4097 and loss is: 0.002250073943287134\n",
      "Iteration is: 4098 and loss is: 0.002227235585451126\n",
      "Iteration is: 4099 and loss is: 0.0022298982366919518\n",
      "Iteration is: 4100 and loss is: 0.0022437255829572678\n",
      "Iteration is: 4101 and loss is: 0.0022441213950514793\n",
      "Iteration is: 4102 and loss is: 0.002228452591225505\n",
      "Iteration is: 4103 and loss is: 0.002217269968241453\n",
      "Iteration is: 4104 and loss is: 0.002222067676484585\n",
      "Iteration is: 4105 and loss is: 0.002229535486549139\n",
      "Iteration is: 4106 and loss is: 0.002227149670943618\n",
      "Iteration is: 4107 and loss is: 0.0022157514467835426\n",
      "Iteration is: 4108 and loss is: 0.0022099025081843138\n",
      "Iteration is: 4109 and loss is: 0.0022123639937490225\n",
      "Iteration is: 4110 and loss is: 0.002216834109276533\n",
      "Iteration is: 4111 and loss is: 0.0022141491062939167\n",
      "Iteration is: 4112 and loss is: 0.002207105280831456\n",
      "Iteration is: 4113 and loss is: 0.002202370436862111\n",
      "Iteration is: 4114 and loss is: 0.0022030812688171864\n",
      "Iteration is: 4115 and loss is: 0.0022051602136343718\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 4116 and loss is: 0.0022037888411432505\n",
      "Iteration is: 4117 and loss is: 0.002199323382228613\n",
      "Iteration is: 4118 and loss is: 0.002195125911384821\n",
      "Iteration is: 4119 and loss is: 0.0021940898150205612\n",
      "Iteration is: 4120 and loss is: 0.0021945436019450426\n",
      "Iteration is: 4121 and loss is: 0.0021941654849797487\n",
      "Iteration is: 4122 and loss is: 0.0021914001554250717\n",
      "Iteration is: 4123 and loss is: 0.002188042737543583\n",
      "Iteration is: 4124 and loss is: 0.0021855966188013554\n",
      "Iteration is: 4125 and loss is: 0.0021848129108548164\n",
      "Iteration is: 4126 and loss is: 0.0021842613350600004\n",
      "Iteration is: 4127 and loss is: 0.0021829530596733093\n",
      "Iteration is: 4128 and loss is: 0.002180529059842229\n",
      "Iteration is: 4129 and loss is: 0.00217795348726213\n",
      "Iteration is: 4130 and loss is: 0.0021759795490652323\n",
      "Iteration is: 4131 and loss is: 0.0021747464779764414\n",
      "Iteration is: 4132 and loss is: 0.0021737082861363888\n",
      "Iteration is: 4133 and loss is: 0.0021723017562180758\n",
      "Iteration is: 4134 and loss is: 0.00217043561860919\n",
      "Iteration is: 4135 and loss is: 0.0021683573722839355\n",
      "Iteration is: 4136 and loss is: 0.0021665114909410477\n",
      "Iteration is: 4137 and loss is: 0.002164966892451048\n",
      "Iteration is: 4138 and loss is: 0.002163686091080308\n",
      "Iteration is: 4139 and loss is: 0.002162379678338766\n",
      "Iteration is: 4140 and loss is: 0.002160959877073765\n",
      "Iteration is: 4141 and loss is: 0.002159364754334092\n",
      "Iteration is: 4142 and loss is: 0.0021577482111752033\n",
      "Iteration is: 4143 and loss is: 0.002156185684725642\n",
      "Iteration is: 4144 and loss is: 0.002154757734388113\n",
      "Iteration is: 4145 and loss is: 0.002153428504243493\n",
      "Iteration is: 4146 and loss is: 0.0021521830931305885\n",
      "Iteration is: 4147 and loss is: 0.002150921383872628\n",
      "Iteration is: 4148 and loss is: 0.002149636624380946\n",
      "Iteration is: 4149 and loss is: 0.002148295287042856\n",
      "Iteration is: 4150 and loss is: 0.0021469295024871826\n",
      "Iteration is: 4151 and loss is: 0.0021455816458910704\n",
      "Iteration is: 4152 and loss is: 0.002144285012036562\n",
      "Iteration is: 4153 and loss is: 0.0021430489141494036\n",
      "Iteration is: 4154 and loss is: 0.0021418642718344927\n",
      "Iteration is: 4155 and loss is: 0.0021407005842775106\n",
      "Iteration is: 4156 and loss is: 0.00213951850309968\n",
      "Iteration is: 4157 and loss is: 0.002138321055099368\n",
      "Iteration is: 4158 and loss is: 0.002137091476470232\n",
      "Iteration is: 4159 and loss is: 0.0021358621306717396\n",
      "Iteration is: 4160 and loss is: 0.002134638139978051\n",
      "Iteration is: 4161 and loss is: 0.002133446279913187\n",
      "Iteration is: 4162 and loss is: 0.0021322653628885746\n",
      "Iteration is: 4163 and loss is: 0.0021311130840331316\n",
      "Iteration is: 4164 and loss is: 0.0021299615036696196\n",
      "Iteration is: 4165 and loss is: 0.002128816908225417\n",
      "Iteration is: 4166 and loss is: 0.0021276664920151234\n",
      "Iteration is: 4167 and loss is: 0.002126519801095128\n",
      "Iteration is: 4168 and loss is: 0.00212537357583642\n",
      "Iteration is: 4169 and loss is: 0.0021242310758680105\n",
      "Iteration is: 4170 and loss is: 0.00212308531627059\n",
      "Iteration is: 4171 and loss is: 0.0021219428163021803\n",
      "Iteration is: 4172 and loss is: 0.0021208024118095636\n",
      "Iteration is: 4173 and loss is: 0.002119664568454027\n",
      "Iteration is: 4174 and loss is: 0.0021185376681387424\n",
      "Iteration is: 4175 and loss is: 0.002117415890097618\n",
      "Iteration is: 4176 and loss is: 0.002116304822266102\n",
      "Iteration is: 4177 and loss is: 0.0021151930559426546\n",
      "Iteration is: 4178 and loss is: 0.0021140938624739647\n",
      "Iteration is: 4179 and loss is: 0.002112993039190769\n",
      "Iteration is: 4180 and loss is: 0.002111888723447919\n",
      "Iteration is: 4181 and loss is: 0.0021107951179146767\n",
      "Iteration is: 4182 and loss is: 0.0021097033750265837\n",
      "Iteration is: 4183 and loss is: 0.002108610700815916\n",
      "Iteration is: 4184 and loss is: 0.0021075240802019835\n",
      "Iteration is: 4185 and loss is: 0.0021064416505396366\n",
      "Iteration is: 4186 and loss is: 0.0021053545642644167\n",
      "Iteration is: 4187 and loss is: 0.0021042802836745977\n",
      "Iteration is: 4188 and loss is: 0.002103201113641262\n",
      "Iteration is: 4189 and loss is: 0.002102123573422432\n",
      "Iteration is: 4190 and loss is: 0.0021010497584939003\n",
      "Iteration is: 4191 and loss is: 0.0020999829284846783\n",
      "Iteration is: 4192 and loss is: 0.002098917728289962\n",
      "Iteration is: 4193 and loss is: 0.002097854856401682\n",
      "Iteration is: 4194 and loss is: 0.0020967964082956314\n",
      "Iteration is: 4195 and loss is: 0.002095745177939534\n",
      "Iteration is: 4196 and loss is: 0.00209469860419631\n",
      "Iteration is: 4197 and loss is: 0.0020936608780175447\n",
      "Iteration is: 4198 and loss is: 0.0020926313009113073\n",
      "Iteration is: 4199 and loss is: 0.0020916229113936424\n",
      "Iteration is: 4200 and loss is: 0.0020906315185129642\n",
      "Iteration is: 4201 and loss is: 0.0020896715577691793\n",
      "Iteration is: 4202 and loss is: 0.002088759792968631\n",
      "Iteration is: 4203 and loss is: 0.0020879150833934546\n",
      "Iteration is: 4204 and loss is: 0.002087176311761141\n",
      "Iteration is: 4205 and loss is: 0.002086596330627799\n",
      "Iteration is: 4206 and loss is: 0.0020862752571702003\n",
      "Iteration is: 4207 and loss is: 0.0020863572135567665\n",
      "Iteration is: 4208 and loss is: 0.0020870771259069443\n",
      "Iteration is: 4209 and loss is: 0.002088855253532529\n",
      "Iteration is: 4210 and loss is: 0.0020923297852277756\n",
      "Iteration is: 4211 and loss is: 0.00209867674857378\n",
      "Iteration is: 4212 and loss is: 0.0021096442360430956\n",
      "Iteration is: 4213 and loss is: 0.002128725638613105\n",
      "Iteration is: 4214 and loss is: 0.0021607372909784317\n",
      "Iteration is: 4215 and loss is: 0.0022162413224577904\n",
      "Iteration is: 4216 and loss is: 0.002308009658008814\n",
      "Iteration is: 4217 and loss is: 0.0024677005130797625\n",
      "Iteration is: 4218 and loss is: 0.002723561367020011\n",
      "Iteration is: 4219 and loss is: 0.0031607705168426037\n",
      "Iteration is: 4220 and loss is: 0.0038010282441973686\n",
      "Iteration is: 4221 and loss is: 0.004791642539203167\n",
      "Iteration is: 4222 and loss is: 0.005961516406387091\n",
      "Iteration is: 4223 and loss is: 0.007267950568348169\n",
      "Iteration is: 4224 and loss is: 0.008256050758063793\n",
      "Iteration is: 4225 and loss is: 0.008464007638394833\n",
      "Iteration is: 4226 and loss is: 0.007629189174622297\n",
      "Iteration is: 4227 and loss is: 0.005928919184952974\n",
      "Iteration is: 4228 and loss is: 0.003872828558087349\n",
      "Iteration is: 4229 and loss is: 0.00258022197522223\n",
      "Iteration is: 4230 and loss is: 0.002752519678324461\n",
      "Iteration is: 4231 and loss is: 0.003890415420755744\n",
      "Iteration is: 4232 and loss is: 0.004876835737377405\n",
      "Iteration is: 4233 and loss is: 0.004413161426782608\n",
      "Iteration is: 4234 and loss is: 0.003045211546123028\n",
      "Iteration is: 4235 and loss is: 0.002235847059637308\n",
      "Iteration is: 4236 and loss is: 0.002595961559563875\n",
      "Iteration is: 4237 and loss is: 0.0033830394968390465\n",
      "Iteration is: 4238 and loss is: 0.0035530366003513336\n",
      "Iteration is: 4239 and loss is: 0.003002847544848919\n",
      "Iteration is: 4240 and loss is: 0.0022623948752880096\n",
      "Iteration is: 4241 and loss is: 0.002142597921192646\n",
      "Iteration is: 4242 and loss is: 0.0025960379280149937\n",
      "Iteration is: 4243 and loss is: 0.0028954208828508854\n",
      "Iteration is: 4244 and loss is: 0.002636723220348358\n",
      "Iteration is: 4245 and loss is: 0.0021887309849262238\n",
      "Iteration is: 4246 and loss is: 0.0021187339443713427\n",
      "Iteration is: 4247 and loss is: 0.0023890151642262936\n",
      "Iteration is: 4248 and loss is: 0.0025584802497178316\n",
      "Iteration is: 4249 and loss is: 0.0024193343706429005\n",
      "Iteration is: 4250 and loss is: 0.0021776335779577494\n",
      "Iteration is: 4251 and loss is: 0.002115434966981411\n",
      "Iteration is: 4252 and loss is: 0.0022363965399563313\n",
      "Iteration is: 4253 and loss is: 0.0023160940036177635\n",
      "Iteration is: 4254 and loss is: 0.0022425316274166107\n",
      "Iteration is: 4255 and loss is: 0.0021215740125626326\n",
      "Iteration is: 4256 and loss is: 0.002104010432958603\n",
      "Iteration is: 4257 and loss is: 0.0021705999970436096\n",
      "Iteration is: 4258 and loss is: 0.002204186748713255\n",
      "Iteration is: 4259 and loss is: 0.002158279065042734\n",
      "Iteration is: 4260 and loss is: 0.0020958012901246548\n",
      "Iteration is: 4261 and loss is: 0.0020881998352706432\n",
      "Iteration is: 4262 and loss is: 0.0021244697272777557\n",
      "Iteration is: 4263 and loss is: 0.00214212853461504\n",
      "Iteration is: 4264 and loss is: 0.0021111788228154182\n",
      "Iteration is: 4265 and loss is: 0.0020682201720774174\n",
      "Iteration is: 4266 and loss is: 0.002060806378722191\n",
      "Iteration is: 4267 and loss is: 0.002086399355903268\n",
      "Iteration is: 4268 and loss is: 0.0021040469873696566\n",
      "Iteration is: 4269 and loss is: 0.0020885169506073\n",
      "Iteration is: 4270 and loss is: 0.002057583536952734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 4271 and loss is: 0.0020431019365787506\n",
      "Iteration is: 4272 and loss is: 0.002053767442703247\n",
      "Iteration is: 4273 and loss is: 0.0020710607059299946\n",
      "Iteration is: 4274 and loss is: 0.002072087721899152\n",
      "Iteration is: 4275 and loss is: 0.00205554673448205\n",
      "Iteration is: 4276 and loss is: 0.0020377179607748985\n",
      "Iteration is: 4277 and loss is: 0.002033933764323592\n",
      "Iteration is: 4278 and loss is: 0.002042352920398116\n",
      "Iteration is: 4279 and loss is: 0.0020501911640167236\n",
      "Iteration is: 4280 and loss is: 0.002048058435320854\n",
      "Iteration is: 4281 and loss is: 0.002037833444774151\n",
      "Iteration is: 4282 and loss is: 0.002028619172051549\n",
      "Iteration is: 4283 and loss is: 0.0020267919171601534\n",
      "Iteration is: 4284 and loss is: 0.0020306336227804422\n",
      "Iteration is: 4285 and loss is: 0.0020338031463325024\n",
      "Iteration is: 4286 and loss is: 0.0020319647155702114\n",
      "Iteration is: 4287 and loss is: 0.0020264489576220512\n",
      "Iteration is: 4288 and loss is: 0.002021307125687599\n",
      "Iteration is: 4289 and loss is: 0.002019608626142144\n",
      "Iteration is: 4290 and loss is: 0.002020767191424966\n",
      "Iteration is: 4291 and loss is: 0.0020219748839735985\n",
      "Iteration is: 4292 and loss is: 0.002020874759182334\n",
      "Iteration is: 4293 and loss is: 0.0020176577381789684\n",
      "Iteration is: 4294 and loss is: 0.002014246303588152\n",
      "Iteration is: 4295 and loss is: 0.002012360142543912\n",
      "Iteration is: 4296 and loss is: 0.00201218924485147\n",
      "Iteration is: 4297 and loss is: 0.0020126004237681627\n",
      "Iteration is: 4298 and loss is: 0.0020121869165450335\n",
      "Iteration is: 4299 and loss is: 0.002010466530919075\n",
      "Iteration is: 4300 and loss is: 0.002008022740483284\n",
      "Iteration is: 4301 and loss is: 0.002005896298214793\n",
      "Iteration is: 4302 and loss is: 0.002004658104851842\n",
      "Iteration is: 4303 and loss is: 0.002004212699830532\n",
      "Iteration is: 4304 and loss is: 0.0020039358641952276\n",
      "Iteration is: 4305 and loss is: 0.0020032294560223818\n",
      "Iteration is: 4306 and loss is: 0.0020019079092890024\n",
      "Iteration is: 4307 and loss is: 0.0020002496894448996\n",
      "Iteration is: 4308 and loss is: 0.0019986776169389486\n",
      "Iteration is: 4309 and loss is: 0.0019974743481725454\n",
      "Iteration is: 4310 and loss is: 0.0019966266117990017\n",
      "Iteration is: 4311 and loss is: 0.0019959250930696726\n",
      "Iteration is: 4312 and loss is: 0.0019951006397604942\n",
      "Iteration is: 4313 and loss is: 0.001994048710912466\n",
      "Iteration is: 4314 and loss is: 0.0019928188994526863\n",
      "Iteration is: 4315 and loss is: 0.0019915704615414143\n",
      "Iteration is: 4316 and loss is: 0.0019904274959117174\n",
      "Iteration is: 4317 and loss is: 0.0019894461147487164\n",
      "Iteration is: 4318 and loss is: 0.0019885667134076357\n",
      "Iteration is: 4319 and loss is: 0.0019876924343407154\n",
      "Iteration is: 4320 and loss is: 0.0019867480732500553\n",
      "Iteration is: 4321 and loss is: 0.001985701732337475\n",
      "Iteration is: 4322 and loss is: 0.0019845927599817514\n",
      "Iteration is: 4323 and loss is: 0.0019834786653518677\n",
      "Iteration is: 4324 and loss is: 0.001982414862141013\n",
      "Iteration is: 4325 and loss is: 0.001981423469260335\n",
      "Iteration is: 4326 and loss is: 0.001980486325919628\n",
      "Iteration is: 4327 and loss is: 0.0019795787520706654\n",
      "Iteration is: 4328 and loss is: 0.0019786502234637737\n",
      "Iteration is: 4329 and loss is: 0.001977692125365138\n",
      "Iteration is: 4330 and loss is: 0.001976697240024805\n",
      "Iteration is: 4331 and loss is: 0.001975681632757187\n",
      "Iteration is: 4332 and loss is: 0.001974665094166994\n",
      "Iteration is: 4333 and loss is: 0.001973670907318592\n",
      "Iteration is: 4334 and loss is: 0.0019726858008652925\n",
      "Iteration is: 4335 and loss is: 0.0019717346876859665\n",
      "Iteration is: 4336 and loss is: 0.001970788696780801\n",
      "Iteration is: 4337 and loss is: 0.001969846896827221\n",
      "Iteration is: 4338 and loss is: 0.0019688985776156187\n",
      "Iteration is: 4339 and loss is: 0.001967940479516983\n",
      "Iteration is: 4340 and loss is: 0.001966973999515176\n",
      "Iteration is: 4341 and loss is: 0.00196600123308599\n",
      "Iteration is: 4342 and loss is: 0.0019650286994874477\n",
      "Iteration is: 4343 and loss is: 0.0019640589598566294\n",
      "Iteration is: 4344 and loss is: 0.001963098533451557\n",
      "Iteration is: 4345 and loss is: 0.0019621485844254494\n",
      "Iteration is: 4346 and loss is: 0.001961208414286375\n",
      "Iteration is: 4347 and loss is: 0.001960268011316657\n",
      "Iteration is: 4348 and loss is: 0.0019593327306210995\n",
      "Iteration is: 4349 and loss is: 0.001958396751433611\n",
      "Iteration is: 4350 and loss is: 0.0019574593752622604\n",
      "Iteration is: 4351 and loss is: 0.0019565292168408632\n",
      "Iteration is: 4352 and loss is: 0.001955605112016201\n",
      "Iteration is: 4353 and loss is: 0.00195468682795763\n",
      "Iteration is: 4354 and loss is: 0.001953776925802231\n",
      "Iteration is: 4355 and loss is: 0.0019528971752151847\n",
      "Iteration is: 4356 and loss is: 0.00195203791372478\n",
      "Iteration is: 4357 and loss is: 0.0019512248691171408\n",
      "Iteration is: 4358 and loss is: 0.0019504736410453916\n",
      "Iteration is: 4359 and loss is: 0.001949820900335908\n",
      "Iteration is: 4360 and loss is: 0.0019493229920044541\n",
      "Iteration is: 4361 and loss is: 0.0019490625709295273\n",
      "Iteration is: 4362 and loss is: 0.0019491726998239756\n",
      "Iteration is: 4363 and loss is: 0.0019498815527185798\n",
      "Iteration is: 4364 and loss is: 0.0019515338353812695\n",
      "Iteration is: 4365 and loss is: 0.0019547464326024055\n",
      "Iteration is: 4366 and loss is: 0.0019604857079684734\n",
      "Iteration is: 4367 and loss is: 0.0019704133737832308\n",
      "Iteration is: 4368 and loss is: 0.0019872519187629223\n",
      "Iteration is: 4369 and loss is: 0.0020156390964984894\n",
      "Iteration is: 4370 and loss is: 0.0020632208324968815\n",
      "Iteration is: 4371 and loss is: 0.002142916666343808\n",
      "Iteration is: 4372 and loss is: 0.0022751200012862682\n",
      "Iteration is: 4373 and loss is: 0.0024923193268477917\n",
      "Iteration is: 4374 and loss is: 0.002838490065187216\n",
      "Iteration is: 4375 and loss is: 0.0033648875541985035\n",
      "Iteration is: 4376 and loss is: 0.004096308257430792\n",
      "Iteration is: 4377 and loss is: 0.004939635284245014\n",
      "Iteration is: 4378 and loss is: 0.005662168841809034\n",
      "Iteration is: 4379 and loss is: 0.005711378995329142\n",
      "Iteration is: 4380 and loss is: 0.005017133429646492\n",
      "Iteration is: 4381 and loss is: 0.003755326382815838\n",
      "Iteration is: 4382 and loss is: 0.002810489619150758\n",
      "Iteration is: 4383 and loss is: 0.0026253843680024147\n",
      "Iteration is: 4384 and loss is: 0.00305075547657907\n",
      "Iteration is: 4385 and loss is: 0.0034526539966464043\n",
      "Iteration is: 4386 and loss is: 0.0034813135862350464\n",
      "Iteration is: 4387 and loss is: 0.0033350447192788124\n",
      "Iteration is: 4388 and loss is: 0.003417958039790392\n",
      "Iteration is: 4389 and loss is: 0.003762970445677638\n",
      "Iteration is: 4390 and loss is: 0.004167596809566021\n",
      "Iteration is: 4391 and loss is: 0.004652476869523525\n",
      "Iteration is: 4392 and loss is: 0.00419343076646328\n",
      "Iteration is: 4393 and loss is: 0.003169880947098136\n",
      "Iteration is: 4394 and loss is: 0.002153684850782156\n",
      "Iteration is: 4395 and loss is: 0.0021307803690433502\n",
      "Iteration is: 4396 and loss is: 0.0028364197351038456\n",
      "Iteration is: 4397 and loss is: 0.0032338243909180164\n",
      "Iteration is: 4398 and loss is: 0.0029957497026771307\n",
      "Iteration is: 4399 and loss is: 0.0024547239299863577\n",
      "Iteration is: 4400 and loss is: 0.002216601511463523\n",
      "Iteration is: 4401 and loss is: 0.0022787137422710657\n",
      "Iteration is: 4402 and loss is: 0.0023782202042639256\n",
      "Iteration is: 4403 and loss is: 0.0023728373926132917\n",
      "Iteration is: 4404 and loss is: 0.002320509171113372\n",
      "Iteration is: 4405 and loss is: 0.0023898640647530556\n",
      "Iteration is: 4406 and loss is: 0.0025377843994647264\n",
      "Iteration is: 4407 and loss is: 0.0024950604420155287\n",
      "Iteration is: 4408 and loss is: 0.002212755847722292\n",
      "Iteration is: 4409 and loss is: 0.001964915543794632\n",
      "Iteration is: 4410 and loss is: 0.001985167386010289\n",
      "Iteration is: 4411 and loss is: 0.0021697424817830324\n",
      "Iteration is: 4412 and loss is: 0.002250146819278598\n",
      "Iteration is: 4413 and loss is: 0.0021453239023685455\n",
      "Iteration is: 4414 and loss is: 0.0020109107717871666\n",
      "Iteration is: 4415 and loss is: 0.0019754315726459026\n",
      "Iteration is: 4416 and loss is: 0.0020066872239112854\n",
      "Iteration is: 4417 and loss is: 0.0020171906799077988\n",
      "Iteration is: 4418 and loss is: 0.001993180252611637\n",
      "Iteration is: 4419 and loss is: 0.0019815529230982065\n",
      "Iteration is: 4420 and loss is: 0.0020016084890812635\n",
      "Iteration is: 4421 and loss is: 0.0020170933566987514\n",
      "Iteration is: 4422 and loss is: 0.001988004194572568\n",
      "Iteration is: 4423 and loss is: 0.0019347169436514378\n",
      "Iteration is: 4424 and loss is: 0.0019093803130090237\n",
      "Iteration is: 4425 and loss is: 0.001933361287228763\n",
      "Iteration is: 4426 and loss is: 0.001970540964975953\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 4427 and loss is: 0.0019760727882385254\n",
      "Iteration is: 4428 and loss is: 0.0019503145013004541\n",
      "Iteration is: 4429 and loss is: 0.0019227697048336267\n",
      "Iteration is: 4430 and loss is: 0.0019142527598887682\n",
      "Iteration is: 4431 and loss is: 0.0019180008675903082\n",
      "Iteration is: 4432 and loss is: 0.0019186334684491158\n",
      "Iteration is: 4433 and loss is: 0.0019134895410388708\n",
      "Iteration is: 4434 and loss is: 0.0019116781186312437\n",
      "Iteration is: 4435 and loss is: 0.0019182339310646057\n",
      "Iteration is: 4436 and loss is: 0.001925719203427434\n",
      "Iteration is: 4437 and loss is: 0.001922967960126698\n",
      "Iteration is: 4438 and loss is: 0.0019094517920166254\n",
      "Iteration is: 4439 and loss is: 0.0018953795079141855\n",
      "Iteration is: 4440 and loss is: 0.0018899907590821385\n",
      "Iteration is: 4441 and loss is: 0.001892841188237071\n",
      "Iteration is: 4442 and loss is: 0.0018968357471749187\n",
      "Iteration is: 4443 and loss is: 0.0018970307428389788\n",
      "Iteration is: 4444 and loss is: 0.001894848421216011\n",
      "Iteration is: 4445 and loss is: 0.0018939611036330462\n",
      "Iteration is: 4446 and loss is: 0.0018949041841551661\n",
      "Iteration is: 4447 and loss is: 0.0018948521465063095\n",
      "Iteration is: 4448 and loss is: 0.0018914681859314442\n",
      "Iteration is: 4449 and loss is: 0.0018856062088161707\n",
      "Iteration is: 4450 and loss is: 0.0018804853316396475\n",
      "Iteration is: 4451 and loss is: 0.001878269948065281\n",
      "Iteration is: 4452 and loss is: 0.0018784507410600781\n",
      "Iteration is: 4453 and loss is: 0.001878888811916113\n",
      "Iteration is: 4454 and loss is: 0.0018782499246299267\n",
      "Iteration is: 4455 and loss is: 0.0018770643509924412\n",
      "Iteration is: 4456 and loss is: 0.0018765234854072332\n",
      "Iteration is: 4457 and loss is: 0.0018768354784697294\n",
      "Iteration is: 4458 and loss is: 0.0018770600436255336\n",
      "Iteration is: 4459 and loss is: 0.0018761659739539027\n",
      "Iteration is: 4460 and loss is: 0.0018741522217169404\n",
      "Iteration is: 4461 and loss is: 0.0018718698993325233\n",
      "Iteration is: 4462 and loss is: 0.001870127860456705\n",
      "Iteration is: 4463 and loss is: 0.0018689667340368032\n",
      "Iteration is: 4464 and loss is: 0.001867875107564032\n",
      "Iteration is: 4465 and loss is: 0.001866486156359315\n",
      "Iteration is: 4466 and loss is: 0.001864878460764885\n",
      "Iteration is: 4467 and loss is: 0.001863437588326633\n",
      "Iteration is: 4468 and loss is: 0.0018624009098857641\n",
      "Iteration is: 4469 and loss is: 0.001861673779785633\n",
      "Iteration is: 4470 and loss is: 0.0018609887920320034\n",
      "Iteration is: 4471 and loss is: 0.0018601460615172982\n",
      "Iteration is: 4472 and loss is: 0.0018591689877212048\n",
      "Iteration is: 4473 and loss is: 0.001858224393799901\n",
      "Iteration is: 4474 and loss is: 0.0018574392888695002\n",
      "Iteration is: 4475 and loss is: 0.0018568127416074276\n",
      "Iteration is: 4476 and loss is: 0.001856234623119235\n",
      "Iteration is: 4477 and loss is: 0.0018555994611233473\n",
      "Iteration is: 4478 and loss is: 0.0018549002707004547\n",
      "Iteration is: 4479 and loss is: 0.001854241476394236\n",
      "Iteration is: 4480 and loss is: 0.0018537407740950584\n",
      "Iteration is: 4481 and loss is: 0.0018534716218709946\n",
      "Iteration is: 4482 and loss is: 0.0018534192349761724\n",
      "Iteration is: 4483 and loss is: 0.0018536312272772193\n",
      "Iteration is: 4484 and loss is: 0.0018541980534791946\n",
      "Iteration is: 4485 and loss is: 0.001855375012382865\n",
      "Iteration is: 4486 and loss is: 0.001857468392699957\n",
      "Iteration is: 4487 and loss is: 0.0018610272090882063\n",
      "Iteration is: 4488 and loss is: 0.0018665737006813288\n",
      "Iteration is: 4489 and loss is: 0.0018753826152533293\n",
      "Iteration is: 4490 and loss is: 0.001888729864731431\n",
      "Iteration is: 4491 and loss is: 0.0019099668134003878\n",
      "Iteration is: 4492 and loss is: 0.0019420781172811985\n",
      "Iteration is: 4493 and loss is: 0.001993871293962002\n",
      "Iteration is: 4494 and loss is: 0.00207141344435513\n",
      "Iteration is: 4495 and loss is: 0.0021972786635160446\n",
      "Iteration is: 4496 and loss is: 0.002379797864705324\n",
      "Iteration is: 4497 and loss is: 0.0026728748343884945\n",
      "Iteration is: 4498 and loss is: 0.003068438498303294\n",
      "Iteration is: 4499 and loss is: 0.0036678542383015156\n",
      "Iteration is: 4500 and loss is: 0.0043676807545125484\n",
      "Iteration is: 4501 and loss is: 0.005251883529126644\n",
      "Iteration is: 4502 and loss is: 0.006027788855135441\n",
      "Iteration is: 4503 and loss is: 0.006560936104506254\n",
      "Iteration is: 4504 and loss is: 0.006448532920330763\n",
      "Iteration is: 4505 and loss is: 0.00559425912797451\n",
      "Iteration is: 4506 and loss is: 0.004037254955619574\n",
      "Iteration is: 4507 and loss is: 0.002554158214479685\n",
      "Iteration is: 4508 and loss is: 0.0018682791851460934\n",
      "Iteration is: 4509 and loss is: 0.0021877498365938663\n",
      "Iteration is: 4510 and loss is: 0.0029953846242278814\n",
      "Iteration is: 4511 and loss is: 0.0035100767854601145\n",
      "Iteration is: 4512 and loss is: 0.0034391803201287985\n",
      "Iteration is: 4513 and loss is: 0.0027703384403139353\n",
      "Iteration is: 4514 and loss is: 0.0021201232448220253\n",
      "Iteration is: 4515 and loss is: 0.0019682880956679583\n",
      "Iteration is: 4516 and loss is: 0.002293006284162402\n",
      "Iteration is: 4517 and loss is: 0.0026387604884803295\n",
      "Iteration is: 4518 and loss is: 0.00266714533790946\n",
      "Iteration is: 4519 and loss is: 0.002461544703692198\n",
      "Iteration is: 4520 and loss is: 0.002171221189200878\n",
      "Iteration is: 4521 and loss is: 0.0020048990845680237\n",
      "Iteration is: 4522 and loss is: 0.0020250037778168917\n",
      "Iteration is: 4523 and loss is: 0.0021336714271456003\n",
      "Iteration is: 4524 and loss is: 0.0021917223930358887\n",
      "Iteration is: 4525 and loss is: 0.0021434370428323746\n",
      "Iteration is: 4526 and loss is: 0.002043085405603051\n",
      "Iteration is: 4527 and loss is: 0.0019503373187035322\n",
      "Iteration is: 4528 and loss is: 0.00191268022172153\n",
      "Iteration is: 4529 and loss is: 0.0019425370264798403\n",
      "Iteration is: 4530 and loss is: 0.0020047188736498356\n",
      "Iteration is: 4531 and loss is: 0.0020353449508547783\n",
      "Iteration is: 4532 and loss is: 0.001993797719478607\n",
      "Iteration is: 4533 and loss is: 0.001905081095173955\n",
      "Iteration is: 4534 and loss is: 0.0018447391921654344\n",
      "Iteration is: 4535 and loss is: 0.0018574282294139266\n",
      "Iteration is: 4536 and loss is: 0.0019169002771377563\n",
      "Iteration is: 4537 and loss is: 0.001955728279426694\n",
      "Iteration is: 4538 and loss is: 0.0019344901666045189\n",
      "Iteration is: 4539 and loss is: 0.0018742247484624386\n",
      "Iteration is: 4540 and loss is: 0.001825246145017445\n",
      "Iteration is: 4541 and loss is: 0.0018214015290141106\n",
      "Iteration is: 4542 and loss is: 0.0018522031605243683\n",
      "Iteration is: 4543 and loss is: 0.0018793395720422268\n",
      "Iteration is: 4544 and loss is: 0.0018774435156956315\n",
      "Iteration is: 4545 and loss is: 0.0018526673084124923\n",
      "Iteration is: 4546 and loss is: 0.0018282243981957436\n",
      "Iteration is: 4547 and loss is: 0.001818440156057477\n",
      "Iteration is: 4548 and loss is: 0.001821705955080688\n",
      "Iteration is: 4549 and loss is: 0.0018286341801285744\n",
      "Iteration is: 4550 and loss is: 0.0018318837974220514\n",
      "Iteration is: 4551 and loss is: 0.0018307246500626206\n",
      "Iteration is: 4552 and loss is: 0.0018273907480761409\n",
      "Iteration is: 4553 and loss is: 0.0018222748767584562\n",
      "Iteration is: 4554 and loss is: 0.0018153236014768481\n",
      "Iteration is: 4555 and loss is: 0.0018084454350173473\n",
      "Iteration is: 4556 and loss is: 0.001804799190722406\n",
      "Iteration is: 4557 and loss is: 0.001805951353162527\n",
      "Iteration is: 4558 and loss is: 0.0018102416070178151\n",
      "Iteration is: 4559 and loss is: 0.0018138291779905558\n",
      "Iteration is: 4560 and loss is: 0.0018128922674804926\n",
      "Iteration is: 4561 and loss is: 0.001807322958484292\n",
      "Iteration is: 4562 and loss is: 0.001800293568521738\n",
      "Iteration is: 4563 and loss is: 0.0017955031944438815\n",
      "Iteration is: 4564 and loss is: 0.001794290030375123\n",
      "Iteration is: 4565 and loss is: 0.0017956641968339682\n",
      "Iteration is: 4566 and loss is: 0.0017974171787500381\n",
      "Iteration is: 4567 and loss is: 0.0017978232353925705\n",
      "Iteration is: 4568 and loss is: 0.0017966581508517265\n",
      "Iteration is: 4569 and loss is: 0.0017947121523320675\n",
      "Iteration is: 4570 and loss is: 0.0017926874570548534\n",
      "Iteration is: 4571 and loss is: 0.0017907172441482544\n",
      "Iteration is: 4572 and loss is: 0.0017887833528220654\n",
      "Iteration is: 4573 and loss is: 0.0017869726289063692\n",
      "Iteration is: 4574 and loss is: 0.0017855152254924178\n",
      "Iteration is: 4575 and loss is: 0.0017847067210823298\n",
      "Iteration is: 4576 and loss is: 0.0017845521215349436\n",
      "Iteration is: 4577 and loss is: 0.0017846516566351056\n",
      "Iteration is: 4578 and loss is: 0.0017844850663095713\n",
      "Iteration is: 4579 and loss is: 0.001783768180757761\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 4580 and loss is: 0.0017824845854192972\n",
      "Iteration is: 4581 and loss is: 0.0017809029668569565\n",
      "Iteration is: 4582 and loss is: 0.0017793625593185425\n",
      "Iteration is: 4583 and loss is: 0.0017780730267986655\n",
      "Iteration is: 4584 and loss is: 0.0017770032864063978\n",
      "Iteration is: 4585 and loss is: 0.0017760619521141052\n",
      "Iteration is: 4586 and loss is: 0.0017751624109223485\n",
      "Iteration is: 4587 and loss is: 0.0017742789350450039\n",
      "Iteration is: 4588 and loss is: 0.0017734644934535027\n",
      "Iteration is: 4589 and loss is: 0.0017727655358612537\n",
      "Iteration is: 4590 and loss is: 0.0017721509793773293\n",
      "Iteration is: 4591 and loss is: 0.001771552488207817\n",
      "Iteration is: 4592 and loss is: 0.0017708921805024147\n",
      "Iteration is: 4593 and loss is: 0.0017701321048662066\n",
      "Iteration is: 4594 and loss is: 0.0017692798282951117\n",
      "Iteration is: 4595 and loss is: 0.001768380869179964\n",
      "Iteration is: 4596 and loss is: 0.0017674848204478621\n",
      "Iteration is: 4597 and loss is: 0.001766617875546217\n",
      "Iteration is: 4598 and loss is: 0.0017657686257734895\n",
      "Iteration is: 4599 and loss is: 0.0017649256624281406\n",
      "Iteration is: 4600 and loss is: 0.001764069776982069\n",
      "Iteration is: 4601 and loss is: 0.0017631978262215853\n",
      "Iteration is: 4602 and loss is: 0.001762315398082137\n",
      "Iteration is: 4603 and loss is: 0.0017614529933780432\n",
      "Iteration is: 4604 and loss is: 0.0017606180626899004\n",
      "Iteration is: 4605 and loss is: 0.0017598156118765473\n",
      "Iteration is: 4606 and loss is: 0.001759040867909789\n",
      "Iteration is: 4607 and loss is: 0.001758278114721179\n",
      "Iteration is: 4608 and loss is: 0.0017575360834598541\n",
      "Iteration is: 4609 and loss is: 0.0017568068578839302\n",
      "Iteration is: 4610 and loss is: 0.0017561110435053706\n",
      "Iteration is: 4611 and loss is: 0.0017554591177031398\n",
      "Iteration is: 4612 and loss is: 0.001754882512614131\n",
      "Iteration is: 4613 and loss is: 0.001754404162056744\n",
      "Iteration is: 4614 and loss is: 0.0017540671397000551\n",
      "Iteration is: 4615 and loss is: 0.0017539209220558405\n",
      "Iteration is: 4616 and loss is: 0.0017540684202685952\n",
      "Iteration is: 4617 and loss is: 0.0017546246526762843\n",
      "Iteration is: 4618 and loss is: 0.0017558294348418713\n",
      "Iteration is: 4619 and loss is: 0.0017579724080860615\n",
      "Iteration is: 4620 and loss is: 0.001761614577844739\n",
      "Iteration is: 4621 and loss is: 0.0017674388363957405\n",
      "Iteration is: 4622 and loss is: 0.001776909688487649\n",
      "Iteration is: 4623 and loss is: 0.0017916456563398242\n",
      "Iteration is: 4624 and loss is: 0.0018155045108869672\n",
      "Iteration is: 4625 and loss is: 0.0018522131722420454\n",
      "Iteration is: 4626 and loss is: 0.0019120409851893783\n",
      "Iteration is: 4627 and loss is: 0.002002422697842121\n",
      "Iteration is: 4628 and loss is: 0.002149903681129217\n",
      "Iteration is: 4629 and loss is: 0.0023628300987184048\n",
      "Iteration is: 4630 and loss is: 0.002700864104554057\n",
      "Iteration is: 4631 and loss is: 0.0031373598612844944\n",
      "Iteration is: 4632 and loss is: 0.0037571450229734182\n",
      "Iteration is: 4633 and loss is: 0.004384348168969154\n",
      "Iteration is: 4634 and loss is: 0.005025777034461498\n",
      "Iteration is: 4635 and loss is: 0.005370787810534239\n",
      "Iteration is: 4636 and loss is: 0.005352814681828022\n",
      "Iteration is: 4637 and loss is: 0.004812299739569426\n",
      "Iteration is: 4638 and loss is: 0.003948515746742487\n",
      "Iteration is: 4639 and loss is: 0.002897242782637477\n",
      "Iteration is: 4640 and loss is: 0.0020876445341855288\n",
      "Iteration is: 4641 and loss is: 0.0017930945614352822\n",
      "Iteration is: 4642 and loss is: 0.0020395719911903143\n",
      "Iteration is: 4643 and loss is: 0.002593620680272579\n",
      "Iteration is: 4644 and loss is: 0.003026831429451704\n",
      "Iteration is: 4645 and loss is: 0.0031165769323706627\n",
      "Iteration is: 4646 and loss is: 0.002712611574679613\n",
      "Iteration is: 4647 and loss is: 0.00213259388692677\n",
      "Iteration is: 4648 and loss is: 0.001777786761522293\n",
      "Iteration is: 4649 and loss is: 0.0018286958802491426\n",
      "Iteration is: 4650 and loss is: 0.0021338665392249823\n",
      "Iteration is: 4651 and loss is: 0.0024114276748150587\n",
      "Iteration is: 4652 and loss is: 0.002522158669307828\n",
      "Iteration is: 4653 and loss is: 0.002374275354668498\n",
      "Iteration is: 4654 and loss is: 0.0021237763576209545\n",
      "Iteration is: 4655 and loss is: 0.0019249272299930453\n",
      "Iteration is: 4656 and loss is: 0.0018853942165151238\n",
      "Iteration is: 4657 and loss is: 0.0019656287040561438\n",
      "Iteration is: 4658 and loss is: 0.002049484523013234\n",
      "Iteration is: 4659 and loss is: 0.002063468797132373\n",
      "Iteration is: 4660 and loss is: 0.0020199776627123356\n",
      "Iteration is: 4661 and loss is: 0.001983870519325137\n",
      "Iteration is: 4662 and loss is: 0.0019598123617470264\n",
      "Iteration is: 4663 and loss is: 0.0019244702998548746\n",
      "Iteration is: 4664 and loss is: 0.0018676398321986198\n",
      "Iteration is: 4665 and loss is: 0.0018091886304318905\n",
      "Iteration is: 4666 and loss is: 0.0017764746444299817\n",
      "Iteration is: 4667 and loss is: 0.0017873258329927921\n",
      "Iteration is: 4668 and loss is: 0.001829242566600442\n",
      "Iteration is: 4669 and loss is: 0.0018659706693142653\n",
      "Iteration is: 4670 and loss is: 0.001861091353930533\n",
      "Iteration is: 4671 and loss is: 0.0018134271958842874\n",
      "Iteration is: 4672 and loss is: 0.0017548310570418835\n",
      "Iteration is: 4673 and loss is: 0.0017213858664035797\n",
      "Iteration is: 4674 and loss is: 0.001725312089547515\n",
      "Iteration is: 4675 and loss is: 0.0017524395370855927\n",
      "Iteration is: 4676 and loss is: 0.0017777620814740658\n",
      "Iteration is: 4677 and loss is: 0.0017833381425589323\n",
      "Iteration is: 4678 and loss is: 0.0017684826161712408\n",
      "Iteration is: 4679 and loss is: 0.0017460298258811235\n",
      "Iteration is: 4680 and loss is: 0.0017303426284343004\n",
      "Iteration is: 4681 and loss is: 0.0017262317705899477\n",
      "Iteration is: 4682 and loss is: 0.0017291707918047905\n",
      "Iteration is: 4683 and loss is: 0.0017314255237579346\n",
      "Iteration is: 4684 and loss is: 0.0017287826631218195\n",
      "Iteration is: 4685 and loss is: 0.0017225230112671852\n",
      "Iteration is: 4686 and loss is: 0.0017175687244161963\n",
      "Iteration is: 4687 and loss is: 0.0017170532373711467\n",
      "Iteration is: 4688 and loss is: 0.0017203092575073242\n",
      "Iteration is: 4689 and loss is: 0.0017237593419849873\n",
      "Iteration is: 4690 and loss is: 0.0017240997403860092\n",
      "Iteration is: 4691 and loss is: 0.0017198047135025263\n",
      "Iteration is: 4692 and loss is: 0.0017125473823398352\n",
      "Iteration is: 4693 and loss is: 0.0017052676994353533\n",
      "Iteration is: 4694 and loss is: 0.0017005631234496832\n",
      "Iteration is: 4695 and loss is: 0.001699392800219357\n",
      "Iteration is: 4696 and loss is: 0.0017008830327540636\n",
      "Iteration is: 4697 and loss is: 0.0017030348535627127\n",
      "Iteration is: 4698 and loss is: 0.0017041554674506187\n",
      "Iteration is: 4699 and loss is: 0.001703697140328586\n",
      "Iteration is: 4700 and loss is: 0.0017021356616169214\n",
      "Iteration is: 4701 and loss is: 0.0017003839602693915\n",
      "Iteration is: 4702 and loss is: 0.0016990440199151635\n",
      "Iteration is: 4703 and loss is: 0.0016982565866783261\n",
      "Iteration is: 4704 and loss is: 0.0016976911574602127\n",
      "Iteration is: 4705 and loss is: 0.0016969302669167519\n",
      "Iteration is: 4706 and loss is: 0.0016956878826022148\n",
      "Iteration is: 4707 and loss is: 0.0016938673797994852\n",
      "Iteration is: 4708 and loss is: 0.0016917000757530332\n",
      "Iteration is: 4709 and loss is: 0.001689587952569127\n",
      "Iteration is: 4710 and loss is: 0.0016878775786608458\n",
      "Iteration is: 4711 and loss is: 0.001686696195974946\n",
      "Iteration is: 4712 and loss is: 0.001685939496383071\n",
      "Iteration is: 4713 and loss is: 0.001685397932305932\n",
      "Iteration is: 4714 and loss is: 0.0016848339000716805\n",
      "Iteration is: 4715 and loss is: 0.0016841223696246743\n",
      "Iteration is: 4716 and loss is: 0.0016832343535497785\n",
      "Iteration is: 4717 and loss is: 0.0016821959288790822\n",
      "Iteration is: 4718 and loss is: 0.0016811353852972388\n",
      "Iteration is: 4719 and loss is: 0.0016801528399810195\n",
      "Iteration is: 4720 and loss is: 0.0016793180257081985\n",
      "Iteration is: 4721 and loss is: 0.0016786346677690744\n",
      "Iteration is: 4722 and loss is: 0.0016780588775873184\n",
      "Iteration is: 4723 and loss is: 0.0016775280237197876\n",
      "Iteration is: 4724 and loss is: 0.0016770106740295887\n",
      "Iteration is: 4725 and loss is: 0.0016764758620411158\n",
      "Iteration is: 4726 and loss is: 0.0016759405843913555\n",
      "Iteration is: 4727 and loss is: 0.0016754436073824763\n",
      "Iteration is: 4728 and loss is: 0.0016750642098486423\n",
      "Iteration is: 4729 and loss is: 0.001674905652180314\n",
      "Iteration is: 4730 and loss is: 0.00167508190497756\n",
      "Iteration is: 4731 and loss is: 0.0016757495468482375\n",
      "Iteration is: 4732 and loss is: 0.0016771629452705383\n",
      "Iteration is: 4733 and loss is: 0.0016796657582744956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 4734 and loss is: 0.0016838848823681474\n",
      "Iteration is: 4735 and loss is: 0.0016907064709812403\n",
      "Iteration is: 4736 and loss is: 0.0017017845530062914\n",
      "Iteration is: 4737 and loss is: 0.001719354884698987\n",
      "Iteration is: 4738 and loss is: 0.0017477949149906635\n",
      "Iteration is: 4739 and loss is: 0.0017923940904438496\n",
      "Iteration is: 4740 and loss is: 0.0018646300304681063\n",
      "Iteration is: 4741 and loss is: 0.0019755433313548565\n",
      "Iteration is: 4742 and loss is: 0.0021534168627113104\n",
      "Iteration is: 4743 and loss is: 0.0024118332657963037\n",
      "Iteration is: 4744 and loss is: 0.0028038553427904844\n",
      "Iteration is: 4745 and loss is: 0.0032952730543911457\n",
      "Iteration is: 4746 and loss is: 0.003904925659298897\n",
      "Iteration is: 4747 and loss is: 0.004419752396643162\n",
      "Iteration is: 4748 and loss is: 0.004699560347944498\n",
      "Iteration is: 4749 and loss is: 0.004551894962787628\n",
      "Iteration is: 4750 and loss is: 0.00400702515617013\n",
      "Iteration is: 4751 and loss is: 0.003261981066316366\n",
      "Iteration is: 4752 and loss is: 0.002625667955726385\n",
      "Iteration is: 4753 and loss is: 0.0022059318143874407\n",
      "Iteration is: 4754 and loss is: 0.0020227935165166855\n",
      "Iteration is: 4755 and loss is: 0.0020532160997390747\n",
      "Iteration is: 4756 and loss is: 0.002265344839543104\n",
      "Iteration is: 4757 and loss is: 0.0026076429057866335\n",
      "Iteration is: 4758 and loss is: 0.0028917291201651096\n",
      "Iteration is: 4759 and loss is: 0.0029591061174869537\n",
      "Iteration is: 4760 and loss is: 0.0026997635141015053\n",
      "Iteration is: 4761 and loss is: 0.0022865012288093567\n",
      "Iteration is: 4762 and loss is: 0.0019963521044701338\n",
      "Iteration is: 4763 and loss is: 0.0019164439290761948\n",
      "Iteration is: 4764 and loss is: 0.002036449732258916\n",
      "Iteration is: 4765 and loss is: 0.002303262008354068\n",
      "Iteration is: 4766 and loss is: 0.0026553827337920666\n",
      "Iteration is: 4767 and loss is: 0.0028504766523838043\n",
      "Iteration is: 4768 and loss is: 0.0027637083549052477\n",
      "Iteration is: 4769 and loss is: 0.0023695556446909904\n",
      "Iteration is: 4770 and loss is: 0.0019528502598404884\n",
      "Iteration is: 4771 and loss is: 0.001806240645237267\n",
      "Iteration is: 4772 and loss is: 0.0019361507147550583\n",
      "Iteration is: 4773 and loss is: 0.0021988695953041315\n",
      "Iteration is: 4774 and loss is: 0.002446681261062622\n",
      "Iteration is: 4775 and loss is: 0.0026241831947118044\n",
      "Iteration is: 4776 and loss is: 0.0025428785011172295\n",
      "Iteration is: 4777 and loss is: 0.0022553845774382353\n",
      "Iteration is: 4778 and loss is: 0.0018923557363450527\n",
      "Iteration is: 4779 and loss is: 0.0017052318435162306\n",
      "Iteration is: 4780 and loss is: 0.001780486200004816\n",
      "Iteration is: 4781 and loss is: 0.001986905001103878\n",
      "Iteration is: 4782 and loss is: 0.002154347952455282\n",
      "Iteration is: 4783 and loss is: 0.0021769250743091106\n",
      "Iteration is: 4784 and loss is: 0.002090658061206341\n",
      "Iteration is: 4785 and loss is: 0.0019078212790191174\n",
      "Iteration is: 4786 and loss is: 0.001736190402880311\n",
      "Iteration is: 4787 and loss is: 0.001654797699302435\n",
      "Iteration is: 4788 and loss is: 0.0016857716254889965\n",
      "Iteration is: 4789 and loss is: 0.0017741265473887324\n",
      "Iteration is: 4790 and loss is: 0.0018408438190817833\n",
      "Iteration is: 4791 and loss is: 0.0018496937118470669\n",
      "Iteration is: 4792 and loss is: 0.0017996309325098991\n",
      "Iteration is: 4793 and loss is: 0.0017297652084380388\n",
      "Iteration is: 4794 and loss is: 0.0016690988559275866\n",
      "Iteration is: 4795 and loss is: 0.0016415356658399105\n",
      "Iteration is: 4796 and loss is: 0.001649353769607842\n",
      "Iteration is: 4797 and loss is: 0.00167562672868371\n",
      "Iteration is: 4798 and loss is: 0.0016974732279777527\n",
      "Iteration is: 4799 and loss is: 0.0017009341390803456\n",
      "Iteration is: 4800 and loss is: 0.0016881829360499978\n",
      "Iteration is: 4801 and loss is: 0.0016674305079504848\n",
      "Iteration is: 4802 and loss is: 0.0016485631931573153\n",
      "Iteration is: 4803 and loss is: 0.0016367852222174406\n",
      "Iteration is: 4804 and loss is: 0.001633604522794485\n",
      "Iteration is: 4805 and loss is: 0.0016371083911508322\n",
      "Iteration is: 4806 and loss is: 0.0016433652490377426\n",
      "Iteration is: 4807 and loss is: 0.0016481323400512338\n",
      "Iteration is: 4808 and loss is: 0.0016488763503730297\n",
      "Iteration is: 4809 and loss is: 0.0016457757446914911\n",
      "Iteration is: 4810 and loss is: 0.0016401621978729963\n",
      "Iteration is: 4811 and loss is: 0.001633818494156003\n",
      "Iteration is: 4812 and loss is: 0.0016280831769108772\n",
      "Iteration is: 4813 and loss is: 0.0016239485703408718\n",
      "Iteration is: 4814 and loss is: 0.0016219158424064517\n",
      "Iteration is: 4815 and loss is: 0.001621921081095934\n",
      "Iteration is: 4816 and loss is: 0.0016232527559623122\n",
      "Iteration is: 4817 and loss is: 0.0016248829197138548\n",
      "Iteration is: 4818 and loss is: 0.001625948934815824\n",
      "Iteration is: 4819 and loss is: 0.0016258726827800274\n",
      "Iteration is: 4820 and loss is: 0.0016245848964899778\n",
      "Iteration is: 4821 and loss is: 0.0016223308630287647\n",
      "Iteration is: 4822 and loss is: 0.0016195641364902258\n",
      "Iteration is: 4823 and loss is: 0.0016168022993952036\n",
      "Iteration is: 4824 and loss is: 0.0016144365072250366\n",
      "Iteration is: 4825 and loss is: 0.00161269074305892\n",
      "Iteration is: 4826 and loss is: 0.0016115701291710138\n",
      "Iteration is: 4827 and loss is: 0.0016109601128846407\n",
      "Iteration is: 4828 and loss is: 0.001610652543604374\n",
      "Iteration is: 4829 and loss is: 0.0016104514943435788\n",
      "Iteration is: 4830 and loss is: 0.001610211911611259\n",
      "Iteration is: 4831 and loss is: 0.001609855331480503\n",
      "Iteration is: 4832 and loss is: 0.00160939060151577\n",
      "Iteration is: 4833 and loss is: 0.0016088437987491488\n",
      "Iteration is: 4834 and loss is: 0.001608263235539198\n",
      "Iteration is: 4835 and loss is: 0.0016076676547527313\n",
      "Iteration is: 4836 and loss is: 0.0016070585697889328\n",
      "Iteration is: 4837 and loss is: 0.0016064468072727323\n",
      "Iteration is: 4838 and loss is: 0.0016058124601840973\n",
      "Iteration is: 4839 and loss is: 0.0016051860293373466\n",
      "Iteration is: 4840 and loss is: 0.0016045405063778162\n",
      "Iteration is: 4841 and loss is: 0.0016039200127124786\n",
      "Iteration is: 4842 and loss is: 0.0016033167485147715\n",
      "Iteration is: 4843 and loss is: 0.0016027831006795168\n",
      "Iteration is: 4844 and loss is: 0.001602323492988944\n",
      "Iteration is: 4845 and loss is: 0.0016019931063055992\n",
      "Iteration is: 4846 and loss is: 0.001601815689355135\n",
      "Iteration is: 4847 and loss is: 0.00160187017172575\n",
      "Iteration is: 4848 and loss is: 0.0016022069612517953\n",
      "Iteration is: 4849 and loss is: 0.001602980075404048\n",
      "Iteration is: 4850 and loss is: 0.0016042985953390598\n",
      "Iteration is: 4851 and loss is: 0.0016064668307080865\n",
      "Iteration is: 4852 and loss is: 0.00160971749573946\n",
      "Iteration is: 4853 and loss is: 0.0016146805137395859\n",
      "Iteration is: 4854 and loss is: 0.001621840987354517\n",
      "Iteration is: 4855 and loss is: 0.0016325986944139004\n",
      "Iteration is: 4856 and loss is: 0.0016479725018143654\n",
      "Iteration is: 4857 and loss is: 0.001671226928010583\n",
      "Iteration is: 4858 and loss is: 0.0017043426632881165\n",
      "Iteration is: 4859 and loss is: 0.0017550720367580652\n",
      "Iteration is: 4860 and loss is: 0.0018262590747326612\n",
      "Iteration is: 4861 and loss is: 0.0019354064716026187\n",
      "Iteration is: 4862 and loss is: 0.0020813955925405025\n",
      "Iteration is: 4863 and loss is: 0.00229833647608757\n",
      "Iteration is: 4864 and loss is: 0.0025580788496881723\n",
      "Iteration is: 4865 and loss is: 0.00290731992572546\n",
      "Iteration is: 4866 and loss is: 0.0032373068388551474\n",
      "Iteration is: 4867 and loss is: 0.003577258437871933\n",
      "Iteration is: 4868 and loss is: 0.003742408473044634\n",
      "Iteration is: 4869 and loss is: 0.003767500165849924\n",
      "Iteration is: 4870 and loss is: 0.0035528428852558136\n",
      "Iteration is: 4871 and loss is: 0.0032280730083584785\n",
      "Iteration is: 4872 and loss is: 0.0028063314966857433\n",
      "Iteration is: 4873 and loss is: 0.00240904837846756\n",
      "Iteration is: 4874 and loss is: 0.002091822912916541\n",
      "Iteration is: 4875 and loss is: 0.0018833426292985678\n",
      "Iteration is: 4876 and loss is: 0.0018275189213454723\n",
      "Iteration is: 4877 and loss is: 0.0019175531342625618\n",
      "Iteration is: 4878 and loss is: 0.0021098367869853973\n",
      "Iteration is: 4879 and loss is: 0.0022966458927839994\n",
      "Iteration is: 4880 and loss is: 0.002391424961388111\n",
      "Iteration is: 4881 and loss is: 0.002289521275088191\n",
      "Iteration is: 4882 and loss is: 0.0020608818158507347\n",
      "Iteration is: 4883 and loss is: 0.001814302522689104\n",
      "Iteration is: 4884 and loss is: 0.0016636612126603723\n",
      "Iteration is: 4885 and loss is: 0.0016491524875164032\n",
      "Iteration is: 4886 and loss is: 0.001749357208609581\n",
      "Iteration is: 4887 and loss is: 0.0019179536029696465\n",
      "Iteration is: 4888 and loss is: 0.002085309475660324\n",
      "Iteration is: 4889 and loss is: 0.0021958290599286556\n",
      "Iteration is: 4890 and loss is: 0.002151267137378454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 4891 and loss is: 0.001974334940314293\n",
      "Iteration is: 4892 and loss is: 0.0017494068015366793\n",
      "Iteration is: 4893 and loss is: 0.0016004799399524927\n",
      "Iteration is: 4894 and loss is: 0.0015827654860913754\n",
      "Iteration is: 4895 and loss is: 0.0016748594352975488\n",
      "Iteration is: 4896 and loss is: 0.0018261224031448364\n",
      "Iteration is: 4897 and loss is: 0.0019825466442853212\n",
      "Iteration is: 4898 and loss is: 0.0021236382890492678\n",
      "Iteration is: 4899 and loss is: 0.002166101709008217\n",
      "Iteration is: 4900 and loss is: 0.0020839867647737265\n",
      "Iteration is: 4901 and loss is: 0.0018924556206911802\n",
      "Iteration is: 4902 and loss is: 0.0016963672824203968\n",
      "Iteration is: 4903 and loss is: 0.0015967232175171375\n",
      "Iteration is: 4904 and loss is: 0.0016186618013307452\n",
      "Iteration is: 4905 and loss is: 0.0017224731855094433\n",
      "Iteration is: 4906 and loss is: 0.0018566057551652193\n",
      "Iteration is: 4907 and loss is: 0.0020067233126610518\n",
      "Iteration is: 4908 and loss is: 0.0021188389509916306\n",
      "Iteration is: 4909 and loss is: 0.0021696118637919426\n",
      "Iteration is: 4910 and loss is: 0.002089613350108266\n",
      "Iteration is: 4911 and loss is: 0.0019038382451981306\n",
      "Iteration is: 4912 and loss is: 0.0017169836210086942\n",
      "Iteration is: 4913 and loss is: 0.0016220859251916409\n",
      "Iteration is: 4914 and loss is: 0.0016420891042798758\n",
      "Iteration is: 4915 and loss is: 0.0017397049814462662\n",
      "Iteration is: 4916 and loss is: 0.0018790338654071093\n",
      "Iteration is: 4917 and loss is: 0.00202211644500494\n",
      "Iteration is: 4918 and loss is: 0.002169682178646326\n",
      "Iteration is: 4919 and loss is: 0.002223796211183071\n",
      "Iteration is: 4920 and loss is: 0.0021361024118959904\n",
      "Iteration is: 4921 and loss is: 0.0019284507725387812\n",
      "Iteration is: 4922 and loss is: 0.0017147964099422097\n",
      "Iteration is: 4923 and loss is: 0.0016129608266055584\n",
      "Iteration is: 4924 and loss is: 0.0016471961280331016\n",
      "Iteration is: 4925 and loss is: 0.0017726310761645436\n",
      "Iteration is: 4926 and loss is: 0.0019374415278434753\n",
      "Iteration is: 4927 and loss is: 0.002139144577085972\n",
      "Iteration is: 4928 and loss is: 0.002295828191563487\n",
      "Iteration is: 4929 and loss is: 0.0023592389188706875\n",
      "Iteration is: 4930 and loss is: 0.002221128437668085\n",
      "Iteration is: 4931 and loss is: 0.001933076768182218\n",
      "Iteration is: 4932 and loss is: 0.0016723759472370148\n",
      "Iteration is: 4933 and loss is: 0.00157720479182899\n",
      "Iteration is: 4934 and loss is: 0.0016617069486528635\n",
      "Iteration is: 4935 and loss is: 0.0018579084426164627\n",
      "Iteration is: 4936 and loss is: 0.00211787736043334\n",
      "Iteration is: 4937 and loss is: 0.0023638573475182056\n",
      "Iteration is: 4938 and loss is: 0.0025782682932913303\n",
      "Iteration is: 4939 and loss is: 0.0025558939669281244\n",
      "Iteration is: 4940 and loss is: 0.0022546485997736454\n",
      "Iteration is: 4941 and loss is: 0.001843504374846816\n",
      "Iteration is: 4942 and loss is: 0.0015822489513084292\n",
      "Iteration is: 4943 and loss is: 0.0015986165963113308\n",
      "Iteration is: 4944 and loss is: 0.0018283871468156576\n",
      "Iteration is: 4945 and loss is: 0.0021507730707526207\n",
      "Iteration is: 4946 and loss is: 0.002455083653330803\n",
      "Iteration is: 4947 and loss is: 0.0027218074537813663\n",
      "Iteration is: 4948 and loss is: 0.002734150504693389\n",
      "Iteration is: 4949 and loss is: 0.002435847418382764\n",
      "Iteration is: 4950 and loss is: 0.0019564235117286444\n",
      "Iteration is: 4951 and loss is: 0.0016285653691738844\n",
      "Iteration is: 4952 and loss is: 0.0016471955459564924\n",
      "Iteration is: 4953 and loss is: 0.0019135554321110249\n",
      "Iteration is: 4954 and loss is: 0.002218838781118393\n",
      "Iteration is: 4955 and loss is: 0.002432991284877062\n",
      "Iteration is: 4956 and loss is: 0.002526403171941638\n",
      "Iteration is: 4957 and loss is: 0.0024216510355472565\n",
      "Iteration is: 4958 and loss is: 0.0021569933742284775\n",
      "Iteration is: 4959 and loss is: 0.0018295205663889647\n",
      "Iteration is: 4960 and loss is: 0.0016599164810031652\n",
      "Iteration is: 4961 and loss is: 0.0017350297421216965\n",
      "Iteration is: 4962 and loss is: 0.0019302095752209425\n",
      "Iteration is: 4963 and loss is: 0.002067528199404478\n",
      "Iteration is: 4964 and loss is: 0.002089720219373703\n",
      "Iteration is: 4965 and loss is: 0.0020151627250015736\n",
      "Iteration is: 4966 and loss is: 0.0018907006597146392\n",
      "Iteration is: 4967 and loss is: 0.0017810086719691753\n",
      "Iteration is: 4968 and loss is: 0.0017114062793552876\n",
      "Iteration is: 4969 and loss is: 0.0017035342752933502\n",
      "Iteration is: 4970 and loss is: 0.001742713269777596\n",
      "Iteration is: 4971 and loss is: 0.0017697976436465979\n",
      "Iteration is: 4972 and loss is: 0.0017488286830484867\n",
      "Iteration is: 4973 and loss is: 0.0016957372426986694\n",
      "Iteration is: 4974 and loss is: 0.0016457133460789919\n",
      "Iteration is: 4975 and loss is: 0.001625510398298502\n",
      "Iteration is: 4976 and loss is: 0.001638063695281744\n",
      "Iteration is: 4977 and loss is: 0.0016628792509436607\n",
      "Iteration is: 4978 and loss is: 0.0016728382324799895\n",
      "Iteration is: 4979 and loss is: 0.0016594043700024486\n",
      "Iteration is: 4980 and loss is: 0.0016197082586586475\n",
      "Iteration is: 4981 and loss is: 0.0015730159357190132\n",
      "Iteration is: 4982 and loss is: 0.0015397770330309868\n",
      "Iteration is: 4983 and loss is: 0.001532323774881661\n",
      "Iteration is: 4984 and loss is: 0.001548062078654766\n",
      "Iteration is: 4985 and loss is: 0.0015732352621853352\n",
      "Iteration is: 4986 and loss is: 0.001593162538483739\n",
      "Iteration is: 4987 and loss is: 0.0015964375343173742\n",
      "Iteration is: 4988 and loss is: 0.001583834644407034\n",
      "Iteration is: 4989 and loss is: 0.0015597597230225801\n",
      "Iteration is: 4990 and loss is: 0.0015356962103396654\n",
      "Iteration is: 4991 and loss is: 0.0015195473097264767\n",
      "Iteration is: 4992 and loss is: 0.0015147964004427195\n",
      "Iteration is: 4993 and loss is: 0.001518902718089521\n",
      "Iteration is: 4994 and loss is: 0.0015264174435287714\n",
      "Iteration is: 4995 and loss is: 0.0015323756961151958\n",
      "Iteration is: 4996 and loss is: 0.0015341558028012514\n",
      "Iteration is: 4997 and loss is: 0.0015320013044402003\n",
      "Iteration is: 4998 and loss is: 0.001527237705886364\n",
      "Iteration is: 4999 and loss is: 0.0015223249793052673\n",
      "Iteration is: 5000 and loss is: 0.0015188422985374928\n",
      "Iteration is: 5001 and loss is: 0.0015174876898527145\n",
      "Iteration is: 5002 and loss is: 0.0015177554450929165\n",
      "Iteration is: 5003 and loss is: 0.0015182977076619864\n",
      "Iteration is: 5004 and loss is: 0.001518014119938016\n",
      "Iteration is: 5005 and loss is: 0.001516265794634819\n",
      "Iteration is: 5006 and loss is: 0.00151314795948565\n",
      "Iteration is: 5007 and loss is: 0.001509233145043254\n",
      "Iteration is: 5008 and loss is: 0.001505351858213544\n",
      "Iteration is: 5009 and loss is: 0.0015023034065961838\n",
      "Iteration is: 5010 and loss is: 0.0015005465829744935\n",
      "Iteration is: 5011 and loss is: 0.00149999326094985\n",
      "Iteration is: 5012 and loss is: 0.0015001663705334067\n",
      "Iteration is: 5013 and loss is: 0.0015004941960796714\n",
      "Iteration is: 5014 and loss is: 0.0015005767345428467\n",
      "Iteration is: 5015 and loss is: 0.0015002567088231444\n",
      "Iteration is: 5016 and loss is: 0.001499526435509324\n",
      "Iteration is: 5017 and loss is: 0.0014984882436692715\n",
      "Iteration is: 5018 and loss is: 0.001497306628152728\n",
      "Iteration is: 5019 and loss is: 0.0014962370041757822\n",
      "Iteration is: 5020 and loss is: 0.0014954973012208939\n",
      "Iteration is: 5021 and loss is: 0.0014951972989365458\n",
      "Iteration is: 5022 and loss is: 0.0014953231438994408\n",
      "Iteration is: 5023 and loss is: 0.0014958459651097655\n",
      "Iteration is: 5024 and loss is: 0.0014967878814786673\n",
      "Iteration is: 5025 and loss is: 0.0014982448192313313\n",
      "Iteration is: 5026 and loss is: 0.0015003785956650972\n",
      "Iteration is: 5027 and loss is: 0.0015034901443868876\n",
      "Iteration is: 5028 and loss is: 0.0015080885495990515\n",
      "Iteration is: 5029 and loss is: 0.001515059033408761\n",
      "Iteration is: 5030 and loss is: 0.0015256472397595644\n",
      "Iteration is: 5031 and loss is: 0.0015421834541484714\n",
      "Iteration is: 5032 and loss is: 0.0015673728194087744\n",
      "Iteration is: 5033 and loss is: 0.0016073675360530615\n",
      "Iteration is: 5034 and loss is: 0.0016675479710102081\n",
      "Iteration is: 5035 and loss is: 0.0017635540571063757\n",
      "Iteration is: 5036 and loss is: 0.0019041660707443953\n",
      "Iteration is: 5037 and loss is: 0.002127496525645256\n",
      "Iteration is: 5038 and loss is: 0.0024397182278335094\n",
      "Iteration is: 5039 and loss is: 0.0029199335258454084\n",
      "Iteration is: 5040 and loss is: 0.003535633906722069\n",
      "Iteration is: 5041 and loss is: 0.004375307355076075\n",
      "Iteration is: 5042 and loss is: 0.00530489394441247\n",
      "Iteration is: 5043 and loss is: 0.006175854243338108\n",
      "Iteration is: 5044 and loss is: 0.006749085150659084\n",
      "Iteration is: 5045 and loss is: 0.006441788282245398\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 5046 and loss is: 0.005177405197173357\n",
      "Iteration is: 5047 and loss is: 0.003231479786336422\n",
      "Iteration is: 5048 and loss is: 0.0017930271569639444\n",
      "Iteration is: 5049 and loss is: 0.0016394936246797442\n",
      "Iteration is: 5050 and loss is: 0.002539797220379114\n",
      "Iteration is: 5051 and loss is: 0.0033820420503616333\n",
      "Iteration is: 5052 and loss is: 0.0033317480701953173\n",
      "Iteration is: 5053 and loss is: 0.0024146074429154396\n",
      "Iteration is: 5054 and loss is: 0.0016077226027846336\n",
      "Iteration is: 5055 and loss is: 0.0016402985202148557\n",
      "Iteration is: 5056 and loss is: 0.002236141823232174\n",
      "Iteration is: 5057 and loss is: 0.0025561493821442127\n",
      "Iteration is: 5058 and loss is: 0.002242241520434618\n",
      "Iteration is: 5059 and loss is: 0.0017354638548567891\n",
      "Iteration is: 5060 and loss is: 0.0015742480754852295\n",
      "Iteration is: 5061 and loss is: 0.0018272544257342815\n",
      "Iteration is: 5062 and loss is: 0.002057203557342291\n",
      "Iteration is: 5063 and loss is: 0.00195423630066216\n",
      "Iteration is: 5064 and loss is: 0.0016385980416089296\n",
      "Iteration is: 5065 and loss is: 0.0015076010022312403\n",
      "Iteration is: 5066 and loss is: 0.0016537327319383621\n",
      "Iteration is: 5067 and loss is: 0.0018217051401734352\n",
      "Iteration is: 5068 and loss is: 0.0017825134564191103\n",
      "Iteration is: 5069 and loss is: 0.0015906852204352617\n",
      "Iteration is: 5070 and loss is: 0.0014841711381450295\n",
      "Iteration is: 5071 and loss is: 0.0015533406985923648\n",
      "Iteration is: 5072 and loss is: 0.0016669579781591892\n",
      "Iteration is: 5073 and loss is: 0.0016734678065404296\n",
      "Iteration is: 5074 and loss is: 0.001569268642924726\n",
      "Iteration is: 5075 and loss is: 0.0014894279884174466\n",
      "Iteration is: 5076 and loss is: 0.0015076888957992196\n",
      "Iteration is: 5077 and loss is: 0.001573901274241507\n",
      "Iteration is: 5078 and loss is: 0.0015908911591395736\n",
      "Iteration is: 5079 and loss is: 0.0015391043853014708\n",
      "Iteration is: 5080 and loss is: 0.001482947962358594\n",
      "Iteration is: 5081 and loss is: 0.0014811940491199493\n",
      "Iteration is: 5082 and loss is: 0.0015195953892543912\n",
      "Iteration is: 5083 and loss is: 0.00154267402831465\n",
      "Iteration is: 5084 and loss is: 0.0015230558346956968\n",
      "Iteration is: 5085 and loss is: 0.0014849819708615541\n",
      "Iteration is: 5086 and loss is: 0.001467777881771326\n",
      "Iteration is: 5087 and loss is: 0.0014811831060796976\n",
      "Iteration is: 5088 and loss is: 0.0015016309916973114\n",
      "Iteration is: 5089 and loss is: 0.0015031405491754413\n",
      "Iteration is: 5090 and loss is: 0.0014855116605758667\n",
      "Iteration is: 5091 and loss is: 0.0014681837055832148\n",
      "Iteration is: 5092 and loss is: 0.0014663257170468569\n",
      "Iteration is: 5093 and loss is: 0.0014764899387955666\n",
      "Iteration is: 5094 and loss is: 0.0014838757924735546\n",
      "Iteration is: 5095 and loss is: 0.0014793651644140482\n",
      "Iteration is: 5096 and loss is: 0.0014672547113150358\n",
      "Iteration is: 5097 and loss is: 0.0014589165803045034\n",
      "Iteration is: 5098 and loss is: 0.0014600271824747324\n",
      "Iteration is: 5099 and loss is: 0.0014662554021924734\n",
      "Iteration is: 5100 and loss is: 0.0014698063023388386\n",
      "Iteration is: 5101 and loss is: 0.0014666501665487885\n",
      "Iteration is: 5102 and loss is: 0.0014598466223105788\n",
      "Iteration is: 5103 and loss is: 0.0014549249317497015\n",
      "Iteration is: 5104 and loss is: 0.0014548096805810928\n",
      "Iteration is: 5105 and loss is: 0.0014574420638382435\n",
      "Iteration is: 5106 and loss is: 0.0014589058700948954\n",
      "Iteration is: 5107 and loss is: 0.0014571649953722954\n",
      "Iteration is: 5108 and loss is: 0.001453327713534236\n",
      "Iteration is: 5109 and loss is: 0.0014501732075586915\n",
      "Iteration is: 5110 and loss is: 0.001449383795261383\n",
      "Iteration is: 5111 and loss is: 0.0014504415448755026\n",
      "Iteration is: 5112 and loss is: 0.0014514194335788488\n",
      "Iteration is: 5113 and loss is: 0.00145093840546906\n",
      "Iteration is: 5114 and loss is: 0.0014490042813122272\n",
      "Iteration is: 5115 and loss is: 0.001446790061891079\n",
      "Iteration is: 5116 and loss is: 0.0014454288175329566\n",
      "Iteration is: 5117 and loss is: 0.001445186440832913\n",
      "Iteration is: 5118 and loss is: 0.0014454215997830033\n",
      "Iteration is: 5119 and loss is: 0.0014452824834734201\n",
      "Iteration is: 5120 and loss is: 0.0014443728141486645\n",
      "Iteration is: 5121 and loss is: 0.0014429384609684348\n",
      "Iteration is: 5122 and loss is: 0.001441548578441143\n",
      "Iteration is: 5123 and loss is: 0.001440660678781569\n",
      "Iteration is: 5124 and loss is: 0.0014402741799131036\n",
      "Iteration is: 5125 and loss is: 0.0014400755753740668\n",
      "Iteration is: 5126 and loss is: 0.001439699437469244\n",
      "Iteration is: 5127 and loss is: 0.001438973704352975\n",
      "Iteration is: 5128 and loss is: 0.0014379958156496286\n",
      "Iteration is: 5129 and loss is: 0.0014370076823979616\n",
      "Iteration is: 5130 and loss is: 0.0014362218789756298\n",
      "Iteration is: 5131 and loss is: 0.0014356750762090087\n",
      "Iteration is: 5132 and loss is: 0.0014352505095303059\n",
      "Iteration is: 5133 and loss is: 0.0014348095282912254\n",
      "Iteration is: 5134 and loss is: 0.0014342314098030329\n",
      "Iteration is: 5135 and loss is: 0.0014335119631141424\n",
      "Iteration is: 5136 and loss is: 0.001432732678949833\n",
      "Iteration is: 5137 and loss is: 0.0014319929759949446\n",
      "Iteration is: 5138 and loss is: 0.0014313572319224477\n",
      "Iteration is: 5139 and loss is: 0.001430816249921918\n",
      "Iteration is: 5140 and loss is: 0.0014303216012194753\n",
      "Iteration is: 5141 and loss is: 0.0014298229943960905\n",
      "Iteration is: 5142 and loss is: 0.001429278519935906\n",
      "Iteration is: 5143 and loss is: 0.0014287005178630352\n",
      "Iteration is: 5144 and loss is: 0.0014281203038990498\n",
      "Iteration is: 5145 and loss is: 0.001427596085704863\n",
      "Iteration is: 5146 and loss is: 0.0014271733816713095\n",
      "Iteration is: 5147 and loss is: 0.0014268849045038223\n",
      "Iteration is: 5148 and loss is: 0.0014267757069319487\n",
      "Iteration is: 5149 and loss is: 0.0014269003877416253\n",
      "Iteration is: 5150 and loss is: 0.0014273582492023706\n",
      "Iteration is: 5151 and loss is: 0.0014283430064097047\n",
      "Iteration is: 5152 and loss is: 0.0014301631599664688\n",
      "Iteration is: 5153 and loss is: 0.0014333174331113696\n",
      "Iteration is: 5154 and loss is: 0.0014386335387825966\n",
      "Iteration is: 5155 and loss is: 0.0014473953051492572\n",
      "Iteration is: 5156 and loss is: 0.0014617166016250849\n",
      "Iteration is: 5157 and loss is: 0.0014850227162241936\n",
      "Iteration is: 5158 and loss is: 0.0015227810945361853\n",
      "Iteration is: 5159 and loss is: 0.0015840156702324748\n",
      "Iteration is: 5160 and loss is: 0.0016823591431602836\n",
      "Iteration is: 5161 and loss is: 0.0018392339115962386\n",
      "Iteration is: 5162 and loss is: 0.002081866143271327\n",
      "Iteration is: 5163 and loss is: 0.0024446139577776194\n",
      "Iteration is: 5164 and loss is: 0.0029361399356275797\n",
      "Iteration is: 5165 and loss is: 0.003532614093273878\n",
      "Iteration is: 5166 and loss is: 0.004023617133498192\n",
      "Iteration is: 5167 and loss is: 0.004226105287671089\n",
      "Iteration is: 5168 and loss is: 0.003739218460395932\n",
      "Iteration is: 5169 and loss is: 0.0028467897791415453\n",
      "Iteration is: 5170 and loss is: 0.0018980815075337887\n",
      "Iteration is: 5171 and loss is: 0.0014432021416723728\n",
      "Iteration is: 5172 and loss is: 0.001593902474269271\n",
      "Iteration is: 5173 and loss is: 0.0021606669761240482\n",
      "Iteration is: 5174 and loss is: 0.00293609662912786\n",
      "Iteration is: 5175 and loss is: 0.0038150781765580177\n",
      "Iteration is: 5176 and loss is: 0.00445348909124732\n",
      "Iteration is: 5177 and loss is: 0.004460523370653391\n",
      "Iteration is: 5178 and loss is: 0.003710453864187002\n",
      "Iteration is: 5179 and loss is: 0.0029525854624807835\n",
      "Iteration is: 5180 and loss is: 0.003254905343055725\n",
      "Iteration is: 5181 and loss is: 0.0035411198623478413\n",
      "Iteration is: 5182 and loss is: 0.0031653749756515026\n",
      "Iteration is: 5183 and loss is: 0.002503545954823494\n",
      "Iteration is: 5184 and loss is: 0.0032571270130574703\n",
      "Iteration is: 5185 and loss is: 0.006198341026902199\n",
      "Iteration is: 5186 and loss is: 0.012103131040930748\n",
      "Iteration is: 5187 and loss is: 0.012225547805428505\n",
      "Iteration is: 5188 and loss is: 0.007615429349243641\n",
      "Iteration is: 5189 and loss is: 0.004317132756114006\n",
      "Iteration is: 5190 and loss is: 0.0023073460906744003\n",
      "Iteration is: 5191 and loss is: 0.001965072238817811\n",
      "Iteration is: 5192 and loss is: 0.002715866081416607\n",
      "Iteration is: 5193 and loss is: 0.0026830299757421017\n",
      "Iteration is: 5194 and loss is: 0.002009985037147999\n",
      "Iteration is: 5195 and loss is: 0.0016815382987260818\n",
      "Iteration is: 5196 and loss is: 0.0023001432418823242\n",
      "Iteration is: 5197 and loss is: 0.00287343910895288\n",
      "Iteration is: 5198 and loss is: 0.0030108855571597815\n",
      "Iteration is: 5199 and loss is: 0.0021042367443442345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 5200 and loss is: 0.0016358279390260577\n",
      "Iteration is: 5201 and loss is: 0.001965352799743414\n",
      "Iteration is: 5202 and loss is: 0.002309519564732909\n",
      "Iteration is: 5203 and loss is: 0.001984284957870841\n",
      "Iteration is: 5204 and loss is: 0.0015631287824362516\n",
      "Iteration is: 5205 and loss is: 0.0016894509317353368\n",
      "Iteration is: 5206 and loss is: 0.001905657583847642\n",
      "Iteration is: 5207 and loss is: 0.0017474174965173006\n",
      "Iteration is: 5208 and loss is: 0.001536511816084385\n",
      "Iteration is: 5209 and loss is: 0.001590609084814787\n",
      "Iteration is: 5210 and loss is: 0.0017223521135747433\n",
      "Iteration is: 5211 and loss is: 0.0016585927223786712\n",
      "Iteration is: 5212 and loss is: 0.0015554344281554222\n",
      "Iteration is: 5213 and loss is: 0.0015341721009463072\n",
      "Iteration is: 5214 and loss is: 0.0015806565061211586\n",
      "Iteration is: 5215 and loss is: 0.00155540625564754\n",
      "Iteration is: 5216 and loss is: 0.001509023830294609\n",
      "Iteration is: 5217 and loss is: 0.0015225228853523731\n",
      "Iteration is: 5218 and loss is: 0.0015069206710904837\n",
      "Iteration is: 5219 and loss is: 0.0014860488008707762\n",
      "Iteration is: 5220 and loss is: 0.00146972993388772\n",
      "Iteration is: 5221 and loss is: 0.0015104184858500957\n",
      "Iteration is: 5222 and loss is: 0.001489795045927167\n",
      "Iteration is: 5223 and loss is: 0.0014428694266825914\n",
      "Iteration is: 5224 and loss is: 0.001429372001439333\n",
      "Iteration is: 5225 and loss is: 0.0014642674941569567\n",
      "Iteration is: 5226 and loss is: 0.0014774692244827747\n",
      "Iteration is: 5227 and loss is: 0.0014393754536285996\n",
      "Iteration is: 5228 and loss is: 0.0014128201873973012\n",
      "Iteration is: 5229 and loss is: 0.001426768722012639\n",
      "Iteration is: 5230 and loss is: 0.001445756177417934\n",
      "Iteration is: 5231 and loss is: 0.001433118712157011\n",
      "Iteration is: 5232 and loss is: 0.0014102302957326174\n",
      "Iteration is: 5233 and loss is: 0.0014116507954895496\n",
      "Iteration is: 5234 and loss is: 0.0014203975442796946\n",
      "Iteration is: 5235 and loss is: 0.0014197491109371185\n",
      "Iteration is: 5236 and loss is: 0.0014065464492887259\n",
      "Iteration is: 5237 and loss is: 0.0014045035932213068\n",
      "Iteration is: 5238 and loss is: 0.0014069171156734228\n",
      "Iteration is: 5239 and loss is: 0.0014068896416574717\n",
      "Iteration is: 5240 and loss is: 0.0014006574638187885\n",
      "Iteration is: 5241 and loss is: 0.0013985205441713333\n",
      "Iteration is: 5242 and loss is: 0.001400969922542572\n",
      "Iteration is: 5243 and loss is: 0.001399376429617405\n",
      "Iteration is: 5244 and loss is: 0.0013943184167146683\n",
      "Iteration is: 5245 and loss is: 0.001391089055687189\n",
      "Iteration is: 5246 and loss is: 0.0013933387817814946\n",
      "Iteration is: 5247 and loss is: 0.0013944972306489944\n",
      "Iteration is: 5248 and loss is: 0.0013909918488934636\n",
      "Iteration is: 5249 and loss is: 0.0013871359406039119\n",
      "Iteration is: 5250 and loss is: 0.0013863628264516592\n",
      "Iteration is: 5251 and loss is: 0.0013885151129215956\n",
      "Iteration is: 5252 and loss is: 0.0013879891484975815\n",
      "Iteration is: 5253 and loss is: 0.001385135343298316\n",
      "Iteration is: 5254 and loss is: 0.0013826885260641575\n",
      "Iteration is: 5255 and loss is: 0.0013824824709445238\n",
      "Iteration is: 5256 and loss is: 0.0013831732794642448\n",
      "Iteration is: 5257 and loss is: 0.0013820966705679893\n",
      "Iteration is: 5258 and loss is: 0.0013806154020130634\n",
      "Iteration is: 5259 and loss is: 0.0013793413527309895\n",
      "Iteration is: 5260 and loss is: 0.0013791535748168826\n",
      "Iteration is: 5261 and loss is: 0.0013785382034257054\n",
      "Iteration is: 5262 and loss is: 0.0013775221304968\n",
      "Iteration is: 5263 and loss is: 0.0013765778858214617\n",
      "Iteration is: 5264 and loss is: 0.0013761506415903568\n",
      "Iteration is: 5265 and loss is: 0.0013757742708548903\n",
      "Iteration is: 5266 and loss is: 0.0013748644851148129\n",
      "Iteration is: 5267 and loss is: 0.0013737219851464033\n",
      "Iteration is: 5268 and loss is: 0.001373059581965208\n",
      "Iteration is: 5269 and loss is: 0.0013727240730077028\n",
      "Iteration is: 5270 and loss is: 0.0013724209275096655\n",
      "Iteration is: 5271 and loss is: 0.0013715249951928854\n",
      "Iteration is: 5272 and loss is: 0.001370608457364142\n",
      "Iteration is: 5273 and loss is: 0.0013698373222723603\n",
      "Iteration is: 5274 and loss is: 0.0013694858644157648\n",
      "Iteration is: 5275 and loss is: 0.0013690488412976265\n",
      "Iteration is: 5276 and loss is: 0.0013684079749509692\n",
      "Iteration is: 5277 and loss is: 0.0013676135567948222\n",
      "Iteration is: 5278 and loss is: 0.0013669428881257772\n",
      "Iteration is: 5279 and loss is: 0.0013664162252098322\n",
      "Iteration is: 5280 and loss is: 0.0013659121468663216\n",
      "Iteration is: 5281 and loss is: 0.0013653072528541088\n",
      "Iteration is: 5282 and loss is: 0.001364678144454956\n",
      "Iteration is: 5283 and loss is: 0.0013640705728903413\n",
      "Iteration is: 5284 and loss is: 0.0013635444920510054\n",
      "Iteration is: 5285 and loss is: 0.0013629808090627193\n",
      "Iteration is: 5286 and loss is: 0.0013623822014778852\n",
      "Iteration is: 5287 and loss is: 0.0013617615913972259\n",
      "Iteration is: 5288 and loss is: 0.0013611981412395835\n",
      "Iteration is: 5289 and loss is: 0.0013606804423034191\n",
      "Iteration is: 5290 and loss is: 0.0013601456303149462\n",
      "Iteration is: 5291 and loss is: 0.0013595754280686378\n",
      "Iteration is: 5292 and loss is: 0.0013589689042419195\n",
      "Iteration is: 5293 and loss is: 0.001358391367830336\n",
      "Iteration is: 5294 and loss is: 0.0013578500365838408\n",
      "Iteration is: 5295 and loss is: 0.0013573248870670795\n",
      "Iteration is: 5296 and loss is: 0.0013567851856350899\n",
      "Iteration is: 5297 and loss is: 0.001356220687739551\n",
      "Iteration is: 5298 and loss is: 0.0013556561898440123\n",
      "Iteration is: 5299 and loss is: 0.0013551057782024145\n",
      "Iteration is: 5300 and loss is: 0.0013545649126172066\n",
      "Iteration is: 5301 and loss is: 0.001354027190245688\n",
      "Iteration is: 5302 and loss is: 0.0013534745667129755\n",
      "Iteration is: 5303 and loss is: 0.0013529283460229635\n",
      "Iteration is: 5304 and loss is: 0.0013523867819458246\n",
      "Iteration is: 5305 and loss is: 0.00135185313411057\n",
      "Iteration is: 5306 and loss is: 0.0013513162266463041\n",
      "Iteration is: 5307 and loss is: 0.0013507779221981764\n",
      "Iteration is: 5308 and loss is: 0.0013502314686775208\n",
      "Iteration is: 5309 and loss is: 0.0013496900210157037\n",
      "Iteration is: 5310 and loss is: 0.0013491525314748287\n",
      "Iteration is: 5311 and loss is: 0.0013486252864822745\n",
      "Iteration is: 5312 and loss is: 0.0013480945490300655\n",
      "Iteration is: 5313 and loss is: 0.001347559504210949\n",
      "Iteration is: 5314 and loss is: 0.0013470237608999014\n",
      "Iteration is: 5315 and loss is: 0.0013464908115565777\n",
      "Iteration is: 5316 and loss is: 0.0013459599576890469\n",
      "Iteration is: 5317 and loss is: 0.0013454335276037455\n",
      "Iteration is: 5318 and loss is: 0.0013449013931676745\n",
      "Iteration is: 5319 and loss is: 0.001344369724392891\n",
      "Iteration is: 5320 and loss is: 0.0013438413152471185\n",
      "Iteration is: 5321 and loss is: 0.0013433131389319897\n",
      "Iteration is: 5322 and loss is: 0.0013427904341369867\n",
      "Iteration is: 5323 and loss is: 0.001342262141406536\n",
      "Iteration is: 5324 and loss is: 0.0013417358277365565\n",
      "Iteration is: 5325 and loss is: 0.0013412099797278643\n",
      "Iteration is: 5326 and loss is: 0.0013406818034127355\n",
      "Iteration is: 5327 and loss is: 0.0013401606120169163\n",
      "Iteration is: 5328 and loss is: 0.0013396348804235458\n",
      "Iteration is: 5329 and loss is: 0.0013391132233664393\n",
      "Iteration is: 5330 and loss is: 0.0013385908678174019\n",
      "Iteration is: 5331 and loss is: 0.0013380685122683644\n",
      "Iteration is: 5332 and loss is: 0.001337544061243534\n",
      "Iteration is: 5333 and loss is: 0.001337024150416255\n",
      "Iteration is: 5334 and loss is: 0.0013365019112825394\n",
      "Iteration is: 5335 and loss is: 0.0013359838631004095\n",
      "Iteration is: 5336 and loss is: 0.0013354616239666939\n",
      "Iteration is: 5337 and loss is: 0.0013349414803087711\n",
      "Iteration is: 5338 and loss is: 0.001334423664957285\n",
      "Iteration is: 5339 and loss is: 0.0013339021243155003\n",
      "Iteration is: 5340 and loss is: 0.0013333873357623816\n",
      "Iteration is: 5341 and loss is: 0.0013328712666407228\n",
      "Iteration is: 5342 and loss is: 0.0013323542661964893\n",
      "Iteration is: 5343 and loss is: 0.001331832492724061\n",
      "Iteration is: 5344 and loss is: 0.0013313177041709423\n",
      "Iteration is: 5345 and loss is: 0.0013308010529726744\n",
      "Iteration is: 5346 and loss is: 0.0013302855659276247\n",
      "Iteration is: 5347 and loss is: 0.001329772174358368\n",
      "Iteration is: 5348 and loss is: 0.0013292538933455944\n",
      "Iteration is: 5349 and loss is: 0.001328739570453763\n",
      "Iteration is: 5350 and loss is: 0.0013282212894409895\n",
      "Iteration is: 5351 and loss is: 0.0013277062680572271\n",
      "Iteration is: 5352 and loss is: 0.0013271942734718323\n",
      "Iteration is: 5353 and loss is: 0.001326682511717081\n",
      "Iteration is: 5354 and loss is: 0.0013261680724099278\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 5355 and loss is: 0.0013256538659334183\n",
      "Iteration is: 5356 and loss is: 0.001325142220593989\n",
      "Iteration is: 5357 and loss is: 0.0013246280141174793\n",
      "Iteration is: 5358 and loss is: 0.0013241188134998083\n",
      "Iteration is: 5359 and loss is: 0.0013236049562692642\n",
      "Iteration is: 5360 and loss is: 0.0013230914482846856\n",
      "Iteration is: 5361 and loss is: 0.0013225796865299344\n",
      "Iteration is: 5362 and loss is: 0.001322070136666298\n",
      "Iteration is: 5363 and loss is: 0.0013215576764196157\n",
      "Iteration is: 5364 and loss is: 0.0013210487086325884\n",
      "Iteration is: 5365 and loss is: 0.0013205402065068483\n",
      "Iteration is: 5366 and loss is: 0.0013200279790908098\n",
      "Iteration is: 5367 and loss is: 0.0013195181963965297\n",
      "Iteration is: 5368 and loss is: 0.0013190116733312607\n",
      "Iteration is: 5369 and loss is: 0.0013185010757297277\n",
      "Iteration is: 5370 and loss is: 0.0013179901288822293\n",
      "Iteration is: 5371 and loss is: 0.0013174836058169603\n",
      "Iteration is: 5372 and loss is: 0.0013169758021831512\n",
      "Iteration is: 5373 and loss is: 0.0013164686970412731\n",
      "Iteration is: 5374 and loss is: 0.001315960194915533\n",
      "Iteration is: 5375 and loss is: 0.0013154505286365747\n",
      "Iteration is: 5376 and loss is: 0.0013149452861398458\n",
      "Iteration is: 5377 and loss is: 0.0013144357362762094\n",
      "Iteration is: 5378 and loss is: 0.0013139302609488368\n",
      "Iteration is: 5379 and loss is: 0.001313424902036786\n",
      "Iteration is: 5380 and loss is: 0.0013129138387739658\n",
      "Iteration is: 5381 and loss is: 0.0013124113902449608\n",
      "Iteration is: 5382 and loss is: 0.001311904052272439\n",
      "Iteration is: 5383 and loss is: 0.0013113964814692736\n",
      "Iteration is: 5384 and loss is: 0.0013108944986015558\n",
      "Iteration is: 5385 and loss is: 0.0013103863457217813\n",
      "Iteration is: 5386 and loss is: 0.0013098835479468107\n",
      "Iteration is: 5387 and loss is: 0.0013093817979097366\n",
      "Iteration is: 5388 and loss is: 0.001308874343521893\n",
      "Iteration is: 5389 and loss is: 0.0013083689846098423\n",
      "Iteration is: 5390 and loss is: 0.00130786735098809\n",
      "Iteration is: 5391 and loss is: 0.001307362806983292\n",
      "Iteration is: 5392 and loss is: 0.0013068628031760454\n",
      "Iteration is: 5393 and loss is: 0.0013063647784292698\n",
      "Iteration is: 5394 and loss is: 0.0013058683834969997\n",
      "Iteration is: 5395 and loss is: 0.001305371057242155\n",
      "Iteration is: 5396 and loss is: 0.001304886769503355\n",
      "Iteration is: 5397 and loss is: 0.0013044173829257488\n",
      "Iteration is: 5398 and loss is: 0.0013039580080658197\n",
      "Iteration is: 5399 and loss is: 0.0013035386800765991\n",
      "Iteration is: 5400 and loss is: 0.0013031649868935347\n",
      "Iteration is: 5401 and loss is: 0.0013028986286371946\n",
      "Iteration is: 5402 and loss is: 0.0013027847744524479\n",
      "Iteration is: 5403 and loss is: 0.001302964985370636\n",
      "Iteration is: 5404 and loss is: 0.001303645665757358\n",
      "Iteration is: 5405 and loss is: 0.0013052247231826186\n",
      "Iteration is: 5406 and loss is: 0.0013083836529403925\n",
      "Iteration is: 5407 and loss is: 0.0013144270051270723\n",
      "Iteration is: 5408 and loss is: 0.0013255809899419546\n",
      "Iteration is: 5409 and loss is: 0.001346216187812388\n",
      "Iteration is: 5410 and loss is: 0.0013835674617439508\n",
      "Iteration is: 5411 and loss is: 0.0014524434227496386\n",
      "Iteration is: 5412 and loss is: 0.0015750087331980467\n",
      "Iteration is: 5413 and loss is: 0.0017985671292990446\n",
      "Iteration is: 5414 and loss is: 0.002174872439354658\n",
      "Iteration is: 5415 and loss is: 0.0028129215352237225\n",
      "Iteration is: 5416 and loss is: 0.0037063579075038433\n",
      "Iteration is: 5417 and loss is: 0.004806371871381998\n",
      "Iteration is: 5418 and loss is: 0.005675882566720247\n",
      "Iteration is: 5419 and loss is: 0.005626404192298651\n",
      "Iteration is: 5420 and loss is: 0.004898382816463709\n",
      "Iteration is: 5421 and loss is: 0.003607453778386116\n",
      "Iteration is: 5422 and loss is: 0.0026111749466508627\n",
      "Iteration is: 5423 and loss is: 0.0021986409556120634\n",
      "Iteration is: 5424 and loss is: 0.002190448809415102\n",
      "Iteration is: 5425 and loss is: 0.0024263947270810604\n",
      "Iteration is: 5426 and loss is: 0.0030348459258675575\n",
      "Iteration is: 5427 and loss is: 0.00405507767572999\n",
      "Iteration is: 5428 and loss is: 0.005153421312570572\n",
      "Iteration is: 5429 and loss is: 0.005022869445383549\n",
      "Iteration is: 5430 and loss is: 0.0038912834133952856\n",
      "Iteration is: 5431 and loss is: 0.003485016990453005\n",
      "Iteration is: 5432 and loss is: 0.003059799550101161\n",
      "Iteration is: 5433 and loss is: 0.002630824688822031\n",
      "Iteration is: 5434 and loss is: 0.003300291020423174\n",
      "Iteration is: 5435 and loss is: 0.006576145067811012\n",
      "Iteration is: 5436 and loss is: 0.010365236550569534\n",
      "Iteration is: 5437 and loss is: 0.013539450243115425\n",
      "Iteration is: 5438 and loss is: 0.014800596982240677\n",
      "Iteration is: 5439 and loss is: 0.005124018527567387\n",
      "Iteration is: 5440 and loss is: 0.0031517099123448133\n",
      "Iteration is: 5441 and loss is: 0.011235302314162254\n",
      "Iteration is: 5442 and loss is: 0.04684990644454956\n",
      "Iteration is: 5443 and loss is: 0.06303823739290237\n",
      "Iteration is: 5444 and loss is: 0.017597587779164314\n",
      "Iteration is: 5445 and loss is: 0.01598527655005455\n",
      "Iteration is: 5446 and loss is: 0.03848830237984657\n",
      "Iteration is: 5447 and loss is: 0.02164175920188427\n",
      "Iteration is: 5448 and loss is: 0.016084246337413788\n",
      "Iteration is: 5449 and loss is: 0.047599274665117264\n",
      "Iteration is: 5450 and loss is: 0.022112799808382988\n",
      "Iteration is: 5451 and loss is: 0.010116016492247581\n",
      "Iteration is: 5452 and loss is: 0.021758927032351494\n",
      "Iteration is: 5453 and loss is: 0.0053637949749827385\n",
      "Iteration is: 5454 and loss is: 0.01996447890996933\n",
      "Iteration is: 5455 and loss is: 0.026972001418471336\n",
      "Iteration is: 5456 and loss is: 0.00696531031280756\n",
      "Iteration is: 5457 and loss is: 0.01895650289952755\n",
      "Iteration is: 5458 and loss is: 0.008909305557608604\n",
      "Iteration is: 5459 and loss is: 0.018350034952163696\n",
      "Iteration is: 5460 and loss is: 0.010553354397416115\n",
      "Iteration is: 5461 and loss is: 0.013673137873411179\n",
      "Iteration is: 5462 and loss is: 0.01126617006957531\n",
      "Iteration is: 5463 and loss is: 0.011604801751673222\n",
      "Iteration is: 5464 and loss is: 0.01300639845430851\n",
      "Iteration is: 5465 and loss is: 0.008541245013475418\n",
      "Iteration is: 5466 and loss is: 0.01332804188132286\n",
      "Iteration is: 5467 and loss is: 0.00644837599247694\n",
      "Iteration is: 5468 and loss is: 0.011438821442425251\n",
      "Iteration is: 5469 and loss is: 0.006148075219243765\n",
      "Iteration is: 5470 and loss is: 0.008663197048008442\n",
      "Iteration is: 5471 and loss is: 0.005495382007211447\n",
      "Iteration is: 5472 and loss is: 0.007612019777297974\n",
      "Iteration is: 5473 and loss is: 0.00428483122959733\n",
      "Iteration is: 5474 and loss is: 0.006178806535899639\n",
      "Iteration is: 5475 and loss is: 0.0039625102654099464\n",
      "Iteration is: 5476 and loss is: 0.004678587429225445\n",
      "Iteration is: 5477 and loss is: 0.004201141186058521\n",
      "Iteration is: 5478 and loss is: 0.0033667273819446564\n",
      "Iteration is: 5479 and loss is: 0.004274733364582062\n",
      "Iteration is: 5480 and loss is: 0.002706896048039198\n",
      "Iteration is: 5481 and loss is: 0.003238776233047247\n",
      "Iteration is: 5482 and loss is: 0.0027723098173737526\n",
      "Iteration is: 5483 and loss is: 0.002606054302304983\n",
      "Iteration is: 5484 and loss is: 0.0025009391829371452\n",
      "Iteration is: 5485 and loss is: 0.0027416939847171307\n",
      "Iteration is: 5486 and loss is: 0.002244497649371624\n",
      "Iteration is: 5487 and loss is: 0.0026331827975809574\n",
      "Iteration is: 5488 and loss is: 0.002329226117581129\n",
      "Iteration is: 5489 and loss is: 0.0022685672156512737\n",
      "Iteration is: 5490 and loss is: 0.002385528292506933\n",
      "Iteration is: 5491 and loss is: 0.0022166669368743896\n",
      "Iteration is: 5492 and loss is: 0.002288679825142026\n",
      "Iteration is: 5493 and loss is: 0.002149553271010518\n",
      "Iteration is: 5494 and loss is: 0.002189433667808771\n",
      "Iteration is: 5495 and loss is: 0.0019713575020432472\n",
      "Iteration is: 5496 and loss is: 0.0021262969821691513\n",
      "Iteration is: 5497 and loss is: 0.0018868743209168315\n",
      "Iteration is: 5498 and loss is: 0.002053885953500867\n",
      "Iteration is: 5499 and loss is: 0.00188457069452852\n",
      "Iteration is: 5500 and loss is: 0.0019314204109832644\n",
      "Iteration is: 5501 and loss is: 0.0018492890521883965\n",
      "Iteration is: 5502 and loss is: 0.001852524816058576\n",
      "Iteration is: 5503 and loss is: 0.0017857940401881933\n",
      "Iteration is: 5504 and loss is: 0.0018080315785482526\n",
      "Iteration is: 5505 and loss is: 0.00176012993324548\n",
      "Iteration is: 5506 and loss is: 0.0017357906326651573\n",
      "Iteration is: 5507 and loss is: 0.0017308155074715614\n",
      "Iteration is: 5508 and loss is: 0.0016683260910212994\n",
      "Iteration is: 5509 and loss is: 0.0016596431378275156\n",
      "Iteration is: 5510 and loss is: 0.0016223685815930367\n",
      "Iteration is: 5511 and loss is: 0.0015992654953151941\n",
      "Iteration is: 5512 and loss is: 0.001581048360094428\n",
      "Iteration is: 5513 and loss is: 0.001571062719449401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 5514 and loss is: 0.0015583774074912071\n",
      "Iteration is: 5515 and loss is: 0.0015476341359317303\n",
      "Iteration is: 5516 and loss is: 0.001538625918328762\n",
      "Iteration is: 5517 and loss is: 0.0015203515067696571\n",
      "Iteration is: 5518 and loss is: 0.0015119051095098257\n",
      "Iteration is: 5519 and loss is: 0.0014972416684031487\n",
      "Iteration is: 5520 and loss is: 0.0014973600627854466\n",
      "Iteration is: 5521 and loss is: 0.0014853713801130652\n",
      "Iteration is: 5522 and loss is: 0.001488230424001813\n",
      "Iteration is: 5523 and loss is: 0.0014757746830582619\n",
      "Iteration is: 5524 and loss is: 0.0014725348446518183\n",
      "Iteration is: 5525 and loss is: 0.0014594623353332281\n",
      "Iteration is: 5526 and loss is: 0.001457597827538848\n",
      "Iteration is: 5527 and loss is: 0.0014471074100583792\n",
      "Iteration is: 5528 and loss is: 0.001447067828848958\n",
      "Iteration is: 5529 and loss is: 0.0014402607921510935\n",
      "Iteration is: 5530 and loss is: 0.0014347496908158064\n",
      "Iteration is: 5531 and loss is: 0.0014286022633314133\n",
      "Iteration is: 5532 and loss is: 0.0014218164142221212\n",
      "Iteration is: 5533 and loss is: 0.0014173986855894327\n",
      "Iteration is: 5534 and loss is: 0.001413075253367424\n",
      "Iteration is: 5535 and loss is: 0.0014116008533164859\n",
      "Iteration is: 5536 and loss is: 0.0014062348054721951\n",
      "Iteration is: 5537 and loss is: 0.001404440845362842\n",
      "Iteration is: 5538 and loss is: 0.0013980318326503038\n",
      "Iteration is: 5539 and loss is: 0.0013955675531178713\n",
      "Iteration is: 5540 and loss is: 0.0013904117513448\n",
      "Iteration is: 5541 and loss is: 0.0013893044088035822\n",
      "Iteration is: 5542 and loss is: 0.001384946284815669\n",
      "Iteration is: 5543 and loss is: 0.0013838645536452532\n",
      "Iteration is: 5544 and loss is: 0.0013795510167255998\n",
      "Iteration is: 5545 and loss is: 0.001377442036755383\n",
      "Iteration is: 5546 and loss is: 0.00137382245156914\n",
      "Iteration is: 5547 and loss is: 0.001371833961457014\n",
      "Iteration is: 5548 and loss is: 0.0013691057683899999\n",
      "Iteration is: 5549 and loss is: 0.0013673445209860802\n",
      "Iteration is: 5550 and loss is: 0.0013649321626871824\n",
      "Iteration is: 5551 and loss is: 0.001362828304991126\n",
      "Iteration is: 5552 and loss is: 0.0013605342246592045\n",
      "Iteration is: 5553 and loss is: 0.0013584059197455645\n",
      "Iteration is: 5554 and loss is: 0.0013564680702984333\n",
      "Iteration is: 5555 and loss is: 0.001354398438706994\n",
      "Iteration is: 5556 and loss is: 0.00135268853046\n",
      "Iteration is: 5557 and loss is: 0.0013505786191672087\n",
      "Iteration is: 5558 and loss is: 0.0013488508993759751\n",
      "Iteration is: 5559 and loss is: 0.001346983714029193\n",
      "Iteration is: 5560 and loss is: 0.00134531338699162\n",
      "Iteration is: 5561 and loss is: 0.0013436577282845974\n",
      "Iteration is: 5562 and loss is: 0.00134209671523422\n",
      "Iteration is: 5563 and loss is: 0.0013404421042650938\n",
      "Iteration is: 5564 and loss is: 0.0013389256782829762\n",
      "Iteration is: 5565 and loss is: 0.0013373850379139185\n",
      "Iteration is: 5566 and loss is: 0.0013358991127461195\n",
      "Iteration is: 5567 and loss is: 0.0013345007319003344\n",
      "Iteration is: 5568 and loss is: 0.001333085703663528\n",
      "Iteration is: 5569 and loss is: 0.0013317095581442118\n",
      "Iteration is: 5570 and loss is: 0.0013303543673828244\n",
      "Iteration is: 5571 and loss is: 0.0013290087226778269\n",
      "Iteration is: 5572 and loss is: 0.0013277074322104454\n",
      "Iteration is: 5573 and loss is: 0.0013264233712106943\n",
      "Iteration is: 5574 and loss is: 0.0013251935597509146\n",
      "Iteration is: 5575 and loss is: 0.0013239217223599553\n",
      "Iteration is: 5576 and loss is: 0.0013227553572505713\n",
      "Iteration is: 5577 and loss is: 0.0013215240323916078\n",
      "Iteration is: 5578 and loss is: 0.0013204042334109545\n",
      "Iteration is: 5579 and loss is: 0.0013192305341362953\n",
      "Iteration is: 5580 and loss is: 0.0013181325048208237\n",
      "Iteration is: 5581 and loss is: 0.0013169877929612994\n",
      "Iteration is: 5582 and loss is: 0.0013159234076738358\n",
      "Iteration is: 5583 and loss is: 0.0013148251455277205\n",
      "Iteration is: 5584 and loss is: 0.0013137954520061612\n",
      "Iteration is: 5585 and loss is: 0.0013127548154443502\n",
      "Iteration is: 5586 and loss is: 0.001311741303652525\n",
      "Iteration is: 5587 and loss is: 0.0013107373379170895\n",
      "Iteration is: 5588 and loss is: 0.001309746177867055\n",
      "Iteration is: 5589 and loss is: 0.0013087706174701452\n",
      "Iteration is: 5590 and loss is: 0.001307810889557004\n",
      "Iteration is: 5591 and loss is: 0.0013068744447082281\n",
      "Iteration is: 5592 and loss is: 0.0013059376506134868\n",
      "Iteration is: 5593 and loss is: 0.0013050218112766743\n",
      "Iteration is: 5594 and loss is: 0.0013041156344115734\n",
      "Iteration is: 5595 and loss is: 0.0013032171409577131\n",
      "Iteration is: 5596 and loss is: 0.0013023415813222528\n",
      "Iteration is: 5597 and loss is: 0.0013014692813158035\n",
      "Iteration is: 5598 and loss is: 0.0013006117660552263\n",
      "Iteration is: 5599 and loss is: 0.0012997626326978207\n",
      "Iteration is: 5600 and loss is: 0.0012989286333322525\n",
      "Iteration is: 5601 and loss is: 0.001298094168305397\n",
      "Iteration is: 5602 and loss is: 0.0012972778640687466\n",
      "Iteration is: 5603 and loss is: 0.0012964728521183133\n",
      "Iteration is: 5604 and loss is: 0.0012956770369783044\n",
      "Iteration is: 5605 and loss is: 0.0012948913499712944\n",
      "Iteration is: 5606 and loss is: 0.0012941110180690885\n",
      "Iteration is: 5607 and loss is: 0.001293340465053916\n",
      "Iteration is: 5608 and loss is: 0.0012925791088491678\n",
      "Iteration is: 5609 and loss is: 0.0012918286956846714\n",
      "Iteration is: 5610 and loss is: 0.00129108433611691\n",
      "Iteration is: 5611 and loss is: 0.0012903467286378145\n",
      "Iteration is: 5612 and loss is: 0.001289617852307856\n",
      "Iteration is: 5613 and loss is: 0.0012888963101431727\n",
      "Iteration is: 5614 and loss is: 0.0012881823349744081\n",
      "Iteration is: 5615 and loss is: 0.0012874733656644821\n",
      "Iteration is: 5616 and loss is: 0.001286775921471417\n",
      "Iteration is: 5617 and loss is: 0.0012860805727541447\n",
      "Iteration is: 5618 and loss is: 0.001285394886508584\n",
      "Iteration is: 5619 and loss is: 0.0012847129255533218\n",
      "Iteration is: 5620 and loss is: 0.0012840398121625185\n",
      "Iteration is: 5621 and loss is: 0.0012833736836910248\n",
      "Iteration is: 5622 and loss is: 0.0012827127939090133\n",
      "Iteration is: 5623 and loss is: 0.0012820563279092312\n",
      "Iteration is: 5624 and loss is: 0.0012814062647521496\n",
      "Iteration is: 5625 and loss is: 0.0012807645834982395\n",
      "Iteration is: 5626 and loss is: 0.001280123949982226\n",
      "Iteration is: 5627 and loss is: 0.001279490883462131\n",
      "Iteration is: 5628 and loss is: 0.001278864685446024\n",
      "Iteration is: 5629 and loss is: 0.0012782395351678133\n",
      "Iteration is: 5630 and loss is: 0.0012776234652847052\n",
      "Iteration is: 5631 and loss is: 0.0012770103057846427\n",
      "Iteration is: 5632 and loss is: 0.0012764025013893843\n",
      "Iteration is: 5633 and loss is: 0.0012758019147440791\n",
      "Iteration is: 5634 and loss is: 0.0012751978356391191\n",
      "Iteration is: 5635 and loss is: 0.0012746083084493876\n",
      "Iteration is: 5636 and loss is: 0.001274014706723392\n",
      "Iteration is: 5637 and loss is: 0.001273431582376361\n",
      "Iteration is: 5638 and loss is: 0.0012728495057672262\n",
      "Iteration is: 5639 and loss is: 0.0012722720857709646\n",
      "Iteration is: 5640 and loss is: 0.0012716990895569324\n",
      "Iteration is: 5641 and loss is: 0.0012711298186331987\n",
      "Iteration is: 5642 and loss is: 0.001270565902814269\n",
      "Iteration is: 5643 and loss is: 0.0012700054794549942\n",
      "Iteration is: 5644 and loss is: 0.0012694424949586391\n",
      "Iteration is: 5645 and loss is: 0.0012688875431194901\n",
      "Iteration is: 5646 and loss is: 0.0012683382956311107\n",
      "Iteration is: 5647 and loss is: 0.0012677896302193403\n",
      "Iteration is: 5648 and loss is: 0.0012672431766986847\n",
      "Iteration is: 5649 and loss is: 0.0012667016126215458\n",
      "Iteration is: 5650 and loss is: 0.0012661644723266363\n",
      "Iteration is: 5651 and loss is: 0.001265628496184945\n",
      "Iteration is: 5652 and loss is: 0.0012651016004383564\n",
      "Iteration is: 5653 and loss is: 0.0012645700480788946\n",
      "Iteration is: 5654 and loss is: 0.001264043734408915\n",
      "Iteration is: 5655 and loss is: 0.001263519050553441\n",
      "Iteration is: 5656 and loss is: 0.0012630022829398513\n",
      "Iteration is: 5657 and loss is: 0.0012624849332496524\n",
      "Iteration is: 5658 and loss is: 0.0012619670014828444\n",
      "Iteration is: 5659 and loss is: 0.0012614559382200241\n",
      "Iteration is: 5660 and loss is: 0.0012609448749572039\n",
      "Iteration is: 5661 and loss is: 0.0012604370713233948\n",
      "Iteration is: 5662 and loss is: 0.001259935786947608\n",
      "Iteration is: 5663 and loss is: 0.001259431941434741\n",
      "Iteration is: 5664 and loss is: 0.0012589297257363796\n",
      "Iteration is: 5665 and loss is: 0.0012584365904331207\n",
      "Iteration is: 5666 and loss is: 0.0012579425238072872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 5667 and loss is: 0.0012574458960443735\n",
      "Iteration is: 5668 and loss is: 0.0012569573009386659\n",
      "Iteration is: 5669 and loss is: 0.0012564663775265217\n",
      "Iteration is: 5670 and loss is: 0.001255979761481285\n",
      "Iteration is: 5671 and loss is: 0.0012554943095892668\n",
      "Iteration is: 5672 and loss is: 0.0012550106039270759\n",
      "Iteration is: 5673 and loss is: 0.0012545327190309763\n",
      "Iteration is: 5674 and loss is: 0.001254052622243762\n",
      "Iteration is: 5675 and loss is: 0.001253575086593628\n",
      "Iteration is: 5676 and loss is: 0.0012530999956652522\n",
      "Iteration is: 5677 and loss is: 0.001252627931535244\n",
      "Iteration is: 5678 and loss is: 0.001252155052497983\n",
      "Iteration is: 5679 and loss is: 0.0012516891583800316\n",
      "Iteration is: 5680 and loss is: 0.0012512183748185635\n",
      "Iteration is: 5681 and loss is: 0.0012507550418376923\n",
      "Iteration is: 5682 and loss is: 0.0012502891477197409\n",
      "Iteration is: 5683 and loss is: 0.0012498260475695133\n",
      "Iteration is: 5684 and loss is: 0.0012493657413870096\n",
      "Iteration is: 5685 and loss is: 0.001248905435204506\n",
      "Iteration is: 5686 and loss is: 0.0012484490871429443\n",
      "Iteration is: 5687 and loss is: 0.0012479929719120264\n",
      "Iteration is: 5688 and loss is: 0.0012475396506488323\n",
      "Iteration is: 5689 and loss is: 0.0012470849324017763\n",
      "Iteration is: 5690 and loss is: 0.0012466318439692259\n",
      "Iteration is: 5691 and loss is: 0.00124617968685925\n",
      "Iteration is: 5692 and loss is: 0.0012457296252250671\n",
      "Iteration is: 5693 and loss is: 0.0012452832888811827\n",
      "Iteration is: 5694 and loss is: 0.001244833692908287\n",
      "Iteration is: 5695 and loss is: 0.0012443925952538848\n",
      "Iteration is: 5696 and loss is: 0.0012439467245712876\n",
      "Iteration is: 5697 and loss is: 0.0012435043463483453\n",
      "Iteration is: 5698 and loss is: 0.0012430595234036446\n",
      "Iteration is: 5699 and loss is: 0.0012426180765032768\n",
      "Iteration is: 5700 and loss is: 0.0012421770952641964\n",
      "Iteration is: 5701 and loss is: 0.0012417408870533109\n",
      "Iteration is: 5702 and loss is: 0.001241306308656931\n",
      "Iteration is: 5703 and loss is: 0.0012408682378008962\n",
      "Iteration is: 5704 and loss is: 0.0012404335429891944\n",
      "Iteration is: 5705 and loss is: 0.0012399997795000672\n",
      "Iteration is: 5706 and loss is: 0.0012395691592246294\n",
      "Iteration is: 5707 and loss is: 0.0012391358613967896\n",
      "Iteration is: 5708 and loss is: 0.0012387046590447426\n",
      "Iteration is: 5709 and loss is: 0.0012382749700918794\n",
      "Iteration is: 5710 and loss is: 0.0012378452811390162\n",
      "Iteration is: 5711 and loss is: 0.0012374215293675661\n",
      "Iteration is: 5712 and loss is: 0.0012369940523058176\n",
      "Iteration is: 5713 and loss is: 0.0012365689035505056\n",
      "Iteration is: 5714 and loss is: 0.0012361448025330901\n",
      "Iteration is: 5715 and loss is: 0.001235717674717307\n",
      "Iteration is: 5716 and loss is: 0.0012352941557765007\n",
      "Iteration is: 5717 and loss is: 0.0012348729651421309\n",
      "Iteration is: 5718 and loss is: 0.0012344508431851864\n",
      "Iteration is: 5719 and loss is: 0.0012340301182121038\n",
      "Iteration is: 5720 and loss is: 0.0012336124200373888\n",
      "Iteration is: 5721 and loss is: 0.0012331909965723753\n",
      "Iteration is: 5722 and loss is: 0.0012327739968895912\n",
      "Iteration is: 5723 and loss is: 0.0012323569972068071\n",
      "Iteration is: 5724 and loss is: 0.0012319376692175865\n",
      "Iteration is: 5725 and loss is: 0.001231528352946043\n",
      "Iteration is: 5726 and loss is: 0.0012311115860939026\n",
      "Iteration is: 5727 and loss is: 0.001230692956596613\n",
      "Iteration is: 5728 and loss is: 0.0012302816612645984\n",
      "Iteration is: 5729 and loss is: 0.0012298689689487219\n",
      "Iteration is: 5730 and loss is: 0.0012294568587094545\n",
      "Iteration is: 5731 and loss is: 0.001229042187333107\n",
      "Iteration is: 5732 and loss is: 0.0012286333367228508\n",
      "Iteration is: 5733 and loss is: 0.001228224951773882\n",
      "Iteration is: 5734 and loss is: 0.0012278123758733273\n",
      "Iteration is: 5735 and loss is: 0.001227404922246933\n",
      "Iteration is: 5736 and loss is: 0.001226998632773757\n",
      "Iteration is: 5737 and loss is: 0.0012265897821635008\n",
      "Iteration is: 5738 and loss is: 0.00122618128079921\n",
      "Iteration is: 5739 and loss is: 0.001225775107741356\n",
      "Iteration is: 5740 and loss is: 0.00122536881826818\n",
      "Iteration is: 5741 and loss is: 0.0012249640421941876\n",
      "Iteration is: 5742 and loss is: 0.0012245613615959883\n",
      "Iteration is: 5743 and loss is: 0.0012241598451510072\n",
      "Iteration is: 5744 and loss is: 0.0012237528571859002\n",
      "Iteration is: 5745 and loss is: 0.0012233495945110917\n",
      "Iteration is: 5746 and loss is: 0.0012229487765580416\n",
      "Iteration is: 5747 and loss is: 0.0012225478421896696\n",
      "Iteration is: 5748 and loss is: 0.0012221451615914702\n",
      "Iteration is: 5749 and loss is: 0.0012217452749609947\n",
      "Iteration is: 5750 and loss is: 0.0012213452719151974\n",
      "Iteration is: 5751 and loss is: 0.001220944570377469\n",
      "Iteration is: 5752 and loss is: 0.0012205478269606829\n",
      "Iteration is: 5753 and loss is: 0.0012201460776850581\n",
      "Iteration is: 5754 and loss is: 0.0012197478208690882\n",
      "Iteration is: 5755 and loss is: 0.0012193529400974512\n",
      "Iteration is: 5756 and loss is: 0.001218954217620194\n",
      "Iteration is: 5757 and loss is: 0.0012185601517558098\n",
      "Iteration is: 5758 and loss is: 0.0012181616621091962\n",
      "Iteration is: 5759 and loss is: 0.0012177678290754557\n",
      "Iteration is: 5760 and loss is: 0.0012173708528280258\n",
      "Iteration is: 5761 and loss is: 0.0012169788824394345\n",
      "Iteration is: 5762 and loss is: 0.0012165799271315336\n",
      "Iteration is: 5763 and loss is: 0.0012161891208961606\n",
      "Iteration is: 5764 and loss is: 0.001215792610310018\n",
      "Iteration is: 5765 and loss is: 0.001215402502566576\n",
      "Iteration is: 5766 and loss is: 0.0012150078546255827\n",
      "Iteration is: 5767 and loss is: 0.001214614138007164\n",
      "Iteration is: 5768 and loss is: 0.0012142236810177565\n",
      "Iteration is: 5769 and loss is: 0.0012138310121372342\n",
      "Iteration is: 5770 and loss is: 0.001213438925333321\n",
      "Iteration is: 5771 and loss is: 0.0012130493996664882\n",
      "Iteration is: 5772 and loss is: 0.0012126602232456207\n",
      "Iteration is: 5773 and loss is: 0.0012122688349336386\n",
      "Iteration is: 5774 and loss is: 0.0012118800077587366\n",
      "Iteration is: 5775 and loss is: 0.0012114890851080418\n",
      "Iteration is: 5776 and loss is: 0.0012111000251024961\n",
      "Iteration is: 5777 and loss is: 0.001210713293403387\n",
      "Iteration is: 5778 and loss is: 0.0012103202752768993\n",
      "Iteration is: 5779 and loss is: 0.001209935056976974\n",
      "Iteration is: 5780 and loss is: 0.0012095479760318995\n",
      "Iteration is: 5781 and loss is: 0.0012091584503650665\n",
      "Iteration is: 5782 and loss is: 0.001208774046972394\n",
      "Iteration is: 5783 and loss is: 0.0012083840556442738\n",
      "Iteration is: 5784 and loss is: 0.0012080003507435322\n",
      "Iteration is: 5785 and loss is: 0.0012076142011210322\n",
      "Iteration is: 5786 and loss is: 0.001207227585837245\n",
      "Iteration is: 5787 and loss is: 0.0012068450450897217\n",
      "Iteration is: 5788 and loss is: 0.0012064574984833598\n",
      "Iteration is: 5789 and loss is: 0.0012060729786753654\n",
      "Iteration is: 5790 and loss is: 0.0012056909035891294\n",
      "Iteration is: 5791 and loss is: 0.0012053041718900204\n",
      "Iteration is: 5792 and loss is: 0.0012049202341586351\n",
      "Iteration is: 5793 and loss is: 0.0012045358307659626\n",
      "Iteration is: 5794 and loss is: 0.0012041511945426464\n",
      "Iteration is: 5795 and loss is: 0.0012037708656862378\n",
      "Iteration is: 5796 and loss is: 0.0012033861130475998\n",
      "Iteration is: 5797 and loss is: 0.0012030068319290876\n",
      "Iteration is: 5798 and loss is: 0.0012026226613670588\n",
      "Iteration is: 5799 and loss is: 0.0012022419832646847\n",
      "Iteration is: 5800 and loss is: 0.001201858278363943\n",
      "Iteration is: 5801 and loss is: 0.0012014806270599365\n",
      "Iteration is: 5802 and loss is: 0.001201097620651126\n",
      "Iteration is: 5803 and loss is: 0.0012007150799036026\n",
      "Iteration is: 5804 and loss is: 0.001200333470478654\n",
      "Iteration is: 5805 and loss is: 0.0011999544221907854\n",
      "Iteration is: 5806 and loss is: 0.0011995743261650205\n",
      "Iteration is: 5807 and loss is: 0.0011991953942924738\n",
      "Iteration is: 5808 and loss is: 0.001198813901282847\n",
      "Iteration is: 5809 and loss is: 0.001198435202240944\n",
      "Iteration is: 5810 and loss is: 0.0011980546405538917\n",
      "Iteration is: 5811 and loss is: 0.0011976751266047359\n",
      "Iteration is: 5812 and loss is: 0.0011972945649176836\n",
      "Iteration is: 5813 and loss is: 0.0011969186598435044\n",
      "Iteration is: 5814 and loss is: 0.0011965412413701415\n",
      "Iteration is: 5815 and loss is: 0.0011961658019572496\n",
      "Iteration is: 5816 and loss is: 0.0011957840761169791\n",
      "Iteration is: 5817 and loss is: 0.0011954065412282944\n",
      "Iteration is: 5818 and loss is: 0.0011950288899242878\n",
      "Iteration is: 5819 and loss is: 0.001194652053527534\n",
      "Iteration is: 5820 and loss is: 0.0011942762648686767\n",
      "Iteration is: 5821 and loss is: 0.0011938984971493483\n",
      "Iteration is: 5822 and loss is: 0.0011935218935832381\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 5823 and loss is: 0.0011931448243558407\n",
      "Iteration is: 5824 and loss is: 0.0011927695013582706\n",
      "Iteration is: 5825 and loss is: 0.0011923935962840915\n",
      "Iteration is: 5826 and loss is: 0.0011920166434720159\n",
      "Iteration is: 5827 and loss is: 0.0011916406219825149\n",
      "Iteration is: 5828 and loss is: 0.0011912633199244738\n",
      "Iteration is: 5829 and loss is: 0.0011908914893865585\n",
      "Iteration is: 5830 and loss is: 0.0011905166320502758\n",
      "Iteration is: 5831 and loss is: 0.001190139795653522\n",
      "Iteration is: 5832 and loss is: 0.0011897663353011012\n",
      "Iteration is: 5833 and loss is: 0.0011893890332430601\n",
      "Iteration is: 5834 and loss is: 0.0011890148743987083\n",
      "Iteration is: 5835 and loss is: 0.0011886407155543566\n",
      "Iteration is: 5836 and loss is: 0.0011882674880325794\n",
      "Iteration is: 5837 and loss is: 0.0011878965888172388\n",
      "Iteration is: 5838 and loss is: 0.001187520450912416\n",
      "Iteration is: 5839 and loss is: 0.0011871455935761333\n",
      "Iteration is: 5840 and loss is: 0.0011867702705785632\n",
      "Iteration is: 5841 and loss is: 0.0011864005355164409\n",
      "Iteration is: 5842 and loss is: 0.001186028472147882\n",
      "Iteration is: 5843 and loss is: 0.0011856568744406104\n",
      "Iteration is: 5844 and loss is: 0.001185280387289822\n",
      "Iteration is: 5845 and loss is: 0.0011849101865664124\n",
      "Iteration is: 5846 and loss is: 0.0011845396365970373\n",
      "Iteration is: 5847 and loss is: 0.0011841676896438003\n",
      "Iteration is: 5848 and loss is: 0.0011837927158921957\n",
      "Iteration is: 5849 and loss is: 0.00118342530913651\n",
      "Iteration is: 5850 and loss is: 0.001183048589155078\n",
      "Iteration is: 5851 and loss is: 0.0011826776899397373\n",
      "Iteration is: 5852 and loss is: 0.0011823084205389023\n",
      "Iteration is: 5853 and loss is: 0.0011819335632026196\n",
      "Iteration is: 5854 and loss is: 0.0011815614998340607\n",
      "Iteration is: 5855 and loss is: 0.0011811923468485475\n",
      "Iteration is: 5856 and loss is: 0.0011808226117864251\n",
      "Iteration is: 5857 and loss is: 0.0011804515961557627\n",
      "Iteration is: 5858 and loss is: 0.0011800792999565601\n",
      "Iteration is: 5859 and loss is: 0.0011797097977250814\n",
      "Iteration is: 5860 and loss is: 0.0011793412268161774\n",
      "Iteration is: 5861 and loss is: 0.0011789698619395494\n",
      "Iteration is: 5862 and loss is: 0.0011786003597080708\n",
      "Iteration is: 5863 and loss is: 0.0011782306246459484\n",
      "Iteration is: 5864 and loss is: 0.0011778597254306078\n",
      "Iteration is: 5865 and loss is: 0.0011774873128160834\n",
      "Iteration is: 5866 and loss is: 0.0011771211866289377\n",
      "Iteration is: 5867 and loss is: 0.0011767512187361717\n",
      "Iteration is: 5868 and loss is: 0.0011763859074562788\n",
      "Iteration is: 5869 and loss is: 0.0011760145425796509\n",
      "Iteration is: 5870 and loss is: 0.001175649231299758\n",
      "Iteration is: 5871 and loss is: 0.0011752750724554062\n",
      "Iteration is: 5872 and loss is: 0.0011749069672077894\n",
      "Iteration is: 5873 and loss is: 0.001174537930637598\n",
      "Iteration is: 5874 and loss is: 0.0011741688940674067\n",
      "Iteration is: 5875 and loss is: 0.0011738036992028356\n",
      "Iteration is: 5876 and loss is: 0.0011734371073544025\n",
      "Iteration is: 5877 and loss is: 0.001173065509647131\n",
      "Iteration is: 5878 and loss is: 0.0011726990342140198\n",
      "Iteration is: 5879 and loss is: 0.0011723274365067482\n",
      "Iteration is: 5880 and loss is: 0.0011719608446583152\n",
      "Iteration is: 5881 and loss is: 0.0011715932050719857\n",
      "Iteration is: 5882 and loss is: 0.0011712266132235527\n",
      "Iteration is: 5883 and loss is: 0.0011708596721291542\n",
      "Iteration is: 5884 and loss is: 0.0011704923817887902\n",
      "Iteration is: 5885 and loss is: 0.0011701222974807024\n",
      "Iteration is: 5886 and loss is: 0.0011697574518620968\n",
      "Iteration is: 5887 and loss is: 0.001169390743598342\n",
      "Iteration is: 5888 and loss is: 0.001169022754766047\n",
      "Iteration is: 5889 and loss is: 0.0011686559300869703\n",
      "Iteration is: 5890 and loss is: 0.001168290269561112\n",
      "Iteration is: 5891 and loss is: 0.0011679248418658972\n",
      "Iteration is: 5892 and loss is: 0.0011675572022795677\n",
      "Iteration is: 5893 and loss is: 0.0011671911925077438\n",
      "Iteration is: 5894 and loss is: 0.0011668262304738164\n",
      "Iteration is: 5895 and loss is: 0.001166459871456027\n",
      "Iteration is: 5896 and loss is: 0.001166094676591456\n",
      "Iteration is: 5897 and loss is: 0.0011657262220978737\n",
      "Iteration is: 5898 and loss is: 0.0011653639376163483\n",
      "Iteration is: 5899 and loss is: 0.0011649960651993752\n",
      "Iteration is: 5900 and loss is: 0.0011646294733509421\n",
      "Iteration is: 5901 and loss is: 0.0011642645113170147\n",
      "Iteration is: 5902 and loss is: 0.001163897686637938\n",
      "Iteration is: 5903 and loss is: 0.0011635349364951253\n",
      "Iteration is: 5904 and loss is: 0.0011631692759692669\n",
      "Iteration is: 5905 and loss is: 0.0011628044303506613\n",
      "Iteration is: 5906 and loss is: 0.001162440050393343\n",
      "Iteration is: 5907 and loss is: 0.001162076136097312\n",
      "Iteration is: 5908 and loss is: 0.0011617090785875916\n",
      "Iteration is: 5909 and loss is: 0.0011613478418439627\n",
      "Iteration is: 5910 and loss is: 0.0011609793873503804\n",
      "Iteration is: 5911 and loss is: 0.0011606148909777403\n",
      "Iteration is: 5912 and loss is: 0.0011602514423429966\n",
      "Iteration is: 5913 and loss is: 0.001159883919171989\n",
      "Iteration is: 5914 and loss is: 0.0011595243122428656\n",
      "Iteration is: 5915 and loss is: 0.0011591571383178234\n",
      "Iteration is: 5916 and loss is: 0.0011587938060984015\n",
      "Iteration is: 5917 and loss is: 0.0011584272142499685\n",
      "Iteration is: 5918 and loss is: 0.001158067025244236\n",
      "Iteration is: 5919 and loss is: 0.0011576996184885502\n",
      "Iteration is: 5920 and loss is: 0.0011573382653295994\n",
      "Iteration is: 5921 and loss is: 0.0011569736525416374\n",
      "Iteration is: 5922 and loss is: 0.0011566132307052612\n",
      "Iteration is: 5923 and loss is: 0.0011562466388568282\n",
      "Iteration is: 5924 and loss is: 0.0011558851692825556\n",
      "Iteration is: 5925 and loss is: 0.0011555199744179845\n",
      "Iteration is: 5926 and loss is: 0.0011551566421985626\n",
      "Iteration is: 5927 and loss is: 0.0011547913309186697\n",
      "Iteration is: 5928 and loss is: 0.0011544320732355118\n",
      "Iteration is: 5929 and loss is: 0.001154066063463688\n",
      "Iteration is: 5930 and loss is: 0.001153704128228128\n",
      "Iteration is: 5931 and loss is: 0.0011533406795933843\n",
      "Iteration is: 5932 and loss is: 0.0011529786279425025\n",
      "Iteration is: 5933 and loss is: 0.001152612967416644\n",
      "Iteration is: 5934 and loss is: 0.0011522539425641298\n",
      "Iteration is: 5935 and loss is: 0.0011518904939293861\n",
      "Iteration is: 5936 and loss is: 0.001151528675109148\n",
      "Iteration is: 5937 and loss is: 0.001151166157796979\n",
      "Iteration is: 5938 and loss is: 0.0011508045718073845\n",
      "Iteration is: 5939 and loss is: 0.0011504413560032845\n",
      "Iteration is: 5940 and loss is: 0.0011500765103846788\n",
      "Iteration is: 5941 and loss is: 0.0011497120140120387\n",
      "Iteration is: 5942 and loss is: 0.001149356714449823\n",
      "Iteration is: 5943 and loss is: 0.001148988027125597\n",
      "Iteration is: 5944 and loss is: 0.0011486291186884046\n",
      "Iteration is: 5945 and loss is: 0.0011482666013762355\n",
      "Iteration is: 5946 and loss is: 0.0011479053646326065\n",
      "Iteration is: 5947 and loss is: 0.0011475434293970466\n",
      "Iteration is: 5948 and loss is: 0.0011471809120848775\n",
      "Iteration is: 5949 and loss is: 0.0011468203738331795\n",
      "Iteration is: 5950 and loss is: 0.001146460184827447\n",
      "Iteration is: 5951 and loss is: 0.0011460990644991398\n",
      "Iteration is: 5952 and loss is: 0.00114573840983212\n",
      "Iteration is: 5953 and loss is: 0.001145377871580422\n",
      "Iteration is: 5954 and loss is: 0.0011450176825746894\n",
      "Iteration is: 5955 and loss is: 0.0011446545831859112\n",
      "Iteration is: 5956 and loss is: 0.001144292764365673\n",
      "Iteration is: 5957 and loss is: 0.0011439310619607568\n",
      "Iteration is: 5958 and loss is: 0.001143570290878415\n",
      "Iteration is: 5959 and loss is: 0.0011432108003646135\n",
      "Iteration is: 5960 and loss is: 0.001142846536822617\n",
      "Iteration is: 5961 and loss is: 0.0011424911208450794\n",
      "Iteration is: 5962 and loss is: 0.001142128836363554\n",
      "Iteration is: 5963 and loss is: 0.001141764922067523\n",
      "Iteration is: 5964 and loss is: 0.0011414055479690433\n",
      "Iteration is: 5965 and loss is: 0.0011410440783947706\n",
      "Iteration is: 5966 and loss is: 0.0011406841222196817\n",
      "Iteration is: 5967 and loss is: 0.0011403235839679837\n",
      "Iteration is: 5968 and loss is: 0.001139962812885642\n",
      "Iteration is: 5969 and loss is: 0.0011396035552024841\n",
      "Iteration is: 5970 and loss is: 0.001139241736382246\n",
      "Iteration is: 5971 and loss is: 0.0011388839920982718\n",
      "Iteration is: 5972 and loss is: 0.0011385225225239992\n",
      "Iteration is: 5973 and loss is: 0.001138162799179554\n",
      "Iteration is: 5974 and loss is: 0.0011378044728189707\n",
      "Iteration is: 5975 and loss is: 0.0011374411405995488\n",
      "Iteration is: 5976 and loss is: 0.0011370803695172071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 5977 and loss is: 0.0011367229744791985\n",
      "Iteration is: 5978 and loss is: 0.0011363608064129949\n",
      "Iteration is: 5979 and loss is: 0.0011360063217580318\n",
      "Iteration is: 5980 and loss is: 0.001135644968599081\n",
      "Iteration is: 5981 and loss is: 0.0011352838482707739\n",
      "Iteration is: 5982 and loss is: 0.0011349234264343977\n",
      "Iteration is: 5983 and loss is: 0.0011345668463036418\n",
      "Iteration is: 5984 and loss is: 0.0011342029320076108\n",
      "Iteration is: 5985 and loss is: 0.0011338472831994295\n",
      "Iteration is: 5986 and loss is: 0.0011334866285324097\n",
      "Iteration is: 5987 and loss is: 0.0011331287678331137\n",
      "Iteration is: 5988 and loss is: 0.001132768695242703\n",
      "Iteration is: 5989 and loss is: 0.001132409437559545\n",
      "Iteration is: 5990 and loss is: 0.0011320523917675018\n",
      "Iteration is: 5991 and loss is: 0.0011316912714391947\n",
      "Iteration is: 5992 and loss is: 0.001131330500356853\n",
      "Iteration is: 5993 and loss is: 0.0011309749679639935\n",
      "Iteration is: 5994 and loss is: 0.0011306137312203646\n",
      "Iteration is: 5995 and loss is: 0.0011302538914605975\n",
      "Iteration is: 5996 and loss is: 0.0011298973113298416\n",
      "Iteration is: 5997 and loss is: 0.0011295379372313619\n",
      "Iteration is: 5998 and loss is: 0.001129178679548204\n",
      "Iteration is: 5999 and loss is: 0.001128818723373115\n",
      "Iteration is: 6000 and loss is: 0.0011284621432423592\n",
      "Iteration is: 6001 and loss is: 0.001128101022914052\n",
      "Iteration is: 6002 and loss is: 0.0011277466546744108\n",
      "Iteration is: 6003 and loss is: 0.0011273868149146438\n",
      "Iteration is: 6004 and loss is: 0.0011270272079855204\n",
      "Iteration is: 6005 and loss is: 0.0011266721412539482\n",
      "Iteration is: 6006 and loss is: 0.001126309740357101\n",
      "Iteration is: 6007 and loss is: 0.0011259536258876324\n",
      "Iteration is: 6008 and loss is: 0.0011255936697125435\n",
      "Iteration is: 6009 and loss is: 0.0011252360418438911\n",
      "Iteration is: 6010 and loss is: 0.0011248781811445951\n",
      "Iteration is: 6011 and loss is: 0.0011245228815823793\n",
      "Iteration is: 6012 and loss is: 0.0011241629254072905\n",
      "Iteration is: 6013 and loss is: 0.0011238068109378219\n",
      "Iteration is: 6014 and loss is: 0.0011234483681619167\n",
      "Iteration is: 6015 and loss is: 0.0011230921372771263\n",
      "Iteration is: 6016 and loss is: 0.0011227327631786466\n",
      "Iteration is: 6017 and loss is: 0.0011223740875720978\n",
      "Iteration is: 6018 and loss is: 0.0011220189044252038\n",
      "Iteration is: 6019 and loss is: 0.0011216583661735058\n",
      "Iteration is: 6020 and loss is: 0.0011212992249056697\n",
      "Iteration is: 6021 and loss is: 0.001120942528359592\n",
      "Iteration is: 6022 and loss is: 0.0011205843184143305\n",
      "Iteration is: 6023 and loss is: 0.0011202278546988964\n",
      "Iteration is: 6024 and loss is: 0.0011198698775842786\n",
      "Iteration is: 6025 and loss is: 0.0011195139959454536\n",
      "Iteration is: 6026 and loss is: 0.0011191563680768013\n",
      "Iteration is: 6027 and loss is: 0.0011188006028532982\n",
      "Iteration is: 6028 and loss is: 0.001118440879508853\n",
      "Iteration is: 6029 and loss is: 0.0011180834844708443\n",
      "Iteration is: 6030 and loss is: 0.0011177259730175138\n",
      "Iteration is: 6031 and loss is: 0.001117371255531907\n",
      "Iteration is: 6032 and loss is: 0.0011170123470947146\n",
      "Iteration is: 6033 and loss is: 0.0011166527401655912\n",
      "Iteration is: 6034 and loss is: 0.001116296392865479\n",
      "Iteration is: 6035 and loss is: 0.001115945284254849\n",
      "Iteration is: 6036 and loss is: 0.0011155827669426799\n",
      "Iteration is: 6037 and loss is: 0.0011152257211506367\n",
      "Iteration is: 6038 and loss is: 0.0011148686753585935\n",
      "Iteration is: 6039 and loss is: 0.0011145134922116995\n",
      "Iteration is: 6040 and loss is: 0.0011141567956656218\n",
      "Iteration is: 6041 and loss is: 0.001113801496103406\n",
      "Iteration is: 6042 and loss is: 0.0011134458472952247\n",
      "Iteration is: 6043 and loss is: 0.001113087753765285\n",
      "Iteration is: 6044 and loss is: 0.0011127322213724256\n",
      "Iteration is: 6045 and loss is: 0.001112375408411026\n",
      "Iteration is: 6046 and loss is: 0.0011120177805423737\n",
      "Iteration is: 6047 and loss is: 0.0011116599198430777\n",
      "Iteration is: 6048 and loss is: 0.0011113067157566547\n",
      "Iteration is: 6049 and loss is: 0.0011109492043033242\n",
      "Iteration is: 6050 and loss is: 0.0011105911107733846\n",
      "Iteration is: 6051 and loss is: 0.0011102370917797089\n",
      "Iteration is: 6052 and loss is: 0.0011098779505118728\n",
      "Iteration is: 6053 and loss is: 0.0011095250956714153\n",
      "Iteration is: 6054 and loss is: 0.0011091683991253376\n",
      "Iteration is: 6055 and loss is: 0.0011088098399341106\n",
      "Iteration is: 6056 and loss is: 0.0011084536090493202\n",
      "Iteration is: 6057 and loss is: 0.0011081004049628973\n",
      "Iteration is: 6058 and loss is: 0.0011077446397393942\n",
      "Iteration is: 6059 and loss is: 0.001107385498471558\n",
      "Iteration is: 6060 and loss is: 0.001107033109292388\n",
      "Iteration is: 6061 and loss is: 0.0011066750157624483\n",
      "Iteration is: 6062 and loss is: 0.0011063164565712214\n",
      "Iteration is: 6063 and loss is: 0.0011059658136218786\n",
      "Iteration is: 6064 and loss is: 0.0011056088842451572\n",
      "Iteration is: 6065 and loss is: 0.001105253235436976\n",
      "Iteration is: 6066 and loss is: 0.0011048943269997835\n",
      "Iteration is: 6067 and loss is: 0.001104538212530315\n",
      "Iteration is: 6068 and loss is: 0.0011041822144761682\n",
      "Iteration is: 6069 and loss is: 0.001103827846236527\n",
      "Iteration is: 6070 and loss is: 0.0011034708004444838\n",
      "Iteration is: 6071 and loss is: 0.0011031177127733827\n",
      "Iteration is: 6072 and loss is: 0.0011027632281184196\n",
      "Iteration is: 6073 and loss is: 0.0011024067644029856\n",
      "Iteration is: 6074 and loss is: 0.0011020529782399535\n",
      "Iteration is: 6075 and loss is: 0.0011016984935849905\n",
      "Iteration is: 6076 and loss is: 0.001101343659684062\n",
      "Iteration is: 6077 and loss is: 0.001100985100492835\n",
      "Iteration is: 6078 and loss is: 0.0011006288696080446\n",
      "Iteration is: 6079 and loss is: 0.0011002728715538979\n",
      "Iteration is: 6080 and loss is: 0.0010999193182215095\n",
      "Iteration is: 6081 and loss is: 0.0010995635529980063\n",
      "Iteration is: 6082 and loss is: 0.0010992081370204687\n",
      "Iteration is: 6083 and loss is: 0.0010988523717969656\n",
      "Iteration is: 6084 and loss is: 0.00109849963337183\n",
      "Iteration is: 6085 and loss is: 0.0010981450323015451\n",
      "Iteration is: 6086 and loss is: 0.0010977876372635365\n",
      "Iteration is: 6087 and loss is: 0.0010974330361932516\n",
      "Iteration is: 6088 and loss is: 0.0010970770381391048\n",
      "Iteration is: 6089 and loss is: 0.0010967215057462454\n",
      "Iteration is: 6090 and loss is: 0.0010963703971356153\n",
      "Iteration is: 6091 and loss is: 0.0010960149811580777\n",
      "Iteration is: 6092 and loss is: 0.001095660962164402\n",
      "Iteration is: 6093 and loss is: 0.0010953048476949334\n",
      "Iteration is: 6094 and loss is: 0.0010949489660561085\n",
      "Iteration is: 6095 and loss is: 0.0010945950634777546\n",
      "Iteration is: 6096 and loss is: 0.0010942398803308606\n",
      "Iteration is: 6097 and loss is: 0.0010938872583210468\n",
      "Iteration is: 6098 and loss is: 0.0010935349855571985\n",
      "Iteration is: 6099 and loss is: 0.0010931773576885462\n",
      "Iteration is: 6100 and loss is: 0.0010928232222795486\n",
      "Iteration is: 6101 and loss is: 0.0010924688540399075\n",
      "Iteration is: 6102 and loss is: 0.0010921135544776917\n",
      "Iteration is: 6103 and loss is: 0.0010917594190686941\n",
      "Iteration is: 6104 and loss is: 0.0010914042359218001\n",
      "Iteration is: 6105 and loss is: 0.0010910512646660209\n",
      "Iteration is: 6106 and loss is: 0.0010906941024586558\n",
      "Iteration is: 6107 and loss is: 0.0010903411312028766\n",
      "Iteration is: 6108 and loss is: 0.0010899854823946953\n",
      "Iteration is: 6109 and loss is: 0.0010896320454776287\n",
      "Iteration is: 6110 and loss is: 0.0010892776772379875\n",
      "Iteration is: 6111 and loss is: 0.00108892482239753\n",
      "Iteration is: 6112 and loss is: 0.00108857243321836\n",
      "Iteration is: 6113 and loss is: 0.0010882171336561441\n",
      "Iteration is: 6114 and loss is: 0.00108785938937217\n",
      "Iteration is: 6115 and loss is: 0.0010875079315155745\n",
      "Iteration is: 6116 and loss is: 0.0010871555423364043\n",
      "Iteration is: 6117 and loss is: 0.0010867995442822576\n",
      "Iteration is: 6118 and loss is: 0.001086444011889398\n",
      "Iteration is: 6119 and loss is: 0.001086090924218297\n",
      "Iteration is: 6120 and loss is: 0.001085735158994794\n",
      "Iteration is: 6121 and loss is: 0.0010853849817067385\n",
      "Iteration is: 6122 and loss is: 0.0010850277030840516\n",
      "Iteration is: 6123 and loss is: 0.0010846760123968124\n",
      "Iteration is: 6124 and loss is: 0.0010843219934031367\n",
      "Iteration is: 6125 and loss is: 0.0010839682072401047\n",
      "Iteration is: 6126 and loss is: 0.001083614770323038\n",
      "Iteration is: 6127 and loss is: 0.001083259703591466\n",
      "Iteration is: 6128 and loss is: 0.001082908594980836\n",
      "Iteration is: 6129 and loss is: 0.0010825529461726546\n",
      "Iteration is: 6130 and loss is: 0.0010821983451023698\n",
      "Iteration is: 6131 and loss is: 0.0010818463051691651\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 6132 and loss is: 0.001081493915989995\n",
      "Iteration is: 6133 and loss is: 0.0010811388492584229\n",
      "Iteration is: 6134 and loss is: 0.0010807907674461603\n",
      "Iteration is: 6135 and loss is: 0.001080437796190381\n",
      "Iteration is: 6136 and loss is: 0.0010800885502249002\n",
      "Iteration is: 6137 and loss is: 0.0010797413997352123\n",
      "Iteration is: 6138 and loss is: 0.0010794011177495122\n",
      "Iteration is: 6139 and loss is: 0.001079061534255743\n",
      "Iteration is: 6140 and loss is: 0.0010787388309836388\n",
      "Iteration is: 6141 and loss is: 0.0010784289333969355\n",
      "Iteration is: 6142 and loss is: 0.001078154775314033\n",
      "Iteration is: 6143 and loss is: 0.0010779335862025619\n",
      "Iteration is: 6144 and loss is: 0.0010778103023767471\n",
      "Iteration is: 6145 and loss is: 0.0010778524447232485\n",
      "Iteration is: 6146 and loss is: 0.001078178989700973\n",
      "Iteration is: 6147 and loss is: 0.0010790113592520356\n",
      "Iteration is: 6148 and loss is: 0.0010807424550876021\n",
      "Iteration is: 6149 and loss is: 0.0010840508621186018\n",
      "Iteration is: 6150 and loss is: 0.00109022855758667\n",
      "Iteration is: 6151 and loss is: 0.0011014530900865793\n",
      "Iteration is: 6152 and loss is: 0.0011220485903322697\n",
      "Iteration is: 6153 and loss is: 0.0011589122004806995\n",
      "Iteration is: 6154 and loss is: 0.0012267592828720808\n",
      "Iteration is: 6155 and loss is: 0.001346132019534707\n",
      "Iteration is: 6156 and loss is: 0.0015653979498893023\n",
      "Iteration is: 6157 and loss is: 0.0019308868795633316\n",
      "Iteration is: 6158 and loss is: 0.00257306732237339\n",
      "Iteration is: 6159 and loss is: 0.003464528825134039\n",
      "Iteration is: 6160 and loss is: 0.004733756184577942\n",
      "Iteration is: 6161 and loss is: 0.0055518848821520805\n",
      "Iteration is: 6162 and loss is: 0.005680260248482227\n",
      "Iteration is: 6163 and loss is: 0.004015070386230946\n",
      "Iteration is: 6164 and loss is: 0.002016405574977398\n",
      "Iteration is: 6165 and loss is: 0.0010854981373995543\n",
      "Iteration is: 6166 and loss is: 0.0017664479091763496\n",
      "Iteration is: 6167 and loss is: 0.002961954101920128\n",
      "Iteration is: 6168 and loss is: 0.003104770090430975\n",
      "Iteration is: 6169 and loss is: 0.002189147984609008\n",
      "Iteration is: 6170 and loss is: 0.0012261421652510762\n",
      "Iteration is: 6171 and loss is: 0.0012576754670590162\n",
      "Iteration is: 6172 and loss is: 0.0019309250637888908\n",
      "Iteration is: 6173 and loss is: 0.0021937743294984102\n",
      "Iteration is: 6174 and loss is: 0.0017507565207779408\n",
      "Iteration is: 6175 and loss is: 0.0011789805721491575\n",
      "Iteration is: 6176 and loss is: 0.0011947732418775558\n",
      "Iteration is: 6177 and loss is: 0.0016086904797703028\n",
      "Iteration is: 6178 and loss is: 0.0017225323244929314\n",
      "Iteration is: 6179 and loss is: 0.001407348201610148\n",
      "Iteration is: 6180 and loss is: 0.0010965915862470865\n",
      "Iteration is: 6181 and loss is: 0.0011707483790814877\n",
      "Iteration is: 6182 and loss is: 0.0014305204385891557\n",
      "Iteration is: 6183 and loss is: 0.0014600652502849698\n",
      "Iteration is: 6184 and loss is: 0.0012355516664683819\n",
      "Iteration is: 6185 and loss is: 0.0010714156087487936\n",
      "Iteration is: 6186 and loss is: 0.0011547368485480547\n",
      "Iteration is: 6187 and loss is: 0.0013033236609771848\n",
      "Iteration is: 6188 and loss is: 0.0012788138119503856\n",
      "Iteration is: 6189 and loss is: 0.001134183956310153\n",
      "Iteration is: 6190 and loss is: 0.0010686889290809631\n",
      "Iteration is: 6191 and loss is: 0.0011402042582631111\n",
      "Iteration is: 6192 and loss is: 0.0012163585051894188\n",
      "Iteration is: 6193 and loss is: 0.0011853970354422927\n",
      "Iteration is: 6194 and loss is: 0.0010983514366671443\n",
      "Iteration is: 6195 and loss is: 0.0010667922906577587\n",
      "Iteration is: 6196 and loss is: 0.0011121148709207773\n",
      "Iteration is: 6197 and loss is: 0.0011533556971699\n",
      "Iteration is: 6198 and loss is: 0.0011306745000183582\n",
      "Iteration is: 6199 and loss is: 0.0010791242821142077\n",
      "Iteration is: 6200 and loss is: 0.0010638060048222542\n",
      "Iteration is: 6201 and loss is: 0.0010928385891020298\n",
      "Iteration is: 6202 and loss is: 0.00111614097841084\n",
      "Iteration is: 6203 and loss is: 0.0011006155982613564\n",
      "Iteration is: 6204 and loss is: 0.0010691969655454159\n",
      "Iteration is: 6205 and loss is: 0.0010605776915326715\n",
      "Iteration is: 6206 and loss is: 0.001078255008906126\n",
      "Iteration is: 6207 and loss is: 0.0010918810730800033\n",
      "Iteration is: 6208 and loss is: 0.0010825798381119967\n",
      "Iteration is: 6209 and loss is: 0.001064002513885498\n",
      "Iteration is: 6210 and loss is: 0.0010581966489553452\n",
      "Iteration is: 6211 and loss is: 0.0010677776299417019\n",
      "Iteration is: 6212 and loss is: 0.0010760693112388253\n",
      "Iteration is: 6213 and loss is: 0.001071686390787363\n",
      "Iteration is: 6214 and loss is: 0.0010608501033857465\n",
      "Iteration is: 6215 and loss is: 0.001055898261256516\n",
      "Iteration is: 6216 and loss is: 0.0010602024849504232\n",
      "Iteration is: 6217 and loss is: 0.001065688207745552\n",
      "Iteration is: 6218 and loss is: 0.001064617419615388\n",
      "Iteration is: 6219 and loss is: 0.0010584021219983697\n",
      "Iteration is: 6220 and loss is: 0.0010539485374465585\n",
      "Iteration is: 6221 and loss is: 0.0010551551822572947\n",
      "Iteration is: 6222 and loss is: 0.0010587020078673959\n",
      "Iteration is: 6223 and loss is: 0.0010592172620818019\n",
      "Iteration is: 6224 and loss is: 0.0010559108341112733\n",
      "Iteration is: 6225 and loss is: 0.0010523718083277345\n",
      "Iteration is: 6226 and loss is: 0.0010519415372982621\n",
      "Iteration is: 6227 and loss is: 0.0010538245551288128\n",
      "Iteration is: 6228 and loss is: 0.0010548173449933529\n",
      "Iteration is: 6229 and loss is: 0.0010534307220950723\n",
      "Iteration is: 6230 and loss is: 0.001051011262461543\n",
      "Iteration is: 6231 and loss is: 0.0010498019400984049\n",
      "Iteration is: 6232 and loss is: 0.0010503074154257774\n",
      "Iteration is: 6233 and loss is: 0.0010511074215173721\n",
      "Iteration is: 6234 and loss is: 0.0010508322156965733\n",
      "Iteration is: 6235 and loss is: 0.001049485057592392\n",
      "Iteration is: 6236 and loss is: 0.0010481877252459526\n",
      "Iteration is: 6237 and loss is: 0.0010478189215064049\n",
      "Iteration is: 6238 and loss is: 0.001048112753778696\n",
      "Iteration is: 6239 and loss is: 0.0010482434881851077\n",
      "Iteration is: 6240 and loss is: 0.0010476778261363506\n",
      "Iteration is: 6241 and loss is: 0.0010466972598806024\n",
      "Iteration is: 6242 and loss is: 0.001045956276357174\n",
      "Iteration is: 6243 and loss is: 0.0010457498719915748\n",
      "Iteration is: 6244 and loss is: 0.0010457995813339949\n",
      "Iteration is: 6245 and loss is: 0.0010456248419359326\n",
      "Iteration is: 6246 and loss is: 0.0010450634872540832\n",
      "Iteration is: 6247 and loss is: 0.0010443662758916616\n",
      "Iteration is: 6248 and loss is: 0.00104387232568115\n",
      "Iteration is: 6249 and loss is: 0.0010436539305374026\n",
      "Iteration is: 6250 and loss is: 0.001043517142534256\n",
      "Iteration is: 6251 and loss is: 0.0010432321578264236\n",
      "Iteration is: 6252 and loss is: 0.0010427526431158185\n",
      "Iteration is: 6253 and loss is: 0.0010422308696433902\n",
      "Iteration is: 6254 and loss is: 0.0010418154997751117\n",
      "Iteration is: 6255 and loss is: 0.0010415416909381747\n",
      "Iteration is: 6256 and loss is: 0.0010413057170808315\n",
      "Iteration is: 6257 and loss is: 0.0010409955866634846\n",
      "Iteration is: 6258 and loss is: 0.0010405891807749867\n",
      "Iteration is: 6259 and loss is: 0.0010401505278423429\n",
      "Iteration is: 6260 and loss is: 0.00103976100217551\n",
      "Iteration is: 6261 and loss is: 0.0010394449345767498\n",
      "Iteration is: 6262 and loss is: 0.0010391668183729053\n",
      "Iteration is: 6263 and loss is: 0.0010388544760644436\n",
      "Iteration is: 6264 and loss is: 0.0010384871857240796\n",
      "Iteration is: 6265 and loss is: 0.0010380964959040284\n",
      "Iteration is: 6266 and loss is: 0.0010377190774306655\n",
      "Iteration is: 6267 and loss is: 0.0010373827535659075\n",
      "Iteration is: 6268 and loss is: 0.0010370755335316062\n",
      "Iteration is: 6269 and loss is: 0.0010367578361183405\n",
      "Iteration is: 6270 and loss is: 0.0010364130139350891\n",
      "Iteration is: 6271 and loss is: 0.0010360523592680693\n",
      "Iteration is: 6272 and loss is: 0.0010356871644034982\n",
      "Iteration is: 6273 and loss is: 0.0010353418765589595\n",
      "Iteration is: 6274 and loss is: 0.0010350117227062583\n",
      "Iteration is: 6275 and loss is: 0.0010346896015107632\n",
      "Iteration is: 6276 and loss is: 0.001034356071613729\n",
      "Iteration is: 6277 and loss is: 0.0010340126464143395\n",
      "Iteration is: 6278 and loss is: 0.0010336608393117785\n",
      "Iteration is: 6279 and loss is: 0.0010333126410841942\n",
      "Iteration is: 6280 and loss is: 0.0010329720098525286\n",
      "Iteration is: 6281 and loss is: 0.0010326395276933908\n",
      "Iteration is: 6282 and loss is: 0.0010323086753487587\n",
      "Iteration is: 6283 and loss is: 0.0010319716529920697\n",
      "Iteration is: 6284 and loss is: 0.0010316307889297605\n",
      "Iteration is: 6285 and loss is: 0.0010312865488231182\n",
      "Iteration is: 6286 and loss is: 0.0010309446370229125\n",
      "Iteration is: 6287 and loss is: 0.001030605286359787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 6288 and loss is: 0.0010302728042006493\n",
      "Iteration is: 6289 and loss is: 0.0010299369459971786\n",
      "Iteration is: 6290 and loss is: 0.0010296038817614317\n",
      "Iteration is: 6291 and loss is: 0.0010292635997757316\n",
      "Iteration is: 6292 and loss is: 0.0010289243655279279\n",
      "Iteration is: 6293 and loss is: 0.0010285836178809404\n",
      "Iteration is: 6294 and loss is: 0.0010282470611855388\n",
      "Iteration is: 6295 and loss is: 0.001027909223921597\n",
      "Iteration is: 6296 and loss is: 0.0010275759268552065\n",
      "Iteration is: 6297 and loss is: 0.0010272415820509195\n",
      "Iteration is: 6298 and loss is: 0.0010269024642184377\n",
      "Iteration is: 6299 and loss is: 0.0010265656746923923\n",
      "Iteration is: 6300 and loss is: 0.0010262280702590942\n",
      "Iteration is: 6301 and loss is: 0.0010258908150717616\n",
      "Iteration is: 6302 and loss is: 0.001025551464408636\n",
      "Iteration is: 6303 and loss is: 0.0010252187494188547\n",
      "Iteration is: 6304 and loss is: 0.001024880213662982\n",
      "Iteration is: 6305 and loss is: 0.0010245477315038443\n",
      "Iteration is: 6306 and loss is: 0.0010242123389616609\n",
      "Iteration is: 6307 and loss is: 0.0010238734539598227\n",
      "Iteration is: 6308 and loss is: 0.0010235360823571682\n",
      "Iteration is: 6309 and loss is: 0.001023200573399663\n",
      "Iteration is: 6310 and loss is: 0.0010228666942566633\n",
      "Iteration is: 6311 and loss is: 0.001022531883791089\n",
      "Iteration is: 6312 and loss is: 0.0010221932316198945\n",
      "Iteration is: 6313 and loss is: 0.0010218608658760786\n",
      "Iteration is: 6314 and loss is: 0.0010215246584266424\n",
      "Iteration is: 6315 and loss is: 0.0010211884509772062\n",
      "Iteration is: 6316 and loss is: 0.0010208517778664827\n",
      "Iteration is: 6317 and loss is: 0.0010205141734331846\n",
      "Iteration is: 6318 and loss is: 0.001020178198814392\n",
      "Iteration is: 6319 and loss is: 0.0010198443196713924\n",
      "Iteration is: 6320 and loss is: 0.0010195111390203238\n",
      "Iteration is: 6321 and loss is: 0.0010191728360950947\n",
      "Iteration is: 6322 and loss is: 0.0010188400046899915\n",
      "Iteration is: 6323 and loss is: 0.0010185055434703827\n",
      "Iteration is: 6324 and loss is: 0.0010181699180975556\n",
      "Iteration is: 6325 and loss is: 0.0010178325464949012\n",
      "Iteration is: 6326 and loss is: 0.0010175001807510853\n",
      "Iteration is: 6327 and loss is: 0.0010171628091484308\n",
      "Iteration is: 6328 and loss is: 0.0010168302105739713\n",
      "Iteration is: 6329 and loss is: 0.0010164931882172823\n",
      "Iteration is: 6330 and loss is: 0.0010161593090742826\n",
      "Iteration is: 6331 and loss is: 0.0010158232180401683\n",
      "Iteration is: 6332 and loss is: 0.0010154894553124905\n",
      "Iteration is: 6333 and loss is: 0.00101515743881464\n",
      "Iteration is: 6334 and loss is: 0.0010148201836273074\n",
      "Iteration is: 6335 and loss is: 0.0010144852567464113\n",
      "Iteration is: 6336 and loss is: 0.0010141500970348716\n",
      "Iteration is: 6337 and loss is: 0.0010138143552467227\n",
      "Iteration is: 6338 and loss is: 0.0010134836193174124\n",
      "Iteration is: 6339 and loss is: 0.0010131476446986198\n",
      "Iteration is: 6340 and loss is: 0.0010128127178177238\n",
      "Iteration is: 6341 and loss is: 0.001012481632642448\n",
      "Iteration is: 6342 and loss is: 0.0010121427476406097\n",
      "Iteration is: 6343 and loss is: 0.0010118099162355065\n",
      "Iteration is: 6344 and loss is: 0.001011473941616714\n",
      "Iteration is: 6345 and loss is: 0.0010111401788890362\n",
      "Iteration is: 6346 and loss is: 0.00101080525200814\n",
      "Iteration is: 6347 and loss is: 0.001010471605695784\n",
      "Iteration is: 6348 and loss is: 0.0010101372608914971\n",
      "Iteration is: 6349 and loss is: 0.0010098029160872102\n",
      "Iteration is: 6350 and loss is: 0.0010094670578837395\n",
      "Iteration is: 6351 and loss is: 0.0010091348085552454\n",
      "Iteration is: 6352 and loss is: 0.001008799416013062\n",
      "Iteration is: 6353 and loss is: 0.0010084679815918207\n",
      "Iteration is: 6354 and loss is: 0.0010081320069730282\n",
      "Iteration is: 6355 and loss is: 0.0010078003397211432\n",
      "Iteration is: 6356 and loss is: 0.0010074663441628218\n",
      "Iteration is: 6357 and loss is: 0.0010071322321891785\n",
      "Iteration is: 6358 and loss is: 0.0010067976545542479\n",
      "Iteration is: 6359 and loss is: 0.0010064642410725355\n",
      "Iteration is: 6360 and loss is: 0.0010061294306069613\n",
      "Iteration is: 6361 and loss is: 0.0010057956678792834\n",
      "Iteration is: 6362 and loss is: 0.0010054619051516056\n",
      "Iteration is: 6363 and loss is: 0.0010051272111013532\n",
      "Iteration is: 6364 and loss is: 0.0010047932155430317\n",
      "Iteration is: 6365 and loss is: 0.0010044608497992158\n",
      "Iteration is: 6366 and loss is: 0.0010041279019787908\n",
      "Iteration is: 6367 and loss is: 0.0010037918109446764\n",
      "Iteration is: 6368 and loss is: 0.001003459095954895\n",
      "Iteration is: 6369 and loss is: 0.001003125449642539\n",
      "Iteration is: 6370 and loss is: 0.0010027934331446886\n",
      "Iteration is: 6371 and loss is: 0.0010024609509855509\n",
      "Iteration is: 6372 and loss is: 0.0010021256748586893\n",
      "Iteration is: 6373 and loss is: 0.0010017906315624714\n",
      "Iteration is: 6374 and loss is: 0.0010014548897743225\n",
      "Iteration is: 6375 and loss is: 0.0010011252015829086\n",
      "Iteration is: 6376 and loss is: 0.0010007908567786217\n",
      "Iteration is: 6377 and loss is: 0.0010004581417888403\n",
      "Iteration is: 6378 and loss is: 0.0010001241462305188\n",
      "Iteration is: 6379 and loss is: 0.0009997895685955882\n",
      "Iteration is: 6380 and loss is: 0.0009994575520977378\n",
      "Iteration is: 6381 and loss is: 0.0009991234401240945\n",
      "Iteration is: 6382 and loss is: 0.000998792820610106\n",
      "Iteration is: 6383 and loss is: 0.000998455099761486\n",
      "Iteration is: 6384 and loss is: 0.0009981243638321757\n",
      "Iteration is: 6385 and loss is: 0.000997790601104498\n",
      "Iteration is: 6386 and loss is: 0.0009974585846066475\n",
      "Iteration is: 6387 and loss is: 0.0009971240069717169\n",
      "Iteration is: 6388 and loss is: 0.0009967917576432228\n",
      "Iteration is: 6389 and loss is: 0.0009964600903913379\n",
      "Iteration is: 6390 and loss is: 0.0009961260948330164\n",
      "Iteration is: 6391 and loss is: 0.0009957915171980858\n",
      "Iteration is: 6392 and loss is: 0.0009954605484381318\n",
      "Iteration is: 6393 and loss is: 0.0009951305110007524\n",
      "Iteration is: 6394 and loss is: 0.0009947995422407985\n",
      "Iteration is: 6395 and loss is: 0.0009944677585735917\n",
      "Iteration is: 6396 and loss is: 0.0009941376047208905\n",
      "Iteration is: 6397 and loss is: 0.0009938114089891315\n",
      "Iteration is: 6398 and loss is: 0.0009934857953339815\n",
      "Iteration is: 6399 and loss is: 0.000993168679997325\n",
      "Iteration is: 6400 and loss is: 0.0009928509825840592\n",
      "Iteration is: 6401 and loss is: 0.000992545043118298\n",
      "Iteration is: 6402 and loss is: 0.0009922587778419256\n",
      "Iteration is: 6403 and loss is: 0.0009919956792145967\n",
      "Iteration is: 6404 and loss is: 0.0009917706483975053\n",
      "Iteration is: 6405 and loss is: 0.0009916076669469476\n",
      "Iteration is: 6406 and loss is: 0.0009915453847497702\n",
      "Iteration is: 6407 and loss is: 0.0009916528360918164\n",
      "Iteration is: 6408 and loss is: 0.00099202711135149\n",
      "Iteration is: 6409 and loss is: 0.000992841087281704\n",
      "Iteration is: 6410 and loss is: 0.0009944040793925524\n",
      "Iteration is: 6411 and loss is: 0.0009971908293664455\n",
      "Iteration is: 6412 and loss is: 0.0010020852787420154\n",
      "Iteration is: 6413 and loss is: 0.0010104505345225334\n",
      "Iteration is: 6414 and loss is: 0.001024969737045467\n",
      "Iteration is: 6415 and loss is: 0.0010494585148990154\n",
      "Iteration is: 6416 and loss is: 0.001092154299840331\n",
      "Iteration is: 6417 and loss is: 0.001163393259048462\n",
      "Iteration is: 6418 and loss is: 0.0012883212184533477\n",
      "Iteration is: 6419 and loss is: 0.00148981180973351\n",
      "Iteration is: 6420 and loss is: 0.001838021446019411\n",
      "Iteration is: 6421 and loss is: 0.002342068823054433\n",
      "Iteration is: 6422 and loss is: 0.003136667888611555\n",
      "Iteration is: 6423 and loss is: 0.0039307726547122\n",
      "Iteration is: 6424 and loss is: 0.0047726696357131\n",
      "Iteration is: 6425 and loss is: 0.0044875480234622955\n",
      "Iteration is: 6426 and loss is: 0.0035760998725891113\n",
      "Iteration is: 6427 and loss is: 0.00195577135309577\n",
      "Iteration is: 6428 and loss is: 0.0010972409509122372\n",
      "Iteration is: 6429 and loss is: 0.0013550438452512026\n",
      "Iteration is: 6430 and loss is: 0.0021816580556333065\n",
      "Iteration is: 6431 and loss is: 0.0028074891306459904\n",
      "Iteration is: 6432 and loss is: 0.0025815265253186226\n",
      "Iteration is: 6433 and loss is: 0.0019429404055699706\n",
      "Iteration is: 6434 and loss is: 0.0013299502898007631\n",
      "Iteration is: 6435 and loss is: 0.0011152650695294142\n",
      "Iteration is: 6436 and loss is: 0.0012675761245191097\n",
      "Iteration is: 6437 and loss is: 0.0015034982934594154\n",
      "Iteration is: 6438 and loss is: 0.0016155196353793144\n",
      "Iteration is: 6439 and loss is: 0.0015776163199916482\n",
      "Iteration is: 6440 and loss is: 0.001503954641520977\n",
      "Iteration is: 6441 and loss is: 0.0014524402795359492\n",
      "Iteration is: 6442 and loss is: 0.0014029863523319364\n",
      "Iteration is: 6443 and loss is: 0.0012991350376978517\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 6444 and loss is: 0.0011756347957998514\n",
      "Iteration is: 6445 and loss is: 0.001093868282623589\n",
      "Iteration is: 6446 and loss is: 0.0011264695785939693\n",
      "Iteration is: 6447 and loss is: 0.0012472285889089108\n",
      "Iteration is: 6448 and loss is: 0.0013447421370074153\n",
      "Iteration is: 6449 and loss is: 0.0013200081884860992\n",
      "Iteration is: 6450 and loss is: 0.0011839964427053928\n",
      "Iteration is: 6451 and loss is: 0.0010465104132890701\n",
      "Iteration is: 6452 and loss is: 0.0009966323850676417\n",
      "Iteration is: 6453 and loss is: 0.0010335028637200594\n",
      "Iteration is: 6454 and loss is: 0.0010957173071801662\n",
      "Iteration is: 6455 and loss is: 0.0011227407958358526\n",
      "Iteration is: 6456 and loss is: 0.0011069657048210502\n",
      "Iteration is: 6457 and loss is: 0.0010757455602288246\n",
      "Iteration is: 6458 and loss is: 0.0010578206274658442\n",
      "Iteration is: 6459 and loss is: 0.0010520359501242638\n",
      "Iteration is: 6460 and loss is: 0.0010393294505774975\n",
      "Iteration is: 6461 and loss is: 0.0010143903782591224\n",
      "Iteration is: 6462 and loss is: 0.0009921867167577147\n",
      "Iteration is: 6463 and loss is: 0.0009899811120703816\n",
      "Iteration is: 6464 and loss is: 0.0010080538922920823\n",
      "Iteration is: 6465 and loss is: 0.001028413767926395\n",
      "Iteration is: 6466 and loss is: 0.0010318015702068806\n",
      "Iteration is: 6467 and loss is: 0.0010164224077016115\n",
      "Iteration is: 6468 and loss is: 0.0009941132739186287\n",
      "Iteration is: 6469 and loss is: 0.0009798649698495865\n",
      "Iteration is: 6470 and loss is: 0.000978827360086143\n",
      "Iteration is: 6471 and loss is: 0.000985149061307311\n",
      "Iteration is: 6472 and loss is: 0.0009899770375341177\n",
      "Iteration is: 6473 and loss is: 0.0009895152179524302\n",
      "Iteration is: 6474 and loss is: 0.0009864625753834844\n",
      "Iteration is: 6475 and loss is: 0.000985068385489285\n",
      "Iteration is: 6476 and loss is: 0.0009863178711384535\n",
      "Iteration is: 6477 and loss is: 0.0009875850519165397\n",
      "Iteration is: 6478 and loss is: 0.00098577665630728\n",
      "Iteration is: 6479 and loss is: 0.0009803450666368008\n",
      "Iteration is: 6480 and loss is: 0.0009740082896314561\n",
      "Iteration is: 6481 and loss is: 0.0009701589005999267\n",
      "Iteration is: 6482 and loss is: 0.0009700667578727007\n",
      "Iteration is: 6483 and loss is: 0.0009722891263663769\n",
      "Iteration is: 6484 and loss is: 0.0009744211565703154\n",
      "Iteration is: 6485 and loss is: 0.0009749424643814564\n",
      "Iteration is: 6486 and loss is: 0.0009739414090290666\n",
      "Iteration is: 6487 and loss is: 0.000972708803601563\n",
      "Iteration is: 6488 and loss is: 0.0009721943642944098\n",
      "Iteration is: 6489 and loss is: 0.000972274225205183\n",
      "Iteration is: 6490 and loss is: 0.000972208974417299\n",
      "Iteration is: 6491 and loss is: 0.0009713460458442569\n",
      "Iteration is: 6492 and loss is: 0.0009696061024442315\n",
      "Iteration is: 6493 and loss is: 0.0009676283807493746\n",
      "Iteration is: 6494 and loss is: 0.0009661411750130355\n",
      "Iteration is: 6495 and loss is: 0.0009654167224653065\n",
      "Iteration is: 6496 and loss is: 0.0009652303997427225\n",
      "Iteration is: 6497 and loss is: 0.0009651322616264224\n",
      "Iteration is: 6498 and loss is: 0.0009647648548707366\n",
      "Iteration is: 6499 and loss is: 0.000964115490205586\n",
      "Iteration is: 6500 and loss is: 0.0009634464513510466\n",
      "Iteration is: 6501 and loss is: 0.0009629962733015418\n",
      "Iteration is: 6502 and loss is: 0.0009628557600080967\n",
      "Iteration is: 6503 and loss is: 0.0009629130945540965\n",
      "Iteration is: 6504 and loss is: 0.0009629473788663745\n",
      "Iteration is: 6505 and loss is: 0.0009628154803067446\n",
      "Iteration is: 6506 and loss is: 0.0009625245002098382\n",
      "Iteration is: 6507 and loss is: 0.0009621906210668385\n",
      "Iteration is: 6508 and loss is: 0.0009619571501389146\n",
      "Iteration is: 6509 and loss is: 0.0009618995245546103\n",
      "Iteration is: 6510 and loss is: 0.0009619814227335155\n",
      "Iteration is: 6511 and loss is: 0.0009621482458896935\n",
      "Iteration is: 6512 and loss is: 0.0009623457444831729\n",
      "Iteration is: 6513 and loss is: 0.0009626062237657607\n",
      "Iteration is: 6514 and loss is: 0.0009630126878619194\n",
      "Iteration is: 6515 and loss is: 0.0009637189214117825\n",
      "Iteration is: 6516 and loss is: 0.000964860082603991\n",
      "Iteration is: 6517 and loss is: 0.0009666238911449909\n",
      "Iteration is: 6518 and loss is: 0.0009691737941466272\n",
      "Iteration is: 6519 and loss is: 0.0009728899458423257\n",
      "Iteration is: 6520 and loss is: 0.0009781171102076769\n",
      "Iteration is: 6521 and loss is: 0.0009857582626864314\n",
      "Iteration is: 6522 and loss is: 0.0009966149227693677\n",
      "Iteration is: 6523 and loss is: 0.0010127215646207333\n",
      "Iteration is: 6524 and loss is: 0.0010356890270486474\n",
      "Iteration is: 6525 and loss is: 0.0010701208375394344\n",
      "Iteration is: 6526 and loss is: 0.001118918415158987\n",
      "Iteration is: 6527 and loss is: 0.0011923529673367739\n",
      "Iteration is: 6528 and loss is: 0.0012940061278641224\n",
      "Iteration is: 6529 and loss is: 0.001444449182599783\n",
      "Iteration is: 6530 and loss is: 0.0016398299485445023\n",
      "Iteration is: 6531 and loss is: 0.0019103672821074724\n",
      "Iteration is: 6532 and loss is: 0.0022148776333779097\n",
      "Iteration is: 6533 and loss is: 0.0025600639637559652\n",
      "Iteration is: 6534 and loss is: 0.0028406328056007624\n",
      "Iteration is: 6535 and loss is: 0.002979259705170989\n",
      "Iteration is: 6536 and loss is: 0.002922765677794814\n",
      "Iteration is: 6537 and loss is: 0.0025912122800946236\n",
      "Iteration is: 6538 and loss is: 0.002109698485583067\n",
      "Iteration is: 6539 and loss is: 0.0015738928923383355\n",
      "Iteration is: 6540 and loss is: 0.0011549151968210936\n",
      "Iteration is: 6541 and loss is: 0.00096744584152475\n",
      "Iteration is: 6542 and loss is: 0.0010266570607200265\n",
      "Iteration is: 6543 and loss is: 0.0012485244078561664\n",
      "Iteration is: 6544 and loss is: 0.0015115032438188791\n",
      "Iteration is: 6545 and loss is: 0.0016794644761830568\n",
      "Iteration is: 6546 and loss is: 0.0017466542776674032\n",
      "Iteration is: 6547 and loss is: 0.0016502689104527235\n",
      "Iteration is: 6548 and loss is: 0.0014543766155838966\n",
      "Iteration is: 6549 and loss is: 0.0012407234171405435\n",
      "Iteration is: 6550 and loss is: 0.001094357343390584\n",
      "Iteration is: 6551 and loss is: 0.0010692665819078684\n",
      "Iteration is: 6552 and loss is: 0.0011314251460134983\n",
      "Iteration is: 6553 and loss is: 0.001225968822836876\n",
      "Iteration is: 6554 and loss is: 0.0013412241823971272\n",
      "Iteration is: 6555 and loss is: 0.0014922285918146372\n",
      "Iteration is: 6556 and loss is: 0.0016139200888574123\n",
      "Iteration is: 6557 and loss is: 0.0016430943505838513\n",
      "Iteration is: 6558 and loss is: 0.0015319217927753925\n",
      "Iteration is: 6559 and loss is: 0.0012873763917014003\n",
      "Iteration is: 6560 and loss is: 0.0010568384313955903\n",
      "Iteration is: 6561 and loss is: 0.0009624326485209167\n",
      "Iteration is: 6562 and loss is: 0.0010310597717761993\n",
      "Iteration is: 6563 and loss is: 0.001180944498628378\n",
      "Iteration is: 6564 and loss is: 0.0013237083330750465\n",
      "Iteration is: 6565 and loss is: 0.0014324208023026586\n",
      "Iteration is: 6566 and loss is: 0.0014986851019784808\n",
      "Iteration is: 6567 and loss is: 0.0014987123431637883\n",
      "Iteration is: 6568 and loss is: 0.0014493828639388084\n",
      "Iteration is: 6569 and loss is: 0.0013165759155526757\n",
      "Iteration is: 6570 and loss is: 0.0011393867898732424\n",
      "Iteration is: 6571 and loss is: 0.00100700743496418\n",
      "Iteration is: 6572 and loss is: 0.000981294084340334\n",
      "Iteration is: 6573 and loss is: 0.0010460292687639594\n",
      "Iteration is: 6574 and loss is: 0.0011490796459838748\n",
      "Iteration is: 6575 and loss is: 0.0012596581364050508\n",
      "Iteration is: 6576 and loss is: 0.0013480709167197347\n",
      "Iteration is: 6577 and loss is: 0.0014150893548503518\n",
      "Iteration is: 6578 and loss is: 0.001402899855747819\n",
      "Iteration is: 6579 and loss is: 0.0013424188364297152\n",
      "Iteration is: 6580 and loss is: 0.0012151285773143172\n",
      "Iteration is: 6581 and loss is: 0.001063646050170064\n",
      "Iteration is: 6582 and loss is: 0.000967398751527071\n",
      "Iteration is: 6583 and loss is: 0.0009637852199375629\n",
      "Iteration is: 6584 and loss is: 0.0010277958353981376\n",
      "Iteration is: 6585 and loss is: 0.001118097803555429\n",
      "Iteration is: 6586 and loss is: 0.0012143837520852685\n",
      "Iteration is: 6587 and loss is: 0.0012913680402562022\n",
      "Iteration is: 6588 and loss is: 0.0013511457946151495\n",
      "Iteration is: 6589 and loss is: 0.0013566932175308466\n",
      "Iteration is: 6590 and loss is: 0.001309902872890234\n",
      "Iteration is: 6591 and loss is: 0.00119834637735039\n",
      "Iteration is: 6592 and loss is: 0.0010596937499940395\n",
      "Iteration is: 6593 and loss is: 0.000964319915510714\n",
      "Iteration is: 6594 and loss is: 0.0009486480848863721\n",
      "Iteration is: 6595 and loss is: 0.000999246258288622\n",
      "Iteration is: 6596 and loss is: 0.0010833315318450332\n",
      "Iteration is: 6597 and loss is: 0.001177800353616476\n",
      "Iteration is: 6598 and loss is: 0.0012634543236345053\n",
      "Iteration is: 6599 and loss is: 0.0013403198681771755\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 6600 and loss is: 0.0013701198622584343\n",
      "Iteration is: 6601 and loss is: 0.0013516416074708104\n",
      "Iteration is: 6602 and loss is: 0.0012550150277093053\n",
      "Iteration is: 6603 and loss is: 0.001113763079047203\n",
      "Iteration is: 6604 and loss is: 0.0009927530772984028\n",
      "Iteration is: 6605 and loss is: 0.0009428102057427168\n",
      "Iteration is: 6606 and loss is: 0.0009684637188911438\n",
      "Iteration is: 6607 and loss is: 0.0010433667339384556\n",
      "Iteration is: 6608 and loss is: 0.0011438574874773622\n",
      "Iteration is: 6609 and loss is: 0.0012518910225480795\n",
      "Iteration is: 6610 and loss is: 0.001368977827951312\n",
      "Iteration is: 6611 and loss is: 0.0014448098372668028\n",
      "Iteration is: 6612 and loss is: 0.0014766142703592777\n",
      "Iteration is: 6613 and loss is: 0.001400180859491229\n",
      "Iteration is: 6614 and loss is: 0.0012353920610621572\n",
      "Iteration is: 6615 and loss is: 0.0010578678920865059\n",
      "Iteration is: 6616 and loss is: 0.000951579655520618\n",
      "Iteration is: 6617 and loss is: 0.0009506852366030216\n",
      "Iteration is: 6618 and loss is: 0.0010295106330886483\n",
      "Iteration is: 6619 and loss is: 0.001152710639871657\n",
      "Iteration is: 6620 and loss is: 0.001299893599934876\n",
      "Iteration is: 6621 and loss is: 0.00147675396874547\n",
      "Iteration is: 6622 and loss is: 0.0016182409599423409\n",
      "Iteration is: 6623 and loss is: 0.001718731946311891\n",
      "Iteration is: 6624 and loss is: 0.0016514810267835855\n",
      "Iteration is: 6625 and loss is: 0.0014224894111976027\n",
      "Iteration is: 6626 and loss is: 0.001148631563410163\n",
      "Iteration is: 6627 and loss is: 0.0009736979845911264\n",
      "Iteration is: 6628 and loss is: 0.0009755345527082682\n",
      "Iteration is: 6629 and loss is: 0.0011039350647479296\n",
      "Iteration is: 6630 and loss is: 0.001284886384382844\n",
      "Iteration is: 6631 and loss is: 0.0014931021723896265\n",
      "Iteration is: 6632 and loss is: 0.001748278271406889\n",
      "Iteration is: 6633 and loss is: 0.0019327629124745727\n",
      "Iteration is: 6634 and loss is: 0.0020435526967048645\n",
      "Iteration is: 6635 and loss is: 0.0018740876112133265\n",
      "Iteration is: 6636 and loss is: 0.0014819374773651361\n",
      "Iteration is: 6637 and loss is: 0.0011220420710742474\n",
      "Iteration is: 6638 and loss is: 0.0009975264547392726\n",
      "Iteration is: 6639 and loss is: 0.001131468452513218\n",
      "Iteration is: 6640 and loss is: 0.0013450903352349997\n",
      "Iteration is: 6641 and loss is: 0.0015012447256594896\n",
      "Iteration is: 6642 and loss is: 0.0016197854420170188\n",
      "Iteration is: 6643 and loss is: 0.0017290482064709067\n",
      "Iteration is: 6644 and loss is: 0.001688153832219541\n",
      "Iteration is: 6645 and loss is: 0.0015749267768114805\n",
      "Iteration is: 6646 and loss is: 0.001359014306217432\n",
      "Iteration is: 6647 and loss is: 0.0011051632463932037\n",
      "Iteration is: 6648 and loss is: 0.000961523677688092\n",
      "Iteration is: 6649 and loss is: 0.0010061634238809347\n",
      "Iteration is: 6650 and loss is: 0.0011423975229263306\n",
      "Iteration is: 6651 and loss is: 0.0012522791512310505\n",
      "Iteration is: 6652 and loss is: 0.0013236801605671644\n",
      "Iteration is: 6653 and loss is: 0.0013512715231627226\n",
      "Iteration is: 6654 and loss is: 0.0013309794012457132\n",
      "Iteration is: 6655 and loss is: 0.001232953043654561\n",
      "Iteration is: 6656 and loss is: 0.001135131111368537\n",
      "Iteration is: 6657 and loss is: 0.0010373910190537572\n",
      "Iteration is: 6658 and loss is: 0.0009511757525615394\n",
      "Iteration is: 6659 and loss is: 0.000938150507863611\n",
      "Iteration is: 6660 and loss is: 0.0009869425557553768\n",
      "Iteration is: 6661 and loss is: 0.0010389008093625307\n",
      "Iteration is: 6662 and loss is: 0.0010790496598929167\n",
      "Iteration is: 6663 and loss is: 0.0011139351408928633\n",
      "Iteration is: 6664 and loss is: 0.0011157580884173512\n",
      "Iteration is: 6665 and loss is: 0.001093353726901114\n",
      "Iteration is: 6666 and loss is: 0.0010563913965597749\n",
      "Iteration is: 6667 and loss is: 0.0010124566033482552\n",
      "Iteration is: 6668 and loss is: 0.0009628495899960399\n",
      "Iteration is: 6669 and loss is: 0.0009287182474508882\n",
      "Iteration is: 6670 and loss is: 0.00092298723757267\n",
      "Iteration is: 6671 and loss is: 0.0009352949564345181\n",
      "Iteration is: 6672 and loss is: 0.0009572879644110799\n",
      "Iteration is: 6673 and loss is: 0.0009842131985351443\n",
      "Iteration is: 6674 and loss is: 0.001005878089927137\n",
      "Iteration is: 6675 and loss is: 0.001014714827761054\n",
      "Iteration is: 6676 and loss is: 0.001016238471493125\n",
      "Iteration is: 6677 and loss is: 0.0010080013889819384\n",
      "Iteration is: 6678 and loss is: 0.000989760970696807\n",
      "Iteration is: 6679 and loss is: 0.0009655056055635214\n",
      "Iteration is: 6680 and loss is: 0.0009426419856026769\n",
      "Iteration is: 6681 and loss is: 0.0009242716478183866\n",
      "Iteration is: 6682 and loss is: 0.000914631993509829\n",
      "Iteration is: 6683 and loss is: 0.0009149037068709731\n",
      "Iteration is: 6684 and loss is: 0.0009222552762366831\n",
      "Iteration is: 6685 and loss is: 0.0009331292239949107\n",
      "Iteration is: 6686 and loss is: 0.0009456146508455276\n",
      "Iteration is: 6687 and loss is: 0.0009580738260410726\n",
      "Iteration is: 6688 and loss is: 0.0009683328680694103\n",
      "Iteration is: 6689 and loss is: 0.0009760349639691412\n",
      "Iteration is: 6690 and loss is: 0.0009781487751752138\n",
      "Iteration is: 6691 and loss is: 0.0009751329780556262\n",
      "Iteration is: 6692 and loss is: 0.000965822662692517\n",
      "Iteration is: 6693 and loss is: 0.0009536848519928753\n",
      "Iteration is: 6694 and loss is: 0.0009403994772583246\n",
      "Iteration is: 6695 and loss is: 0.0009288025903515518\n",
      "Iteration is: 6696 and loss is: 0.0009199751075357199\n",
      "Iteration is: 6697 and loss is: 0.0009139180765487254\n",
      "Iteration is: 6698 and loss is: 0.0009097887668758631\n",
      "Iteration is: 6699 and loss is: 0.0009073778055608273\n",
      "Iteration is: 6700 and loss is: 0.0009063823381438851\n",
      "Iteration is: 6701 and loss is: 0.0009063223842531443\n",
      "Iteration is: 6702 and loss is: 0.0009069661609828472\n",
      "Iteration is: 6703 and loss is: 0.0009081608150154352\n",
      "Iteration is: 6704 and loss is: 0.0009095367277041078\n",
      "Iteration is: 6705 and loss is: 0.0009109606617130339\n",
      "Iteration is: 6706 and loss is: 0.0009126481018029153\n",
      "Iteration is: 6707 and loss is: 0.0009146872325800359\n",
      "Iteration is: 6708 and loss is: 0.000917206984013319\n",
      "Iteration is: 6709 and loss is: 0.0009202742367051542\n",
      "Iteration is: 6710 and loss is: 0.0009243666427209973\n",
      "Iteration is: 6711 and loss is: 0.000929558533243835\n",
      "Iteration is: 6712 and loss is: 0.0009364938596263528\n",
      "Iteration is: 6713 and loss is: 0.0009451835649088025\n",
      "Iteration is: 6714 and loss is: 0.0009567445376887918\n",
      "Iteration is: 6715 and loss is: 0.0009707342251203954\n",
      "Iteration is: 6716 and loss is: 0.0009896394331008196\n",
      "Iteration is: 6717 and loss is: 0.0010130682494491339\n",
      "Iteration is: 6718 and loss is: 0.001045972341671586\n",
      "Iteration is: 6719 and loss is: 0.0010873109567910433\n",
      "Iteration is: 6720 and loss is: 0.001146875787526369\n",
      "Iteration is: 6721 and loss is: 0.0012192369904369116\n",
      "Iteration is: 6722 and loss is: 0.0013212726917117834\n",
      "Iteration is: 6723 and loss is: 0.0014329596888273954\n",
      "Iteration is: 6724 and loss is: 0.0015753754414618015\n",
      "Iteration is: 6725 and loss is: 0.0017009895527735353\n",
      "Iteration is: 6726 and loss is: 0.001827038824558258\n",
      "Iteration is: 6727 and loss is: 0.0018862599972635508\n",
      "Iteration is: 6728 and loss is: 0.0018987392541021109\n",
      "Iteration is: 6729 and loss is: 0.0018234571907669306\n",
      "Iteration is: 6730 and loss is: 0.0016984359826892614\n",
      "Iteration is: 6731 and loss is: 0.001526126405224204\n",
      "Iteration is: 6732 and loss is: 0.0013487192336469889\n",
      "Iteration is: 6733 and loss is: 0.0011789256241172552\n",
      "Iteration is: 6734 and loss is: 0.0010416016448289156\n",
      "Iteration is: 6735 and loss is: 0.0009468073258176446\n",
      "Iteration is: 6736 and loss is: 0.0009022614685818553\n",
      "Iteration is: 6737 and loss is: 0.0009043281897902489\n",
      "Iteration is: 6738 and loss is: 0.0009395689703524113\n",
      "Iteration is: 6739 and loss is: 0.0009896333795040846\n",
      "Iteration is: 6740 and loss is: 0.0010371396783739328\n",
      "Iteration is: 6741 and loss is: 0.0010723161976784468\n",
      "Iteration is: 6742 and loss is: 0.0010892781428992748\n",
      "Iteration is: 6743 and loss is: 0.0010944943642243743\n",
      "Iteration is: 6744 and loss is: 0.0010889001423493028\n",
      "Iteration is: 6745 and loss is: 0.0010868003591895103\n",
      "Iteration is: 6746 and loss is: 0.0010854533175006509\n",
      "Iteration is: 6747 and loss is: 0.0010909729171544313\n",
      "Iteration is: 6748 and loss is: 0.0010975637705996633\n",
      "Iteration is: 6749 and loss is: 0.0011003694962710142\n",
      "Iteration is: 6750 and loss is: 0.0010974599281325936\n",
      "Iteration is: 6751 and loss is: 0.0010835056891664863\n",
      "Iteration is: 6752 and loss is: 0.001063544419594109\n",
      "Iteration is: 6753 and loss is: 0.0010381708852946758\n",
      "Iteration is: 6754 and loss is: 0.0010145901469513774\n",
      "Iteration is: 6755 and loss is: 0.000995962880551815\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 6756 and loss is: 0.000984985614195466\n",
      "Iteration is: 6757 and loss is: 0.0009828041074797511\n",
      "Iteration is: 6758 and loss is: 0.0009859860874712467\n",
      "Iteration is: 6759 and loss is: 0.0009946830105036497\n",
      "Iteration is: 6760 and loss is: 0.00100324850063771\n",
      "Iteration is: 6761 and loss is: 0.0010140349622815847\n",
      "Iteration is: 6762 and loss is: 0.001022492884658277\n",
      "Iteration is: 6763 and loss is: 0.0010340465232729912\n",
      "Iteration is: 6764 and loss is: 0.0010451548732817173\n",
      "Iteration is: 6765 and loss is: 0.0010633011115714908\n",
      "Iteration is: 6766 and loss is: 0.0010841934708878398\n",
      "Iteration is: 6767 and loss is: 0.0011161735747009516\n",
      "Iteration is: 6768 and loss is: 0.0011524164583534002\n",
      "Iteration is: 6769 and loss is: 0.001200736267492175\n",
      "Iteration is: 6770 and loss is: 0.0012502175522968173\n",
      "Iteration is: 6771 and loss is: 0.0013064004015177488\n",
      "Iteration is: 6772 and loss is: 0.0013526688562706113\n",
      "Iteration is: 6773 and loss is: 0.0013927662512287498\n",
      "Iteration is: 6774 and loss is: 0.001406642491929233\n",
      "Iteration is: 6775 and loss is: 0.001402510330080986\n",
      "Iteration is: 6776 and loss is: 0.0013657694216817617\n",
      "Iteration is: 6777 and loss is: 0.0013155053602531552\n",
      "Iteration is: 6778 and loss is: 0.0012466502375900745\n",
      "Iteration is: 6779 and loss is: 0.0011829499853774905\n",
      "Iteration is: 6780 and loss is: 0.0011198974680155516\n",
      "Iteration is: 6781 and loss is: 0.0010727319167926908\n",
      "Iteration is: 6782 and loss is: 0.0010326402261853218\n",
      "Iteration is: 6783 and loss is: 0.001005222788080573\n",
      "Iteration is: 6784 and loss is: 0.0009829627815634012\n",
      "Iteration is: 6785 and loss is: 0.0009683073731139302\n",
      "Iteration is: 6786 and loss is: 0.0009572801645845175\n",
      "Iteration is: 6787 and loss is: 0.0009517312282696366\n",
      "Iteration is: 6788 and loss is: 0.0009493400575593114\n",
      "Iteration is: 6789 and loss is: 0.0009515326237305999\n",
      "Iteration is: 6790 and loss is: 0.000956568110268563\n",
      "Iteration is: 6791 and loss is: 0.000965812592767179\n",
      "Iteration is: 6792 and loss is: 0.0009783008135855198\n",
      "Iteration is: 6793 and loss is: 0.0009960448369383812\n",
      "Iteration is: 6794 and loss is: 0.0010188929736614227\n",
      "Iteration is: 6795 and loss is: 0.0010504549136385322\n",
      "Iteration is: 6796 and loss is: 0.0010901354253292084\n",
      "Iteration is: 6797 and loss is: 0.0011440643575042486\n",
      "Iteration is: 6798 and loss is: 0.001207171124406159\n",
      "Iteration is: 6799 and loss is: 0.0012884704628959298\n",
      "Iteration is: 6800 and loss is: 0.0013711783103644848\n",
      "Iteration is: 6801 and loss is: 0.0014670111704617739\n",
      "Iteration is: 6802 and loss is: 0.0015410392079502344\n",
      "Iteration is: 6803 and loss is: 0.0016096893232315779\n",
      "Iteration is: 6804 and loss is: 0.0016270538326352835\n",
      "Iteration is: 6805 and loss is: 0.0016211267793551087\n",
      "Iteration is: 6806 and loss is: 0.00155910465400666\n",
      "Iteration is: 6807 and loss is: 0.0014826323604211211\n",
      "Iteration is: 6808 and loss is: 0.0013792928075417876\n",
      "Iteration is: 6809 and loss is: 0.0012849257327616215\n",
      "Iteration is: 6810 and loss is: 0.0011901166290044785\n",
      "Iteration is: 6811 and loss is: 0.001109802396968007\n",
      "Iteration is: 6812 and loss is: 0.0010369140654802322\n",
      "Iteration is: 6813 and loss is: 0.0009774451609700918\n",
      "Iteration is: 6814 and loss is: 0.0009314710041508079\n",
      "Iteration is: 6815 and loss is: 0.0009020168799906969\n",
      "Iteration is: 6816 and loss is: 0.0008878969820216298\n",
      "Iteration is: 6817 and loss is: 0.0008864141418598592\n",
      "Iteration is: 6818 and loss is: 0.0008929867180995643\n",
      "Iteration is: 6819 and loss is: 0.0009027615888044238\n",
      "Iteration is: 6820 and loss is: 0.0009113510022871196\n",
      "Iteration is: 6821 and loss is: 0.0009165479568764567\n",
      "Iteration is: 6822 and loss is: 0.0009162415517494082\n",
      "Iteration is: 6823 and loss is: 0.0009116564178839326\n",
      "Iteration is: 6824 and loss is: 0.000903112580999732\n",
      "Iteration is: 6825 and loss is: 0.0008934200159274042\n",
      "Iteration is: 6826 and loss is: 0.0008840793161652982\n",
      "Iteration is: 6827 and loss is: 0.0008769426494836807\n",
      "Iteration is: 6828 and loss is: 0.0008726500673219562\n",
      "Iteration is: 6829 and loss is: 0.0008712585549801588\n",
      "Iteration is: 6830 and loss is: 0.0008722678758203983\n",
      "Iteration is: 6831 and loss is: 0.0008749376866035163\n",
      "Iteration is: 6832 and loss is: 0.0008785605896264315\n",
      "Iteration is: 6833 and loss is: 0.0008825903641991317\n",
      "Iteration is: 6834 and loss is: 0.0008867335855029523\n",
      "Iteration is: 6835 and loss is: 0.000891085306648165\n",
      "Iteration is: 6836 and loss is: 0.0008961408166214824\n",
      "Iteration is: 6837 and loss is: 0.0009032184025272727\n",
      "Iteration is: 6838 and loss is: 0.0009140162728726864\n",
      "Iteration is: 6839 and loss is: 0.0009319206001237035\n",
      "Iteration is: 6840 and loss is: 0.0009611856658011675\n",
      "Iteration is: 6841 and loss is: 0.0010090447030961514\n",
      "Iteration is: 6842 and loss is: 0.0010868130484595895\n",
      "Iteration is: 6843 and loss is: 0.0012074278201907873\n",
      "Iteration is: 6844 and loss is: 0.0013989300932735205\n",
      "Iteration is: 6845 and loss is: 0.0016661406261846423\n",
      "Iteration is: 6846 and loss is: 0.0020508498419076204\n",
      "Iteration is: 6847 and loss is: 0.0024589321110397577\n",
      "Iteration is: 6848 and loss is: 0.0028549819253385067\n",
      "Iteration is: 6849 and loss is: 0.003025745740160346\n",
      "Iteration is: 6850 and loss is: 0.0028649885207414627\n",
      "Iteration is: 6851 and loss is: 0.0025902078486979008\n",
      "Iteration is: 6852 and loss is: 0.0022614053450524807\n",
      "Iteration is: 6853 and loss is: 0.0021923352032899857\n",
      "Iteration is: 6854 and loss is: 0.0021782810799777508\n",
      "Iteration is: 6855 and loss is: 0.002153197769075632\n",
      "Iteration is: 6856 and loss is: 0.0018946565687656403\n",
      "Iteration is: 6857 and loss is: 0.001749804592691362\n",
      "Iteration is: 6858 and loss is: 0.002425913931801915\n",
      "Iteration is: 6859 and loss is: 0.0051268236711621284\n",
      "Iteration is: 6860 and loss is: 0.00907820649445057\n",
      "Iteration is: 6861 and loss is: 0.014210065826773643\n",
      "Iteration is: 6862 and loss is: 0.023127226158976555\n",
      "Iteration is: 6863 and loss is: 0.016144774854183197\n",
      "Iteration is: 6864 and loss is: 0.0054952348582446575\n",
      "Iteration is: 6865 and loss is: 0.003482073312625289\n",
      "Iteration is: 6866 and loss is: 0.013517141342163086\n",
      "Iteration is: 6867 and loss is: 0.05981350690126419\n",
      "Iteration is: 6868 and loss is: 0.13904431462287903\n",
      "Iteration is: 6869 and loss is: 0.1181294322013855\n",
      "Iteration is: 6870 and loss is: 0.030567649751901627\n",
      "Iteration is: 6871 and loss is: 0.09861081838607788\n",
      "Iteration is: 6872 and loss is: 0.042151570320129395\n",
      "Iteration is: 6873 and loss is: 0.07868790626525879\n",
      "Iteration is: 6874 and loss is: 0.04027402028441429\n",
      "Iteration is: 6875 and loss is: 0.06558390706777573\n",
      "Iteration is: 6876 and loss is: 0.04731132090091705\n",
      "Iteration is: 6877 and loss is: 0.04664594680070877\n",
      "Iteration is: 6878 and loss is: 0.04538716375827789\n",
      "Iteration is: 6879 and loss is: 0.0400797575712204\n",
      "Iteration is: 6880 and loss is: 0.040498100221157074\n",
      "Iteration is: 6881 and loss is: 0.037483349442481995\n",
      "Iteration is: 6882 and loss is: 0.03347719833254814\n",
      "Iteration is: 6883 and loss is: 0.036924008280038834\n",
      "Iteration is: 6884 and loss is: 0.029827844351530075\n",
      "Iteration is: 6885 and loss is: 0.030435612425208092\n",
      "Iteration is: 6886 and loss is: 0.023052819073200226\n",
      "Iteration is: 6887 and loss is: 0.030163174495100975\n",
      "Iteration is: 6888 and loss is: 0.018788142129778862\n",
      "Iteration is: 6889 and loss is: 0.018495066091418266\n",
      "Iteration is: 6890 and loss is: 0.01774224080145359\n",
      "Iteration is: 6891 and loss is: 0.013610612601041794\n",
      "Iteration is: 6892 and loss is: 0.013576664961874485\n",
      "Iteration is: 6893 and loss is: 0.011185616254806519\n",
      "Iteration is: 6894 and loss is: 0.010642200708389282\n",
      "Iteration is: 6895 and loss is: 0.00838642567396164\n",
      "Iteration is: 6896 and loss is: 0.007591488305479288\n",
      "Iteration is: 6897 and loss is: 0.006973618175834417\n",
      "Iteration is: 6898 and loss is: 0.00725206732749939\n",
      "Iteration is: 6899 and loss is: 0.004948992282152176\n",
      "Iteration is: 6900 and loss is: 0.005843667313456535\n",
      "Iteration is: 6901 and loss is: 0.00469159334897995\n",
      "Iteration is: 6902 and loss is: 0.005439054686576128\n",
      "Iteration is: 6903 and loss is: 0.00423236470669508\n",
      "Iteration is: 6904 and loss is: 0.00459715910255909\n",
      "Iteration is: 6905 and loss is: 0.00440709525719285\n",
      "Iteration is: 6906 and loss is: 0.004305701702833176\n",
      "Iteration is: 6907 and loss is: 0.004098743665963411\n",
      "Iteration is: 6908 and loss is: 0.003920425660908222\n",
      "Iteration is: 6909 and loss is: 0.004195055924355984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 6910 and loss is: 0.0036738375201821327\n",
      "Iteration is: 6911 and loss is: 0.0038669828791171312\n",
      "Iteration is: 6912 and loss is: 0.003709982382133603\n",
      "Iteration is: 6913 and loss is: 0.003854508977383375\n",
      "Iteration is: 6914 and loss is: 0.00334886834025383\n",
      "Iteration is: 6915 and loss is: 0.003436970990151167\n",
      "Iteration is: 6916 and loss is: 0.0033349446021020412\n",
      "Iteration is: 6917 and loss is: 0.003179781138896942\n",
      "Iteration is: 6918 and loss is: 0.003034194465726614\n",
      "Iteration is: 6919 and loss is: 0.002929799258708954\n",
      "Iteration is: 6920 and loss is: 0.002837624866515398\n",
      "Iteration is: 6921 and loss is: 0.0026971253100782633\n",
      "Iteration is: 6922 and loss is: 0.0026327031664550304\n",
      "Iteration is: 6923 and loss is: 0.002466360805556178\n",
      "Iteration is: 6924 and loss is: 0.002398543292656541\n",
      "Iteration is: 6925 and loss is: 0.0022893694695085287\n",
      "Iteration is: 6926 and loss is: 0.0022156292106956244\n",
      "Iteration is: 6927 and loss is: 0.0021477569825947285\n",
      "Iteration is: 6928 and loss is: 0.002060561440885067\n",
      "Iteration is: 6929 and loss is: 0.001985540147870779\n",
      "Iteration is: 6930 and loss is: 0.001970053883269429\n",
      "Iteration is: 6931 and loss is: 0.0019430129323154688\n",
      "Iteration is: 6932 and loss is: 0.0018494443502277136\n",
      "Iteration is: 6933 and loss is: 0.001804226078093052\n",
      "Iteration is: 6934 and loss is: 0.0017606320325285196\n",
      "Iteration is: 6935 and loss is: 0.0017198796849697828\n",
      "Iteration is: 6936 and loss is: 0.0016912997234612703\n",
      "Iteration is: 6937 and loss is: 0.0016646640142425895\n",
      "Iteration is: 6938 and loss is: 0.0016377238789573312\n",
      "Iteration is: 6939 and loss is: 0.0016283034346997738\n",
      "Iteration is: 6940 and loss is: 0.0016192885814234614\n",
      "Iteration is: 6941 and loss is: 0.001581973279826343\n",
      "Iteration is: 6942 and loss is: 0.0015695039182901382\n",
      "Iteration is: 6943 and loss is: 0.0015342957340180874\n",
      "Iteration is: 6944 and loss is: 0.0015214458107948303\n",
      "Iteration is: 6945 and loss is: 0.0014879537047818303\n",
      "Iteration is: 6946 and loss is: 0.001465303124859929\n",
      "Iteration is: 6947 and loss is: 0.001432423247024417\n",
      "Iteration is: 6948 and loss is: 0.0014309261459857225\n",
      "Iteration is: 6949 and loss is: 0.0014070417964830995\n",
      "Iteration is: 6950 and loss is: 0.001397573621943593\n",
      "Iteration is: 6951 and loss is: 0.0013829972594976425\n",
      "Iteration is: 6952 and loss is: 0.0013741918373852968\n",
      "Iteration is: 6953 and loss is: 0.001362381735816598\n",
      "Iteration is: 6954 and loss is: 0.0013453238643705845\n",
      "Iteration is: 6955 and loss is: 0.0013280848506838083\n",
      "Iteration is: 6956 and loss is: 0.0013160816160961986\n",
      "Iteration is: 6957 and loss is: 0.0013052142458036542\n",
      "Iteration is: 6958 and loss is: 0.0012932996032759547\n",
      "Iteration is: 6959 and loss is: 0.0012850568164139986\n",
      "Iteration is: 6960 and loss is: 0.0012753414921462536\n",
      "Iteration is: 6961 and loss is: 0.0012686543632298708\n",
      "Iteration is: 6962 and loss is: 0.0012586952652782202\n",
      "Iteration is: 6963 and loss is: 0.0012489273212850094\n",
      "Iteration is: 6964 and loss is: 0.0012389891780912876\n",
      "Iteration is: 6965 and loss is: 0.0012323406990617514\n",
      "Iteration is: 6966 and loss is: 0.0012226386461406946\n",
      "Iteration is: 6967 and loss is: 0.0012160663027316332\n",
      "Iteration is: 6968 and loss is: 0.0012068281648680568\n",
      "Iteration is: 6969 and loss is: 0.0012014142703264952\n",
      "Iteration is: 6970 and loss is: 0.0011943257413804531\n",
      "Iteration is: 6971 and loss is: 0.0011875755153596401\n",
      "Iteration is: 6972 and loss is: 0.0011808403069153428\n",
      "Iteration is: 6973 and loss is: 0.0011745569063350558\n",
      "Iteration is: 6974 and loss is: 0.0011682228650897741\n",
      "Iteration is: 6975 and loss is: 0.0011617847485467792\n",
      "Iteration is: 6976 and loss is: 0.0011553498916327953\n",
      "Iteration is: 6977 and loss is: 0.0011495060753077269\n",
      "Iteration is: 6978 and loss is: 0.0011442189570516348\n",
      "Iteration is: 6979 and loss is: 0.0011384999379515648\n",
      "Iteration is: 6980 and loss is: 0.0011332465801388025\n",
      "Iteration is: 6981 and loss is: 0.0011276976438239217\n",
      "Iteration is: 6982 and loss is: 0.001122844754718244\n",
      "Iteration is: 6983 and loss is: 0.001117564970627427\n",
      "Iteration is: 6984 and loss is: 0.001112527446821332\n",
      "Iteration is: 6985 and loss is: 0.00110730342566967\n",
      "Iteration is: 6986 and loss is: 0.0011026648571714759\n",
      "Iteration is: 6987 and loss is: 0.001097709871828556\n",
      "Iteration is: 6988 and loss is: 0.0010931461583822966\n",
      "Iteration is: 6989 and loss is: 0.001088485587388277\n",
      "Iteration is: 6990 and loss is: 0.0010841658804565668\n",
      "Iteration is: 6991 and loss is: 0.0010798382572829723\n",
      "Iteration is: 6992 and loss is: 0.0010754842078313231\n",
      "Iteration is: 6993 and loss is: 0.001071240403689444\n",
      "Iteration is: 6994 and loss is: 0.0010671033523976803\n",
      "Iteration is: 6995 and loss is: 0.0010631043696776032\n",
      "Iteration is: 6996 and loss is: 0.0010592094622552395\n",
      "Iteration is: 6997 and loss is: 0.0010553685715422034\n",
      "Iteration is: 6998 and loss is: 0.0010517057962715626\n",
      "Iteration is: 6999 and loss is: 0.001048161881044507\n",
      "Iteration is: 7000 and loss is: 0.001044672098942101\n",
      "Iteration is: 7001 and loss is: 0.0010412965202704072\n",
      "Iteration is: 7002 and loss is: 0.0010379815939813852\n",
      "Iteration is: 7003 and loss is: 0.0010348019422963262\n",
      "Iteration is: 7004 and loss is: 0.0010316723491996527\n",
      "Iteration is: 7005 and loss is: 0.0010286171454936266\n",
      "Iteration is: 7006 and loss is: 0.0010256904643028975\n",
      "Iteration is: 7007 and loss is: 0.0010228344472125173\n",
      "Iteration is: 7008 and loss is: 0.001020076684653759\n",
      "Iteration is: 7009 and loss is: 0.0010173679329454899\n",
      "Iteration is: 7010 and loss is: 0.0010147305438295007\n",
      "Iteration is: 7011 and loss is: 0.0010121794184669852\n",
      "Iteration is: 7012 and loss is: 0.001009677303954959\n",
      "Iteration is: 7013 and loss is: 0.0010072493460029364\n",
      "Iteration is: 7014 and loss is: 0.0010048731928691268\n",
      "Iteration is: 7015 and loss is: 0.001002562465146184\n",
      "Iteration is: 7016 and loss is: 0.0010003159986808896\n",
      "Iteration is: 7017 and loss is: 0.0009981215698644519\n",
      "Iteration is: 7018 and loss is: 0.0009959712624549866\n",
      "Iteration is: 7019 and loss is: 0.0009938857983797789\n",
      "Iteration is: 7020 and loss is: 0.0009918315336108208\n",
      "Iteration is: 7021 and loss is: 0.000989833613857627\n",
      "Iteration is: 7022 and loss is: 0.0009878724813461304\n",
      "Iteration is: 7023 and loss is: 0.0009859568672254682\n",
      "Iteration is: 7024 and loss is: 0.000984079553745687\n",
      "Iteration is: 7025 and loss is: 0.0009822397259995341\n",
      "Iteration is: 7026 and loss is: 0.0009804402943700552\n",
      "Iteration is: 7027 and loss is: 0.0009786769514903426\n",
      "Iteration is: 7028 and loss is: 0.0009769487660378218\n",
      "Iteration is: 7029 and loss is: 0.0009752515470609069\n",
      "Iteration is: 7030 and loss is: 0.0009735905914567411\n",
      "Iteration is: 7031 and loss is: 0.0009719547815620899\n",
      "Iteration is: 7032 and loss is: 0.0009703529649414122\n",
      "Iteration is: 7033 and loss is: 0.0009687786223366857\n",
      "Iteration is: 7034 and loss is: 0.0009672344895079732\n",
      "Iteration is: 7035 and loss is: 0.0009657107875682414\n",
      "Iteration is: 7036 and loss is: 0.0009642213117331266\n",
      "Iteration is: 7037 and loss is: 0.000962756690569222\n",
      "Iteration is: 7038 and loss is: 0.0009613146539777517\n",
      "Iteration is: 7039 and loss is: 0.0009598973556421697\n",
      "Iteration is: 7040 and loss is: 0.0009585011866874993\n",
      "Iteration is: 7041 and loss is: 0.0009571333648636937\n",
      "Iteration is: 7042 and loss is: 0.0009557846933603287\n",
      "Iteration is: 7043 and loss is: 0.0009544581407681108\n",
      "Iteration is: 7044 and loss is: 0.0009531516116112471\n",
      "Iteration is: 7045 and loss is: 0.0009518624283373356\n",
      "Iteration is: 7046 and loss is: 0.0009505972848273814\n",
      "Iteration is: 7047 and loss is: 0.000949347042478621\n",
      "Iteration is: 7048 and loss is: 0.0009481193264946342\n",
      "Iteration is: 7049 and loss is: 0.0009469042415730655\n",
      "Iteration is: 7050 and loss is: 0.0009457076666876674\n",
      "Iteration is: 7051 and loss is: 0.000944535480812192\n",
      "Iteration is: 7052 and loss is: 0.0009433733066543937\n",
      "Iteration is: 7053 and loss is: 0.0009422306902706623\n",
      "Iteration is: 7054 and loss is: 0.0009411011124029756\n",
      "Iteration is: 7055 and loss is: 0.0009399894624948502\n",
      "Iteration is: 7056 and loss is: 0.0009388899779878557\n",
      "Iteration is: 7057 and loss is: 0.000937808072194457\n",
      "Iteration is: 7058 and loss is: 0.0009367362363263965\n",
      "Iteration is: 7059 and loss is: 0.0009356789523735642\n",
      "Iteration is: 7060 and loss is: 0.0009346380829811096\n",
      "Iteration is: 7061 and loss is: 0.0009336101356893778\n",
      "Iteration is: 7062 and loss is: 0.000932596973143518\n",
      "Iteration is: 7063 and loss is: 0.000931589980609715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 7064 and loss is: 0.0009305946296080947\n",
      "Iteration is: 7065 and loss is: 0.000929619709495455\n",
      "Iteration is: 7066 and loss is: 0.0009286505519412458\n",
      "Iteration is: 7067 and loss is: 0.000927695888094604\n",
      "Iteration is: 7068 and loss is: 0.0009267491404898465\n",
      "Iteration is: 7069 and loss is: 0.000925810425542295\n",
      "Iteration is: 7070 and loss is: 0.0009248900460079312\n",
      "Iteration is: 7071 and loss is: 0.000923974032048136\n",
      "Iteration is: 7072 and loss is: 0.0009230745490640402\n",
      "Iteration is: 7073 and loss is: 0.0009221805958077312\n",
      "Iteration is: 7074 and loss is: 0.0009212943259626627\n",
      "Iteration is: 7075 and loss is: 0.000920423655770719\n",
      "Iteration is: 7076 and loss is: 0.0009195569436997175\n",
      "Iteration is: 7077 and loss is: 0.0009187044342979789\n",
      "Iteration is: 7078 and loss is: 0.0009178570471704006\n",
      "Iteration is: 7079 and loss is: 0.000917015306185931\n",
      "Iteration is: 7080 and loss is: 0.0009161884663626552\n",
      "Iteration is: 7081 and loss is: 0.000915366574190557\n",
      "Iteration is: 7082 and loss is: 0.0009145590593107045\n",
      "Iteration is: 7083 and loss is: 0.0009137537563219666\n",
      "Iteration is: 7084 and loss is: 0.0009129565441980958\n",
      "Iteration is: 7085 and loss is: 0.0009121671901084483\n",
      "Iteration is: 7086 and loss is: 0.0009113854030147195\n",
      "Iteration is: 7087 and loss is: 0.000910615490283817\n",
      "Iteration is: 7088 and loss is: 0.0009098491864278913\n",
      "Iteration is: 7089 and loss is: 0.0009090877720154822\n",
      "Iteration is: 7090 and loss is: 0.0009083388140425086\n",
      "Iteration is: 7091 and loss is: 0.0009075942216441035\n",
      "Iteration is: 7092 and loss is: 0.0009068547515198588\n",
      "Iteration is: 7093 and loss is: 0.0009061258751899004\n",
      "Iteration is: 7094 and loss is: 0.0009054015390574932\n",
      "Iteration is: 7095 and loss is: 0.0009046836639754474\n",
      "Iteration is: 7096 and loss is: 0.0009039707947522402\n",
      "Iteration is: 7097 and loss is: 0.0009032655507326126\n",
      "Iteration is: 7098 and loss is: 0.0009025635663419962\n",
      "Iteration is: 7099 and loss is: 0.0009018724085763097\n",
      "Iteration is: 7100 and loss is: 0.0009011836955323815\n",
      "Iteration is: 7101 and loss is: 0.0009005032479763031\n",
      "Iteration is: 7102 and loss is: 0.0008998285047709942\n",
      "Iteration is: 7103 and loss is: 0.0008991555077955127\n",
      "Iteration is: 7104 and loss is: 0.0008984917658381164\n",
      "Iteration is: 7105 and loss is: 0.0008978353580459952\n",
      "Iteration is: 7106 and loss is: 0.0008971794741228223\n",
      "Iteration is: 7107 and loss is: 0.0008965309825725853\n",
      "Iteration is: 7108 and loss is: 0.0008958879625424743\n",
      "Iteration is: 7109 and loss is: 0.0008952461648732424\n",
      "Iteration is: 7110 and loss is: 0.0008946144953370094\n",
      "Iteration is: 7111 and loss is: 0.000893985154107213\n",
      "Iteration is: 7112 and loss is: 0.0008933596545830369\n",
      "Iteration is: 7113 and loss is: 0.0008927445160225034\n",
      "Iteration is: 7114 and loss is: 0.0008921284461393952\n",
      "Iteration is: 7115 and loss is: 0.0008915161015465856\n",
      "Iteration is: 7116 and loss is: 0.0008909121388569474\n",
      "Iteration is: 7117 and loss is: 0.0008903139969334006\n",
      "Iteration is: 7118 and loss is: 0.0008897176594473422\n",
      "Iteration is: 7119 and loss is: 0.0008891216712072492\n",
      "Iteration is: 7120 and loss is: 0.0008885335410013795\n",
      "Iteration is: 7121 and loss is: 0.0008879521628841758\n",
      "Iteration is: 7122 and loss is: 0.0008873747428879142\n",
      "Iteration is: 7123 and loss is: 0.0008867945289239287\n",
      "Iteration is: 7124 and loss is: 0.0008862235117703676\n",
      "Iteration is: 7125 and loss is: 0.0008856523782014847\n",
      "Iteration is: 7126 and loss is: 0.0008850875310599804\n",
      "Iteration is: 7127 and loss is: 0.0008845275151543319\n",
      "Iteration is: 7128 and loss is: 0.0008839724468998611\n",
      "Iteration is: 7129 and loss is: 0.0008834188338369131\n",
      "Iteration is: 7130 and loss is: 0.0008828697027638555\n",
      "Iteration is: 7131 and loss is: 0.0008823230164125562\n",
      "Iteration is: 7132 and loss is: 0.0008817813941277564\n",
      "Iteration is: 7133 and loss is: 0.000881244195625186\n",
      "Iteration is: 7134 and loss is: 0.0008807039121165872\n",
      "Iteration is: 7135 and loss is: 0.0008801755029708147\n",
      "Iteration is: 7136 and loss is: 0.0008796474430710077\n",
      "Iteration is: 7137 and loss is: 0.0008791198488324881\n",
      "Iteration is: 7138 and loss is: 0.0008785946993157268\n",
      "Iteration is: 7139 and loss is: 0.0008780735079199076\n",
      "Iteration is: 7140 and loss is: 0.0008775604073889554\n",
      "Iteration is: 7141 and loss is: 0.0008770463755354285\n",
      "Iteration is: 7142 and loss is: 0.0008765358943492174\n",
      "Iteration is: 7143 and loss is: 0.0008760286727920175\n",
      "Iteration is: 7144 and loss is: 0.0008755253511480987\n",
      "Iteration is: 7145 and loss is: 0.0008750227279961109\n",
      "Iteration is: 7146 and loss is: 0.0008745254599489272\n",
      "Iteration is: 7147 and loss is: 0.0008740277262404561\n",
      "Iteration is: 7148 and loss is: 0.0008735338924452662\n",
      "Iteration is: 7149 and loss is: 0.0008730440167710185\n",
      "Iteration is: 7150 and loss is: 0.0008725570514798164\n",
      "Iteration is: 7151 and loss is: 0.0008720729383639991\n",
      "Iteration is: 7152 and loss is: 0.0008715863805264235\n",
      "Iteration is: 7153 and loss is: 0.0008711088448762894\n",
      "Iteration is: 7154 and loss is: 0.000870629446581006\n",
      "Iteration is: 7155 and loss is: 0.0008701528422534466\n",
      "Iteration is: 7156 and loss is: 0.0008696840377524495\n",
      "Iteration is: 7157 and loss is: 0.0008692069677636027\n",
      "Iteration is: 7158 and loss is: 0.0008687409572303295\n",
      "Iteration is: 7159 and loss is: 0.0008682734915055335\n",
      "Iteration is: 7160 and loss is: 0.0008678127196617424\n",
      "Iteration is: 7161 and loss is: 0.0008673481643199921\n",
      "Iteration is: 7162 and loss is: 0.0008668919326737523\n",
      "Iteration is: 7163 and loss is: 0.0008664332563057542\n",
      "Iteration is: 7164 and loss is: 0.00086597929475829\n",
      "Iteration is: 7165 and loss is: 0.0008655282435938716\n",
      "Iteration is: 7166 and loss is: 0.0008650770178064704\n",
      "Iteration is: 7167 and loss is: 0.000864627945702523\n",
      "Iteration is: 7168 and loss is: 0.0008641820168122649\n",
      "Iteration is: 7169 and loss is: 0.000863739347551018\n",
      "Iteration is: 7170 and loss is: 0.0008632958633825183\n",
      "Iteration is: 7171 and loss is: 0.0008628544164821506\n",
      "Iteration is: 7172 and loss is: 0.000862418208271265\n",
      "Iteration is: 7173 and loss is: 0.0008619811851531267\n",
      "Iteration is: 7174 and loss is: 0.0008615487604402006\n",
      "Iteration is: 7175 and loss is: 0.0008611163357272744\n",
      "Iteration is: 7176 and loss is: 0.0008606806513853371\n",
      "Iteration is: 7177 and loss is: 0.0008602555608376861\n",
      "Iteration is: 7178 and loss is: 0.0008598256972618401\n",
      "Iteration is: 7179 and loss is: 0.0008594019454903901\n",
      "Iteration is: 7180 and loss is: 0.000858975516166538\n",
      "Iteration is: 7181 and loss is: 0.0008585522300563753\n",
      "Iteration is: 7182 and loss is: 0.0008581326110288501\n",
      "Iteration is: 7183 and loss is: 0.0008577136322855949\n",
      "Iteration is: 7184 and loss is: 0.00085729721467942\n",
      "Iteration is: 7185 and loss is: 0.0008568869670853019\n",
      "Iteration is: 7186 and loss is: 0.0008564702002331614\n",
      "Iteration is: 7187 and loss is: 0.0008560596033930779\n",
      "Iteration is: 7188 and loss is: 0.0008556466782465577\n",
      "Iteration is: 7189 and loss is: 0.0008552413783036172\n",
      "Iteration is: 7190 and loss is: 0.0008548314217478037\n",
      "Iteration is: 7191 and loss is: 0.0008544227457605302\n",
      "Iteration is: 7192 and loss is: 0.0008540189010091126\n",
      "Iteration is: 7193 and loss is: 0.0008536160457879305\n",
      "Iteration is: 7194 and loss is: 0.0008532166248187423\n",
      "Iteration is: 7195 and loss is: 0.0008528148173354566\n",
      "Iteration is: 7196 and loss is: 0.000852417666465044\n",
      "Iteration is: 7197 and loss is: 0.0008520180126652122\n",
      "Iteration is: 7198 and loss is: 0.000851621909532696\n",
      "Iteration is: 7199 and loss is: 0.000851226388476789\n",
      "Iteration is: 7200 and loss is: 0.0008508312166668475\n",
      "Iteration is: 7201 and loss is: 0.0008504441357217729\n",
      "Iteration is: 7202 and loss is: 0.0008500515250489116\n",
      "Iteration is: 7203 and loss is: 0.0008496604859828949\n",
      "Iteration is: 7204 and loss is: 0.0008492730557918549\n",
      "Iteration is: 7205 and loss is: 0.0008488834137097001\n",
      "Iteration is: 7206 and loss is: 0.0008485019207000732\n",
      "Iteration is: 7207 and loss is: 0.0008481189724989235\n",
      "Iteration is: 7208 and loss is: 0.0008477320661768317\n",
      "Iteration is: 7209 and loss is: 0.0008473499910905957\n",
      "Iteration is: 7210 and loss is: 0.0008469708845950663\n",
      "Iteration is: 7211 and loss is: 0.0008465900318697095\n",
      "Iteration is: 7212 and loss is: 0.0008462112164124846\n",
      "Iteration is: 7213 and loss is: 0.0008458335651084781\n",
      "Iteration is: 7214 and loss is: 0.0008454578928649426\n",
      "Iteration is: 7215 and loss is: 0.0008450840832665563\n",
      "Iteration is: 7216 and loss is: 0.0008447130676358938\n",
      "Iteration is: 7217 and loss is: 0.0008443383267149329\n",
      "Iteration is: 7218 and loss is: 0.0008439663797616959\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 7219 and loss is: 0.0008435919298790395\n",
      "Iteration is: 7220 and loss is: 0.000843226327560842\n",
      "Iteration is: 7221 and loss is: 0.0008428567089140415\n",
      "Iteration is: 7222 and loss is: 0.0008424879051744938\n",
      "Iteration is: 7223 and loss is: 0.0008421210804954171\n",
      "Iteration is: 7224 and loss is: 0.0008417575154453516\n",
      "Iteration is: 7225 and loss is: 0.0008413924369961023\n",
      "Iteration is: 7226 and loss is: 0.0008410298032686114\n",
      "Iteration is: 7227 and loss is: 0.0008406683919019997\n",
      "Iteration is: 7228 and loss is: 0.0008403061656281352\n",
      "Iteration is: 7229 and loss is: 0.0008399439975619316\n",
      "Iteration is: 7230 and loss is: 0.0008395871846005321\n",
      "Iteration is: 7231 and loss is: 0.0008392268791794777\n",
      "Iteration is: 7232 and loss is: 0.0008388706482946873\n",
      "Iteration is: 7233 and loss is: 0.0008385126711800694\n",
      "Iteration is: 7234 and loss is: 0.0008381551597267389\n",
      "Iteration is: 7235 and loss is: 0.0008378021302632987\n",
      "Iteration is: 7236 and loss is: 0.0008374480530619621\n",
      "Iteration is: 7237 and loss is: 0.0008370941504836082\n",
      "Iteration is: 7238 and loss is: 0.0008367422269657254\n",
      "Iteration is: 7239 and loss is: 0.0008363882079720497\n",
      "Iteration is: 7240 and loss is: 0.000836039602290839\n",
      "Iteration is: 7241 and loss is: 0.0008356897160410881\n",
      "Iteration is: 7242 and loss is: 0.0008353387238457799\n",
      "Iteration is: 7243 and loss is: 0.0008349908166565001\n",
      "Iteration is: 7244 and loss is: 0.0008346465183421969\n",
      "Iteration is: 7245 and loss is: 0.0008342976798303425\n",
      "Iteration is: 7246 and loss is: 0.0008339523337781429\n",
      "Iteration is: 7247 and loss is: 0.0008336082100868225\n",
      "Iteration is: 7248 and loss is: 0.0008332626894116402\n",
      "Iteration is: 7249 and loss is: 0.0008329233969561756\n",
      "Iteration is: 7250 and loss is: 0.0008325750241056085\n",
      "Iteration is: 7251 and loss is: 0.0008322350913658738\n",
      "Iteration is: 7252 and loss is: 0.0008318934706039727\n",
      "Iteration is: 7253 and loss is: 0.0008315580198541284\n",
      "Iteration is: 7254 and loss is: 0.0008312126155942678\n",
      "Iteration is: 7255 and loss is: 0.000830876815598458\n",
      "Iteration is: 7256 and loss is: 0.0008305368246510625\n",
      "Iteration is: 7257 and loss is: 0.0008301971247419715\n",
      "Iteration is: 7258 and loss is: 0.0008298635948449373\n",
      "Iteration is: 7259 and loss is: 0.0008295252919197083\n",
      "Iteration is: 7260 and loss is: 0.000829189782962203\n",
      "Iteration is: 7261 and loss is: 0.000828851480036974\n",
      "Iteration is: 7262 and loss is: 0.0008285202784463763\n",
      "Iteration is: 7263 and loss is: 0.0008281880291178823\n",
      "Iteration is: 7264 and loss is: 0.0008278547320514917\n",
      "Iteration is: 7265 and loss is: 0.0008275243453681469\n",
      "Iteration is: 7266 and loss is: 0.0008271948900073767\n",
      "Iteration is: 7267 and loss is: 0.0008268600795418024\n",
      "Iteration is: 7268 and loss is: 0.0008265330106951296\n",
      "Iteration is: 7269 and loss is: 0.0008262021001428366\n",
      "Iteration is: 7270 and loss is: 0.0008258736343123019\n",
      "Iteration is: 7271 and loss is: 0.0008255417924374342\n",
      "Iteration is: 7272 and loss is: 0.0008252182160504162\n",
      "Iteration is: 7273 and loss is: 0.0008248928934335709\n",
      "Iteration is: 7274 and loss is: 0.0008245662320405245\n",
      "Iteration is: 7275 and loss is: 0.000824232934974134\n",
      "Iteration is: 7276 and loss is: 0.0008239122689701617\n",
      "Iteration is: 7277 and loss is: 0.0008235883433371782\n",
      "Iteration is: 7278 and loss is: 0.0008232634281739593\n",
      "Iteration is: 7279 and loss is: 0.0008229387458413839\n",
      "Iteration is: 7280 and loss is: 0.0008226176723837852\n",
      "Iteration is: 7281 and loss is: 0.0008222954929806292\n",
      "Iteration is: 7282 and loss is: 0.0008219753508456051\n",
      "Iteration is: 7283 and loss is: 0.0008216549758799374\n",
      "Iteration is: 7284 and loss is: 0.000821330351755023\n",
      "Iteration is: 7285 and loss is: 0.0008210086380131543\n",
      "Iteration is: 7286 and loss is: 0.0008206889033317566\n",
      "Iteration is: 7287 and loss is: 0.000820370449218899\n",
      "Iteration is: 7288 and loss is: 0.0008200533920899034\n",
      "Iteration is: 7289 and loss is: 0.0008197343559004366\n",
      "Iteration is: 7290 and loss is: 0.0008194185793399811\n",
      "Iteration is: 7291 and loss is: 0.0008191023953258991\n",
      "Iteration is: 7292 and loss is: 0.0008187842322513461\n",
      "Iteration is: 7293 and loss is: 0.0008184696780517697\n",
      "Iteration is: 7294 and loss is: 0.0008181538432836533\n",
      "Iteration is: 7295 and loss is: 0.0008178389398381114\n",
      "Iteration is: 7296 and loss is: 0.0008175261318683624\n",
      "Iteration is: 7297 and loss is: 0.0008172103553079069\n",
      "Iteration is: 7298 and loss is: 0.000816897489130497\n",
      "Iteration is: 7299 and loss is: 0.0008165831677615643\n",
      "Iteration is: 7300 and loss is: 0.000816270534414798\n",
      "Iteration is: 7301 and loss is: 0.0008159582503139973\n",
      "Iteration is: 7302 and loss is: 0.0008156482363119721\n",
      "Iteration is: 7303 and loss is: 0.0008153381058946252\n",
      "Iteration is: 7304 and loss is: 0.0008150272187776864\n",
      "Iteration is: 7305 and loss is: 0.0008147170301526785\n",
      "Iteration is: 7306 and loss is: 0.0008144068997353315\n",
      "Iteration is: 7307 and loss is: 0.000814095779787749\n",
      "Iteration is: 7308 and loss is: 0.0008137876284308732\n",
      "Iteration is: 7309 and loss is: 0.0008134797681123018\n",
      "Iteration is: 7310 and loss is: 0.0008131730719469488\n",
      "Iteration is: 7311 and loss is: 0.000812862126622349\n",
      "Iteration is: 7312 and loss is: 0.0008125588065013289\n",
      "Iteration is: 7313 and loss is: 0.0008122521685436368\n",
      "Iteration is: 7314 and loss is: 0.0008119437843561172\n",
      "Iteration is: 7315 and loss is: 0.0008116401731967926\n",
      "Iteration is: 7316 and loss is: 0.000811333185993135\n",
      "Iteration is: 7317 and loss is: 0.0008110275375656784\n",
      "Iteration is: 7318 and loss is: 0.0008107262547127903\n",
      "Iteration is: 7319 and loss is: 0.0008104201988317072\n",
      "Iteration is: 7320 and loss is: 0.0008101182174868882\n",
      "Iteration is: 7321 and loss is: 0.0008098110556602478\n",
      "Iteration is: 7322 and loss is: 0.0008095105877146125\n",
      "Iteration is: 7323 and loss is: 0.0008092100033536553\n",
      "Iteration is: 7324 and loss is: 0.0008089065668173134\n",
      "Iteration is: 7325 and loss is: 0.0008086029556579888\n",
      "Iteration is: 7326 and loss is: 0.0008083058055490255\n",
      "Iteration is: 7327 and loss is: 0.0008080030092969537\n",
      "Iteration is: 7328 and loss is: 0.0008077031234279275\n",
      "Iteration is: 7329 and loss is: 0.0008074036450125277\n",
      "Iteration is: 7330 and loss is: 0.0008071045158430934\n",
      "Iteration is: 7331 and loss is: 0.0008068031747825444\n",
      "Iteration is: 7332 and loss is: 0.0008065021829679608\n",
      "Iteration is: 7333 and loss is: 0.0008062033448368311\n",
      "Iteration is: 7334 and loss is: 0.0008059078827500343\n",
      "Iteration is: 7335 and loss is: 0.0008056083461269736\n",
      "Iteration is: 7336 and loss is: 0.0008053138153627515\n",
      "Iteration is: 7337 and loss is: 0.0008050171891227365\n",
      "Iteration is: 7338 and loss is: 0.0008047193987295032\n",
      "Iteration is: 7339 and loss is: 0.0008044207934290171\n",
      "Iteration is: 7340 and loss is: 0.0008041250985115767\n",
      "Iteration is: 7341 and loss is: 0.0008038296946324408\n",
      "Iteration is: 7342 and loss is: 0.0008035340579226613\n",
      "Iteration is: 7343 and loss is: 0.0008032353362068534\n",
      "Iteration is: 7344 and loss is: 0.0008029456366784871\n",
      "Iteration is: 7345 and loss is: 0.0008026502910070121\n",
      "Iteration is: 7346 and loss is: 0.0008023557020351291\n",
      "Iteration is: 7347 and loss is: 0.0008020622772164643\n",
      "Iteration is: 7348 and loss is: 0.0008017656509764493\n",
      "Iteration is: 7349 and loss is: 0.0008014753693714738\n",
      "Iteration is: 7350 and loss is: 0.0008011798490770161\n",
      "Iteration is: 7351 and loss is: 0.0008008878212422132\n",
      "Iteration is: 7352 and loss is: 0.0008005976560525596\n",
      "Iteration is: 7353 and loss is: 0.0008003088878467679\n",
      "Iteration is: 7354 and loss is: 0.0008000137167982757\n",
      "Iteration is: 7355 and loss is: 0.000799724250100553\n",
      "Iteration is: 7356 and loss is: 0.000799432338681072\n",
      "Iteration is: 7357 and loss is: 0.0007991428719833493\n",
      "Iteration is: 7358 and loss is: 0.0007988507277332246\n",
      "Iteration is: 7359 and loss is: 0.0007985597476363182\n",
      "Iteration is: 7360 and loss is: 0.0007982719107531011\n",
      "Iteration is: 7361 and loss is: 0.0007979818619787693\n",
      "Iteration is: 7362 and loss is: 0.0007976912893354893\n",
      "Iteration is: 7363 and loss is: 0.0007974067702889442\n",
      "Iteration is: 7364 and loss is: 0.0007971153245307505\n",
      "Iteration is: 7365 and loss is: 0.0007968281861394644\n",
      "Iteration is: 7366 and loss is: 0.0007965419208630919\n",
      "Iteration is: 7367 and loss is: 0.0007962523959577084\n",
      "Iteration is: 7368 and loss is: 0.0007959665963426232\n",
      "Iteration is: 7369 and loss is: 0.0007956781191751361\n",
      "Iteration is: 7370 and loss is: 0.0007953881868161261\n",
      "Iteration is: 7371 and loss is: 0.0007951065199449658\n",
      "Iteration is: 7372 and loss is: 0.0007948186248540878\n",
      "Iteration is: 7373 and loss is: 0.000794533989392221\n",
      "Iteration is: 7374 and loss is: 0.0007942452793940902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 7375 and loss is: 0.0007939626229926944\n",
      "Iteration is: 7376 and loss is: 0.0007936762413010001\n",
      "Iteration is: 7377 and loss is: 0.0007933910237625241\n",
      "Iteration is: 7378 and loss is: 0.0007931062136776745\n",
      "Iteration is: 7379 and loss is: 0.0007928221602924168\n",
      "Iteration is: 7380 and loss is: 0.0007925395620986819\n",
      "Iteration is: 7381 and loss is: 0.0007922546355985105\n",
      "Iteration is: 7382 and loss is: 0.0007919692434370518\n",
      "Iteration is: 7383 and loss is: 0.0007916863542050123\n",
      "Iteration is: 7384 and loss is: 0.0007914023008197546\n",
      "Iteration is: 7385 and loss is: 0.0007911223219707608\n",
      "Iteration is: 7386 and loss is: 0.0007908328552730381\n",
      "Iteration is: 7387 and loss is: 0.000790556485299021\n",
      "Iteration is: 7388 and loss is: 0.0007902749348431826\n",
      "Iteration is: 7389 and loss is: 0.0007899912307038903\n",
      "Iteration is: 7390 and loss is: 0.0007897099712863564\n",
      "Iteration is: 7391 and loss is: 0.0007894300506450236\n",
      "Iteration is: 7392 and loss is: 0.0007891493150964379\n",
      "Iteration is: 7393 and loss is: 0.000788866076618433\n",
      "Iteration is: 7394 and loss is: 0.0007885830709710717\n",
      "Iteration is: 7395 and loss is: 0.0007883069338276982\n",
      "Iteration is: 7396 and loss is: 0.0007880243938416243\n",
      "Iteration is: 7397 and loss is: 0.0007877453463152051\n",
      "Iteration is: 7398 and loss is: 0.0007874650182202458\n",
      "Iteration is: 7399 and loss is: 0.0007871869020164013\n",
      "Iteration is: 7400 and loss is: 0.0007869075052440166\n",
      "Iteration is: 7401 and loss is: 0.0007866299711167812\n",
      "Iteration is: 7402 and loss is: 0.0007863454520702362\n",
      "Iteration is: 7403 and loss is: 0.0007860702462494373\n",
      "Iteration is: 7404 and loss is: 0.0007857930613681674\n",
      "Iteration is: 7405 and loss is: 0.0007855147123336792\n",
      "Iteration is: 7406 and loss is: 0.0007852375856600702\n",
      "Iteration is: 7407 and loss is: 0.0007849594694562256\n",
      "Iteration is: 7408 and loss is: 0.0007846831576898694\n",
      "Iteration is: 7409 and loss is: 0.0007844051579013467\n",
      "Iteration is: 7410 and loss is: 0.0007841276237741113\n",
      "Iteration is: 7411 and loss is: 0.0007838532910682261\n",
      "Iteration is: 7412 and loss is: 0.0007835736032575369\n",
      "Iteration is: 7413 and loss is: 0.0007832982810214162\n",
      "Iteration is: 7414 and loss is: 0.0007830209797248244\n",
      "Iteration is: 7415 and loss is: 0.0007827453664503992\n",
      "Iteration is: 7416 and loss is: 0.0007824718486517668\n",
      "Iteration is: 7417 and loss is: 0.0007821943145245314\n",
      "Iteration is: 7418 and loss is: 0.000781916780397296\n",
      "Iteration is: 7419 and loss is: 0.0007816453580744565\n",
      "Iteration is: 7420 and loss is: 0.0007813718402758241\n",
      "Iteration is: 7421 and loss is: 0.0007810929091647267\n",
      "Iteration is: 7422 and loss is: 0.0007808199152350426\n",
      "Iteration is: 7423 and loss is: 0.0007805440109223127\n",
      "Iteration is: 7424 and loss is: 0.0007802696200087667\n",
      "Iteration is: 7425 and loss is: 0.0007799972081556916\n",
      "Iteration is: 7426 and loss is: 0.0007797233411110938\n",
      "Iteration is: 7427 and loss is: 0.0007794485427439213\n",
      "Iteration is: 7428 and loss is: 0.000779176945798099\n",
      "Iteration is: 7429 and loss is: 0.0007789010996930301\n",
      "Iteration is: 7430 and loss is: 0.0007786269998177886\n",
      "Iteration is: 7431 and loss is: 0.000778356974478811\n",
      "Iteration is: 7432 and loss is: 0.0007780806045047939\n",
      "Iteration is: 7433 and loss is: 0.0007778102299198508\n",
      "Iteration is: 7434 and loss is: 0.0007775373524054885\n",
      "Iteration is: 7435 and loss is: 0.00077726726885885\n",
      "Iteration is: 7436 and loss is: 0.000776993518229574\n",
      "Iteration is: 7437 and loss is: 0.0007767200004309416\n",
      "Iteration is: 7438 and loss is: 0.000776452012360096\n",
      "Iteration is: 7439 and loss is: 0.0007761775050312281\n",
      "Iteration is: 7440 and loss is: 0.0007759063737466931\n",
      "Iteration is: 7441 and loss is: 0.0007756378035992384\n",
      "Iteration is: 7442 and loss is: 0.0007753657409921288\n",
      "Iteration is: 7443 and loss is: 0.0007750953081995249\n",
      "Iteration is: 7444 and loss is: 0.0007748258067294955\n",
      "Iteration is: 7445 and loss is: 0.000774553045630455\n",
      "Iteration is: 7446 and loss is: 0.0007742823217995465\n",
      "Iteration is: 7447 and loss is: 0.0007740141591057181\n",
      "Iteration is: 7448 and loss is: 0.0007737462874501944\n",
      "Iteration is: 7449 and loss is: 0.0007734745740890503\n",
      "Iteration is: 7450 and loss is: 0.0007732032681815326\n",
      "Iteration is: 7451 and loss is: 0.0007729335920885205\n",
      "Iteration is: 7452 and loss is: 0.0007726664189249277\n",
      "Iteration is: 7453 and loss is: 0.000772396451793611\n",
      "Iteration is: 7454 and loss is: 0.0007721306174062192\n",
      "Iteration is: 7455 and loss is: 0.0007718604756519198\n",
      "Iteration is: 7456 and loss is: 0.0007715914980508387\n",
      "Iteration is: 7457 and loss is: 0.0007713204249739647\n",
      "Iteration is: 7458 and loss is: 0.0007710525533184409\n",
      "Iteration is: 7459 and loss is: 0.0007707863696850836\n",
      "Iteration is: 7460 and loss is: 0.0007705213502049446\n",
      "Iteration is: 7461 and loss is: 0.0007702502189204097\n",
      "Iteration is: 7462 and loss is: 0.0007699797861278057\n",
      "Iteration is: 7463 and loss is: 0.0007697188993915915\n",
      "Iteration is: 7464 and loss is: 0.0007694476516917348\n",
      "Iteration is: 7465 and loss is: 0.0007691786158829927\n",
      "Iteration is: 7466 and loss is: 0.0007689128397032619\n",
      "Iteration is: 7467 and loss is: 0.0007686465396545827\n",
      "Iteration is: 7468 and loss is: 0.0007683818694204092\n",
      "Iteration is: 7469 and loss is: 0.0007681158022023737\n",
      "Iteration is: 7470 and loss is: 0.000767846591770649\n",
      "Iteration is: 7471 and loss is: 0.0007675827364437282\n",
      "Iteration is: 7472 and loss is: 0.0007673161453567445\n",
      "Iteration is: 7473 and loss is: 0.0007670485065318644\n",
      "Iteration is: 7474 and loss is: 0.0007667827885597944\n",
      "Iteration is: 7475 and loss is: 0.0007665162556804717\n",
      "Iteration is: 7476 and loss is: 0.0007662501884624362\n",
      "Iteration is: 7477 and loss is: 0.0007659849943593144\n",
      "Iteration is: 7478 and loss is: 0.0007657207315787673\n",
      "Iteration is: 7479 and loss is: 0.0007654541404917836\n",
      "Iteration is: 7480 and loss is: 0.0007651906926184893\n",
      "Iteration is: 7481 and loss is: 0.0007649261970072985\n",
      "Iteration is: 7482 and loss is: 0.0007646635640412569\n",
      "Iteration is: 7483 and loss is: 0.0007643967983312905\n",
      "Iteration is: 7484 and loss is: 0.000764132128097117\n",
      "Iteration is: 7485 and loss is: 0.0007638626848347485\n",
      "Iteration is: 7486 and loss is: 0.0007636021473444998\n",
      "Iteration is: 7487 and loss is: 0.0007633377099409699\n",
      "Iteration is: 7488 and loss is: 0.0007630732143297791\n",
      "Iteration is: 7489 and loss is: 0.0007628104649484158\n",
      "Iteration is: 7490 and loss is: 0.0007625470170751214\n",
      "Iteration is: 7491 and loss is: 0.0007622868288308382\n",
      "Iteration is: 7492 and loss is: 0.0007620167452841997\n",
      "Iteration is: 7493 and loss is: 0.0007617558585479856\n",
      "Iteration is: 7494 and loss is: 0.0007614929927513003\n",
      "Iteration is: 7495 and loss is: 0.0007612305926159024\n",
      "Iteration is: 7496 and loss is: 0.0007609673193655908\n",
      "Iteration is: 7497 and loss is: 0.0007607049774378538\n",
      "Iteration is: 7498 and loss is: 0.0007604433340020478\n",
      "Iteration is: 7499 and loss is: 0.0007601782563142478\n",
      "Iteration is: 7500 and loss is: 0.0007599161472171545\n",
      "Iteration is: 7501 and loss is: 0.0007596557261422276\n",
      "Iteration is: 7502 and loss is: 0.0007593921618536115\n",
      "Iteration is: 7503 and loss is: 0.0007591320900246501\n",
      "Iteration is: 7504 and loss is: 0.0007588661974295974\n",
      "Iteration is: 7505 and loss is: 0.0007586066494695842\n",
      "Iteration is: 7506 and loss is: 0.0007583452388644218\n",
      "Iteration is: 7507 and loss is: 0.0007580826641060412\n",
      "Iteration is: 7508 and loss is: 0.0007578239310532808\n",
      "Iteration is: 7509 and loss is: 0.0007575592026114464\n",
      "Iteration is: 7510 and loss is: 0.0007573012262582779\n",
      "Iteration is: 7511 and loss is: 0.0007570391753688455\n",
      "Iteration is: 7512 and loss is: 0.0007567760767415166\n",
      "Iteration is: 7513 and loss is: 0.0007565133273601532\n",
      "Iteration is: 7514 and loss is: 0.0007562519749626517\n",
      "Iteration is: 7515 and loss is: 0.0007559928926639259\n",
      "Iteration is: 7516 and loss is: 0.0007557350909337401\n",
      "Iteration is: 7517 and loss is: 0.0007554739131592214\n",
      "Iteration is: 7518 and loss is: 0.0007552144816145301\n",
      "Iteration is: 7519 and loss is: 0.0007549509173259139\n",
      "Iteration is: 7520 and loss is: 0.0007546931155957282\n",
      "Iteration is: 7521 and loss is: 0.000754431588575244\n",
      "Iteration is: 7522 and loss is: 0.0007541764061897993\n",
      "Iteration is: 7523 and loss is: 0.0007539152866229415\n",
      "Iteration is: 7524 and loss is: 0.0007536524208262563\n",
      "Iteration is: 7525 and loss is: 0.0007533931639045477\n",
      "Iteration is: 7526 and loss is: 0.00075313332490623\n",
      "Iteration is: 7527 and loss is: 0.0007528751157224178\n",
      "Iteration is: 7528 and loss is: 0.0007526173139922321\n",
      "Iteration is: 7529 and loss is: 0.0007523558451794088\n",
      "Iteration is: 7530 and loss is: 0.0007520945509895682\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 7531 and loss is: 0.0007518361089751124\n",
      "Iteration is: 7532 and loss is: 0.0007515795296058059\n",
      "Iteration is: 7533 and loss is: 0.000751321786083281\n",
      "Iteration is: 7534 and loss is: 0.0007510615396313369\n",
      "Iteration is: 7535 and loss is: 0.0007508022245019674\n",
      "Iteration is: 7536 and loss is: 0.0007505454705096781\n",
      "Iteration is: 7537 and loss is: 0.0007502913940697908\n",
      "Iteration is: 7538 and loss is: 0.0007500273641198874\n",
      "Iteration is: 7539 and loss is: 0.0007497709011659026\n",
      "Iteration is: 7540 and loss is: 0.0007495104800909758\n",
      "Iteration is: 7541 and loss is: 0.0007492511067539454\n",
      "Iteration is: 7542 and loss is: 0.0007489956915378571\n",
      "Iteration is: 7543 and loss is: 0.0007487377151846886\n",
      "Iteration is: 7544 and loss is: 0.000748481834307313\n",
      "Iteration is: 7545 and loss is: 0.0007482219953089952\n",
      "Iteration is: 7546 and loss is: 0.0007479699561372399\n",
      "Iteration is: 7547 and loss is: 0.000747711630538106\n",
      "Iteration is: 7548 and loss is: 0.0007474494050256908\n",
      "Iteration is: 7549 and loss is: 0.0007471964345313609\n",
      "Iteration is: 7550 and loss is: 0.0007469390984624624\n",
      "Iteration is: 7551 and loss is: 0.0007466803072020411\n",
      "Iteration is: 7552 and loss is: 0.0007464233785867691\n",
      "Iteration is: 7553 and loss is: 0.000746167846955359\n",
      "Iteration is: 7554 and loss is: 0.0007459089392796159\n",
      "Iteration is: 7555 and loss is: 0.0007456549210473895\n",
      "Iteration is: 7556 and loss is: 0.0007453948492184281\n",
      "Iteration is: 7557 and loss is: 0.0007451416458934546\n",
      "Iteration is: 7558 and loss is: 0.0007448828546330333\n",
      "Iteration is: 7559 and loss is: 0.0007446241797879338\n",
      "Iteration is: 7560 and loss is: 0.0007443709182552993\n",
      "Iteration is: 7561 and loss is: 0.0007441153866238892\n",
      "Iteration is: 7562 and loss is: 0.0007438590400852263\n",
      "Iteration is: 7563 and loss is: 0.0007436032174155116\n",
      "Iteration is: 7564 and loss is: 0.0007433486171066761\n",
      "Iteration is: 7565 and loss is: 0.0007430899422615767\n",
      "Iteration is: 7566 and loss is: 0.0007428376120515168\n",
      "Iteration is: 7567 and loss is: 0.0007425803923979402\n",
      "Iteration is: 7568 and loss is: 0.0007423249771818519\n",
      "Iteration is: 7569 and loss is: 0.0007420696783810854\n",
      "Iteration is: 7570 and loss is: 0.0007418142631649971\n",
      "Iteration is: 7571 and loss is: 0.0007415591389872134\n",
      "Iteration is: 7572 and loss is: 0.000741301744710654\n",
      "Iteration is: 7573 and loss is: 0.000741046154871583\n",
      "Iteration is: 7574 and loss is: 0.0007407953380607069\n",
      "Iteration is: 7575 and loss is: 0.0007405392825603485\n",
      "Iteration is: 7576 and loss is: 0.0007402849150821567\n",
      "Iteration is: 7577 and loss is: 0.0007400288013741374\n",
      "Iteration is: 7578 and loss is: 0.0007397732115350664\n",
      "Iteration is: 7579 and loss is: 0.0007395187858492136\n",
      "Iteration is: 7580 and loss is: 0.0007392654661089182\n",
      "Iteration is: 7581 and loss is: 0.0007390128448605537\n",
      "Iteration is: 7582 and loss is: 0.0007387577788904309\n",
      "Iteration is: 7583 and loss is: 0.0007385028293356299\n",
      "Iteration is: 7584 and loss is: 0.0007382456096820533\n",
      "Iteration is: 7585 and loss is: 0.0007379949674941599\n",
      "Iteration is: 7586 and loss is: 0.0007377378642559052\n",
      "Iteration is: 7587 and loss is: 0.000737485708668828\n",
      "Iteration is: 7588 and loss is: 0.0007372298277914524\n",
      "Iteration is: 7589 and loss is: 0.0007369765662588179\n",
      "Iteration is: 7590 and loss is: 0.0007367218495346606\n",
      "Iteration is: 7591 and loss is: 0.0007364705088548362\n",
      "Iteration is: 7592 and loss is: 0.0007362178293988109\n",
      "Iteration is: 7593 and loss is: 0.0007359598530456424\n",
      "Iteration is: 7594 and loss is: 0.0007357096183113754\n",
      "Iteration is: 7595 and loss is: 0.0007354563567787409\n",
      "Iteration is: 7596 and loss is: 0.0007352036191150546\n",
      "Iteration is: 7597 and loss is: 0.0007349519291892648\n",
      "Iteration is: 7598 and loss is: 0.000734695524442941\n",
      "Iteration is: 7599 and loss is: 0.0007344435434788465\n",
      "Iteration is: 7600 and loss is: 0.0007341873133555055\n",
      "Iteration is: 7601 and loss is: 0.000733937427867204\n",
      "Iteration is: 7602 and loss is: 0.0007336870185099542\n",
      "Iteration is: 7603 and loss is: 0.0007334287511184812\n",
      "Iteration is: 7604 and loss is: 0.000733180670067668\n",
      "Iteration is: 7605 and loss is: 0.0007329245563596487\n",
      "Iteration is: 7606 and loss is: 0.0007326746708713472\n",
      "Iteration is: 7607 and loss is: 0.000732419197447598\n",
      "Iteration is: 7608 and loss is: 0.0007321682642214\n",
      "Iteration is: 7609 and loss is: 0.0007319137803278863\n",
      "Iteration is: 7610 and loss is: 0.0007316621486097574\n",
      "Iteration is: 7611 and loss is: 0.0007314126705750823\n",
      "Iteration is: 7612 and loss is: 0.000731155276298523\n",
      "Iteration is: 7613 and loss is: 0.0007309068460017443\n",
      "Iteration is: 7614 and loss is: 0.0007306537008844316\n",
      "Iteration is: 7615 and loss is: 0.0007304011378437281\n",
      "Iteration is: 7616 and loss is: 0.0007301485165953636\n",
      "Iteration is: 7617 and loss is: 0.0007298997370526195\n",
      "Iteration is: 7618 and loss is: 0.0007296453113667667\n",
      "Iteration is: 7619 and loss is: 0.0007293953094631433\n",
      "Iteration is: 7620 and loss is: 0.00072914530755952\n",
      "Iteration is: 7621 and loss is: 0.0007288915221579373\n",
      "Iteration is: 7622 and loss is: 0.0007286393665708601\n",
      "Iteration is: 7623 and loss is: 0.0007283887825906277\n",
      "Iteration is: 7624 and loss is: 0.0007281386060640216\n",
      "Iteration is: 7625 and loss is: 0.0007278849370777607\n",
      "Iteration is: 7626 and loss is: 0.000727633130736649\n",
      "Iteration is: 7627 and loss is: 0.0007273831870406866\n",
      "Iteration is: 7628 and loss is: 0.0007271317881532013\n",
      "Iteration is: 7629 and loss is: 0.0007268801564350724\n",
      "Iteration is: 7630 and loss is: 0.0007266300963237882\n",
      "Iteration is: 7631 and loss is: 0.000726378639228642\n",
      "Iteration is: 7632 and loss is: 0.0007261271821334958\n",
      "Iteration is: 7633 and loss is: 0.0007258764817379415\n",
      "Iteration is: 7634 and loss is: 0.0007256255485117435\n",
      "Iteration is: 7635 and loss is: 0.0007253764197230339\n",
      "Iteration is: 7636 and loss is: 0.000725122052244842\n",
      "Iteration is: 7637 and loss is: 0.0007248758338391781\n",
      "Iteration is: 7638 and loss is: 0.0007246239110827446\n",
      "Iteration is: 7639 and loss is: 0.0007243723957799375\n",
      "Iteration is: 7640 and loss is: 0.0007241233251988888\n",
      "Iteration is: 7641 and loss is: 0.0007238711696118116\n",
      "Iteration is: 7642 and loss is: 0.0007236192468553782\n",
      "Iteration is: 7643 and loss is: 0.0007233719807118177\n",
      "Iteration is: 7644 and loss is: 0.0007231223280541599\n",
      "Iteration is: 7645 and loss is: 0.0007228705799207091\n",
      "Iteration is: 7646 and loss is: 0.0007226219167932868\n",
      "Iteration is: 7647 and loss is: 0.000722366152331233\n",
      "Iteration is: 7648 and loss is: 0.0007221230189315975\n",
      "Iteration is: 7649 and loss is: 0.0007218702230602503\n",
      "Iteration is: 7650 and loss is: 0.0007216219091787934\n",
      "Iteration is: 7651 and loss is: 0.000721368589438498\n",
      "Iteration is: 7652 and loss is: 0.0007211195188574493\n",
      "Iteration is: 7653 and loss is: 0.0007208687020465732\n",
      "Iteration is: 7654 and loss is: 0.0007206189329735935\n",
      "Iteration is: 7655 and loss is: 0.0007203678833320737\n",
      "Iteration is: 7656 and loss is: 0.000720120151527226\n",
      "Iteration is: 7657 and loss is: 0.000719871954061091\n",
      "Iteration is: 7658 and loss is: 0.0007196229416877031\n",
      "Iteration is: 7659 and loss is: 0.0007193718338385224\n",
      "Iteration is: 7660 and loss is: 0.0007191243348643184\n",
      "Iteration is: 7661 and loss is: 0.0007188753224909306\n",
      "Iteration is: 7662 and loss is: 0.0007186278235167265\n",
      "Iteration is: 7663 and loss is: 0.0007183783454820514\n",
      "Iteration is: 7664 and loss is: 0.0007181313121691346\n",
      "Iteration is: 7665 and loss is: 0.0007178877131082118\n",
      "Iteration is: 7666 and loss is: 0.0007176412618719041\n",
      "Iteration is: 7667 and loss is: 0.0007174036582000554\n",
      "Iteration is: 7668 and loss is: 0.0007171648903749883\n",
      "Iteration is: 7669 and loss is: 0.0007169462624005973\n",
      "Iteration is: 7670 and loss is: 0.0007167381700128317\n",
      "Iteration is: 7671 and loss is: 0.0007165632559917867\n",
      "Iteration is: 7672 and loss is: 0.0007164374692365527\n",
      "Iteration is: 7673 and loss is: 0.0007163980626501143\n",
      "Iteration is: 7674 and loss is: 0.0007165158167481422\n",
      "Iteration is: 7675 and loss is: 0.0007169037708081305\n",
      "Iteration is: 7676 and loss is: 0.000717765826266259\n",
      "Iteration is: 7677 and loss is: 0.0007194885984063148\n",
      "Iteration is: 7678 and loss is: 0.000722711265552789\n",
      "Iteration is: 7679 and loss is: 0.0007287387270480394\n",
      "Iteration is: 7680 and loss is: 0.0007396286819130182\n",
      "Iteration is: 7681 and loss is: 0.0007599190576002002\n",
      "Iteration is: 7682 and loss is: 0.0007961790543049574\n",
      "Iteration is: 7683 and loss is: 0.0008648118819110096\n",
      "Iteration is: 7684 and loss is: 0.000985311926342547\n",
      "Iteration is: 7685 and loss is: 0.0012185419909656048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 7686 and loss is: 0.0016083908267319202\n",
      "Iteration is: 7687 and loss is: 0.0023727272637188435\n",
      "Iteration is: 7688 and loss is: 0.0034781056456267834\n",
      "Iteration is: 7689 and loss is: 0.005544171668589115\n",
      "Iteration is: 7690 and loss is: 0.00743460888043046\n",
      "Iteration is: 7691 and loss is: 0.01005876250565052\n",
      "Iteration is: 7692 and loss is: 0.009390119463205338\n",
      "Iteration is: 7693 and loss is: 0.00721227191388607\n",
      "Iteration is: 7694 and loss is: 0.0030987621285021305\n",
      "Iteration is: 7695 and loss is: 0.0008182157762348652\n",
      "Iteration is: 7696 and loss is: 0.0017936999211087823\n",
      "Iteration is: 7697 and loss is: 0.003965807147324085\n",
      "Iteration is: 7698 and loss is: 0.004342535976320505\n",
      "Iteration is: 7699 and loss is: 0.002279733307659626\n",
      "Iteration is: 7700 and loss is: 0.0009110178798437119\n",
      "Iteration is: 7701 and loss is: 0.001561818178743124\n",
      "Iteration is: 7702 and loss is: 0.002629233291372657\n",
      "Iteration is: 7703 and loss is: 0.0025000479072332382\n",
      "Iteration is: 7704 and loss is: 0.0012613015715032816\n",
      "Iteration is: 7705 and loss is: 0.0008770375861786306\n",
      "Iteration is: 7706 and loss is: 0.0016258780378848314\n",
      "Iteration is: 7707 and loss is: 0.0019968177657574415\n",
      "Iteration is: 7708 and loss is: 0.0014024998527020216\n",
      "Iteration is: 7709 and loss is: 0.0007826687069609761\n",
      "Iteration is: 7710 and loss is: 0.0010759865399450064\n",
      "Iteration is: 7711 and loss is: 0.0015529390657320619\n",
      "Iteration is: 7712 and loss is: 0.001347733661532402\n",
      "Iteration is: 7713 and loss is: 0.0009711275342851877\n",
      "Iteration is: 7714 and loss is: 0.0008738778415136039\n",
      "Iteration is: 7715 and loss is: 0.0010104842949658632\n",
      "Iteration is: 7716 and loss is: 0.001160101848654449\n",
      "Iteration is: 7717 and loss is: 0.001081111142411828\n",
      "Iteration is: 7718 and loss is: 0.0008301796042360365\n",
      "Iteration is: 7719 and loss is: 0.0007776364218443632\n",
      "Iteration is: 7720 and loss is: 0.0009742177207954228\n",
      "Iteration is: 7721 and loss is: 0.001023729331791401\n",
      "Iteration is: 7722 and loss is: 0.0008379627834074199\n",
      "Iteration is: 7723 and loss is: 0.0007442579371854663\n",
      "Iteration is: 7724 and loss is: 0.0008309857221320271\n",
      "Iteration is: 7725 and loss is: 0.0008974430384114385\n",
      "Iteration is: 7726 and loss is: 0.0008616651175543666\n",
      "Iteration is: 7727 and loss is: 0.0007859568577259779\n",
      "Iteration is: 7728 and loss is: 0.000743825570680201\n",
      "Iteration is: 7729 and loss is: 0.0007708484772592783\n",
      "Iteration is: 7730 and loss is: 0.0008282141061499715\n",
      "Iteration is: 7731 and loss is: 0.0008236609282903373\n",
      "Iteration is: 7732 and loss is: 0.0007511042058467865\n",
      "Iteration is: 7733 and loss is: 0.0007180738030001521\n",
      "Iteration is: 7734 and loss is: 0.0007525347173213959\n",
      "Iteration is: 7735 and loss is: 0.0007813751581124961\n",
      "Iteration is: 7736 and loss is: 0.0007691642385907471\n",
      "Iteration is: 7737 and loss is: 0.00074104341911152\n",
      "Iteration is: 7738 and loss is: 0.0007234818767756224\n",
      "Iteration is: 7739 and loss is: 0.0007240298436954618\n",
      "Iteration is: 7740 and loss is: 0.0007408763049170375\n",
      "Iteration is: 7741 and loss is: 0.000752118939999491\n",
      "Iteration is: 7742 and loss is: 0.0007375439163297415\n",
      "Iteration is: 7743 and loss is: 0.0007145988056436181\n",
      "Iteration is: 7744 and loss is: 0.0007113015744835138\n",
      "Iteration is: 7745 and loss is: 0.0007236193632707\n",
      "Iteration is: 7746 and loss is: 0.0007308162748813629\n",
      "Iteration is: 7747 and loss is: 0.0007272269576787949\n",
      "Iteration is: 7748 and loss is: 0.0007187440642155707\n",
      "Iteration is: 7749 and loss is: 0.0007101356168277562\n",
      "Iteration is: 7750 and loss is: 0.000707463244907558\n",
      "Iteration is: 7751 and loss is: 0.0007130743470042944\n",
      "Iteration is: 7752 and loss is: 0.0007189086172729731\n",
      "Iteration is: 7753 and loss is: 0.0007167430594563484\n",
      "Iteration is: 7754 and loss is: 0.0007095017936080694\n",
      "Iteration is: 7755 and loss is: 0.0007051529246382415\n",
      "Iteration is: 7756 and loss is: 0.0007052312139421701\n",
      "Iteration is: 7757 and loss is: 0.0007071034051477909\n",
      "Iteration is: 7758 and loss is: 0.0007090824656188488\n",
      "Iteration is: 7759 and loss is: 0.0007092742016538978\n",
      "Iteration is: 7760 and loss is: 0.0007063739467412233\n",
      "Iteration is: 7761 and loss is: 0.0007025967934168875\n",
      "Iteration is: 7762 and loss is: 0.0007013727445155382\n",
      "Iteration is: 7763 and loss is: 0.0007026531966403127\n",
      "Iteration is: 7764 and loss is: 0.0007038941257633269\n",
      "Iteration is: 7765 and loss is: 0.0007038832409307361\n",
      "Iteration is: 7766 and loss is: 0.0007029110565781593\n",
      "Iteration is: 7767 and loss is: 0.0007012772839516401\n",
      "Iteration is: 7768 and loss is: 0.0006996249430812895\n",
      "Iteration is: 7769 and loss is: 0.0006990745896473527\n",
      "Iteration is: 7770 and loss is: 0.0006997036980465055\n",
      "Iteration is: 7771 and loss is: 0.0007002772763371468\n",
      "Iteration is: 7772 and loss is: 0.0006999978213571012\n",
      "Iteration is: 7773 and loss is: 0.0006991602713242173\n",
      "Iteration is: 7774 and loss is: 0.0006982542108744383\n",
      "Iteration is: 7775 and loss is: 0.0006974436691962183\n",
      "Iteration is: 7776 and loss is: 0.0006969728856347501\n",
      "Iteration is: 7777 and loss is: 0.000697021430823952\n",
      "Iteration is: 7778 and loss is: 0.0006972328992560506\n",
      "Iteration is: 7779 and loss is: 0.0006970934919081628\n",
      "Iteration is: 7780 and loss is: 0.0006965925567783415\n",
      "Iteration is: 7781 and loss is: 0.000696005707141012\n",
      "Iteration is: 7782 and loss is: 0.0006954772397875786\n",
      "Iteration is: 7783 and loss is: 0.0006950414972379804\n",
      "Iteration is: 7784 and loss is: 0.0006948013324290514\n",
      "Iteration is: 7785 and loss is: 0.0006947431247681379\n",
      "Iteration is: 7786 and loss is: 0.0006946665816940367\n",
      "Iteration is: 7787 and loss is: 0.0006944079650565982\n",
      "Iteration is: 7788 and loss is: 0.0006940268795005977\n",
      "Iteration is: 7789 and loss is: 0.0006936368881724775\n",
      "Iteration is: 7790 and loss is: 0.0006932577816769481\n",
      "Iteration is: 7791 and loss is: 0.0006929183728061616\n",
      "Iteration is: 7792 and loss is: 0.0006926727946847677\n",
      "Iteration is: 7793 and loss is: 0.0006925058551132679\n",
      "Iteration is: 7794 and loss is: 0.000692333560436964\n",
      "Iteration is: 7795 and loss is: 0.0006920955493114889\n",
      "Iteration is: 7796 and loss is: 0.0006918178405612707\n",
      "Iteration is: 7797 and loss is: 0.0006915230769664049\n",
      "Iteration is: 7798 and loss is: 0.0006912096287123859\n",
      "Iteration is: 7799 and loss is: 0.0006909011281095445\n",
      "Iteration is: 7800 and loss is: 0.0006906327325850725\n",
      "Iteration is: 7801 and loss is: 0.0006904019392095506\n",
      "Iteration is: 7802 and loss is: 0.0006901848828420043\n",
      "Iteration is: 7803 and loss is: 0.0006899572326801717\n",
      "Iteration is: 7804 and loss is: 0.0006897234125062823\n",
      "Iteration is: 7805 and loss is: 0.000689480104483664\n",
      "Iteration is: 7806 and loss is: 0.0006892180535942316\n",
      "Iteration is: 7807 and loss is: 0.0006889464566484094\n",
      "Iteration is: 7808 and loss is: 0.0006886750925332308\n",
      "Iteration is: 7809 and loss is: 0.0006884192698635161\n",
      "Iteration is: 7810 and loss is: 0.0006881728768348694\n",
      "Iteration is: 7811 and loss is: 0.0006879287539049983\n",
      "Iteration is: 7812 and loss is: 0.0006876928964629769\n",
      "Iteration is: 7813 and loss is: 0.0006874620448797941\n",
      "Iteration is: 7814 and loss is: 0.0006872254307381809\n",
      "Iteration is: 7815 and loss is: 0.0006869860226288438\n",
      "Iteration is: 7816 and loss is: 0.0006867380579933524\n",
      "Iteration is: 7817 and loss is: 0.0006864932947792113\n",
      "Iteration is: 7818 and loss is: 0.0006862435257062316\n",
      "Iteration is: 7819 and loss is: 0.0006859961431473494\n",
      "Iteration is: 7820 and loss is: 0.0006857475964352489\n",
      "Iteration is: 7821 and loss is: 0.000685502658598125\n",
      "Iteration is: 7822 and loss is: 0.000685267848894\n",
      "Iteration is: 7823 and loss is: 0.0006850240752100945\n",
      "Iteration is: 7824 and loss is: 0.0006847860058769584\n",
      "Iteration is: 7825 and loss is: 0.0006845497991889715\n",
      "Iteration is: 7826 and loss is: 0.00068431202089414\n",
      "Iteration is: 7827 and loss is: 0.0006840737187303603\n",
      "Iteration is: 7828 and loss is: 0.0006838370463810861\n",
      "Iteration is: 7829 and loss is: 0.0006835982203483582\n",
      "Iteration is: 7830 and loss is: 0.0006833595689386129\n",
      "Iteration is: 7831 and loss is: 0.0006831197533756495\n",
      "Iteration is: 7832 and loss is: 0.000682879239320755\n",
      "Iteration is: 7833 and loss is: 0.0006826386088505387\n",
      "Iteration is: 7834 and loss is: 0.0006824024021625519\n",
      "Iteration is: 7835 and loss is: 0.0006821593269705772\n",
      "Iteration is: 7836 and loss is: 0.0006819218397140503\n",
      "Iteration is: 7837 and loss is: 0.000681683886796236\n",
      "Iteration is: 7838 and loss is: 0.0006814461667090654\n",
      "Iteration is: 7839 and loss is: 0.0006812087376601994\n",
      "Iteration is: 7840 and loss is: 0.0006809717742726207\n",
      "Iteration is: 7841 and loss is: 0.0006807351601310074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 7842 and loss is: 0.0006804961012676358\n",
      "Iteration is: 7843 and loss is: 0.0006802594871260226\n",
      "Iteration is: 7844 and loss is: 0.0006800235132686794\n",
      "Iteration is: 7845 and loss is: 0.0006797880050726235\n",
      "Iteration is: 7846 and loss is: 0.0006795543013140559\n",
      "Iteration is: 7847 and loss is: 0.0006793151842430234\n",
      "Iteration is: 7848 and loss is: 0.0006790815386921167\n",
      "Iteration is: 7849 and loss is: 0.0006788412574678659\n",
      "Iteration is: 7850 and loss is: 0.0006786094745621085\n",
      "Iteration is: 7851 and loss is: 0.0006783773424103856\n",
      "Iteration is: 7852 and loss is: 0.0006781392730772495\n",
      "Iteration is: 7853 and loss is: 0.0006779043469578028\n",
      "Iteration is: 7854 and loss is: 0.0006776684895157814\n",
      "Iteration is: 7855 and loss is: 0.0006774347857572138\n",
      "Iteration is: 7856 and loss is: 0.0006772039923816919\n",
      "Iteration is: 7857 and loss is: 0.0006769702304154634\n",
      "Iteration is: 7858 and loss is: 0.000676737108733505\n",
      "Iteration is: 7859 and loss is: 0.0006765036378055811\n",
      "Iteration is: 7860 and loss is: 0.0006762754637748003\n",
      "Iteration is: 7861 and loss is: 0.000676048337481916\n",
      "Iteration is: 7862 and loss is: 0.0006758258678019047\n",
      "Iteration is: 7863 and loss is: 0.0006756058428436518\n",
      "Iteration is: 7864 and loss is: 0.0006753879133611917\n",
      "Iteration is: 7865 and loss is: 0.0006751841283403337\n",
      "Iteration is: 7866 and loss is: 0.0006749957101419568\n",
      "Iteration is: 7867 and loss is: 0.0006748282467015088\n",
      "Iteration is: 7868 and loss is: 0.0006746947183273733\n",
      "Iteration is: 7869 and loss is: 0.0006746156141161919\n",
      "Iteration is: 7870 and loss is: 0.0006746146827936172\n",
      "Iteration is: 7871 and loss is: 0.0006747454172000289\n",
      "Iteration is: 7872 and loss is: 0.0006750815082341433\n",
      "Iteration is: 7873 and loss is: 0.0006757530500181019\n",
      "Iteration is: 7874 and loss is: 0.000676955096423626\n",
      "Iteration is: 7875 and loss is: 0.00067903968738392\n",
      "Iteration is: 7876 and loss is: 0.0006825471064075828\n",
      "Iteration is: 7877 and loss is: 0.0006884455215185881\n",
      "Iteration is: 7878 and loss is: 0.0006982561899349093\n",
      "Iteration is: 7879 and loss is: 0.0007147989235818386\n",
      "Iteration is: 7880 and loss is: 0.0007423234055750072\n",
      "Iteration is: 7881 and loss is: 0.0007891865679994226\n",
      "Iteration is: 7882 and loss is: 0.0008671518298797309\n",
      "Iteration is: 7883 and loss is: 0.001001065829768777\n",
      "Iteration is: 7884 and loss is: 0.001220697071403265\n",
      "Iteration is: 7885 and loss is: 0.0015954290283843875\n",
      "Iteration is: 7886 and loss is: 0.002175839152187109\n",
      "Iteration is: 7887 and loss is: 0.0031083154026418924\n",
      "Iteration is: 7888 and loss is: 0.004301540087908506\n",
      "Iteration is: 7889 and loss is: 0.0057937935926020145\n",
      "Iteration is: 7890 and loss is: 0.006551994476467371\n",
      "Iteration is: 7891 and loss is: 0.006373786833137274\n",
      "Iteration is: 7892 and loss is: 0.004184530582278967\n",
      "Iteration is: 7893 and loss is: 0.0019104108214378357\n",
      "Iteration is: 7894 and loss is: 0.0008278301684185863\n",
      "Iteration is: 7895 and loss is: 0.0014723220374435186\n",
      "Iteration is: 7896 and loss is: 0.002686808817088604\n",
      "Iteration is: 7897 and loss is: 0.0027446576859802008\n",
      "Iteration is: 7898 and loss is: 0.0018457588739693165\n",
      "Iteration is: 7899 and loss is: 0.0008485470898449421\n",
      "Iteration is: 7900 and loss is: 0.0011375618632882833\n",
      "Iteration is: 7901 and loss is: 0.0019988813437521458\n",
      "Iteration is: 7902 and loss is: 0.0017767399549484253\n",
      "Iteration is: 7903 and loss is: 0.0010295193642377853\n",
      "Iteration is: 7904 and loss is: 0.0007798083825036883\n",
      "Iteration is: 7905 and loss is: 0.001239089760929346\n",
      "Iteration is: 7906 and loss is: 0.0015425276942551136\n",
      "Iteration is: 7907 and loss is: 0.0011541050625965\n",
      "Iteration is: 7908 and loss is: 0.0007625106372870505\n",
      "Iteration is: 7909 and loss is: 0.000858002807945013\n",
      "Iteration is: 7910 and loss is: 0.0011533359065651894\n",
      "Iteration is: 7911 and loss is: 0.0011796191101893783\n",
      "Iteration is: 7912 and loss is: 0.000894606055226177\n",
      "Iteration is: 7913 and loss is: 0.0007128702709451318\n",
      "Iteration is: 7914 and loss is: 0.0008354008896276355\n",
      "Iteration is: 7915 and loss is: 0.0010147226275876164\n",
      "Iteration is: 7916 and loss is: 0.0009818196995183825\n",
      "Iteration is: 7917 and loss is: 0.0007914683665148914\n",
      "Iteration is: 7918 and loss is: 0.0007058390765450895\n",
      "Iteration is: 7919 and loss is: 0.0007878968026489019\n",
      "Iteration is: 7920 and loss is: 0.0008882249239832163\n",
      "Iteration is: 7921 and loss is: 0.0008673511911183596\n",
      "Iteration is: 7922 and loss is: 0.0007547872373834252\n",
      "Iteration is: 7923 and loss is: 0.0006981256883591413\n",
      "Iteration is: 7924 and loss is: 0.0007463996298611164\n",
      "Iteration is: 7925 and loss is: 0.0008061100961640477\n",
      "Iteration is: 7926 and loss is: 0.0007914194138720632\n",
      "Iteration is: 7927 and loss is: 0.0007258817786350846\n",
      "Iteration is: 7928 and loss is: 0.0006939427112229168\n",
      "Iteration is: 7929 and loss is: 0.0007230426999740303\n",
      "Iteration is: 7930 and loss is: 0.0007578292861580849\n",
      "Iteration is: 7931 and loss is: 0.0007470092969015241\n",
      "Iteration is: 7932 and loss is: 0.000706557824742049\n",
      "Iteration is: 7933 and loss is: 0.0006908824434503913\n",
      "Iteration is: 7934 and loss is: 0.0007100912043824792\n",
      "Iteration is: 7935 and loss is: 0.000728564104065299\n",
      "Iteration is: 7936 and loss is: 0.0007199298124760389\n",
      "Iteration is: 7937 and loss is: 0.0006960718892514706\n",
      "Iteration is: 7938 and loss is: 0.000687181600369513\n",
      "Iteration is: 7939 and loss is: 0.0006989655666984618\n",
      "Iteration is: 7940 and loss is: 0.00071002944605425\n",
      "Iteration is: 7941 and loss is: 0.0007044340018182993\n",
      "Iteration is: 7942 and loss is: 0.0006902981549501419\n",
      "Iteration is: 7943 and loss is: 0.0006844839081168175\n",
      "Iteration is: 7944 and loss is: 0.0006906451308168471\n",
      "Iteration is: 7945 and loss is: 0.0006975939031690359\n",
      "Iteration is: 7946 and loss is: 0.0006955935386940837\n",
      "Iteration is: 7947 and loss is: 0.000687155406922102\n",
      "Iteration is: 7948 and loss is: 0.0006823037401773036\n",
      "Iteration is: 7949 and loss is: 0.0006847443873994052\n",
      "Iteration is: 7950 and loss is: 0.0006891526281833649\n",
      "Iteration is: 7951 and loss is: 0.0006892436067573726\n",
      "Iteration is: 7952 and loss is: 0.000684671220369637\n",
      "Iteration is: 7953 and loss is: 0.0006806203164160252\n",
      "Iteration is: 7954 and loss is: 0.0006807416211813688\n",
      "Iteration is: 7955 and loss is: 0.0006833113147877157\n",
      "Iteration is: 7956 and loss is: 0.0006842095172032714\n",
      "Iteration is: 7957 and loss is: 0.0006821545539423823\n",
      "Iteration is: 7958 and loss is: 0.0006793129723519087\n",
      "Iteration is: 7959 and loss is: 0.0006783030112273991\n",
      "Iteration is: 7960 and loss is: 0.0006793535430915654\n",
      "Iteration is: 7961 and loss is: 0.0006803979631513357\n",
      "Iteration is: 7962 and loss is: 0.000679708318784833\n",
      "Iteration is: 7963 and loss is: 0.0006779029499739408\n",
      "Iteration is: 7964 and loss is: 0.0006766642327420413\n",
      "Iteration is: 7965 and loss is: 0.0006767050363123417\n",
      "Iteration is: 7966 and loss is: 0.0006773134227842093\n",
      "Iteration is: 7967 and loss is: 0.000677321688272059\n",
      "Iteration is: 7968 and loss is: 0.0006764059653505683\n",
      "Iteration is: 7969 and loss is: 0.0006752908229827881\n",
      "Iteration is: 7970 and loss is: 0.0006748043233528733\n",
      "Iteration is: 7971 and loss is: 0.0006749376188963652\n",
      "Iteration is: 7972 and loss is: 0.0006750694592483342\n",
      "Iteration is: 7973 and loss is: 0.0006747315637767315\n",
      "Iteration is: 7974 and loss is: 0.00067401013802737\n",
      "Iteration is: 7975 and loss is: 0.0006733615300618112\n",
      "Iteration is: 7976 and loss is: 0.0006730994209647179\n",
      "Iteration is: 7977 and loss is: 0.0006730984896421432\n",
      "Iteration is: 7978 and loss is: 0.0006729962769895792\n",
      "Iteration is: 7979 and loss is: 0.0006726116989739239\n",
      "Iteration is: 7980 and loss is: 0.0006720841629430652\n",
      "Iteration is: 7981 and loss is: 0.0006716559873893857\n",
      "Iteration is: 7982 and loss is: 0.0006714323535561562\n",
      "Iteration is: 7983 and loss is: 0.0006713179172948003\n",
      "Iteration is: 7984 and loss is: 0.0006711221649311483\n",
      "Iteration is: 7985 and loss is: 0.000670777983032167\n",
      "Iteration is: 7986 and loss is: 0.0006703746621496975\n",
      "Iteration is: 7987 and loss is: 0.0006700347294099629\n",
      "Iteration is: 7988 and loss is: 0.0006698038196191192\n",
      "Iteration is: 7989 and loss is: 0.0006696216878481209\n",
      "Iteration is: 7990 and loss is: 0.000669399683829397\n",
      "Iteration is: 7991 and loss is: 0.0006690967129543424\n",
      "Iteration is: 7992 and loss is: 0.0006687641725875437\n",
      "Iteration is: 7993 and loss is: 0.0006684667896479368\n",
      "Iteration is: 7994 and loss is: 0.0006682224338874221\n",
      "Iteration is: 7995 and loss is: 0.0006680088117718697\n",
      "Iteration is: 7996 and loss is: 0.0006677795900031924\n",
      "Iteration is: 7997 and loss is: 0.0006675079930573702\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 7998 and loss is: 0.0006672190502285957\n",
      "Iteration is: 7999 and loss is: 0.0006669374415650964\n",
      "Iteration is: 8000 and loss is: 0.000666684762109071\n",
      "Iteration is: 8001 and loss is: 0.0006664562970399857\n",
      "Iteration is: 8002 and loss is: 0.0006662232335656881\n",
      "Iteration is: 8003 and loss is: 0.0006659767823293805\n",
      "Iteration is: 8004 and loss is: 0.0006657065823674202\n",
      "Iteration is: 8005 and loss is: 0.0006654405733570457\n",
      "Iteration is: 8006 and loss is: 0.000665188068524003\n",
      "Iteration is: 8007 and loss is: 0.0006649513961747289\n",
      "Iteration is: 8008 and loss is: 0.0006647170521318913\n",
      "Iteration is: 8009 and loss is: 0.0006644770037382841\n",
      "Iteration is: 8010 and loss is: 0.0006642297375947237\n",
      "Iteration is: 8011 and loss is: 0.000663973274640739\n",
      "Iteration is: 8012 and loss is: 0.0006637239130213857\n",
      "Iteration is: 8013 and loss is: 0.0006634824676439166\n",
      "Iteration is: 8014 and loss is: 0.0006632483564317226\n",
      "Iteration is: 8015 and loss is: 0.0006630127318203449\n",
      "Iteration is: 8016 and loss is: 0.0006627708789892495\n",
      "Iteration is: 8017 and loss is: 0.0006625294918194413\n",
      "Iteration is: 8018 and loss is: 0.0006622850778512657\n",
      "Iteration is: 8019 and loss is: 0.0006620432832278311\n",
      "Iteration is: 8020 and loss is: 0.0006618055049329996\n",
      "Iteration is: 8021 and loss is: 0.0006615715101361275\n",
      "Iteration is: 8022 and loss is: 0.0006613374571315944\n",
      "Iteration is: 8023 and loss is: 0.0006611007265746593\n",
      "Iteration is: 8024 and loss is: 0.0006608616095036268\n",
      "Iteration is: 8025 and loss is: 0.0006606258684769273\n",
      "Iteration is: 8026 and loss is: 0.0006603890797123313\n",
      "Iteration is: 8027 and loss is: 0.0006601557834073901\n",
      "Iteration is: 8028 and loss is: 0.0006599227781407535\n",
      "Iteration is: 8029 and loss is: 0.0006596898892894387\n",
      "Iteration is: 8030 and loss is: 0.0006594575825147331\n",
      "Iteration is: 8031 and loss is: 0.000659224868286401\n",
      "Iteration is: 8032 and loss is: 0.0006589932017959654\n",
      "Iteration is: 8033 and loss is: 0.0006587603711523116\n",
      "Iteration is: 8034 and loss is: 0.0006585277733393013\n",
      "Iteration is: 8035 and loss is: 0.0006582971545867622\n",
      "Iteration is: 8036 and loss is: 0.0006580656627193093\n",
      "Iteration is: 8037 and loss is: 0.0006578376633115113\n",
      "Iteration is: 8038 and loss is: 0.0006576059386134148\n",
      "Iteration is: 8039 and loss is: 0.0006573768332600594\n",
      "Iteration is: 8040 and loss is: 0.0006571479607373476\n",
      "Iteration is: 8041 and loss is: 0.0006569213001057506\n",
      "Iteration is: 8042 and loss is: 0.0006566888187080622\n",
      "Iteration is: 8043 and loss is: 0.0006564640789292753\n",
      "Iteration is: 8044 and loss is: 0.0006562361377291381\n",
      "Iteration is: 8045 and loss is: 0.0006560106994584203\n",
      "Iteration is: 8046 and loss is: 0.0006557816523127258\n",
      "Iteration is: 8047 and loss is: 0.0006555532454513013\n",
      "Iteration is: 8048 and loss is: 0.0006553269922733307\n",
      "Iteration is: 8049 and loss is: 0.00065510073909536\n",
      "Iteration is: 8050 and loss is: 0.0006548776873387396\n",
      "Iteration is: 8051 and loss is: 0.0006546513177454472\n",
      "Iteration is: 8052 and loss is: 0.0006544252391904593\n",
      "Iteration is: 8053 and loss is: 0.0006542018963955343\n",
      "Iteration is: 8054 and loss is: 0.0006539768655784428\n",
      "Iteration is: 8055 and loss is: 0.0006537515437230468\n",
      "Iteration is: 8056 and loss is: 0.0006535270367749035\n",
      "Iteration is: 8057 and loss is: 0.0006533045088872313\n",
      "Iteration is: 8058 and loss is: 0.0006530835526064038\n",
      "Iteration is: 8059 and loss is: 0.0006528593949042261\n",
      "Iteration is: 8060 and loss is: 0.0006526383222080767\n",
      "Iteration is: 8061 and loss is: 0.0006524125928990543\n",
      "Iteration is: 8062 and loss is: 0.0006521933246403933\n",
      "Iteration is: 8063 and loss is: 0.0006519705639220774\n",
      "Iteration is: 8064 and loss is: 0.0006517490837723017\n",
      "Iteration is: 8065 and loss is: 0.0006515263230539858\n",
      "Iteration is: 8066 and loss is: 0.00065130420261994\n",
      "Iteration is: 8067 and loss is: 0.0006510853418149054\n",
      "Iteration is: 8068 and loss is: 0.0006508648511953652\n",
      "Iteration is: 8069 and loss is: 0.0006506456411443651\n",
      "Iteration is: 8070 and loss is: 0.0006504225311800838\n",
      "Iteration is: 8071 and loss is: 0.0006502041942439973\n",
      "Iteration is: 8072 and loss is: 0.0006499821320176125\n",
      "Iteration is: 8073 and loss is: 0.0006497661815956235\n",
      "Iteration is: 8074 and loss is: 0.0006495449924841523\n",
      "Iteration is: 8075 and loss is: 0.0006493250839412212\n",
      "Iteration is: 8076 and loss is: 0.0006491082604043186\n",
      "Iteration is: 8077 and loss is: 0.0006488870712928474\n",
      "Iteration is: 8078 and loss is: 0.0006486704805865884\n",
      "Iteration is: 8079 and loss is: 0.0006484549958258867\n",
      "Iteration is: 8080 and loss is: 0.0006482349708676338\n",
      "Iteration is: 8081 and loss is: 0.0006480166921392083\n",
      "Iteration is: 8082 and loss is: 0.0006478005670942366\n",
      "Iteration is: 8083 and loss is: 0.0006475814152508974\n",
      "Iteration is: 8084 and loss is: 0.0006473643006756902\n",
      "Iteration is: 8085 and loss is: 0.0006471446249634027\n",
      "Iteration is: 8086 and loss is: 0.0006469303625635803\n",
      "Iteration is: 8087 and loss is: 0.0006467106286436319\n",
      "Iteration is: 8088 and loss is: 0.0006464947946369648\n",
      "Iteration is: 8089 and loss is: 0.0006462808232754469\n",
      "Iteration is: 8090 and loss is: 0.0006460635340772569\n",
      "Iteration is: 8091 and loss is: 0.000645848922431469\n",
      "Iteration is: 8092 and loss is: 0.0006456344854086637\n",
      "Iteration is: 8093 and loss is: 0.0006454162066802382\n",
      "Iteration is: 8094 and loss is: 0.0006452018860727549\n",
      "Iteration is: 8095 and loss is: 0.0006449876818805933\n",
      "Iteration is: 8096 and loss is: 0.0006447708001360297\n",
      "Iteration is: 8097 and loss is: 0.0006445558974519372\n",
      "Iteration is: 8098 and loss is: 0.0006443444290198386\n",
      "Iteration is: 8099 and loss is: 0.0006441287114284933\n",
      "Iteration is: 8100 and loss is: 0.0006439141579903662\n",
      "Iteration is: 8101 and loss is: 0.00064369902247563\n",
      "Iteration is: 8102 and loss is: 0.0006434835959225893\n",
      "Iteration is: 8103 and loss is: 0.0006432720692828298\n",
      "Iteration is: 8104 and loss is: 0.00064305798150599\n",
      "Iteration is: 8105 and loss is: 0.0006428466876968741\n",
      "Iteration is: 8106 and loss is: 0.0006426324835047126\n",
      "Iteration is: 8107 and loss is: 0.0006424194434657693\n",
      "Iteration is: 8108 and loss is: 0.000642205704934895\n",
      "Iteration is: 8109 and loss is: 0.0006419916753657162\n",
      "Iteration is: 8110 and loss is: 0.0006417787517420948\n",
      "Iteration is: 8111 and loss is: 0.0006415679235942662\n",
      "Iteration is: 8112 and loss is: 0.0006413561059162021\n",
      "Iteration is: 8113 and loss is: 0.000641141552478075\n",
      "Iteration is: 8114 and loss is: 0.0006409300258383155\n",
      "Iteration is: 8115 and loss is: 0.0006407197215594351\n",
      "Iteration is: 8116 and loss is: 0.0006405103486031294\n",
      "Iteration is: 8117 and loss is: 0.0006402962608262897\n",
      "Iteration is: 8118 and loss is: 0.0006400842685252428\n",
      "Iteration is: 8119 and loss is: 0.0006398740806616843\n",
      "Iteration is: 8120 and loss is: 0.0006396640092134476\n",
      "Iteration is: 8121 and loss is: 0.0006394523661583662\n",
      "Iteration is: 8122 and loss is: 0.0006392404902726412\n",
      "Iteration is: 8123 and loss is: 0.0006390314665623009\n",
      "Iteration is: 8124 and loss is: 0.0006388197070918977\n",
      "Iteration is: 8125 and loss is: 0.0006386081222444773\n",
      "Iteration is: 8126 and loss is: 0.000638399098534137\n",
      "Iteration is: 8127 and loss is: 0.0006381868734024465\n",
      "Iteration is: 8128 and loss is: 0.0006379785481840372\n",
      "Iteration is: 8129 and loss is: 0.0006377659738063812\n",
      "Iteration is: 8130 and loss is: 0.0006375561933964491\n",
      "Iteration is: 8131 and loss is: 0.000637347053270787\n",
      "Iteration is: 8132 and loss is: 0.0006371400086209178\n",
      "Iteration is: 8133 and loss is: 0.0006369278416968882\n",
      "Iteration is: 8134 and loss is: 0.0006367205642163754\n",
      "Iteration is: 8135 and loss is: 0.000636510259937495\n",
      "Iteration is: 8136 and loss is: 0.0006363008869811893\n",
      "Iteration is: 8137 and loss is: 0.0006360935512930155\n",
      "Iteration is: 8138 and loss is: 0.000635881326161325\n",
      "Iteration is: 8139 and loss is: 0.0006356734083965421\n",
      "Iteration is: 8140 and loss is: 0.0006354675278998911\n",
      "Iteration is: 8141 and loss is: 0.0006352600757963955\n",
      "Iteration is: 8142 and loss is: 0.0006350516923703253\n",
      "Iteration is: 8143 and loss is: 0.0006348468014039099\n",
      "Iteration is: 8144 and loss is: 0.0006346385926008224\n",
      "Iteration is: 8145 and loss is: 0.0006344346329569817\n",
      "Iteration is: 8146 and loss is: 0.0006342328269965947\n",
      "Iteration is: 8147 and loss is: 0.0006340356776490808\n",
      "Iteration is: 8148 and loss is: 0.0006338439998216927\n",
      "Iteration is: 8149 and loss is: 0.0006336663500405848\n",
      "Iteration is: 8150 and loss is: 0.0006335051148198545\n",
      "Iteration is: 8151 and loss is: 0.000633378280326724\n",
      "Iteration is: 8152 and loss is: 0.0006333062774501741\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 8153 and loss is: 0.0006333307246677577\n",
      "Iteration is: 8154 and loss is: 0.0006335240323096514\n",
      "Iteration is: 8155 and loss is: 0.0006340107065625489\n",
      "Iteration is: 8156 and loss is: 0.0006350149633362889\n",
      "Iteration is: 8157 and loss is: 0.0006369564216583967\n",
      "Iteration is: 8158 and loss is: 0.0006405219901353121\n",
      "Iteration is: 8159 and loss is: 0.0006471268134191632\n",
      "Iteration is: 8160 and loss is: 0.0006589573458768427\n",
      "Iteration is: 8161 and loss is: 0.0006809361511841416\n",
      "Iteration is: 8162 and loss is: 0.000719832198228687\n",
      "Iteration is: 8163 and loss is: 0.0007932204171083868\n",
      "Iteration is: 8164 and loss is: 0.0009198958287015557\n",
      "Iteration is: 8165 and loss is: 0.0011625828919932246\n",
      "Iteration is: 8166 and loss is: 0.0015509662916883826\n",
      "Iteration is: 8167 and loss is: 0.002284260466694832\n",
      "Iteration is: 8168 and loss is: 0.003204141277819872\n",
      "Iteration is: 8169 and loss is: 0.004684929270297289\n",
      "Iteration is: 8170 and loss is: 0.00518425740301609\n",
      "Iteration is: 8171 and loss is: 0.005106315482407808\n",
      "Iteration is: 8172 and loss is: 0.0026471849996596575\n",
      "Iteration is: 8173 and loss is: 0.0009745431016199291\n",
      "Iteration is: 8174 and loss is: 0.0010552146704867482\n",
      "Iteration is: 8175 and loss is: 0.0021637140307575464\n",
      "Iteration is: 8176 and loss is: 0.0029164650477468967\n",
      "Iteration is: 8177 and loss is: 0.0015744476113468409\n",
      "Iteration is: 8178 and loss is: 0.0007681185379624367\n",
      "Iteration is: 8179 and loss is: 0.001300428295508027\n",
      "Iteration is: 8180 and loss is: 0.00179946213029325\n",
      "Iteration is: 8181 and loss is: 0.001574565190821886\n",
      "Iteration is: 8182 and loss is: 0.0008477008668705821\n",
      "Iteration is: 8183 and loss is: 0.000855480320751667\n",
      "Iteration is: 8184 and loss is: 0.0013544477988034487\n",
      "Iteration is: 8185 and loss is: 0.001377020264044404\n",
      "Iteration is: 8186 and loss is: 0.0009506623027846217\n",
      "Iteration is: 8187 and loss is: 0.0006901285378262401\n",
      "Iteration is: 8188 and loss is: 0.0009047784842550755\n",
      "Iteration is: 8189 and loss is: 0.001195393269881606\n",
      "Iteration is: 8190 and loss is: 0.001076204003766179\n",
      "Iteration is: 8191 and loss is: 0.0007670202758163214\n",
      "Iteration is: 8192 and loss is: 0.0006619133055210114\n",
      "Iteration is: 8193 and loss is: 0.0008425383130088449\n",
      "Iteration is: 8194 and loss is: 0.0009991928236559033\n",
      "Iteration is: 8195 and loss is: 0.0009001902653835714\n",
      "Iteration is: 8196 and loss is: 0.0007109700236469507\n",
      "Iteration is: 8197 and loss is: 0.0006573999999091029\n",
      "Iteration is: 8198 and loss is: 0.0007540853694081306\n",
      "Iteration is: 8199 and loss is: 0.0008402004023082554\n",
      "Iteration is: 8200 and loss is: 0.0008109075715765357\n",
      "Iteration is: 8201 and loss is: 0.0007095042965374887\n",
      "Iteration is: 8202 and loss is: 0.0006462714518420398\n",
      "Iteration is: 8203 and loss is: 0.0006725034909322858\n",
      "Iteration is: 8204 and loss is: 0.0007416332373395562\n",
      "Iteration is: 8205 and loss is: 0.0007629953324794769\n",
      "Iteration is: 8206 and loss is: 0.0007154513150453568\n",
      "Iteration is: 8207 and loss is: 0.0006532956613227725\n",
      "Iteration is: 8208 and loss is: 0.0006360551342368126\n",
      "Iteration is: 8209 and loss is: 0.0006618588231503963\n",
      "Iteration is: 8210 and loss is: 0.0006963086198084056\n",
      "Iteration is: 8211 and loss is: 0.0007067973492667079\n",
      "Iteration is: 8212 and loss is: 0.0006878206040710211\n",
      "Iteration is: 8213 and loss is: 0.0006536159198731184\n",
      "Iteration is: 8214 and loss is: 0.0006313069025054574\n",
      "Iteration is: 8215 and loss is: 0.0006327069131657481\n",
      "Iteration is: 8216 and loss is: 0.0006499219452962279\n",
      "Iteration is: 8217 and loss is: 0.0006644998793490231\n",
      "Iteration is: 8218 and loss is: 0.0006667406414635479\n",
      "Iteration is: 8219 and loss is: 0.0006562271155416965\n",
      "Iteration is: 8220 and loss is: 0.0006401051068678498\n",
      "Iteration is: 8221 and loss is: 0.0006281221285462379\n",
      "Iteration is: 8222 and loss is: 0.0006270608282648027\n",
      "Iteration is: 8223 and loss is: 0.0006340524414554238\n",
      "Iteration is: 8224 and loss is: 0.0006419151904992759\n",
      "Iteration is: 8225 and loss is: 0.0006446086335927248\n",
      "Iteration is: 8226 and loss is: 0.0006414935342036188\n",
      "Iteration is: 8227 and loss is: 0.000634322757832706\n",
      "Iteration is: 8228 and loss is: 0.000627139350399375\n",
      "Iteration is: 8229 and loss is: 0.0006234465399757028\n",
      "Iteration is: 8230 and loss is: 0.0006243513780646026\n",
      "Iteration is: 8231 and loss is: 0.000627568457275629\n",
      "Iteration is: 8232 and loss is: 0.0006305574206635356\n",
      "Iteration is: 8233 and loss is: 0.0006317660445347428\n",
      "Iteration is: 8234 and loss is: 0.0006307153380475938\n",
      "Iteration is: 8235 and loss is: 0.00062772654928267\n",
      "Iteration is: 8236 and loss is: 0.0006242230301722884\n",
      "Iteration is: 8237 and loss is: 0.0006217233603820205\n",
      "Iteration is: 8238 and loss is: 0.0006208288250491023\n",
      "Iteration is: 8239 and loss is: 0.0006211600266396999\n",
      "Iteration is: 8240 and loss is: 0.0006222131778486073\n",
      "Iteration is: 8241 and loss is: 0.0006233180756680667\n",
      "Iteration is: 8242 and loss is: 0.0006238309433683753\n",
      "Iteration is: 8243 and loss is: 0.000623387168161571\n",
      "Iteration is: 8244 and loss is: 0.0006222620722837746\n",
      "Iteration is: 8245 and loss is: 0.0006208773120306432\n",
      "Iteration is: 8246 and loss is: 0.0006195887690410018\n",
      "Iteration is: 8247 and loss is: 0.0006186104146763682\n",
      "Iteration is: 8248 and loss is: 0.0006181268836371601\n",
      "Iteration is: 8249 and loss is: 0.0006181142525747418\n",
      "Iteration is: 8250 and loss is: 0.0006183419609442353\n",
      "Iteration is: 8251 and loss is: 0.0006185707170516253\n",
      "Iteration is: 8252 and loss is: 0.0006187045946717262\n",
      "Iteration is: 8253 and loss is: 0.0006186642567627132\n",
      "Iteration is: 8254 and loss is: 0.0006183846853673458\n",
      "Iteration is: 8255 and loss is: 0.0006179079646244645\n",
      "Iteration is: 8256 and loss is: 0.0006173469009809196\n",
      "Iteration is: 8257 and loss is: 0.000616785662714392\n",
      "Iteration is: 8258 and loss is: 0.0006162543431855738\n",
      "Iteration is: 8259 and loss is: 0.0006157916504889727\n",
      "Iteration is: 8260 and loss is: 0.0006154400180093944\n",
      "Iteration is: 8261 and loss is: 0.0006151929264888167\n",
      "Iteration is: 8262 and loss is: 0.0006150134140625596\n",
      "Iteration is: 8263 and loss is: 0.00061487325001508\n",
      "Iteration is: 8264 and loss is: 0.0006147664971649647\n",
      "Iteration is: 8265 and loss is: 0.0006146666128188372\n",
      "Iteration is: 8266 and loss is: 0.0006145492661744356\n",
      "Iteration is: 8267 and loss is: 0.0006144046201370656\n",
      "Iteration is: 8268 and loss is: 0.0006142409984022379\n",
      "Iteration is: 8269 and loss is: 0.0006140618352219462\n",
      "Iteration is: 8270 and loss is: 0.0006138568860478699\n",
      "Iteration is: 8271 and loss is: 0.0006136306328698993\n",
      "Iteration is: 8272 and loss is: 0.0006134042632766068\n",
      "Iteration is: 8273 and loss is: 0.0006131761474534869\n",
      "Iteration is: 8274 and loss is: 0.0006129445391707122\n",
      "Iteration is: 8275 and loss is: 0.0006127109518274665\n",
      "Iteration is: 8276 and loss is: 0.0006124881329014897\n",
      "Iteration is: 8277 and loss is: 0.0006122730555944145\n",
      "Iteration is: 8278 and loss is: 0.0006120662437751889\n",
      "Iteration is: 8279 and loss is: 0.0006118657765910029\n",
      "Iteration is: 8280 and loss is: 0.0006116795702837408\n",
      "Iteration is: 8281 and loss is: 0.0006115090218372643\n",
      "Iteration is: 8282 and loss is: 0.0006113571580499411\n",
      "Iteration is: 8283 and loss is: 0.000611225375905633\n",
      "Iteration is: 8284 and loss is: 0.0006111294496804476\n",
      "Iteration is: 8285 and loss is: 0.0006110745598562062\n",
      "Iteration is: 8286 and loss is: 0.0006110762478783727\n",
      "Iteration is: 8287 and loss is: 0.0006111534312367439\n",
      "Iteration is: 8288 and loss is: 0.0006113374838605523\n",
      "Iteration is: 8289 and loss is: 0.0006116744480095804\n",
      "Iteration is: 8290 and loss is: 0.0006122355116531253\n",
      "Iteration is: 8291 and loss is: 0.000613105483353138\n",
      "Iteration is: 8292 and loss is: 0.0006144442595541477\n",
      "Iteration is: 8293 and loss is: 0.0006164467195048928\n",
      "Iteration is: 8294 and loss is: 0.0006194696179591119\n",
      "Iteration is: 8295 and loss is: 0.0006239402573555708\n",
      "Iteration is: 8296 and loss is: 0.0006307131261564791\n",
      "Iteration is: 8297 and loss is: 0.0006407242617569864\n",
      "Iteration is: 8298 and loss is: 0.000655983341857791\n",
      "Iteration is: 8299 and loss is: 0.0006784570286981761\n",
      "Iteration is: 8300 and loss is: 0.0007128410506993532\n",
      "Iteration is: 8301 and loss is: 0.0007625739672221243\n",
      "Iteration is: 8302 and loss is: 0.0008376135956496\n",
      "Iteration is: 8303 and loss is: 0.0009400802664458752\n",
      "Iteration is: 8304 and loss is: 0.00108528567943722\n",
      "Iteration is: 8305 and loss is: 0.0012544100172817707\n",
      "Iteration is: 8306 and loss is: 0.0014469296438619494\n",
      "Iteration is: 8307 and loss is: 0.0015739982482045889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 8308 and loss is: 0.0015803864225745201\n",
      "Iteration is: 8309 and loss is: 0.00139111396856606\n",
      "Iteration is: 8310 and loss is: 0.0010468014515936375\n",
      "Iteration is: 8311 and loss is: 0.0007509096758440137\n",
      "Iteration is: 8312 and loss is: 0.0006391412462107837\n",
      "Iteration is: 8313 and loss is: 0.0007318987045437098\n",
      "Iteration is: 8314 and loss is: 0.0008976860553957522\n",
      "Iteration is: 8315 and loss is: 0.0009477109415456653\n",
      "Iteration is: 8316 and loss is: 0.0008610209915786982\n",
      "Iteration is: 8317 and loss is: 0.0006901157903485\n",
      "Iteration is: 8318 and loss is: 0.0006259260699152946\n",
      "Iteration is: 8319 and loss is: 0.000690265791490674\n",
      "Iteration is: 8320 and loss is: 0.0007746038027107716\n",
      "Iteration is: 8321 and loss is: 0.0008056173101067543\n",
      "Iteration is: 8322 and loss is: 0.000755688757635653\n",
      "Iteration is: 8323 and loss is: 0.0006782773416489363\n",
      "Iteration is: 8324 and loss is: 0.0006247570272535086\n",
      "Iteration is: 8325 and loss is: 0.0006289026932790875\n",
      "Iteration is: 8326 and loss is: 0.0006764251738786697\n",
      "Iteration is: 8327 and loss is: 0.0007190684555098414\n",
      "Iteration is: 8328 and loss is: 0.0007238356629386544\n",
      "Iteration is: 8329 and loss is: 0.0006931493408046663\n",
      "Iteration is: 8330 and loss is: 0.0006524355849251151\n",
      "Iteration is: 8331 and loss is: 0.0006218149792402983\n",
      "Iteration is: 8332 and loss is: 0.0006158082978799939\n",
      "Iteration is: 8333 and loss is: 0.0006322849658317864\n",
      "Iteration is: 8334 and loss is: 0.0006547714583575726\n",
      "Iteration is: 8335 and loss is: 0.0006674610776826739\n",
      "Iteration is: 8336 and loss is: 0.0006629170384258032\n",
      "Iteration is: 8337 and loss is: 0.0006485903868451715\n",
      "Iteration is: 8338 and loss is: 0.0006312832701951265\n",
      "Iteration is: 8339 and loss is: 0.0006164328078739345\n",
      "Iteration is: 8340 and loss is: 0.0006081777391955256\n",
      "Iteration is: 8341 and loss is: 0.0006081136525608599\n",
      "Iteration is: 8342 and loss is: 0.0006143624195829034\n",
      "Iteration is: 8343 and loss is: 0.0006237399065867066\n",
      "Iteration is: 8344 and loss is: 0.0006347464513964951\n",
      "Iteration is: 8345 and loss is: 0.0006484595942310989\n",
      "Iteration is: 8346 and loss is: 0.0006666478584520519\n",
      "Iteration is: 8347 and loss is: 0.0006933626136742532\n",
      "Iteration is: 8348 and loss is: 0.0007347118807956576\n",
      "Iteration is: 8349 and loss is: 0.000803790520876646\n",
      "Iteration is: 8350 and loss is: 0.0009110883693210781\n",
      "Iteration is: 8351 and loss is: 0.0010850504040718079\n",
      "Iteration is: 8352 and loss is: 0.0013150987215340137\n",
      "Iteration is: 8353 and loss is: 0.0016175922937691212\n",
      "Iteration is: 8354 and loss is: 0.001850704662501812\n",
      "Iteration is: 8355 and loss is: 0.0019050419796258211\n",
      "Iteration is: 8356 and loss is: 0.0016280589625239372\n",
      "Iteration is: 8357 and loss is: 0.0011042330879718065\n",
      "Iteration is: 8358 and loss is: 0.0007403354393318295\n",
      "Iteration is: 8359 and loss is: 0.0006931686075404286\n",
      "Iteration is: 8360 and loss is: 0.0008891531033441424\n",
      "Iteration is: 8361 and loss is: 0.001111612655222416\n",
      "Iteration is: 8362 and loss is: 0.0010412922129034996\n",
      "Iteration is: 8363 and loss is: 0.0008087914902716875\n",
      "Iteration is: 8364 and loss is: 0.000630598864518106\n",
      "Iteration is: 8365 and loss is: 0.0006979823810979724\n",
      "Iteration is: 8366 and loss is: 0.0008476224029436707\n",
      "Iteration is: 8367 and loss is: 0.0008724714862182736\n",
      "Iteration is: 8368 and loss is: 0.0008311502169817686\n",
      "Iteration is: 8369 and loss is: 0.0007528407732024789\n",
      "Iteration is: 8370 and loss is: 0.0006648849230259657\n",
      "Iteration is: 8371 and loss is: 0.0006197548937052488\n",
      "Iteration is: 8372 and loss is: 0.0006566370138898492\n",
      "Iteration is: 8373 and loss is: 0.0007389180245809257\n",
      "Iteration is: 8374 and loss is: 0.000798360793851316\n",
      "Iteration is: 8375 and loss is: 0.000801753718405962\n",
      "Iteration is: 8376 and loss is: 0.0007733601378276944\n",
      "Iteration is: 8377 and loss is: 0.0007285951869562268\n",
      "Iteration is: 8378 and loss is: 0.0006717456271871924\n",
      "Iteration is: 8379 and loss is: 0.0006250281585380435\n",
      "Iteration is: 8380 and loss is: 0.0006043020403012633\n",
      "Iteration is: 8381 and loss is: 0.0006084091728553176\n",
      "Iteration is: 8382 and loss is: 0.0006215850007720292\n",
      "Iteration is: 8383 and loss is: 0.0006366503657773137\n",
      "Iteration is: 8384 and loss is: 0.0006611224380321801\n",
      "Iteration is: 8385 and loss is: 0.0006994307041168213\n",
      "Iteration is: 8386 and loss is: 0.0007451344281435013\n",
      "Iteration is: 8387 and loss is: 0.0008031246834434569\n",
      "Iteration is: 8388 and loss is: 0.000871438009198755\n",
      "Iteration is: 8389 and loss is: 0.0009647691622376442\n",
      "Iteration is: 8390 and loss is: 0.0010611164616420865\n",
      "Iteration is: 8391 and loss is: 0.0011465633288025856\n",
      "Iteration is: 8392 and loss is: 0.0011735563166439533\n",
      "Iteration is: 8393 and loss is: 0.001121958834119141\n",
      "Iteration is: 8394 and loss is: 0.0009728028671815991\n",
      "Iteration is: 8395 and loss is: 0.0007855145959183574\n",
      "Iteration is: 8396 and loss is: 0.000645999563857913\n",
      "Iteration is: 8397 and loss is: 0.000612739531788975\n",
      "Iteration is: 8398 and loss is: 0.0006696972413919866\n",
      "Iteration is: 8399 and loss is: 0.0007469542906619608\n",
      "Iteration is: 8400 and loss is: 0.0007774027762934566\n",
      "Iteration is: 8401 and loss is: 0.0007453042780980468\n",
      "Iteration is: 8402 and loss is: 0.0006654891767539084\n",
      "Iteration is: 8403 and loss is: 0.0006104623898863792\n",
      "Iteration is: 8404 and loss is: 0.0006145534571260214\n",
      "Iteration is: 8405 and loss is: 0.0006576488958671689\n",
      "Iteration is: 8406 and loss is: 0.0006929046357981861\n",
      "Iteration is: 8407 and loss is: 0.0006869369535706937\n",
      "Iteration is: 8408 and loss is: 0.000655726995319128\n",
      "Iteration is: 8409 and loss is: 0.0006206966354511678\n",
      "Iteration is: 8410 and loss is: 0.0006030604708939791\n",
      "Iteration is: 8411 and loss is: 0.0006098798476159573\n",
      "Iteration is: 8412 and loss is: 0.0006313691264949739\n",
      "Iteration is: 8413 and loss is: 0.0006486061029136181\n",
      "Iteration is: 8414 and loss is: 0.0006485463818535209\n",
      "Iteration is: 8415 and loss is: 0.0006337379454635084\n",
      "Iteration is: 8416 and loss is: 0.0006148117245174944\n",
      "Iteration is: 8417 and loss is: 0.0006022794987075031\n",
      "Iteration is: 8418 and loss is: 0.0006000255234539509\n",
      "Iteration is: 8419 and loss is: 0.0006067472859285772\n",
      "Iteration is: 8420 and loss is: 0.0006164471269585192\n",
      "Iteration is: 8421 and loss is: 0.0006214078748598695\n",
      "Iteration is: 8422 and loss is: 0.0006191205466166139\n",
      "Iteration is: 8423 and loss is: 0.0006112692644819617\n",
      "Iteration is: 8424 and loss is: 0.0006031483062542975\n",
      "Iteration is: 8425 and loss is: 0.0005981359863653779\n",
      "Iteration is: 8426 and loss is: 0.0005975472158752382\n",
      "Iteration is: 8427 and loss is: 0.0006005883333273232\n",
      "Iteration is: 8428 and loss is: 0.0006046320777386427\n",
      "Iteration is: 8429 and loss is: 0.0006067806389182806\n",
      "Iteration is: 8430 and loss is: 0.0006057687569409609\n",
      "Iteration is: 8431 and loss is: 0.000602556043304503\n",
      "Iteration is: 8432 and loss is: 0.000598847575020045\n",
      "Iteration is: 8433 and loss is: 0.0005961206625215709\n",
      "Iteration is: 8434 and loss is: 0.0005950784543529153\n",
      "Iteration is: 8435 and loss is: 0.0005957364337518811\n",
      "Iteration is: 8436 and loss is: 0.0005972893559373915\n",
      "Iteration is: 8437 and loss is: 0.0005985466414131224\n",
      "Iteration is: 8438 and loss is: 0.0005988162592984736\n",
      "Iteration is: 8439 and loss is: 0.0005980342393741012\n",
      "Iteration is: 8440 and loss is: 0.0005966349854134023\n",
      "Iteration is: 8441 and loss is: 0.0005950432387180626\n",
      "Iteration is: 8442 and loss is: 0.0005937214009463787\n",
      "Iteration is: 8443 and loss is: 0.0005930244224146008\n",
      "Iteration is: 8444 and loss is: 0.0005929870530962944\n",
      "Iteration is: 8445 and loss is: 0.0005933339707553387\n",
      "Iteration is: 8446 and loss is: 0.0005937177920714021\n",
      "Iteration is: 8447 and loss is: 0.0005939212278462946\n",
      "Iteration is: 8448 and loss is: 0.0005938331596553326\n",
      "Iteration is: 8449 and loss is: 0.0005934327491559088\n",
      "Iteration is: 8450 and loss is: 0.0005927737802267075\n",
      "Iteration is: 8451 and loss is: 0.0005920502590015531\n",
      "Iteration is: 8452 and loss is: 0.0005914251087233424\n",
      "Iteration is: 8453 and loss is: 0.0005909684114158154\n",
      "Iteration is: 8454 and loss is: 0.0005906848818995059\n",
      "Iteration is: 8455 and loss is: 0.0005905497819185257\n",
      "Iteration is: 8456 and loss is: 0.000590518640819937\n",
      "Iteration is: 8457 and loss is: 0.0005905130528844893\n",
      "Iteration is: 8458 and loss is: 0.0005904510035179555\n",
      "Iteration is: 8459 and loss is: 0.0005903057754039764\n",
      "Iteration is: 8460 and loss is: 0.0005900824326090515\n",
      "Iteration is: 8461 and loss is: 0.0005897949449717999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 8462 and loss is: 0.0005894617061130702\n",
      "Iteration is: 8463 and loss is: 0.0005891083856113255\n",
      "Iteration is: 8464 and loss is: 0.0005887736333534122\n",
      "Iteration is: 8465 and loss is: 0.0005884724669158459\n",
      "Iteration is: 8466 and loss is: 0.0005882066325284541\n",
      "Iteration is: 8467 and loss is: 0.0005879787495359778\n",
      "Iteration is: 8468 and loss is: 0.0005877816583961248\n",
      "Iteration is: 8469 and loss is: 0.0005876083741895854\n",
      "Iteration is: 8470 and loss is: 0.0005874458001926541\n",
      "Iteration is: 8471 and loss is: 0.0005872867768630385\n",
      "Iteration is: 8472 and loss is: 0.0005871179164387286\n",
      "Iteration is: 8473 and loss is: 0.0005869463202543557\n",
      "Iteration is: 8474 and loss is: 0.0005867625586688519\n",
      "Iteration is: 8475 and loss is: 0.0005865677958354354\n",
      "Iteration is: 8476 and loss is: 0.0005863599944859743\n",
      "Iteration is: 8477 and loss is: 0.000586150970775634\n",
      "Iteration is: 8478 and loss is: 0.0005859354278072715\n",
      "Iteration is: 8479 and loss is: 0.0005857161013409495\n",
      "Iteration is: 8480 and loss is: 0.0005854950286448002\n",
      "Iteration is: 8481 and loss is: 0.0005852769245393574\n",
      "Iteration is: 8482 and loss is: 0.0005850624293088913\n",
      "Iteration is: 8483 and loss is: 0.0005848475848324597\n",
      "Iteration is: 8484 and loss is: 0.0005846328567713499\n",
      "Iteration is: 8485 and loss is: 0.0005844187107868493\n",
      "Iteration is: 8486 and loss is: 0.0005842132377438247\n",
      "Iteration is: 8487 and loss is: 0.0005840064259245992\n",
      "Iteration is: 8488 and loss is: 0.0005838049110025167\n",
      "Iteration is: 8489 and loss is: 0.0005836045020259917\n",
      "Iteration is: 8490 and loss is: 0.0005834032199345529\n",
      "Iteration is: 8491 and loss is: 0.0005832084571011364\n",
      "Iteration is: 8492 and loss is: 0.0005830142181366682\n",
      "Iteration is: 8493 and loss is: 0.0005828234716318548\n",
      "Iteration is: 8494 and loss is: 0.0005826359847560525\n",
      "Iteration is: 8495 and loss is: 0.0005824534455314279\n",
      "Iteration is: 8496 and loss is: 0.0005822808016091585\n",
      "Iteration is: 8497 and loss is: 0.0005821129307150841\n",
      "Iteration is: 8498 and loss is: 0.000581954198423773\n",
      "Iteration is: 8499 and loss is: 0.0005818162462674081\n",
      "Iteration is: 8500 and loss is: 0.0005816992488689721\n",
      "Iteration is: 8501 and loss is: 0.0005816149641759694\n",
      "Iteration is: 8502 and loss is: 0.0005815810873173177\n",
      "Iteration is: 8503 and loss is: 0.0005816133925691247\n",
      "Iteration is: 8504 and loss is: 0.0005817466881126165\n",
      "Iteration is: 8505 and loss is: 0.0005820348160341382\n",
      "Iteration is: 8506 and loss is: 0.0005825415137223899\n",
      "Iteration is: 8507 and loss is: 0.0005834061885252595\n",
      "Iteration is: 8508 and loss is: 0.0005847759312018752\n",
      "Iteration is: 8509 and loss is: 0.0005869767628610134\n",
      "Iteration is: 8510 and loss is: 0.0005903821438550949\n",
      "Iteration is: 8511 and loss is: 0.0005958240944892168\n",
      "Iteration is: 8512 and loss is: 0.0006041678134351969\n",
      "Iteration is: 8513 and loss is: 0.0006176551105454564\n",
      "Iteration is: 8514 and loss is: 0.0006382354185916483\n",
      "Iteration is: 8515 and loss is: 0.0006720851524733007\n",
      "Iteration is: 8516 and loss is: 0.000722964818123728\n",
      "Iteration is: 8517 and loss is: 0.000808097654953599\n",
      "Iteration is: 8518 and loss is: 0.0009306749561801553\n",
      "Iteration is: 8519 and loss is: 0.0011370494030416012\n",
      "Iteration is: 8520 and loss is: 0.0014021587558090687\n",
      "Iteration is: 8521 and loss is: 0.0018349955789744854\n",
      "Iteration is: 8522 and loss is: 0.002237549750134349\n",
      "Iteration is: 8523 and loss is: 0.002812388353049755\n",
      "Iteration is: 8524 and loss is: 0.0028666071593761444\n",
      "Iteration is: 8525 and loss is: 0.0028459045570343733\n",
      "Iteration is: 8526 and loss is: 0.0019965670071542263\n",
      "Iteration is: 8527 and loss is: 0.0012513094116002321\n",
      "Iteration is: 8528 and loss is: 0.0007195582147687674\n",
      "Iteration is: 8529 and loss is: 0.0007121438393369317\n",
      "Iteration is: 8530 and loss is: 0.0010767534840852022\n",
      "Iteration is: 8531 and loss is: 0.0014137269463390112\n",
      "Iteration is: 8532 and loss is: 0.001587586710229516\n",
      "Iteration is: 8533 and loss is: 0.0013687224127352238\n",
      "Iteration is: 8534 and loss is: 0.0010456404415890574\n",
      "Iteration is: 8535 and loss is: 0.0007603075355291367\n",
      "Iteration is: 8536 and loss is: 0.0006221386720426381\n",
      "Iteration is: 8537 and loss is: 0.0006558476015925407\n",
      "Iteration is: 8538 and loss is: 0.0007881755009293556\n",
      "Iteration is: 8539 and loss is: 0.0009047072962857783\n",
      "Iteration is: 8540 and loss is: 0.0009277823846787214\n",
      "Iteration is: 8541 and loss is: 0.0008551825303584337\n",
      "Iteration is: 8542 and loss is: 0.0007600074168294668\n",
      "Iteration is: 8543 and loss is: 0.0006918872240930796\n",
      "Iteration is: 8544 and loss is: 0.0006540181348100305\n",
      "Iteration is: 8545 and loss is: 0.0006424720631912351\n",
      "Iteration is: 8546 and loss is: 0.0006606158567592502\n",
      "Iteration is: 8547 and loss is: 0.0006955459248274565\n",
      "Iteration is: 8548 and loss is: 0.0007204740541055799\n",
      "Iteration is: 8549 and loss is: 0.0007035364396870136\n",
      "Iteration is: 8550 and loss is: 0.0006635078461840749\n",
      "Iteration is: 8551 and loss is: 0.000622758932877332\n",
      "Iteration is: 8552 and loss is: 0.0005974058294668794\n",
      "Iteration is: 8553 and loss is: 0.0005897831288166344\n",
      "Iteration is: 8554 and loss is: 0.0006012467201799154\n",
      "Iteration is: 8555 and loss is: 0.0006273877806961536\n",
      "Iteration is: 8556 and loss is: 0.0006498752627521753\n",
      "Iteration is: 8557 and loss is: 0.0006523454794660211\n",
      "Iteration is: 8558 and loss is: 0.0006331488839350641\n",
      "Iteration is: 8559 and loss is: 0.0006105748470872641\n",
      "Iteration is: 8560 and loss is: 0.0005976594984531403\n",
      "Iteration is: 8561 and loss is: 0.0005926335579715669\n",
      "Iteration is: 8562 and loss is: 0.0005872002802789211\n",
      "Iteration is: 8563 and loss is: 0.0005826209089718759\n",
      "Iteration is: 8564 and loss is: 0.0005838537472300231\n",
      "Iteration is: 8565 and loss is: 0.0005902302218601108\n",
      "Iteration is: 8566 and loss is: 0.0005958968540653586\n",
      "Iteration is: 8567 and loss is: 0.0005962576833553612\n",
      "Iteration is: 8568 and loss is: 0.0005935134249739349\n",
      "Iteration is: 8569 and loss is: 0.0005889693857170641\n",
      "Iteration is: 8570 and loss is: 0.0005827440181747079\n",
      "Iteration is: 8571 and loss is: 0.0005760893691331148\n",
      "Iteration is: 8572 and loss is: 0.00057197455316782\n",
      "Iteration is: 8573 and loss is: 0.000572222110349685\n",
      "Iteration is: 8574 and loss is: 0.0005749205010943115\n",
      "Iteration is: 8575 and loss is: 0.000576862134039402\n",
      "Iteration is: 8576 and loss is: 0.0005772168515250087\n",
      "Iteration is: 8577 and loss is: 0.0005774079472757876\n",
      "Iteration is: 8578 and loss is: 0.0005784797249361873\n",
      "Iteration is: 8579 and loss is: 0.0005796393961645663\n",
      "Iteration is: 8580 and loss is: 0.0005795083707198501\n",
      "Iteration is: 8581 and loss is: 0.0005781210493296385\n",
      "Iteration is: 8582 and loss is: 0.000576151767745614\n",
      "Iteration is: 8583 and loss is: 0.0005741348140873015\n",
      "Iteration is: 8584 and loss is: 0.0005721071502193809\n",
      "Iteration is: 8585 and loss is: 0.0005702815833501518\n",
      "Iteration is: 8586 and loss is: 0.000569111667573452\n",
      "Iteration is: 8587 and loss is: 0.000568654271773994\n",
      "Iteration is: 8588 and loss is: 0.0005684358184225857\n",
      "Iteration is: 8589 and loss is: 0.0005680288304574788\n",
      "Iteration is: 8590 and loss is: 0.000567555776797235\n",
      "Iteration is: 8591 and loss is: 0.0005674138083122671\n",
      "Iteration is: 8592 and loss is: 0.0005677134613506496\n",
      "Iteration is: 8593 and loss is: 0.0005681783659383655\n",
      "Iteration is: 8594 and loss is: 0.000568539253436029\n",
      "Iteration is: 8595 and loss is: 0.0005687461816705763\n",
      "Iteration is: 8596 and loss is: 0.0005689443205483258\n",
      "Iteration is: 8597 and loss is: 0.0005691942060366273\n",
      "Iteration is: 8598 and loss is: 0.0005694521823897958\n",
      "Iteration is: 8599 and loss is: 0.0005697596934624016\n",
      "Iteration is: 8600 and loss is: 0.0005701830959878862\n",
      "Iteration is: 8601 and loss is: 0.0005707991076633334\n",
      "Iteration is: 8602 and loss is: 0.0005715717561542988\n",
      "Iteration is: 8603 and loss is: 0.0005724253715015948\n",
      "Iteration is: 8604 and loss is: 0.0005734420847147703\n",
      "Iteration is: 8605 and loss is: 0.0005747363320551813\n",
      "Iteration is: 8606 and loss is: 0.0005765180685557425\n",
      "Iteration is: 8607 and loss is: 0.0005789248971268535\n",
      "Iteration is: 8608 and loss is: 0.0005820866208523512\n",
      "Iteration is: 8609 and loss is: 0.0005862854886800051\n",
      "Iteration is: 8610 and loss is: 0.0005918914102949202\n",
      "Iteration is: 8611 and loss is: 0.0005994824459776282\n",
      "Iteration is: 8612 and loss is: 0.000609881361015141\n",
      "Iteration is: 8613 and loss is: 0.0006240117363631725\n",
      "Iteration is: 8614 and loss is: 0.0006435354007408023\n",
      "Iteration is: 8615 and loss is: 0.0006701621459797025\n",
      "Iteration is: 8616 and loss is: 0.0007066010730341077\n",
      "Iteration is: 8617 and loss is: 0.0007554906769655645\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 8618 and loss is: 0.000820028712041676\n",
      "Iteration is: 8619 and loss is: 0.000901528459507972\n",
      "Iteration is: 8620 and loss is: 0.0010014238068833947\n",
      "Iteration is: 8621 and loss is: 0.001108925323933363\n",
      "Iteration is: 8622 and loss is: 0.0012208650587126613\n",
      "Iteration is: 8623 and loss is: 0.0012939535081386566\n",
      "Iteration is: 8624 and loss is: 0.0013345690676942468\n",
      "Iteration is: 8625 and loss is: 0.0012713694013655186\n",
      "Iteration is: 8626 and loss is: 0.0011646938510239124\n",
      "Iteration is: 8627 and loss is: 0.0009775893995538354\n",
      "Iteration is: 8628 and loss is: 0.0008059610263444483\n",
      "Iteration is: 8629 and loss is: 0.0006623064400628209\n",
      "Iteration is: 8630 and loss is: 0.0005847045686095953\n",
      "Iteration is: 8631 and loss is: 0.0005701366462744772\n",
      "Iteration is: 8632 and loss is: 0.0006044398178346455\n",
      "Iteration is: 8633 and loss is: 0.0006731398170813918\n",
      "Iteration is: 8634 and loss is: 0.0007674149819649756\n",
      "Iteration is: 8635 and loss is: 0.0008941917913034558\n",
      "Iteration is: 8636 and loss is: 0.0010637067025527358\n",
      "Iteration is: 8637 and loss is: 0.0012652098666876554\n",
      "Iteration is: 8638 and loss is: 0.0015021393774077296\n",
      "Iteration is: 8639 and loss is: 0.0016619242960587144\n",
      "Iteration is: 8640 and loss is: 0.0016745944740250707\n",
      "Iteration is: 8641 and loss is: 0.0014894116902723908\n",
      "Iteration is: 8642 and loss is: 0.0011097658425569534\n",
      "Iteration is: 8643 and loss is: 0.0008078220998868346\n",
      "Iteration is: 8644 and loss is: 0.0006362614803947508\n",
      "Iteration is: 8645 and loss is: 0.0006717563373968005\n",
      "Iteration is: 8646 and loss is: 0.0008587274933233857\n",
      "Iteration is: 8647 and loss is: 0.0010247325990349054\n",
      "Iteration is: 8648 and loss is: 0.0011013587936758995\n",
      "Iteration is: 8649 and loss is: 0.0010040118359029293\n",
      "Iteration is: 8650 and loss is: 0.0009324460988864303\n",
      "Iteration is: 8651 and loss is: 0.0009345756843686104\n",
      "Iteration is: 8652 and loss is: 0.0009470631484873593\n",
      "Iteration is: 8653 and loss is: 0.0009175101295113564\n",
      "Iteration is: 8654 and loss is: 0.0008293489227071404\n",
      "Iteration is: 8655 and loss is: 0.0007562304381281137\n",
      "Iteration is: 8656 and loss is: 0.0007306530023925006\n",
      "Iteration is: 8657 and loss is: 0.0007082377560436726\n",
      "Iteration is: 8658 and loss is: 0.0006550299585796893\n",
      "Iteration is: 8659 and loss is: 0.0005859991069883108\n",
      "Iteration is: 8660 and loss is: 0.0005707344971597195\n",
      "Iteration is: 8661 and loss is: 0.0006146845989860594\n",
      "Iteration is: 8662 and loss is: 0.0006553076673299074\n",
      "Iteration is: 8663 and loss is: 0.0006609527627006173\n",
      "Iteration is: 8664 and loss is: 0.0006442872690968215\n",
      "Iteration is: 8665 and loss is: 0.0006487937644124031\n",
      "Iteration is: 8666 and loss is: 0.000672575959470123\n",
      "Iteration is: 8667 and loss is: 0.0006862838054075837\n",
      "Iteration is: 8668 and loss is: 0.0006780686089769006\n",
      "Iteration is: 8669 and loss is: 0.0006627366528846323\n",
      "Iteration is: 8670 and loss is: 0.0006631495198234916\n",
      "Iteration is: 8671 and loss is: 0.0006741620018146932\n",
      "Iteration is: 8672 and loss is: 0.0006715986528433859\n",
      "Iteration is: 8673 and loss is: 0.0006536609726026654\n",
      "Iteration is: 8674 and loss is: 0.0006233695894479752\n",
      "Iteration is: 8675 and loss is: 0.0005999350687488914\n",
      "Iteration is: 8676 and loss is: 0.000584761262871325\n",
      "Iteration is: 8677 and loss is: 0.00057134625967592\n",
      "Iteration is: 8678 and loss is: 0.0005595601396635175\n",
      "Iteration is: 8679 and loss is: 0.0005550810601562262\n",
      "Iteration is: 8680 and loss is: 0.0005608814535662532\n",
      "Iteration is: 8681 and loss is: 0.0005726937670260668\n",
      "Iteration is: 8682 and loss is: 0.0005846048588864505\n",
      "Iteration is: 8683 and loss is: 0.0005965352756902575\n",
      "Iteration is: 8684 and loss is: 0.0006112513365224004\n",
      "Iteration is: 8685 and loss is: 0.0006323595298454165\n",
      "Iteration is: 8686 and loss is: 0.0006589473923668265\n",
      "Iteration is: 8687 and loss is: 0.00068929314147681\n",
      "Iteration is: 8688 and loss is: 0.0007244679145514965\n",
      "Iteration is: 8689 and loss is: 0.0007646324811503291\n",
      "Iteration is: 8690 and loss is: 0.0008037497755140066\n",
      "Iteration is: 8691 and loss is: 0.0008394951582886279\n",
      "Iteration is: 8692 and loss is: 0.0008554239757359028\n",
      "Iteration is: 8693 and loss is: 0.0008537954417988658\n",
      "Iteration is: 8694 and loss is: 0.0008303942158818245\n",
      "Iteration is: 8695 and loss is: 0.000787126540672034\n",
      "Iteration is: 8696 and loss is: 0.0007377326255664229\n",
      "Iteration is: 8697 and loss is: 0.0006806827150285244\n",
      "Iteration is: 8698 and loss is: 0.0006339758401736617\n",
      "Iteration is: 8699 and loss is: 0.0005986584583297372\n",
      "Iteration is: 8700 and loss is: 0.0005765619571320713\n",
      "Iteration is: 8701 and loss is: 0.0005630782688967884\n",
      "Iteration is: 8702 and loss is: 0.0005549814086407423\n",
      "Iteration is: 8703 and loss is: 0.0005522036226466298\n",
      "Iteration is: 8704 and loss is: 0.0005560435238294303\n",
      "Iteration is: 8705 and loss is: 0.0005664836498908699\n",
      "Iteration is: 8706 and loss is: 0.0005822517559863627\n",
      "Iteration is: 8707 and loss is: 0.0006021668668836355\n",
      "Iteration is: 8708 and loss is: 0.0006291564786806703\n",
      "Iteration is: 8709 and loss is: 0.0006654852186329663\n",
      "Iteration is: 8710 and loss is: 0.000716579903382808\n",
      "Iteration is: 8711 and loss is: 0.0007821877952665091\n",
      "Iteration is: 8712 and loss is: 0.0008610189543105662\n",
      "Iteration is: 8713 and loss is: 0.000947279972024262\n",
      "Iteration is: 8714 and loss is: 0.0010303003946319222\n",
      "Iteration is: 8715 and loss is: 0.001095823012292385\n",
      "Iteration is: 8716 and loss is: 0.0011278395541012287\n",
      "Iteration is: 8717 and loss is: 0.0011180664878338575\n",
      "Iteration is: 8718 and loss is: 0.0010579712688922882\n",
      "Iteration is: 8719 and loss is: 0.000972893787547946\n",
      "Iteration is: 8720 and loss is: 0.0008619487634859979\n",
      "Iteration is: 8721 and loss is: 0.000764576019719243\n",
      "Iteration is: 8722 and loss is: 0.000678617914672941\n",
      "Iteration is: 8723 and loss is: 0.000620015780441463\n",
      "Iteration is: 8724 and loss is: 0.0005828136345371604\n",
      "Iteration is: 8725 and loss is: 0.000565244466997683\n",
      "Iteration is: 8726 and loss is: 0.0005643746117129922\n",
      "Iteration is: 8727 and loss is: 0.00058088602963835\n",
      "Iteration is: 8728 and loss is: 0.0006171124405227602\n",
      "Iteration is: 8729 and loss is: 0.0006762556731700897\n",
      "Iteration is: 8730 and loss is: 0.0007580177625641227\n",
      "Iteration is: 8731 and loss is: 0.0008727662498131394\n",
      "Iteration is: 8732 and loss is: 0.0010078216437250376\n",
      "Iteration is: 8733 and loss is: 0.0011730572441592813\n",
      "Iteration is: 8734 and loss is: 0.0013114882167428732\n",
      "Iteration is: 8735 and loss is: 0.0013871279079467058\n",
      "Iteration is: 8736 and loss is: 0.0013495499733835459\n",
      "Iteration is: 8737 and loss is: 0.0011615324765443802\n",
      "Iteration is: 8738 and loss is: 0.0009337081573903561\n",
      "Iteration is: 8739 and loss is: 0.0007074569584801793\n",
      "Iteration is: 8740 and loss is: 0.0005893399938941002\n",
      "Iteration is: 8741 and loss is: 0.0005887019215151668\n",
      "Iteration is: 8742 and loss is: 0.0006791966734454036\n",
      "Iteration is: 8743 and loss is: 0.0008189688669517636\n",
      "Iteration is: 8744 and loss is: 0.0009825527667999268\n",
      "Iteration is: 8745 and loss is: 0.001213858020491898\n",
      "Iteration is: 8746 and loss is: 0.0015818800311535597\n",
      "Iteration is: 8747 and loss is: 0.002048817463219166\n",
      "Iteration is: 8748 and loss is: 0.002558178501203656\n",
      "Iteration is: 8749 and loss is: 0.0024888531770557165\n",
      "Iteration is: 8750 and loss is: 0.0018365029245615005\n",
      "Iteration is: 8751 and loss is: 0.0012116024736315012\n",
      "Iteration is: 8752 and loss is: 0.0009960110764950514\n",
      "Iteration is: 8753 and loss is: 0.00108185107819736\n",
      "Iteration is: 8754 and loss is: 0.001128769014030695\n",
      "Iteration is: 8755 and loss is: 0.0011351382127031684\n",
      "Iteration is: 8756 and loss is: 0.0012336097424849868\n",
      "Iteration is: 8757 and loss is: 0.0011083377758041024\n",
      "Iteration is: 8758 and loss is: 0.0009549793321639299\n",
      "Iteration is: 8759 and loss is: 0.0009329835884273052\n",
      "Iteration is: 8760 and loss is: 0.0010795039124786854\n",
      "Iteration is: 8761 and loss is: 0.0010822216281667352\n",
      "Iteration is: 8762 and loss is: 0.0008258136222139001\n",
      "Iteration is: 8763 and loss is: 0.0006647057016380131\n",
      "Iteration is: 8764 and loss is: 0.0007994245970621705\n",
      "Iteration is: 8765 and loss is: 0.0011069603497162461\n",
      "Iteration is: 8766 and loss is: 0.001445754081942141\n",
      "Iteration is: 8767 and loss is: 0.0019351327791810036\n",
      "Iteration is: 8768 and loss is: 0.0032667105551809072\n",
      "Iteration is: 8769 and loss is: 0.00527411513030529\n",
      "Iteration is: 8770 and loss is: 0.006584147922694683\n",
      "Iteration is: 8771 and loss is: 0.004225694574415684\n",
      "Iteration is: 8772 and loss is: 0.0015158127062022686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 8773 and loss is: 0.002157881623134017\n",
      "Iteration is: 8774 and loss is: 0.007255326956510544\n",
      "Iteration is: 8775 and loss is: 0.016278276219964027\n",
      "Iteration is: 8776 and loss is: 0.027724791318178177\n",
      "Iteration is: 8777 and loss is: 0.04122146964073181\n",
      "Iteration is: 8778 and loss is: 0.07272066175937653\n",
      "Iteration is: 8779 and loss is: 0.016544712707400322\n",
      "Iteration is: 8780 and loss is: 0.024165287613868713\n",
      "Iteration is: 8781 and loss is: 0.07332051545381546\n",
      "Iteration is: 8782 and loss is: 0.014295272529125214\n",
      "Iteration is: 8783 and loss is: 0.0493130087852478\n",
      "Iteration is: 8784 and loss is: 0.023563358932733536\n",
      "Iteration is: 8785 and loss is: 0.04611840844154358\n",
      "Iteration is: 8786 and loss is: 0.02127678692340851\n",
      "Iteration is: 8787 and loss is: 0.043480806052684784\n",
      "Iteration is: 8788 and loss is: 0.02554922364652157\n",
      "Iteration is: 8789 and loss is: 0.0371464341878891\n",
      "Iteration is: 8790 and loss is: 0.03151969239115715\n",
      "Iteration is: 8791 and loss is: 0.02803170308470726\n",
      "Iteration is: 8792 and loss is: 0.02936747670173645\n",
      "Iteration is: 8793 and loss is: 0.02459052950143814\n",
      "Iteration is: 8794 and loss is: 0.023141108453273773\n",
      "Iteration is: 8795 and loss is: 0.020894166082143784\n",
      "Iteration is: 8796 and loss is: 0.018669677898287773\n",
      "Iteration is: 8797 and loss is: 0.016089728102087975\n",
      "Iteration is: 8798 and loss is: 0.013601357117295265\n",
      "Iteration is: 8799 and loss is: 0.013216136023402214\n",
      "Iteration is: 8800 and loss is: 0.00976802222430706\n",
      "Iteration is: 8801 and loss is: 0.010050985962152481\n",
      "Iteration is: 8802 and loss is: 0.00917004607617855\n",
      "Iteration is: 8803 and loss is: 0.0067680226638913155\n",
      "Iteration is: 8804 and loss is: 0.007553831674158573\n",
      "Iteration is: 8805 and loss is: 0.006478983908891678\n",
      "Iteration is: 8806 and loss is: 0.003420851659029722\n",
      "Iteration is: 8807 and loss is: 0.005977240856736898\n",
      "Iteration is: 8808 and loss is: 0.004157449584454298\n",
      "Iteration is: 8809 and loss is: 0.0038619856350123882\n",
      "Iteration is: 8810 and loss is: 0.006088670343160629\n",
      "Iteration is: 8811 and loss is: 0.004299980588257313\n",
      "Iteration is: 8812 and loss is: 0.007401107344776392\n",
      "Iteration is: 8813 and loss is: 0.006435740739107132\n",
      "Iteration is: 8814 and loss is: 0.005085691809654236\n",
      "Iteration is: 8815 and loss is: 0.006276990752667189\n",
      "Iteration is: 8816 and loss is: 0.005213463678956032\n",
      "Iteration is: 8817 and loss is: 0.006126148626208305\n",
      "Iteration is: 8818 and loss is: 0.00887772161513567\n",
      "Iteration is: 8819 and loss is: 0.011142333038151264\n",
      "Iteration is: 8820 and loss is: 0.015230949968099594\n",
      "Iteration is: 8821 and loss is: 0.018701504915952682\n",
      "Iteration is: 8822 and loss is: 0.012601991184055805\n",
      "Iteration is: 8823 and loss is: 0.005787150003015995\n",
      "Iteration is: 8824 and loss is: 0.0042703477665781975\n",
      "Iteration is: 8825 and loss is: 0.008792254142463207\n",
      "Iteration is: 8826 and loss is: 0.009013348259031773\n",
      "Iteration is: 8827 and loss is: 0.003063482465222478\n",
      "Iteration is: 8828 and loss is: 0.004247584380209446\n",
      "Iteration is: 8829 and loss is: 0.004908796399831772\n",
      "Iteration is: 8830 and loss is: 0.0021365806460380554\n",
      "Iteration is: 8831 and loss is: 0.003369510406628251\n",
      "Iteration is: 8832 and loss is: 0.002115047536790371\n",
      "Iteration is: 8833 and loss is: 0.0027255425229668617\n",
      "Iteration is: 8834 and loss is: 0.002962319180369377\n",
      "Iteration is: 8835 and loss is: 0.0017357515171170235\n",
      "Iteration is: 8836 and loss is: 0.002784617245197296\n",
      "Iteration is: 8837 and loss is: 0.002668411936610937\n",
      "Iteration is: 8838 and loss is: 0.0016550082946196198\n",
      "Iteration is: 8839 and loss is: 0.0022820262238383293\n",
      "Iteration is: 8840 and loss is: 0.0022737630642950535\n",
      "Iteration is: 8841 and loss is: 0.001504270825535059\n",
      "Iteration is: 8842 and loss is: 0.0017630262300372124\n",
      "Iteration is: 8843 and loss is: 0.002042336855083704\n",
      "Iteration is: 8844 and loss is: 0.001362290931865573\n",
      "Iteration is: 8845 and loss is: 0.0014232851099222898\n",
      "Iteration is: 8846 and loss is: 0.0017480595270171762\n",
      "Iteration is: 8847 and loss is: 0.0012133517302572727\n",
      "Iteration is: 8848 and loss is: 0.001228715293109417\n",
      "Iteration is: 8849 and loss is: 0.0014289880637079477\n",
      "Iteration is: 8850 and loss is: 0.001051338855177164\n",
      "Iteration is: 8851 and loss is: 0.001160678919404745\n",
      "Iteration is: 8852 and loss is: 0.0011978975962847471\n",
      "Iteration is: 8853 and loss is: 0.000949795008637011\n",
      "Iteration is: 8854 and loss is: 0.0011234164703637362\n",
      "Iteration is: 8855 and loss is: 0.0009910383960232139\n",
      "Iteration is: 8856 and loss is: 0.0009487525094300508\n",
      "Iteration is: 8857 and loss is: 0.0010225014993920922\n",
      "Iteration is: 8858 and loss is: 0.0008931162301450968\n",
      "Iteration is: 8859 and loss is: 0.0009764492278918624\n",
      "Iteration is: 8860 and loss is: 0.0008889792952686548\n",
      "Iteration is: 8861 and loss is: 0.0008942561689764261\n",
      "Iteration is: 8862 and loss is: 0.00089930061949417\n",
      "Iteration is: 8863 and loss is: 0.0008312704740092158\n",
      "Iteration is: 8864 and loss is: 0.0008732181158848107\n",
      "Iteration is: 8865 and loss is: 0.0008028442971408367\n",
      "Iteration is: 8866 and loss is: 0.0008346113609150052\n",
      "Iteration is: 8867 and loss is: 0.0008001603418961167\n",
      "Iteration is: 8868 and loss is: 0.0007848768145777285\n",
      "Iteration is: 8869 and loss is: 0.0007933444576337934\n",
      "Iteration is: 8870 and loss is: 0.0007574890041723847\n",
      "Iteration is: 8871 and loss is: 0.0007747329655103385\n",
      "Iteration is: 8872 and loss is: 0.0007469832198694348\n",
      "Iteration is: 8873 and loss is: 0.0007469800184480846\n",
      "Iteration is: 8874 and loss is: 0.0007434719009324908\n",
      "Iteration is: 8875 and loss is: 0.0007261200225912035\n",
      "Iteration is: 8876 and loss is: 0.0007342852768488228\n",
      "Iteration is: 8877 and loss is: 0.0007159418892115355\n",
      "Iteration is: 8878 and loss is: 0.0007167724543251097\n",
      "Iteration is: 8879 and loss is: 0.0007135319174267352\n",
      "Iteration is: 8880 and loss is: 0.00070106954080984\n",
      "Iteration is: 8881 and loss is: 0.0007052056607790291\n",
      "Iteration is: 8882 and loss is: 0.0006920484593138099\n",
      "Iteration is: 8883 and loss is: 0.0006918841972947121\n",
      "Iteration is: 8884 and loss is: 0.0006882149027660489\n",
      "Iteration is: 8885 and loss is: 0.0006789357867091894\n",
      "Iteration is: 8886 and loss is: 0.0006811395287513733\n",
      "Iteration is: 8887 and loss is: 0.0006728828884661198\n",
      "Iteration is: 8888 and loss is: 0.0006711704190820456\n",
      "Iteration is: 8889 and loss is: 0.0006684648687951267\n",
      "Iteration is: 8890 and loss is: 0.0006625704700127244\n",
      "Iteration is: 8891 and loss is: 0.0006633274024352431\n",
      "Iteration is: 8892 and loss is: 0.0006578186294063926\n",
      "Iteration is: 8893 and loss is: 0.0006564497016370296\n",
      "Iteration is: 8894 and loss is: 0.000654265284538269\n",
      "Iteration is: 8895 and loss is: 0.0006503337644971907\n",
      "Iteration is: 8896 and loss is: 0.0006499132141470909\n",
      "Iteration is: 8897 and loss is: 0.0006456546252593398\n",
      "Iteration is: 8898 and loss is: 0.0006444050231948495\n",
      "Iteration is: 8899 and loss is: 0.0006419742712751031\n",
      "Iteration is: 8900 and loss is: 0.000639228499494493\n",
      "Iteration is: 8901 and loss is: 0.0006382805877365172\n",
      "Iteration is: 8902 and loss is: 0.0006351324263960123\n",
      "Iteration is: 8903 and loss is: 0.0006341133848764002\n",
      "Iteration is: 8904 and loss is: 0.0006321595283225179\n",
      "Iteration is: 8905 and loss is: 0.0006300224922597408\n",
      "Iteration is: 8906 and loss is: 0.0006289982702583075\n",
      "Iteration is: 8907 and loss is: 0.0006266706623136997\n",
      "Iteration is: 8908 and loss is: 0.0006255533080548048\n",
      "Iteration is: 8909 and loss is: 0.0006239148788154125\n",
      "Iteration is: 8910 and loss is: 0.0006220970535650849\n",
      "Iteration is: 8911 and loss is: 0.0006211436120793223\n",
      "Iteration is: 8912 and loss is: 0.0006192971486598253\n",
      "Iteration is: 8913 and loss is: 0.0006181790377013385\n",
      "Iteration is: 8914 and loss is: 0.0006168540567159653\n",
      "Iteration is: 8915 and loss is: 0.0006153834401629865\n",
      "Iteration is: 8916 and loss is: 0.0006144355284050107\n",
      "Iteration is: 8917 and loss is: 0.0006129755638539791\n",
      "Iteration is: 8918 and loss is: 0.0006119305035099387\n",
      "Iteration is: 8919 and loss is: 0.000610795512329787\n",
      "Iteration is: 8920 and loss is: 0.0006095593562349677\n",
      "Iteration is: 8921 and loss is: 0.000608605972956866\n",
      "Iteration is: 8922 and loss is: 0.0006073993863537908\n",
      "Iteration is: 8923 and loss is: 0.0006063904729671776\n",
      "Iteration is: 8924 and loss is: 0.0006053890101611614\n",
      "Iteration is: 8925 and loss is: 0.0006043115281499922\n",
      "Iteration is: 8926 and loss is: 0.0006033998215571046\n",
      "Iteration is: 8927 and loss is: 0.000602393236476928\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 8928 and loss is: 0.0006014610407873988\n",
      "Iteration is: 8929 and loss is: 0.0006005731411278248\n",
      "Iteration is: 8930 and loss is: 0.0005996188847348094\n",
      "Iteration is: 8931 and loss is: 0.0005987748736515641\n",
      "Iteration is: 8932 and loss is: 0.0005978975677862763\n",
      "Iteration is: 8933 and loss is: 0.000597034755628556\n",
      "Iteration is: 8934 and loss is: 0.0005962309660390019\n",
      "Iteration is: 8935 and loss is: 0.000595384044572711\n",
      "Iteration is: 8936 and loss is: 0.0005946088349446654\n",
      "Iteration is: 8937 and loss is: 0.0005938226240687072\n",
      "Iteration is: 8938 and loss is: 0.00059303711168468\n",
      "Iteration is: 8939 and loss is: 0.0005923081771470606\n",
      "Iteration is: 8940 and loss is: 0.000591545831412077\n",
      "Iteration is: 8941 and loss is: 0.0005908275488764048\n",
      "Iteration is: 8942 and loss is: 0.0005901141557842493\n",
      "Iteration is: 8943 and loss is: 0.0005893965717405081\n",
      "Iteration is: 8944 and loss is: 0.0005887197330594063\n",
      "Iteration is: 8945 and loss is: 0.000588029739446938\n",
      "Iteration is: 8946 and loss is: 0.0005873634363524616\n",
      "Iteration is: 8947 and loss is: 0.0005867069121450186\n",
      "Iteration is: 8948 and loss is: 0.0005860513774678111\n",
      "Iteration is: 8949 and loss is: 0.0005854206974618137\n",
      "Iteration is: 8950 and loss is: 0.0005847889697179198\n",
      "Iteration is: 8951 and loss is: 0.0005841676611453295\n",
      "Iteration is: 8952 and loss is: 0.0005835647461935878\n",
      "Iteration is: 8953 and loss is: 0.0005829581641592085\n",
      "Iteration is: 8954 and loss is: 0.000582370616029948\n",
      "Iteration is: 8955 and loss is: 0.0005817858036607504\n",
      "Iteration is: 8956 and loss is: 0.0005812090239487588\n",
      "Iteration is: 8957 and loss is: 0.0005806460976600647\n",
      "Iteration is: 8958 and loss is: 0.000580084859393537\n",
      "Iteration is: 8959 and loss is: 0.0005795345059596002\n",
      "Iteration is: 8960 and loss is: 0.0005789923598058522\n",
      "Iteration is: 8961 and loss is: 0.0005784569075331092\n",
      "Iteration is: 8962 and loss is: 0.0005779277416877449\n",
      "Iteration is: 8963 and loss is: 0.0005774048622697592\n",
      "Iteration is: 8964 and loss is: 0.0005768896080553532\n",
      "Iteration is: 8965 and loss is: 0.0005763829685747623\n",
      "Iteration is: 8966 and loss is: 0.0005758774932473898\n",
      "Iteration is: 8967 and loss is: 0.000575382960960269\n",
      "Iteration is: 8968 and loss is: 0.0005748935509473085\n",
      "Iteration is: 8969 and loss is: 0.0005744084483012557\n",
      "Iteration is: 8970 and loss is: 0.000573932018596679\n",
      "Iteration is: 8971 and loss is: 0.0005734589649364352\n",
      "Iteration is: 8972 and loss is: 0.0005729931872338057\n",
      "Iteration is: 8973 and loss is: 0.0005725347436964512\n",
      "Iteration is: 8974 and loss is: 0.0005720768240280449\n",
      "Iteration is: 8975 and loss is: 0.0005716243758797646\n",
      "Iteration is: 8976 and loss is: 0.0005711810663342476\n",
      "Iteration is: 8977 and loss is: 0.0005707406671717763\n",
      "Iteration is: 8978 and loss is: 0.0005703048082068563\n",
      "Iteration is: 8979 and loss is: 0.000569873140193522\n",
      "Iteration is: 8980 and loss is: 0.0005694456631317735\n",
      "Iteration is: 8981 and loss is: 0.0005690259858965874\n",
      "Iteration is: 8982 and loss is: 0.0005686082295142114\n",
      "Iteration is: 8983 and loss is: 0.0005681944312527776\n",
      "Iteration is: 8984 and loss is: 0.0005677865701727569\n",
      "Iteration is: 8985 and loss is: 0.0005673813866451383\n",
      "Iteration is: 8986 and loss is: 0.0005669817328453064\n",
      "Iteration is: 8987 and loss is: 0.0005665859789587557\n",
      "Iteration is: 8988 and loss is: 0.0005661917966790497\n",
      "Iteration is: 8989 and loss is: 0.0005658051231876016\n",
      "Iteration is: 8990 and loss is: 0.0005654194392263889\n",
      "Iteration is: 8991 and loss is: 0.0005650372477248311\n",
      "Iteration is: 8992 and loss is: 0.0005646585486829281\n",
      "Iteration is: 8993 and loss is: 0.0005642840988002717\n",
      "Iteration is: 8994 and loss is: 0.0005639137816615403\n",
      "Iteration is: 8995 and loss is: 0.0005635496927425265\n",
      "Iteration is: 8996 and loss is: 0.0005631822859868407\n",
      "Iteration is: 8997 and loss is: 0.0005628223298117518\n",
      "Iteration is: 8998 and loss is: 0.0005624655168503523\n",
      "Iteration is: 8999 and loss is: 0.0005621110321953893\n",
      "Iteration is: 9000 and loss is: 0.0005617616116069257\n",
      "Iteration is: 9001 and loss is: 0.0005614104447886348\n",
      "Iteration is: 9002 and loss is: 0.000561065215151757\n",
      "Iteration is: 9003 and loss is: 0.0005607236525975168\n",
      "Iteration is: 9004 and loss is: 0.0005603819154202938\n",
      "Iteration is: 9005 and loss is: 0.0005600444274023175\n",
      "Iteration is: 9006 and loss is: 0.0005597132258117199\n",
      "Iteration is: 9007 and loss is: 0.0005593793466687202\n",
      "Iteration is: 9008 and loss is: 0.0005590523360297084\n",
      "Iteration is: 9009 and loss is: 0.0005587254418060184\n",
      "Iteration is: 9010 and loss is: 0.0005583990132436156\n",
      "Iteration is: 9011 and loss is: 0.000558080559130758\n",
      "Iteration is: 9012 and loss is: 0.0005577596020884812\n",
      "Iteration is: 9013 and loss is: 0.0005574427777901292\n",
      "Iteration is: 9014 and loss is: 0.0005571290384978056\n",
      "Iteration is: 9015 and loss is: 0.0005568190244957805\n",
      "Iteration is: 9016 and loss is: 0.000556509243324399\n",
      "Iteration is: 9017 and loss is: 0.000556199811398983\n",
      "Iteration is: 9018 and loss is: 0.0005558964912779629\n",
      "Iteration is: 9019 and loss is: 0.0005555934621952474\n",
      "Iteration is: 9020 and loss is: 0.0005552918300963938\n",
      "Iteration is: 9021 and loss is: 0.0005549920024350286\n",
      "Iteration is: 9022 and loss is: 0.0005546962493099272\n",
      "Iteration is: 9023 and loss is: 0.0005544013110920787\n",
      "Iteration is: 9024 and loss is: 0.0005541110876947641\n",
      "Iteration is: 9025 and loss is: 0.0005538175464607775\n",
      "Iteration is: 9026 and loss is: 0.0005535291857086122\n",
      "Iteration is: 9027 and loss is: 0.0005532418144866824\n",
      "Iteration is: 9028 and loss is: 0.0005529558402486145\n",
      "Iteration is: 9029 and loss is: 0.0005526720779016614\n",
      "Iteration is: 9030 and loss is: 0.0005523924482986331\n",
      "Iteration is: 9031 and loss is: 0.0005521130515262485\n",
      "Iteration is: 9032 and loss is: 0.0005518323159776628\n",
      "Iteration is: 9033 and loss is: 0.0005515571683645248\n",
      "Iteration is: 9034 and loss is: 0.0005512825446203351\n",
      "Iteration is: 9035 and loss is: 0.0005510087357833982\n",
      "Iteration is: 9036 and loss is: 0.0005507390596903861\n",
      "Iteration is: 9037 and loss is: 0.0005504692671820521\n",
      "Iteration is: 9038 and loss is: 0.0005501993582583964\n",
      "Iteration is: 9039 and loss is: 0.0005499339895322919\n",
      "Iteration is: 9040 and loss is: 0.0005496671074070036\n",
      "Iteration is: 9041 and loss is: 0.0005494024371728301\n",
      "Iteration is: 9042 and loss is: 0.0005491406773217022\n",
      "Iteration is: 9043 and loss is: 0.0005488803144544363\n",
      "Iteration is: 9044 and loss is: 0.0005486198351718485\n",
      "Iteration is: 9045 and loss is: 0.0005483624991029501\n",
      "Iteration is: 9046 and loss is: 0.0005481068510562181\n",
      "Iteration is: 9047 and loss is: 0.0005478515522554517\n",
      "Iteration is: 9048 and loss is: 0.0005475953221321106\n",
      "Iteration is: 9049 and loss is: 0.0005473438068293035\n",
      "Iteration is: 9050 and loss is: 0.0005470915930345654\n",
      "Iteration is: 9051 and loss is: 0.0005468418821692467\n",
      "Iteration is: 9052 and loss is: 0.0005465936847031116\n",
      "Iteration is: 9053 and loss is: 0.0005463461857289076\n",
      "Iteration is: 9054 and loss is: 0.0005461002001538873\n",
      "Iteration is: 9055 and loss is: 0.0005458556115627289\n",
      "Iteration is: 9056 and loss is: 0.0005456131184473634\n",
      "Iteration is: 9057 and loss is: 0.0005453674239106476\n",
      "Iteration is: 9058 and loss is: 0.0005451273755170405\n",
      "Iteration is: 9059 and loss is: 0.0005448877927847207\n",
      "Iteration is: 9060 and loss is: 0.0005446468712761998\n",
      "Iteration is: 9061 and loss is: 0.0005444107227958739\n",
      "Iteration is: 9062 and loss is: 0.0005441727116703987\n",
      "Iteration is: 9063 and loss is: 0.0005439359229058027\n",
      "Iteration is: 9064 and loss is: 0.0005437004147097468\n",
      "Iteration is: 9065 and loss is: 0.0005434679333120584\n",
      "Iteration is: 9066 and loss is: 0.0005432356847450137\n",
      "Iteration is: 9067 and loss is: 0.0005430056480690837\n",
      "Iteration is: 9068 and loss is: 0.0005427731084637344\n",
      "Iteration is: 9069 and loss is: 0.0005425439449027181\n",
      "Iteration is: 9070 and loss is: 0.0005423155380412936\n",
      "Iteration is: 9071 and loss is: 0.0005420846864581108\n",
      "Iteration is: 9072 and loss is: 0.0005418608197942376\n",
      "Iteration is: 9073 and loss is: 0.0005416362546384335\n",
      "Iteration is: 9074 and loss is: 0.0005414090119302273\n",
      "Iteration is: 9075 and loss is: 0.0005411878228187561\n",
      "Iteration is: 9076 and loss is: 0.0005409664008766413\n",
      "Iteration is: 9077 and loss is: 0.0005407427670434117\n",
      "Iteration is: 9078 and loss is: 0.0005405191332101822\n",
      "Iteration is: 9079 and loss is: 0.0005402985261753201\n",
      "Iteration is: 9080 and loss is: 0.000540081353392452\n",
      "Iteration is: 9081 and loss is: 0.0005398625507950783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 9082 and loss is: 0.0005396458436734974\n",
      "Iteration is: 9083 and loss is: 0.0005394280888140202\n",
      "Iteration is: 9084 and loss is: 0.0005392124294303358\n",
      "Iteration is: 9085 and loss is: 0.0005389968864619732\n",
      "Iteration is: 9086 and loss is: 0.0005387823330238461\n",
      "Iteration is: 9087 and loss is: 0.0005385685944929719\n",
      "Iteration is: 9088 and loss is: 0.0005383563693612814\n",
      "Iteration is: 9089 and loss is: 0.000538146123290062\n",
      "Iteration is: 9090 and loss is: 0.0005379326175898314\n",
      "Iteration is: 9091 and loss is: 0.0005377224879339337\n",
      "Iteration is: 9092 and loss is: 0.0005375129985623062\n",
      "Iteration is: 9093 and loss is: 0.0005373036256060004\n",
      "Iteration is: 9094 and loss is: 0.000537096755579114\n",
      "Iteration is: 9095 and loss is: 0.0005368892452679574\n",
      "Iteration is: 9096 and loss is: 0.0005366806290112436\n",
      "Iteration is: 9097 and loss is: 0.0005364759126678109\n",
      "Iteration is: 9098 and loss is: 0.0005362704396247864\n",
      "Iteration is: 9099 and loss is: 0.0005360660725273192\n",
      "Iteration is: 9100 and loss is: 0.0005358616472221911\n",
      "Iteration is: 9101 and loss is: 0.0005356587935239077\n",
      "Iteration is: 9102 and loss is: 0.0005354559398256242\n",
      "Iteration is: 9103 and loss is: 0.0005352544831112027\n",
      "Iteration is: 9104 and loss is: 0.0005350529681891203\n",
      "Iteration is: 9105 and loss is: 0.0005348522681742907\n",
      "Iteration is: 9106 and loss is: 0.0005346531979739666\n",
      "Iteration is: 9107 and loss is: 0.0005344528472051024\n",
      "Iteration is: 9108 and loss is: 0.0005342524964362383\n",
      "Iteration is: 9109 and loss is: 0.0005340576171875\n",
      "Iteration is: 9110 and loss is: 0.0005338593618944287\n",
      "Iteration is: 9111 and loss is: 0.0005336645408533514\n",
      "Iteration is: 9112 and loss is: 0.0005334675661288202\n",
      "Iteration is: 9113 and loss is: 0.000533270591404289\n",
      "Iteration is: 9114 and loss is: 0.0005330758867785335\n",
      "Iteration is: 9115 and loss is: 0.0005328812403604388\n",
      "Iteration is: 9116 and loss is: 0.0005326895043253899\n",
      "Iteration is: 9117 and loss is: 0.000532493635546416\n",
      "Iteration is: 9118 and loss is: 0.0005323021323420107\n",
      "Iteration is: 9119 and loss is: 0.0005321098724380136\n",
      "Iteration is: 9120 and loss is: 0.0005319180199876428\n",
      "Iteration is: 9121 and loss is: 0.0005317256436683238\n",
      "Iteration is: 9122 and loss is: 0.0005315367598086596\n",
      "Iteration is: 9123 and loss is: 0.0005313461879268289\n",
      "Iteration is: 9124 and loss is: 0.0005311580025590956\n",
      "Iteration is: 9125 and loss is: 0.0005309685366228223\n",
      "Iteration is: 9126 and loss is: 0.0005307800020091236\n",
      "Iteration is: 9127 and loss is: 0.0005305925733409822\n",
      "Iteration is: 9128 and loss is: 0.0005304051446728408\n",
      "Iteration is: 9129 and loss is: 0.0005302175413817167\n",
      "Iteration is: 9130 and loss is: 0.0005300308112055063\n",
      "Iteration is: 9131 and loss is: 0.0005298446048982441\n",
      "Iteration is: 9132 and loss is: 0.0005296602612361312\n",
      "Iteration is: 9133 and loss is: 0.0005294773145578802\n",
      "Iteration is: 9134 and loss is: 0.00052929314551875\n",
      "Iteration is: 9135 and loss is: 0.000529108801856637\n",
      "Iteration is: 9136 and loss is: 0.0005289232358336449\n",
      "Iteration is: 9137 and loss is: 0.0005287433159537613\n",
      "Iteration is: 9138 and loss is: 0.0005285604856908321\n",
      "Iteration is: 9139 and loss is: 0.0005283781210891902\n",
      "Iteration is: 9140 and loss is: 0.0005281962803564966\n",
      "Iteration is: 9141 and loss is: 0.0005280147306621075\n",
      "Iteration is: 9142 and loss is: 0.0005278337048366666\n",
      "Iteration is: 9143 and loss is: 0.000527654483448714\n",
      "Iteration is: 9144 and loss is: 0.0005274750874377787\n",
      "Iteration is: 9145 and loss is: 0.0005272965063340962\n",
      "Iteration is: 9146 and loss is: 0.0005271193222142756\n",
      "Iteration is: 9147 and loss is: 0.0005269396351650357\n",
      "Iteration is: 9148 and loss is: 0.0005267597734928131\n",
      "Iteration is: 9149 and loss is: 0.0005265831132419407\n",
      "Iteration is: 9150 and loss is: 0.0005264057544991374\n",
      "Iteration is: 9151 and loss is: 0.0005262302584014833\n",
      "Iteration is: 9152 and loss is: 0.0005260532489046454\n",
      "Iteration is: 9153 and loss is: 0.0005258776363916695\n",
      "Iteration is: 9154 and loss is: 0.0005257033626548946\n",
      "Iteration is: 9155 and loss is: 0.0005255274008959532\n",
      "Iteration is: 9156 and loss is: 0.0005253524868749082\n",
      "Iteration is: 9157 and loss is: 0.0005251787952147424\n",
      "Iteration is: 9158 and loss is: 0.0005250065005384386\n",
      "Iteration is: 9159 and loss is: 0.0005248325178399682\n",
      "Iteration is: 9160 and loss is: 0.0005246599903330207\n",
      "Iteration is: 9161 and loss is: 0.0005244857165962458\n",
      "Iteration is: 9162 and loss is: 0.0005243145278654993\n",
      "Iteration is: 9163 and loss is: 0.0005241415929049253\n",
      "Iteration is: 9164 and loss is: 0.0005239720339886844\n",
      "Iteration is: 9165 and loss is: 0.0005238009616732597\n",
      "Iteration is: 9166 and loss is: 0.0005236297147348523\n",
      "Iteration is: 9167 and loss is: 0.0005234583513811231\n",
      "Iteration is: 9168 and loss is: 0.0005232869880273938\n",
      "Iteration is: 9169 and loss is: 0.000523120048455894\n",
      "Iteration is: 9170 and loss is: 0.0005229495000094175\n",
      "Iteration is: 9171 and loss is: 0.000522781047038734\n",
      "Iteration is: 9172 and loss is: 0.0005226120119914412\n",
      "Iteration is: 9173 and loss is: 0.0005224422202445567\n",
      "Iteration is: 9174 and loss is: 0.0005222749314270914\n",
      "Iteration is: 9175 and loss is: 0.0005221067694947124\n",
      "Iteration is: 9176 and loss is: 0.0005219407612457871\n",
      "Iteration is: 9177 and loss is: 0.0005217719590291381\n",
      "Iteration is: 9178 and loss is: 0.0005216085119172931\n",
      "Iteration is: 9179 and loss is: 0.0005214407574385405\n",
      "Iteration is: 9180 and loss is: 0.0005212740506976843\n",
      "Iteration is: 9181 and loss is: 0.0005211080424487591\n",
      "Iteration is: 9182 and loss is: 0.000520944653544575\n",
      "Iteration is: 9183 and loss is: 0.0005207785870879889\n",
      "Iteration is: 9184 and loss is: 0.0005206135101616383\n",
      "Iteration is: 9185 and loss is: 0.0005204483750276268\n",
      "Iteration is: 9186 and loss is: 0.0005202851607464254\n",
      "Iteration is: 9187 and loss is: 0.0005201220628805459\n",
      "Iteration is: 9188 and loss is: 0.0005199583829380572\n",
      "Iteration is: 9189 and loss is: 0.0005197942373342812\n",
      "Iteration is: 9190 and loss is: 0.0005196323618292809\n",
      "Iteration is: 9191 and loss is: 0.0005194671102799475\n",
      "Iteration is: 9192 and loss is: 0.000519305991474539\n",
      "Iteration is: 9193 and loss is: 0.000519145280122757\n",
      "Iteration is: 9194 and loss is: 0.0005189839284867048\n",
      "Iteration is: 9195 and loss is: 0.0005188206559978426\n",
      "Iteration is: 9196 and loss is: 0.0005186606431379914\n",
      "Iteration is: 9197 and loss is: 0.0005185010959394276\n",
      "Iteration is: 9198 and loss is: 0.0005183396860957146\n",
      "Iteration is: 9199 and loss is: 0.000518179323989898\n",
      "Iteration is: 9200 and loss is: 0.0005180188454687595\n",
      "Iteration is: 9201 and loss is: 0.0005178603460080922\n",
      "Iteration is: 9202 and loss is: 0.000517700333148241\n",
      "Iteration is: 9203 and loss is: 0.0005175415426492691\n",
      "Iteration is: 9204 and loss is: 0.00051738356705755\n",
      "Iteration is: 9205 and loss is: 0.0005172240198589861\n",
      "Iteration is: 9206 and loss is: 0.0005170646472834051\n",
      "Iteration is: 9207 and loss is: 0.000516905914992094\n",
      "Iteration is: 9208 and loss is: 0.0005167491617612541\n",
      "Iteration is: 9209 and loss is: 0.0005165914190001786\n",
      "Iteration is: 9210 and loss is: 0.0005164327449165285\n",
      "Iteration is: 9211 and loss is: 0.0005162772140465677\n",
      "Iteration is: 9212 and loss is: 0.0005161195294931531\n",
      "Iteration is: 9213 and loss is: 0.0005159619031473994\n",
      "Iteration is: 9214 and loss is: 0.0005158061976544559\n",
      "Iteration is: 9215 and loss is: 0.0005156511906534433\n",
      "Iteration is: 9216 and loss is: 0.0005154964746907353\n",
      "Iteration is: 9217 and loss is: 0.0005153390811756253\n",
      "Iteration is: 9218 and loss is: 0.0005151834920980036\n",
      "Iteration is: 9219 and loss is: 0.0005150287179276347\n",
      "Iteration is: 9220 and loss is: 0.0005148755735717714\n",
      "Iteration is: 9221 and loss is: 0.0005147178890183568\n",
      "Iteration is: 9222 and loss is: 0.0005145642790012062\n",
      "Iteration is: 9223 and loss is: 0.0005144107271917164\n",
      "Iteration is: 9224 and loss is: 0.0005142557201907039\n",
      "Iteration is: 9225 and loss is: 0.0005141032161191106\n",
      "Iteration is: 9226 and loss is: 0.0005139505956321955\n",
      "Iteration is: 9227 and loss is: 0.0005137987900525331\n",
      "Iteration is: 9228 and loss is: 0.0005136438994668424\n",
      "Iteration is: 9229 and loss is: 0.0005134913371875882\n",
      "Iteration is: 9230 and loss is: 0.0005133377853780985\n",
      "Iteration is: 9231 and loss is: 0.0005131859215907753\n",
      "Iteration is: 9232 and loss is: 0.00051303202053532\n",
      "Iteration is: 9233 and loss is: 0.0005128807970322669\n",
      "Iteration is: 9234 and loss is: 0.0005127298645675182\n",
      "Iteration is: 9235 and loss is: 0.0005125798634253442\n",
      "Iteration is: 9236 and loss is: 0.0005124264862388372\n",
      "Iteration is: 9237 and loss is: 0.0005122770089656115\n",
      "Iteration is: 9238 and loss is: 0.0005121256690472364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 9239 and loss is: 0.0005119743291288614\n",
      "Iteration is: 9240 and loss is: 0.0005118229892104864\n",
      "Iteration is: 9241 and loss is: 0.000511673919390887\n",
      "Iteration is: 9242 and loss is: 0.0005115233943797648\n",
      "Iteration is: 9243 and loss is: 0.0005113737424835563\n",
      "Iteration is: 9244 and loss is: 0.0005112250801175833\n",
      "Iteration is: 9245 and loss is: 0.0005110768834128976\n",
      "Iteration is: 9246 and loss is: 0.0005109241465106606\n",
      "Iteration is: 9247 and loss is: 0.0005107765318825841\n",
      "Iteration is: 9248 and loss is: 0.0005106257740408182\n",
      "Iteration is: 9249 and loss is: 0.0005104778101667762\n",
      "Iteration is: 9250 and loss is: 0.0005103298462927341\n",
      "Iteration is: 9251 and loss is: 0.0005101808346807957\n",
      "Iteration is: 9252 and loss is: 0.000510034675244242\n",
      "Iteration is: 9253 and loss is: 0.0005098850233480334\n",
      "Iteration is: 9254 and loss is: 0.0005097389803268015\n",
      "Iteration is: 9255 and loss is: 0.0005095907836221159\n",
      "Iteration is: 9256 and loss is: 0.0005094438674859703\n",
      "Iteration is: 9257 and loss is: 0.0005092953215353191\n",
      "Iteration is: 9258 and loss is: 0.0005091503844596446\n",
      "Iteration is: 9259 and loss is: 0.0005090021877549589\n",
      "Iteration is: 9260 and loss is: 0.0005088556790724397\n",
      "Iteration is: 9261 and loss is: 0.0005087078898213804\n",
      "Iteration is: 9262 and loss is: 0.0005085619050078094\n",
      "Iteration is: 9263 and loss is: 0.0005084165604785085\n",
      "Iteration is: 9264 and loss is: 0.000508268887642771\n",
      "Iteration is: 9265 and loss is: 0.0005081252311356366\n",
      "Iteration is: 9266 and loss is: 0.0005079779075458646\n",
      "Iteration is: 9267 and loss is: 0.0005078337271697819\n",
      "Iteration is: 9268 and loss is: 0.0005076866364106536\n",
      "Iteration is: 9269 and loss is: 0.0005075432127341628\n",
      "Iteration is: 9270 and loss is: 0.0005074003129266202\n",
      "Iteration is: 9271 and loss is: 0.0005072529893368483\n",
      "Iteration is: 9272 and loss is: 0.0005071074119769037\n",
      "Iteration is: 9273 and loss is: 0.0005069626495242119\n",
      "Iteration is: 9274 and loss is: 0.0005068196915090084\n",
      "Iteration is: 9275 and loss is: 0.000506675336509943\n",
      "Iteration is: 9276 and loss is: 0.0005065303412266076\n",
      "Iteration is: 9277 and loss is: 0.000506388139910996\n",
      "Iteration is: 9278 and loss is: 0.0005062435520812869\n",
      "Iteration is: 9279 and loss is: 0.000506100885104388\n",
      "Iteration is: 9280 and loss is: 0.0005059583345428109\n",
      "Iteration is: 9281 and loss is: 0.0005058135720901191\n",
      "Iteration is: 9282 and loss is: 0.0005056700902059674\n",
      "Iteration is: 9283 and loss is: 0.0005055287037976086\n",
      "Iteration is: 9284 and loss is: 0.0005053849890828133\n",
      "Iteration is: 9285 and loss is: 0.0005052416818216443\n",
      "Iteration is: 9286 and loss is: 0.0005051008774898946\n",
      "Iteration is: 9287 and loss is: 0.0005049575702287257\n",
      "Iteration is: 9288 and loss is: 0.0005048165912739933\n",
      "Iteration is: 9289 and loss is: 0.0005046738078817725\n",
      "Iteration is: 9290 and loss is: 0.0005045318393968046\n",
      "Iteration is: 9291 and loss is: 0.0005043905111961067\n",
      "Iteration is: 9292 and loss is: 0.0005042492412030697\n",
      "Iteration is: 9293 and loss is: 0.0005041075055487454\n",
      "Iteration is: 9294 and loss is: 0.0005039643729105592\n",
      "Iteration is: 9295 and loss is: 0.0005038265371695161\n",
      "Iteration is: 9296 and loss is: 0.0005036834627389908\n",
      "Iteration is: 9297 and loss is: 0.0005035429494455457\n",
      "Iteration is: 9298 and loss is: 0.0005034016212448478\n",
      "Iteration is: 9299 and loss is: 0.0005032605840824544\n",
      "Iteration is: 9300 and loss is: 0.0005031199543736875\n",
      "Iteration is: 9301 and loss is: 0.0005029804306104779\n",
      "Iteration is: 9302 and loss is: 0.0005028396844863892\n",
      "Iteration is: 9303 and loss is: 0.0005027006845921278\n",
      "Iteration is: 9304 and loss is: 0.0005025616846978664\n",
      "Iteration is: 9305 and loss is: 0.0005024200072512031\n",
      "Iteration is: 9306 and loss is: 0.0005022803088650107\n",
      "Iteration is: 9307 and loss is: 0.0005021420074626803\n",
      "Iteration is: 9308 and loss is: 0.0005019999807700515\n",
      "Iteration is: 9309 and loss is: 0.0005018641822971404\n",
      "Iteration is: 9310 and loss is: 0.0005017247167415917\n",
      "Iteration is: 9311 and loss is: 0.0005015840288251638\n",
      "Iteration is: 9312 and loss is: 0.0005014466587454081\n",
      "Iteration is: 9313 and loss is: 0.000501304748468101\n",
      "Iteration is: 9314 and loss is: 0.0005011689499951899\n",
      "Iteration is: 9315 and loss is: 0.0005010293098166585\n",
      "Iteration is: 9316 and loss is: 0.0005008908919990063\n",
      "Iteration is: 9317 and loss is: 0.0005007513100281358\n",
      "Iteration is: 9318 and loss is: 0.0005006126593798399\n",
      "Iteration is: 9319 and loss is: 0.0005004737759009004\n",
      "Iteration is: 9320 and loss is: 0.0005003383266739547\n",
      "Iteration is: 9321 and loss is: 0.0005002005491405725\n",
      "Iteration is: 9322 and loss is: 0.0005000628298148513\n",
      "Iteration is: 9323 and loss is: 0.0004999246448278427\n",
      "Iteration is: 9324 and loss is: 0.0004997888463549316\n",
      "Iteration is: 9325 and loss is: 0.0004996501957066357\n",
      "Iteration is: 9326 and loss is: 0.0004995116032660007\n",
      "Iteration is: 9327 and loss is: 0.0004993763868696988\n",
      "Iteration is: 9328 and loss is: 0.0004992382600903511\n",
      "Iteration is: 9329 and loss is: 0.0004991012974642217\n",
      "Iteration is: 9330 and loss is: 0.0004989634035155177\n",
      "Iteration is: 9331 and loss is: 0.0004988268483430147\n",
      "Iteration is: 9332 and loss is: 0.000498690758831799\n",
      "Iteration is: 9333 and loss is: 0.0004985553096048534\n",
      "Iteration is: 9334 and loss is: 0.0004984196275472641\n",
      "Iteration is: 9335 and loss is: 0.0004982836544513702\n",
      "Iteration is: 9336 and loss is: 0.0004981463425792754\n",
      "Iteration is: 9337 and loss is: 0.0004980109515599906\n",
      "Iteration is: 9338 and loss is: 0.0004978736978955567\n",
      "Iteration is: 9339 and loss is: 0.0004977381904609501\n",
      "Iteration is: 9340 and loss is: 0.0004976027412340045\n",
      "Iteration is: 9341 and loss is: 0.0004974664188921452\n",
      "Iteration is: 9342 and loss is: 0.0004973307950422168\n",
      "Iteration is: 9343 and loss is: 0.0004971969174221158\n",
      "Iteration is: 9344 and loss is: 0.0004970611771568656\n",
      "Iteration is: 9345 and loss is: 0.0004969247966073453\n",
      "Iteration is: 9346 and loss is: 0.0004967895802110434\n",
      "Iteration is: 9347 and loss is: 0.0004966545384377241\n",
      "Iteration is: 9348 and loss is: 0.0004965189145877957\n",
      "Iteration is: 9349 and loss is: 0.0004963843384757638\n",
      "Iteration is: 9350 and loss is: 0.0004962493549101055\n",
      "Iteration is: 9351 and loss is: 0.0004961159429512918\n",
      "Iteration is: 9352 and loss is: 0.0004959792131558061\n",
      "Iteration is: 9353 and loss is: 0.0004958450444974005\n",
      "Iteration is: 9354 and loss is: 0.0004957126220688224\n",
      "Iteration is: 9355 and loss is: 0.0004955778131261468\n",
      "Iteration is: 9356 and loss is: 0.0004954423638992012\n",
      "Iteration is: 9357 and loss is: 0.0004953093593940139\n",
      "Iteration is: 9358 and loss is: 0.0004951750743202865\n",
      "Iteration is: 9359 and loss is: 0.0004950413713231683\n",
      "Iteration is: 9360 and loss is: 0.0004949071444571018\n",
      "Iteration is: 9361 and loss is: 0.00049477448919788\n",
      "Iteration is: 9362 and loss is: 0.0004946377011947334\n",
      "Iteration is: 9363 and loss is: 0.0004945066757500172\n",
      "Iteration is: 9364 and loss is: 0.0004943714593537152\n",
      "Iteration is: 9365 and loss is: 0.0004942400264553726\n",
      "Iteration is: 9366 and loss is: 0.0004941084771417081\n",
      "Iteration is: 9367 and loss is: 0.0004939745995216072\n",
      "Iteration is: 9368 and loss is: 0.0004938406054861844\n",
      "Iteration is: 9369 and loss is: 0.0004937065532431006\n",
      "Iteration is: 9370 and loss is: 0.0004935741308145225\n",
      "Iteration is: 9371 and loss is: 0.0004934411263093352\n",
      "Iteration is: 9372 and loss is: 0.0004933091113343835\n",
      "Iteration is: 9373 and loss is: 0.0004931747680529952\n",
      "Iteration is: 9374 and loss is: 0.0004930433351546526\n",
      "Iteration is: 9375 and loss is: 0.0004929094575345516\n",
      "Iteration is: 9376 and loss is: 0.0004927776753902435\n",
      "Iteration is: 9377 and loss is: 0.0004926439141854644\n",
      "Iteration is: 9378 and loss is: 0.00049251300515607\n",
      "Iteration is: 9379 and loss is: 0.0004923802916891873\n",
      "Iteration is: 9380 and loss is: 0.0004922468797303736\n",
      "Iteration is: 9381 and loss is: 0.0004921145737171173\n",
      "Iteration is: 9382 and loss is: 0.0004919841303490102\n",
      "Iteration is: 9383 and loss is: 0.0004918516497127712\n",
      "Iteration is: 9384 and loss is: 0.0004917209153063595\n",
      "Iteration is: 9385 and loss is: 0.0004915900062769651\n",
      "Iteration is: 9386 and loss is: 0.0004914554301649332\n",
      "Iteration is: 9387 and loss is: 0.0004913235898129642\n",
      "Iteration is: 9388 and loss is: 0.0004911931464448571\n",
      "Iteration is: 9389 and loss is: 0.0004910629359073937\n",
      "Iteration is: 9390 and loss is: 0.0004909280105493963\n",
      "Iteration is: 9391 and loss is: 0.0004907971015200019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 9392 and loss is: 0.0004906647372990847\n",
      "Iteration is: 9393 and loss is: 0.0004905357491225004\n",
      "Iteration is: 9394 and loss is: 0.0004904041416011751\n",
      "Iteration is: 9395 and loss is: 0.0004902741638943553\n",
      "Iteration is: 9396 and loss is: 0.0004901417996734381\n",
      "Iteration is: 9397 and loss is: 0.0004900122294202447\n",
      "Iteration is: 9398 and loss is: 0.0004898795741610229\n",
      "Iteration is: 9399 and loss is: 0.0004897487815469503\n",
      "Iteration is: 9400 and loss is: 0.0004896190948784351\n",
      "Iteration is: 9401 and loss is: 0.0004894867888651788\n",
      "Iteration is: 9402 and loss is: 0.0004893572768196464\n",
      "Iteration is: 9403 and loss is: 0.000489225669298321\n",
      "Iteration is: 9404 and loss is: 0.0004890955751761794\n",
      "Iteration is: 9405 and loss is: 0.0004889657720923424\n",
      "Iteration is: 9406 and loss is: 0.0004888344556093216\n",
      "Iteration is: 9407 and loss is: 0.00048870372120291\n",
      "Iteration is: 9408 and loss is: 0.0004885731032118201\n",
      "Iteration is: 9409 and loss is: 0.0004884438822045922\n",
      "Iteration is: 9410 and loss is: 0.000488314195536077\n",
      "Iteration is: 9411 and loss is: 0.00048818273353390396\n",
      "Iteration is: 9412 and loss is: 0.00048805464757606387\n",
      "Iteration is: 9413 and loss is: 0.00048792260349728167\n",
      "Iteration is: 9414 and loss is: 0.0004877914907410741\n",
      "Iteration is: 9415 and loss is: 0.0004876640159636736\n",
      "Iteration is: 9416 and loss is: 0.0004875333106610924\n",
      "Iteration is: 9417 and loss is: 0.0004874038859270513\n",
      "Iteration is: 9418 and loss is: 0.00048727437388151884\n",
      "Iteration is: 9419 and loss is: 0.00048714448348619044\n",
      "Iteration is: 9420 and loss is: 0.00048701625200919807\n",
      "Iteration is: 9421 and loss is: 0.00048688348033465445\n",
      "Iteration is: 9422 and loss is: 0.00048675452126190066\n",
      "Iteration is: 9423 and loss is: 0.00048662605695426464\n",
      "Iteration is: 9424 and loss is: 0.0004864948568865657\n",
      "Iteration is: 9425 and loss is: 0.0004863662179559469\n",
      "Iteration is: 9426 and loss is: 0.0004862372879870236\n",
      "Iteration is: 9427 and loss is: 0.0004861077177338302\n",
      "Iteration is: 9428 and loss is: 0.0004859770415350795\n",
      "Iteration is: 9429 and loss is: 0.0004858482861891389\n",
      "Iteration is: 9430 and loss is: 0.0004857199965044856\n",
      "Iteration is: 9431 and loss is: 0.00048558932030573487\n",
      "Iteration is: 9432 and loss is: 0.00048546172911301255\n",
      "Iteration is: 9433 and loss is: 0.00048533239169046283\n",
      "Iteration is: 9434 and loss is: 0.0004852020647376776\n",
      "Iteration is: 9435 and loss is: 0.0004850720870308578\n",
      "Iteration is: 9436 and loss is: 0.00048494484508410096\n",
      "Iteration is: 9437 and loss is: 0.0004848153330385685\n",
      "Iteration is: 9438 and loss is: 0.00048468756722286344\n",
      "Iteration is: 9439 and loss is: 0.00048455651267431676\n",
      "Iteration is: 9440 and loss is: 0.0004844271461479366\n",
      "Iteration is: 9441 and loss is: 0.00048429903108626604\n",
      "Iteration is: 9442 and loss is: 0.00048417053767479956\n",
      "Iteration is: 9443 and loss is: 0.0004840419569518417\n",
      "Iteration is: 9444 and loss is: 0.0004839142202399671\n",
      "Iteration is: 9445 and loss is: 0.00048378558130934834\n",
      "Iteration is: 9446 and loss is: 0.0004836563894059509\n",
      "Iteration is: 9447 and loss is: 0.0004835269646719098\n",
      "Iteration is: 9448 and loss is: 0.000483397685457021\n",
      "Iteration is: 9449 and loss is: 0.000483269221149385\n",
      "Iteration is: 9450 and loss is: 0.00048314151354134083\n",
      "Iteration is: 9451 and loss is: 0.0004830103716813028\n",
      "Iteration is: 9452 and loss is: 0.0004828838864341378\n",
      "Iteration is: 9453 and loss is: 0.00048275385051965714\n",
      "Iteration is: 9454 and loss is: 0.000482626142911613\n",
      "Iteration is: 9455 and loss is: 0.0004824956995435059\n",
      "Iteration is: 9456 and loss is: 0.0004823692434001714\n",
      "Iteration is: 9457 and loss is: 0.00048224261263385415\n",
      "Iteration is: 9458 and loss is: 0.0004821102484129369\n",
      "Iteration is: 9459 and loss is: 0.0004819839377887547\n",
      "Iteration is: 9460 and loss is: 0.00048185361083596945\n",
      "Iteration is: 9461 and loss is: 0.00048172782408073545\n",
      "Iteration is: 9462 and loss is: 0.00048159845755435526\n",
      "Iteration is: 9463 and loss is: 0.0004814684798475355\n",
      "Iteration is: 9464 and loss is: 0.00048133969539776444\n",
      "Iteration is: 9465 and loss is: 0.0004812110564671457\n",
      "Iteration is: 9466 and loss is: 0.0004810839018318802\n",
      "Iteration is: 9467 and loss is: 0.00048095619422383606\n",
      "Iteration is: 9468 and loss is: 0.000480825052363798\n",
      "Iteration is: 9469 and loss is: 0.00048069667536765337\n",
      "Iteration is: 9470 and loss is: 0.00048056989908218384\n",
      "Iteration is: 9471 and loss is: 0.00048043945571407676\n",
      "Iteration is: 9472 and loss is: 0.0004803117481060326\n",
      "Iteration is: 9473 and loss is: 0.00048018229426816106\n",
      "Iteration is: 9474 and loss is: 0.0004800526949111372\n",
      "Iteration is: 9475 and loss is: 0.0004799244343303144\n",
      "Iteration is: 9476 and loss is: 0.0004797950678039342\n",
      "Iteration is: 9477 and loss is: 0.0004796672146767378\n",
      "Iteration is: 9478 and loss is: 0.00047953639295883477\n",
      "Iteration is: 9479 and loss is: 0.00047940906370058656\n",
      "Iteration is: 9480 and loss is: 0.00047927998821251094\n",
      "Iteration is: 9481 and loss is: 0.0004791507381014526\n",
      "Iteration is: 9482 and loss is: 0.00047902174992486835\n",
      "Iteration is: 9483 and loss is: 0.0004788936348631978\n",
      "Iteration is: 9484 and loss is: 0.0004787649377249181\n",
      "Iteration is: 9485 and loss is: 0.0004786352510564029\n",
      "Iteration is: 9486 and loss is: 0.0004785062628798187\n",
      "Iteration is: 9487 and loss is: 0.0004783783224411309\n",
      "Iteration is: 9488 and loss is: 0.00047824805369600654\n",
      "Iteration is: 9489 and loss is: 0.000478117261081934\n",
      "Iteration is: 9490 and loss is: 0.0004779860028065741\n",
      "Iteration is: 9491 and loss is: 0.00047785756760276854\n",
      "Iteration is: 9492 and loss is: 0.00047772741527296603\n",
      "Iteration is: 9493 and loss is: 0.00047759641893208027\n",
      "Iteration is: 9494 and loss is: 0.000477466790471226\n",
      "Iteration is: 9495 and loss is: 0.0004773343971464783\n",
      "Iteration is: 9496 and loss is: 0.0004772063111886382\n",
      "Iteration is: 9497 and loss is: 0.00047707429621368647\n",
      "Iteration is: 9498 and loss is: 0.00047694603563286364\n",
      "Iteration is: 9499 and loss is: 0.0004768133512698114\n",
      "Iteration is: 9500 and loss is: 0.0004766837228089571\n",
      "Iteration is: 9501 and loss is: 0.00047655051457695663\n",
      "Iteration is: 9502 and loss is: 0.0004764189652632922\n",
      "Iteration is: 9503 and loss is: 0.0004762865137308836\n",
      "Iteration is: 9504 and loss is: 0.00047615531366318464\n",
      "Iteration is: 9505 and loss is: 0.00047602286213077605\n",
      "Iteration is: 9506 and loss is: 0.0004758909344673157\n",
      "Iteration is: 9507 and loss is: 0.0004757578717544675\n",
      "Iteration is: 9508 and loss is: 0.00047562539111822844\n",
      "Iteration is: 9509 and loss is: 0.00047549145529046655\n",
      "Iteration is: 9510 and loss is: 0.00047535530757158995\n",
      "Iteration is: 9511 and loss is: 0.00047522212844341993\n",
      "Iteration is: 9512 and loss is: 0.0004750880762003362\n",
      "Iteration is: 9513 and loss is: 0.0004749558283947408\n",
      "Iteration is: 9514 and loss is: 0.00047482061199843884\n",
      "Iteration is: 9515 and loss is: 0.00047468155389651656\n",
      "Iteration is: 9516 and loss is: 0.0004745469777844846\n",
      "Iteration is: 9517 and loss is: 0.00047440925845876336\n",
      "Iteration is: 9518 and loss is: 0.0004742732271552086\n",
      "Iteration is: 9519 and loss is: 0.00047413527499884367\n",
      "Iteration is: 9520 and loss is: 0.0004739969444926828\n",
      "Iteration is: 9521 and loss is: 0.00047385701327584684\n",
      "Iteration is: 9522 and loss is: 0.00047371687833219767\n",
      "Iteration is: 9523 and loss is: 0.0004735759866889566\n",
      "Iteration is: 9524 and loss is: 0.0004734363465104252\n",
      "Iteration is: 9525 and loss is: 0.00047329074004665017\n",
      "Iteration is: 9526 and loss is: 0.00047314749099314213\n",
      "Iteration is: 9527 and loss is: 0.00047300418373197317\n",
      "Iteration is: 9528 and loss is: 0.0004728590720333159\n",
      "Iteration is: 9529 and loss is: 0.0004727084015030414\n",
      "Iteration is: 9530 and loss is: 0.0004725611361209303\n",
      "Iteration is: 9531 and loss is: 0.0004724112222902477\n",
      "Iteration is: 9532 and loss is: 0.000472259649541229\n",
      "Iteration is: 9533 and loss is: 0.000472106970846653\n",
      "Iteration is: 9534 and loss is: 0.0004719540593214333\n",
      "Iteration is: 9535 and loss is: 0.00047179823741316795\n",
      "Iteration is: 9536 and loss is: 0.00047163726412691176\n",
      "Iteration is: 9537 and loss is: 0.00047147725126706064\n",
      "Iteration is: 9538 and loss is: 0.00047131715109571815\n",
      "Iteration is: 9539 and loss is: 0.0004711493675131351\n",
      "Iteration is: 9540 and loss is: 0.00047098335926420987\n",
      "Iteration is: 9541 and loss is: 0.0004708140331786126\n",
      "Iteration is: 9542 and loss is: 0.0004706414183601737\n",
      "Iteration is: 9543 and loss is: 0.00047046365216374397\n",
      "Iteration is: 9544 and loss is: 0.0004702839651145041\n",
      "Iteration is: 9545 and loss is: 0.00047010736307129264\n",
      "Iteration is: 9546 and loss is: 0.00046992063289508224\n",
      "Iteration is: 9547 and loss is: 0.0004697341937571764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 9548 and loss is: 0.00046954298159107566\n",
      "Iteration is: 9549 and loss is: 0.00046935174032114446\n",
      "Iteration is: 9550 and loss is: 0.0004691547073889524\n",
      "Iteration is: 9551 and loss is: 0.000468958867713809\n",
      "Iteration is: 9552 and loss is: 0.00046876072883605957\n",
      "Iteration is: 9553 and loss is: 0.0004685603198595345\n",
      "Iteration is: 9554 and loss is: 0.00046835956163704395\n",
      "Iteration is: 9555 and loss is: 0.0004681585414800793\n",
      "Iteration is: 9556 and loss is: 0.0004679570556618273\n",
      "Iteration is: 9557 and loss is: 0.00046775868395343423\n",
      "Iteration is: 9558 and loss is: 0.0004675578966271132\n",
      "Iteration is: 9559 and loss is: 0.0004673610383179039\n",
      "Iteration is: 9560 and loss is: 0.0004671643837355077\n",
      "Iteration is: 9561 and loss is: 0.00046697008656337857\n",
      "Iteration is: 9562 and loss is: 0.00046677872887812555\n",
      "Iteration is: 9563 and loss is: 0.00046658830251544714\n",
      "Iteration is: 9564 and loss is: 0.0004663997679017484\n",
      "Iteration is: 9565 and loss is: 0.00046621597721241415\n",
      "Iteration is: 9566 and loss is: 0.00046602761722169816\n",
      "Iteration is: 9567 and loss is: 0.00046584586380049586\n",
      "Iteration is: 9568 and loss is: 0.0004656636156141758\n",
      "Iteration is: 9569 and loss is: 0.0004654847434721887\n",
      "Iteration is: 9570 and loss is: 0.0004653091309592128\n",
      "Iteration is: 9571 and loss is: 0.00046513904817402363\n",
      "Iteration is: 9572 and loss is: 0.0004649764159694314\n",
      "Iteration is: 9573 and loss is: 0.00046482880134135485\n",
      "Iteration is: 9574 and loss is: 0.0004647037712857127\n",
      "Iteration is: 9575 and loss is: 0.0004646278393920511\n",
      "Iteration is: 9576 and loss is: 0.00046464032493531704\n",
      "Iteration is: 9577 and loss is: 0.00046480901073664427\n",
      "Iteration is: 9578 and loss is: 0.00046528520761057734\n",
      "Iteration is: 9579 and loss is: 0.0004663470317609608\n",
      "Iteration is: 9580 and loss is: 0.0004685515013989061\n",
      "Iteration is: 9581 and loss is: 0.0004729939391836524\n",
      "Iteration is: 9582 and loss is: 0.0004818450834136456\n",
      "Iteration is: 9583 and loss is: 0.0004995025228708982\n",
      "Iteration is: 9584 and loss is: 0.0005345879471860826\n",
      "Iteration is: 9585 and loss is: 0.0006046135094948113\n",
      "Iteration is: 9586 and loss is: 0.0007419005851261318\n",
      "Iteration is: 9587 and loss is: 0.001007271814160049\n",
      "Iteration is: 9588 and loss is: 0.0014804985839873552\n",
      "Iteration is: 9589 and loss is: 0.0022301189601421356\n",
      "Iteration is: 9590 and loss is: 0.003011284628883004\n",
      "Iteration is: 9591 and loss is: 0.003112285165116191\n",
      "Iteration is: 9592 and loss is: 0.0017969824839383364\n",
      "Iteration is: 9593 and loss is: 0.0006956163560971618\n",
      "Iteration is: 9594 and loss is: 0.001411521341651678\n",
      "Iteration is: 9595 and loss is: 0.002371464390307665\n",
      "Iteration is: 9596 and loss is: 0.0017315056174993515\n",
      "Iteration is: 9597 and loss is: 0.0007556643686257303\n",
      "Iteration is: 9598 and loss is: 0.0008918707026168704\n",
      "Iteration is: 9599 and loss is: 0.0013381884200498462\n",
      "Iteration is: 9600 and loss is: 0.0011531031923368573\n",
      "Iteration is: 9601 and loss is: 0.0008546715835109353\n",
      "Iteration is: 9602 and loss is: 0.0007214464712888002\n",
      "Iteration is: 9603 and loss is: 0.0007980081718415022\n",
      "Iteration is: 9604 and loss is: 0.0009099624585360289\n",
      "Iteration is: 9605 and loss is: 0.0012131102848798037\n",
      "Iteration is: 9606 and loss is: 0.0015907882479950786\n",
      "Iteration is: 9607 and loss is: 0.001775296637788415\n",
      "Iteration is: 9608 and loss is: 0.0015805189032107592\n",
      "Iteration is: 9609 and loss is: 0.0010403324849903584\n",
      "Iteration is: 9610 and loss is: 0.0005796478362753987\n",
      "Iteration is: 9611 and loss is: 0.0005557129043154418\n",
      "Iteration is: 9612 and loss is: 0.0008911235490813851\n",
      "Iteration is: 9613 and loss is: 0.0009321238612756133\n",
      "Iteration is: 9614 and loss is: 0.0006489845691248775\n",
      "Iteration is: 9615 and loss is: 0.0005444379057735205\n",
      "Iteration is: 9616 and loss is: 0.0006924290210008621\n",
      "Iteration is: 9617 and loss is: 0.0007477311883121729\n",
      "Iteration is: 9618 and loss is: 0.0006309833843261003\n",
      "Iteration is: 9619 and loss is: 0.0005598394200205803\n",
      "Iteration is: 9620 and loss is: 0.0005613304674625397\n",
      "Iteration is: 9621 and loss is: 0.0006192055880092084\n",
      "Iteration is: 9622 and loss is: 0.0006553566781803966\n",
      "Iteration is: 9623 and loss is: 0.0006254658219404519\n",
      "Iteration is: 9624 and loss is: 0.0005459693493321538\n",
      "Iteration is: 9625 and loss is: 0.00048805458936840296\n",
      "Iteration is: 9626 and loss is: 0.0005198706639930606\n",
      "Iteration is: 9627 and loss is: 0.0005875923670828342\n",
      "Iteration is: 9628 and loss is: 0.0006016016704961658\n",
      "Iteration is: 9629 and loss is: 0.0005456891958601773\n",
      "Iteration is: 9630 and loss is: 0.0004850452532991767\n",
      "Iteration is: 9631 and loss is: 0.0004756835987791419\n",
      "Iteration is: 9632 and loss is: 0.000507829652633518\n",
      "Iteration is: 9633 and loss is: 0.000527735857758671\n",
      "Iteration is: 9634 and loss is: 0.0005165690672583878\n",
      "Iteration is: 9635 and loss is: 0.0004913771990686655\n",
      "Iteration is: 9636 and loss is: 0.0004751957021653652\n",
      "Iteration is: 9637 and loss is: 0.00048000761307775974\n",
      "Iteration is: 9638 and loss is: 0.0004917556652799249\n",
      "Iteration is: 9639 and loss is: 0.0004971339949406683\n",
      "Iteration is: 9640 and loss is: 0.00048714166041463614\n",
      "Iteration is: 9641 and loss is: 0.0004714540846180171\n",
      "Iteration is: 9642 and loss is: 0.0004657277022488415\n",
      "Iteration is: 9643 and loss is: 0.0004727211780846119\n",
      "Iteration is: 9644 and loss is: 0.000482879055198282\n",
      "Iteration is: 9645 and loss is: 0.0004830120597034693\n",
      "Iteration is: 9646 and loss is: 0.00047349545639008284\n",
      "Iteration is: 9647 and loss is: 0.0004635695950128138\n",
      "Iteration is: 9648 and loss is: 0.0004623095446731895\n",
      "Iteration is: 9649 and loss is: 0.00046841363655403256\n",
      "Iteration is: 9650 and loss is: 0.0004735986585728824\n",
      "Iteration is: 9651 and loss is: 0.00047259763232432306\n",
      "Iteration is: 9652 and loss is: 0.00046635890612378716\n",
      "Iteration is: 9653 and loss is: 0.0004613151540979743\n",
      "Iteration is: 9654 and loss is: 0.0004605504800565541\n",
      "Iteration is: 9655 and loss is: 0.0004632371128536761\n",
      "Iteration is: 9656 and loss is: 0.0004655357333831489\n",
      "Iteration is: 9657 and loss is: 0.0004651416384149343\n",
      "Iteration is: 9658 and loss is: 0.00046285256394185126\n",
      "Iteration is: 9659 and loss is: 0.00046039518201723695\n",
      "Iteration is: 9660 and loss is: 0.0004595548671204597\n",
      "Iteration is: 9661 and loss is: 0.00046011433005332947\n",
      "Iteration is: 9662 and loss is: 0.00046128593385219574\n",
      "Iteration is: 9663 and loss is: 0.00046166154788807034\n",
      "Iteration is: 9664 and loss is: 0.00046073627891018987\n",
      "Iteration is: 9665 and loss is: 0.0004591462784446776\n",
      "Iteration is: 9666 and loss is: 0.0004579705710057169\n",
      "Iteration is: 9667 and loss is: 0.0004580328823067248\n",
      "Iteration is: 9668 and loss is: 0.00045880110701546073\n",
      "Iteration is: 9669 and loss is: 0.00045938248513266444\n",
      "Iteration is: 9670 and loss is: 0.00045909517211839557\n",
      "Iteration is: 9671 and loss is: 0.00045818538637831807\n",
      "Iteration is: 9672 and loss is: 0.00045735889580100775\n",
      "Iteration is: 9673 and loss is: 0.0004569998709484935\n",
      "Iteration is: 9674 and loss is: 0.00045709399273619056\n",
      "Iteration is: 9675 and loss is: 0.0004572619218379259\n",
      "Iteration is: 9676 and loss is: 0.00045728959958069026\n",
      "Iteration is: 9677 and loss is: 0.00045710126869380474\n",
      "Iteration is: 9678 and loss is: 0.0004567535361275077\n",
      "Iteration is: 9679 and loss is: 0.00045635434798896313\n",
      "Iteration is: 9680 and loss is: 0.0004560240777209401\n",
      "Iteration is: 9681 and loss is: 0.0004558835062198341\n",
      "Iteration is: 9682 and loss is: 0.0004558935179375112\n",
      "Iteration is: 9683 and loss is: 0.0004559461958706379\n",
      "Iteration is: 9684 and loss is: 0.0004558637738227844\n",
      "Iteration is: 9685 and loss is: 0.00045562576269730926\n",
      "Iteration is: 9686 and loss is: 0.0004553095786832273\n",
      "Iteration is: 9687 and loss is: 0.0004550364101305604\n",
      "Iteration is: 9688 and loss is: 0.00045488192699849606\n",
      "Iteration is: 9689 and loss is: 0.0004548055003397167\n",
      "Iteration is: 9690 and loss is: 0.000454750785138458\n",
      "Iteration is: 9691 and loss is: 0.0004546664422377944\n",
      "Iteration is: 9692 and loss is: 0.0004545376868918538\n",
      "Iteration is: 9693 and loss is: 0.0004543734830804169\n",
      "Iteration is: 9694 and loss is: 0.000454183726105839\n",
      "Iteration is: 9695 and loss is: 0.0004539877991192043\n",
      "Iteration is: 9696 and loss is: 0.00045381090603768826\n",
      "Iteration is: 9697 and loss is: 0.0004536659980658442\n",
      "Iteration is: 9698 and loss is: 0.00045355159090831876\n",
      "Iteration is: 9699 and loss is: 0.0004534542968031019\n",
      "Iteration is: 9700 and loss is: 0.0004533423634711653\n",
      "Iteration is: 9701 and loss is: 0.0004532052844297141\n",
      "Iteration is: 9702 and loss is: 0.0004530484729912132\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 9703 and loss is: 0.00045288424007594585\n",
      "Iteration is: 9704 and loss is: 0.0004527201526798308\n",
      "Iteration is: 9705 and loss is: 0.00045256305020302534\n",
      "Iteration is: 9706 and loss is: 0.00045241197221912444\n",
      "Iteration is: 9707 and loss is: 0.00045225885696709156\n",
      "Iteration is: 9708 and loss is: 0.0004521160153672099\n",
      "Iteration is: 9709 and loss is: 0.0004519734065979719\n",
      "Iteration is: 9710 and loss is: 0.0004518320201896131\n",
      "Iteration is: 9711 and loss is: 0.0004516866756603122\n",
      "Iteration is: 9712 and loss is: 0.00045153143582865596\n",
      "Iteration is: 9713 and loss is: 0.00045137066626921296\n",
      "Iteration is: 9714 and loss is: 0.00045119825517758727\n",
      "Iteration is: 9715 and loss is: 0.00045102860894985497\n",
      "Iteration is: 9716 and loss is: 0.00045085063902661204\n",
      "Iteration is: 9717 and loss is: 0.00045067135943099856\n",
      "Iteration is: 9718 and loss is: 0.0004504826501943171\n",
      "Iteration is: 9719 and loss is: 0.000450292369350791\n",
      "Iteration is: 9720 and loss is: 0.00045009588939137757\n",
      "Iteration is: 9721 and loss is: 0.000449896149802953\n",
      "Iteration is: 9722 and loss is: 0.0004496909386944026\n",
      "Iteration is: 9723 and loss is: 0.0004494778986554593\n",
      "Iteration is: 9724 and loss is: 0.00044925883412361145\n",
      "Iteration is: 9725 and loss is: 0.0004490309511311352\n",
      "Iteration is: 9726 and loss is: 0.00044880027417093515\n",
      "Iteration is: 9727 and loss is: 0.000448561564553529\n",
      "Iteration is: 9728 and loss is: 0.0004483146476559341\n",
      "Iteration is: 9729 and loss is: 0.00044806808000430465\n",
      "Iteration is: 9730 and loss is: 0.00044781947508454323\n",
      "Iteration is: 9731 and loss is: 0.00044756982242688537\n",
      "Iteration is: 9732 and loss is: 0.0004473211010918021\n",
      "Iteration is: 9733 and loss is: 0.0004470768035389483\n",
      "Iteration is: 9734 and loss is: 0.0004468371334951371\n",
      "Iteration is: 9735 and loss is: 0.00044660549610853195\n",
      "Iteration is: 9736 and loss is: 0.0004463820660021156\n",
      "Iteration is: 9737 and loss is: 0.00044616643572226167\n",
      "Iteration is: 9738 and loss is: 0.0004459595656953752\n",
      "Iteration is: 9739 and loss is: 0.000445763289462775\n",
      "Iteration is: 9740 and loss is: 0.00044558433000929654\n",
      "Iteration is: 9741 and loss is: 0.0004454486770555377\n",
      "Iteration is: 9742 and loss is: 0.0004453950677998364\n",
      "Iteration is: 9743 and loss is: 0.00044553951011039317\n",
      "Iteration is: 9744 and loss is: 0.0004461584612727165\n",
      "Iteration is: 9745 and loss is: 0.00044788751984015107\n",
      "Iteration is: 9746 and loss is: 0.0004523135139606893\n",
      "Iteration is: 9747 and loss is: 0.0004632165946532041\n",
      "Iteration is: 9748 and loss is: 0.0004899954656139016\n",
      "Iteration is: 9749 and loss is: 0.000554898229893297\n",
      "Iteration is: 9750 and loss is: 0.0007139014196582139\n",
      "Iteration is: 9751 and loss is: 0.0010875414591282606\n",
      "Iteration is: 9752 and loss is: 0.001956959255039692\n",
      "Iteration is: 9753 and loss is: 0.0036146598868072033\n",
      "Iteration is: 9754 and loss is: 0.006075755227357149\n",
      "Iteration is: 9755 and loss is: 0.006267627235502005\n",
      "Iteration is: 9756 and loss is: 0.0031823664903640747\n",
      "Iteration is: 9757 and loss is: 0.0018435448873788118\n",
      "Iteration is: 9758 and loss is: 0.0032003128435462713\n",
      "Iteration is: 9759 and loss is: 0.007353931199759245\n",
      "Iteration is: 9760 and loss is: 0.01224733516573906\n",
      "Iteration is: 9761 and loss is: 0.005147889256477356\n",
      "Iteration is: 9762 and loss is: 0.0013124047545716166\n",
      "Iteration is: 9763 and loss is: 0.008696465753018856\n",
      "Iteration is: 9764 and loss is: 0.020503288134932518\n",
      "Iteration is: 9765 and loss is: 0.01984001137316227\n",
      "Iteration is: 9766 and loss is: 0.0047408174723386765\n",
      "Iteration is: 9767 and loss is: 0.028698455542325974\n",
      "Iteration is: 9768 and loss is: 0.06172452121973038\n",
      "Iteration is: 9769 and loss is: 0.04014161229133606\n",
      "Iteration is: 9770 and loss is: 0.03194130212068558\n",
      "Iteration is: 9771 and loss is: 0.0443246066570282\n",
      "Iteration is: 9772 and loss is: 0.07458236068487167\n",
      "Iteration is: 9773 and loss is: 0.036407437175512314\n",
      "Iteration is: 9774 and loss is: 0.043009836226701736\n",
      "Iteration is: 9775 and loss is: 0.023421522229909897\n",
      "Iteration is: 9776 and loss is: 0.03395151346921921\n",
      "Iteration is: 9777 and loss is: 0.026813097298145294\n",
      "Iteration is: 9778 and loss is: 0.022186610847711563\n",
      "Iteration is: 9779 and loss is: 0.026569021865725517\n",
      "Iteration is: 9780 and loss is: 0.01657213643193245\n",
      "Iteration is: 9781 and loss is: 0.019794756546616554\n",
      "Iteration is: 9782 and loss is: 0.014583435840904713\n",
      "Iteration is: 9783 and loss is: 0.012958118692040443\n",
      "Iteration is: 9784 and loss is: 0.016399135813117027\n",
      "Iteration is: 9785 and loss is: 0.006631131283938885\n",
      "Iteration is: 9786 and loss is: 0.016165439039468765\n",
      "Iteration is: 9787 and loss is: 0.0064153652638196945\n",
      "Iteration is: 9788 and loss is: 0.009548268280923367\n",
      "Iteration is: 9789 and loss is: 0.009870661422610283\n",
      "Iteration is: 9790 and loss is: 0.00521233631297946\n",
      "Iteration is: 9791 and loss is: 0.007111882790923119\n",
      "Iteration is: 9792 and loss is: 0.007570864632725716\n",
      "Iteration is: 9793 and loss is: 0.0035225320607423782\n",
      "Iteration is: 9794 and loss is: 0.0070977648720145226\n",
      "Iteration is: 9795 and loss is: 0.004199618473649025\n",
      "Iteration is: 9796 and loss is: 0.0037896204739809036\n",
      "Iteration is: 9797 and loss is: 0.005589022766798735\n",
      "Iteration is: 9798 and loss is: 0.002678773133084178\n",
      "Iteration is: 9799 and loss is: 0.0043603042140603065\n",
      "Iteration is: 9800 and loss is: 0.0032680360600352287\n",
      "Iteration is: 9801 and loss is: 0.0032322253100574017\n",
      "Iteration is: 9802 and loss is: 0.0027452162466943264\n",
      "Iteration is: 9803 and loss is: 0.00345591944642365\n",
      "Iteration is: 9804 and loss is: 0.0019310946809127927\n",
      "Iteration is: 9805 and loss is: 0.0029069145675748587\n",
      "Iteration is: 9806 and loss is: 0.0022846742067486048\n",
      "Iteration is: 9807 and loss is: 0.002039101906120777\n",
      "Iteration is: 9808 and loss is: 0.0023413589224219322\n",
      "Iteration is: 9809 and loss is: 0.001987060997635126\n",
      "Iteration is: 9810 and loss is: 0.0016578387003391981\n",
      "Iteration is: 9811 and loss is: 0.0020470749586820602\n",
      "Iteration is: 9812 and loss is: 0.0015850133495405316\n",
      "Iteration is: 9813 and loss is: 0.0015748562291264534\n",
      "Iteration is: 9814 and loss is: 0.0017806871328502893\n",
      "Iteration is: 9815 and loss is: 0.0013036273885518312\n",
      "Iteration is: 9816 and loss is: 0.0015806520823389292\n",
      "Iteration is: 9817 and loss is: 0.0013897777535021305\n",
      "Iteration is: 9818 and loss is: 0.001286159735172987\n",
      "Iteration is: 9819 and loss is: 0.0013218849198892713\n",
      "Iteration is: 9820 and loss is: 0.0012269705766811967\n",
      "Iteration is: 9821 and loss is: 0.001170020317658782\n",
      "Iteration is: 9822 and loss is: 0.001131701748818159\n",
      "Iteration is: 9823 and loss is: 0.00111674377694726\n",
      "Iteration is: 9824 and loss is: 0.0010199651587754488\n",
      "Iteration is: 9825 and loss is: 0.001069442369043827\n",
      "Iteration is: 9826 and loss is: 0.00099570385646075\n",
      "Iteration is: 9827 and loss is: 0.0009504499612376094\n",
      "Iteration is: 9828 and loss is: 0.0009548838716000319\n",
      "Iteration is: 9829 and loss is: 0.0008996978867799044\n",
      "Iteration is: 9830 and loss is: 0.0008821574738249183\n",
      "Iteration is: 9831 and loss is: 0.000881871092133224\n",
      "Iteration is: 9832 and loss is: 0.0008287216769531369\n",
      "Iteration is: 9833 and loss is: 0.0008136235410347581\n",
      "Iteration is: 9834 and loss is: 0.000822903704829514\n",
      "Iteration is: 9835 and loss is: 0.0007553481264039874\n",
      "Iteration is: 9836 and loss is: 0.0007900466443970799\n",
      "Iteration is: 9837 and loss is: 0.0007461516070179641\n",
      "Iteration is: 9838 and loss is: 0.000723030767403543\n",
      "Iteration is: 9839 and loss is: 0.0007422666531056166\n",
      "Iteration is: 9840 and loss is: 0.0006869528442621231\n",
      "Iteration is: 9841 and loss is: 0.0007050498388707638\n",
      "Iteration is: 9842 and loss is: 0.0006869544740766287\n",
      "Iteration is: 9843 and loss is: 0.0006652654847130179\n",
      "Iteration is: 9844 and loss is: 0.000670977751724422\n",
      "Iteration is: 9845 and loss is: 0.0006502684555016458\n",
      "Iteration is: 9846 and loss is: 0.0006393122021108866\n",
      "Iteration is: 9847 and loss is: 0.0006430466310121119\n",
      "Iteration is: 9848 and loss is: 0.0006222414085641503\n",
      "Iteration is: 9849 and loss is: 0.0006188557599671185\n",
      "Iteration is: 9850 and loss is: 0.0006179020274430513\n",
      "Iteration is: 9851 and loss is: 0.0005986944888718426\n",
      "Iteration is: 9852 and loss is: 0.000606418470852077\n",
      "Iteration is: 9853 and loss is: 0.0005923662101849914\n",
      "Iteration is: 9854 and loss is: 0.000586089794524014\n",
      "Iteration is: 9855 and loss is: 0.0005865677376277745\n",
      "Iteration is: 9856 and loss is: 0.0005749348783865571\n",
      "Iteration is: 9857 and loss is: 0.0005743519868701696\n",
      "Iteration is: 9858 and loss is: 0.0005694600404240191\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 9859 and loss is: 0.0005627849604934454\n",
      "Iteration is: 9860 and loss is: 0.0005604000180028379\n",
      "Iteration is: 9861 and loss is: 0.0005567250773310661\n",
      "Iteration is: 9862 and loss is: 0.0005509174079634249\n",
      "Iteration is: 9863 and loss is: 0.0005497282254509628\n",
      "Iteration is: 9864 and loss is: 0.0005445358110591769\n",
      "Iteration is: 9865 and loss is: 0.0005416416097432375\n",
      "Iteration is: 9866 and loss is: 0.0005391797749325633\n",
      "Iteration is: 9867 and loss is: 0.0005353911546990275\n",
      "Iteration is: 9868 and loss is: 0.0005332015571184456\n",
      "Iteration is: 9869 and loss is: 0.0005299634649418294\n",
      "Iteration is: 9870 and loss is: 0.000527696858625859\n",
      "Iteration is: 9871 and loss is: 0.0005246393266133964\n",
      "Iteration is: 9872 and loss is: 0.0005226500215940177\n",
      "Iteration is: 9873 and loss is: 0.0005201437743380666\n",
      "Iteration is: 9874 and loss is: 0.0005179313011467457\n",
      "Iteration is: 9875 and loss is: 0.0005156153929419816\n",
      "Iteration is: 9876 and loss is: 0.0005138120613992214\n",
      "Iteration is: 9877 and loss is: 0.0005115300882607698\n",
      "Iteration is: 9878 and loss is: 0.0005096830427646637\n",
      "Iteration is: 9879 and loss is: 0.0005080935079604387\n",
      "Iteration is: 9880 and loss is: 0.0005057356902398169\n",
      "Iteration is: 9881 and loss is: 0.000504489813465625\n",
      "Iteration is: 9882 and loss is: 0.0005026678554713726\n",
      "Iteration is: 9883 and loss is: 0.0005009134183637798\n",
      "Iteration is: 9884 and loss is: 0.0004995292401872575\n",
      "Iteration is: 9885 and loss is: 0.0004979072255082428\n",
      "Iteration is: 9886 and loss is: 0.0004963398678228259\n",
      "Iteration is: 9887 and loss is: 0.0004951058072037995\n",
      "Iteration is: 9888 and loss is: 0.0004935497418045998\n",
      "Iteration is: 9889 and loss is: 0.0004922632360830903\n",
      "Iteration is: 9890 and loss is: 0.0004909421550109982\n",
      "Iteration is: 9891 and loss is: 0.000489646103233099\n",
      "Iteration is: 9892 and loss is: 0.0004884721711277962\n",
      "Iteration is: 9893 and loss is: 0.0004871414275839925\n",
      "Iteration is: 9894 and loss is: 0.000486110569909215\n",
      "Iteration is: 9895 and loss is: 0.00048485753359273076\n",
      "Iteration is: 9896 and loss is: 0.00048376445192843676\n",
      "Iteration is: 9897 and loss is: 0.0004827376105822623\n",
      "Iteration is: 9898 and loss is: 0.0004816182772628963\n",
      "Iteration is: 9899 and loss is: 0.0004806046490557492\n",
      "Iteration is: 9900 and loss is: 0.000479620648548007\n",
      "Iteration is: 9901 and loss is: 0.0004785974742844701\n",
      "Iteration is: 9902 and loss is: 0.0004776642599608749\n",
      "Iteration is: 9903 and loss is: 0.0004767179489135742\n",
      "Iteration is: 9904 and loss is: 0.0004758058348670602\n",
      "Iteration is: 9905 and loss is: 0.00047491141594946384\n",
      "Iteration is: 9906 and loss is: 0.00047402497148141265\n",
      "Iteration is: 9907 and loss is: 0.00047319900477305055\n",
      "Iteration is: 9908 and loss is: 0.0004723279853351414\n",
      "Iteration is: 9909 and loss is: 0.0004715257091447711\n",
      "Iteration is: 9910 and loss is: 0.0004707233456429094\n",
      "Iteration is: 9911 and loss is: 0.0004699327109847218\n",
      "Iteration is: 9912 and loss is: 0.00046916509745642543\n",
      "Iteration is: 9913 and loss is: 0.00046841788571327925\n",
      "Iteration is: 9914 and loss is: 0.00046767026651650667\n",
      "Iteration is: 9915 and loss is: 0.000466948957182467\n",
      "Iteration is: 9916 and loss is: 0.0004662410938180983\n",
      "Iteration is: 9917 and loss is: 0.0004655446973629296\n",
      "Iteration is: 9918 and loss is: 0.00046485959319397807\n",
      "Iteration is: 9919 and loss is: 0.00046418464626185596\n",
      "Iteration is: 9920 and loss is: 0.0004635307122953236\n",
      "Iteration is: 9921 and loss is: 0.0004628809983842075\n",
      "Iteration is: 9922 and loss is: 0.00046224630204960704\n",
      "Iteration is: 9923 and loss is: 0.00046162647777237\n",
      "Iteration is: 9924 and loss is: 0.00046101020416244864\n",
      "Iteration is: 9925 and loss is: 0.00046040897723287344\n",
      "Iteration is: 9926 and loss is: 0.0004598207597155124\n",
      "Iteration is: 9927 and loss is: 0.0004592353361658752\n",
      "Iteration is: 9928 and loss is: 0.0004586644936352968\n",
      "Iteration is: 9929 and loss is: 0.0004580985405482352\n",
      "Iteration is: 9930 and loss is: 0.00045754597522318363\n",
      "Iteration is: 9931 and loss is: 0.00045700138434767723\n",
      "Iteration is: 9932 and loss is: 0.0004564638074953109\n",
      "Iteration is: 9933 and loss is: 0.0004559433145914227\n",
      "Iteration is: 9934 and loss is: 0.00045541999861598015\n",
      "Iteration is: 9935 and loss is: 0.0004549063160084188\n",
      "Iteration is: 9936 and loss is: 0.0004544037801679224\n",
      "Iteration is: 9937 and loss is: 0.0004539070650935173\n",
      "Iteration is: 9938 and loss is: 0.00045341975055634975\n",
      "Iteration is: 9939 and loss is: 0.00045293866423889995\n",
      "Iteration is: 9940 and loss is: 0.0004524633986875415\n",
      "Iteration is: 9941 and loss is: 0.0004519961657933891\n",
      "Iteration is: 9942 and loss is: 0.0004515328910201788\n",
      "Iteration is: 9943 and loss is: 0.00045107980258762836\n",
      "Iteration is: 9944 and loss is: 0.0004506309924181551\n",
      "Iteration is: 9945 and loss is: 0.0004501880903262645\n",
      "Iteration is: 9946 and loss is: 0.0004497526679188013\n",
      "Iteration is: 9947 and loss is: 0.00044932280434295535\n",
      "Iteration is: 9948 and loss is: 0.0004489014099817723\n",
      "Iteration is: 9949 and loss is: 0.00044848263496533036\n",
      "Iteration is: 9950 and loss is: 0.00044806714868173003\n",
      "Iteration is: 9951 and loss is: 0.00044765963684767485\n",
      "Iteration is: 9952 and loss is: 0.00044725858606398106\n",
      "Iteration is: 9953 and loss is: 0.0004468602128326893\n",
      "Iteration is: 9954 and loss is: 0.0004464648081921041\n",
      "Iteration is: 9955 and loss is: 0.0004460795898921788\n",
      "Iteration is: 9956 and loss is: 0.00044569323654286563\n",
      "Iteration is: 9957 and loss is: 0.0004453173605725169\n",
      "Iteration is: 9958 and loss is: 0.0004449437838047743\n",
      "Iteration is: 9959 and loss is: 0.0004445721860975027\n",
      "Iteration is: 9960 and loss is: 0.000444208737462759\n",
      "Iteration is: 9961 and loss is: 0.00044384694774635136\n",
      "Iteration is: 9962 and loss is: 0.0004434905422385782\n",
      "Iteration is: 9963 and loss is: 0.00044314004480838776\n",
      "Iteration is: 9964 and loss is: 0.0004427875974215567\n",
      "Iteration is: 9965 and loss is: 0.00044244565651752055\n",
      "Iteration is: 9966 and loss is: 0.000442102609667927\n",
      "Iteration is: 9967 and loss is: 0.00044176256051287055\n",
      "Iteration is: 9968 and loss is: 0.00044143066043034196\n",
      "Iteration is: 9969 and loss is: 0.00044110254384577274\n",
      "Iteration is: 9970 and loss is: 0.00044077259371988475\n",
      "Iteration is: 9971 and loss is: 0.00044045032700523734\n",
      "Iteration is: 9972 and loss is: 0.0004401289043016732\n",
      "Iteration is: 9973 and loss is: 0.0004398115852382034\n",
      "Iteration is: 9974 and loss is: 0.00043950072722509503\n",
      "Iteration is: 9975 and loss is: 0.00043918780284002423\n",
      "Iteration is: 9976 and loss is: 0.00043887837091460824\n",
      "Iteration is: 9977 and loss is: 0.0004385761567391455\n",
      "Iteration is: 9978 and loss is: 0.0004382735933177173\n",
      "Iteration is: 9979 and loss is: 0.00043797591933980584\n",
      "Iteration is: 9980 and loss is: 0.00043767859460785985\n",
      "Iteration is: 9981 and loss is: 0.0004373858100734651\n",
      "Iteration is: 9982 and loss is: 0.00043709532474167645\n",
      "Iteration is: 9983 and loss is: 0.0004368044901639223\n",
      "Iteration is: 9984 and loss is: 0.0004365210188552737\n",
      "Iteration is: 9985 and loss is: 0.00043623812962323427\n",
      "Iteration is: 9986 and loss is: 0.0004359563463367522\n",
      "Iteration is: 9987 and loss is: 0.0004356799181550741\n",
      "Iteration is: 9988 and loss is: 0.00043540276237763464\n",
      "Iteration is: 9989 and loss is: 0.0004351313691586256\n",
      "Iteration is: 9990 and loss is: 0.0004348586080595851\n",
      "Iteration is: 9991 and loss is: 0.00043459096923470497\n",
      "Iteration is: 9992 and loss is: 0.00043432795791886747\n",
      "Iteration is: 9993 and loss is: 0.0004340617451816797\n",
      "Iteration is: 9994 and loss is: 0.00043379905400797725\n",
      "Iteration is: 9995 and loss is: 0.0004335386911407113\n",
      "Iteration is: 9996 and loss is: 0.0004332796379458159\n",
      "Iteration is: 9997 and loss is: 0.0004330235533416271\n",
      "Iteration is: 9998 and loss is: 0.00043276860378682613\n",
      "Iteration is: 9999 and loss is: 0.0004325191839598119\n",
      "Iteration is: 10000 and loss is: 0.000432267552241683\n",
      "Iteration is: 10001 and loss is: 0.00043201990774832666\n",
      "Iteration is: 10002 and loss is: 0.00043177130282856524\n",
      "Iteration is: 10003 and loss is: 0.0004315268015488982\n",
      "Iteration is: 10004 and loss is: 0.0004312837263569236\n",
      "Iteration is: 10005 and loss is: 0.00043104286305606365\n",
      "Iteration is: 10006 and loss is: 0.0004308023490011692\n",
      "Iteration is: 10007 and loss is: 0.00043056783033534884\n",
      "Iteration is: 10008 and loss is: 0.0004303297318983823\n",
      "Iteration is: 10009 and loss is: 0.0004300943692214787\n",
      "Iteration is: 10010 and loss is: 0.00042986319749616086\n",
      "Iteration is: 10011 and loss is: 0.000429630366852507\n",
      "Iteration is: 10012 and loss is: 0.00042939913691952825\n",
      "Iteration is: 10013 and loss is: 0.0004291711957193911\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 10014 and loss is: 0.0004289440985303372\n",
      "Iteration is: 10015 and loss is: 0.00042871912592090666\n",
      "Iteration is: 10016 and loss is: 0.00042849284363910556\n",
      "Iteration is: 10017 and loss is: 0.0004282707523088902\n",
      "Iteration is: 10018 and loss is: 0.00042805192060768604\n",
      "Iteration is: 10019 and loss is: 0.0004278331180103123\n",
      "Iteration is: 10020 and loss is: 0.0004276090767234564\n",
      "Iteration is: 10021 and loss is: 0.00042739714263007045\n",
      "Iteration is: 10022 and loss is: 0.0004271774960216135\n",
      "Iteration is: 10023 and loss is: 0.000426962913479656\n",
      "Iteration is: 10024 and loss is: 0.0004267486510798335\n",
      "Iteration is: 10025 and loss is: 0.00042653927812352777\n",
      "Iteration is: 10026 and loss is: 0.00042632699478417635\n",
      "Iteration is: 10027 and loss is: 0.00042611476965248585\n",
      "Iteration is: 10028 and loss is: 0.0004259073466528207\n",
      "Iteration is: 10029 and loss is: 0.00042569811921566725\n",
      "Iteration is: 10030 and loss is: 0.0004254917148500681\n",
      "Iteration is: 10031 and loss is: 0.0004252864746376872\n",
      "Iteration is: 10032 and loss is: 0.0004250840866006911\n",
      "Iteration is: 10033 and loss is: 0.00042487445170991123\n",
      "Iteration is: 10034 and loss is: 0.0004246749740559608\n",
      "Iteration is: 10035 and loss is: 0.0004244761948939413\n",
      "Iteration is: 10036 and loss is: 0.00042427328298799694\n",
      "Iteration is: 10037 and loss is: 0.0004240743292029947\n",
      "Iteration is: 10038 and loss is: 0.00042387587018311024\n",
      "Iteration is: 10039 and loss is: 0.0004236786044202745\n",
      "Iteration is: 10040 and loss is: 0.00042348241549916565\n",
      "Iteration is: 10041 and loss is: 0.00042328640120103955\n",
      "Iteration is: 10042 and loss is: 0.000423091696575284\n",
      "Iteration is: 10043 and loss is: 0.00042289914563298225\n",
      "Iteration is: 10044 and loss is: 0.0004227047902531922\n",
      "Iteration is: 10045 and loss is: 0.00042251398554071784\n",
      "Iteration is: 10046 and loss is: 0.00042232294799759984\n",
      "Iteration is: 10047 and loss is: 0.000422132812673226\n",
      "Iteration is: 10048 and loss is: 0.0004219438415020704\n",
      "Iteration is: 10049 and loss is: 0.00042175373528152704\n",
      "Iteration is: 10050 and loss is: 0.00042156767449341714\n",
      "Iteration is: 10051 and loss is: 0.0004213802167214453\n",
      "Iteration is: 10052 and loss is: 0.00042119278805330396\n",
      "Iteration is: 10053 and loss is: 0.0004210100742056966\n",
      "Iteration is: 10054 and loss is: 0.00042082718573510647\n",
      "Iteration is: 10055 and loss is: 0.00042064202716574073\n",
      "Iteration is: 10056 and loss is: 0.00042045762529596686\n",
      "Iteration is: 10057 and loss is: 0.0004202784039080143\n",
      "Iteration is: 10058 and loss is: 0.00042009458411484957\n",
      "Iteration is: 10059 and loss is: 0.00041991425678133965\n",
      "Iteration is: 10060 and loss is: 0.00041973317274823785\n",
      "Iteration is: 10061 and loss is: 0.00041955712367780507\n",
      "Iteration is: 10062 and loss is: 0.0004193777567707002\n",
      "Iteration is: 10063 and loss is: 0.00041920016519725323\n",
      "Iteration is: 10064 and loss is: 0.0004190218751318753\n",
      "Iteration is: 10065 and loss is: 0.0004188431485090405\n",
      "Iteration is: 10066 and loss is: 0.00041866739047691226\n",
      "Iteration is: 10067 and loss is: 0.00041849567787721753\n",
      "Iteration is: 10068 and loss is: 0.00041831767885014415\n",
      "Iteration is: 10069 and loss is: 0.00041814480209723115\n",
      "Iteration is: 10070 and loss is: 0.00041797145968303084\n",
      "Iteration is: 10071 and loss is: 0.0004177983500994742\n",
      "Iteration is: 10072 and loss is: 0.0004176284419372678\n",
      "Iteration is: 10073 and loss is: 0.0004174556233920157\n",
      "Iteration is: 10074 and loss is: 0.0004172836779616773\n",
      "Iteration is: 10075 and loss is: 0.00041711371159181\n",
      "Iteration is: 10076 and loss is: 0.0004169464809820056\n",
      "Iteration is: 10077 and loss is: 0.0004167752340435982\n",
      "Iteration is: 10078 and loss is: 0.00041660689748823643\n",
      "Iteration is: 10079 and loss is: 0.0004164378624409437\n",
      "Iteration is: 10080 and loss is: 0.00041627028258517385\n",
      "Iteration is: 10081 and loss is: 0.00041610386688262224\n",
      "Iteration is: 10082 and loss is: 0.000415936519857496\n",
      "Iteration is: 10083 and loss is: 0.0004157730145379901\n",
      "Iteration is: 10084 and loss is: 0.0004156050272285938\n",
      "Iteration is: 10085 and loss is: 0.000415440124925226\n",
      "Iteration is: 10086 and loss is: 0.0004152789479121566\n",
      "Iteration is: 10087 and loss is: 0.00041511235758662224\n",
      "Iteration is: 10088 and loss is: 0.00041495036566630006\n",
      "Iteration is: 10089 and loss is: 0.00041478703496977687\n",
      "Iteration is: 10090 and loss is: 0.00041462460649199784\n",
      "Iteration is: 10091 and loss is: 0.00041446220711804926\n",
      "Iteration is: 10092 and loss is: 0.0004143010010011494\n",
      "Iteration is: 10093 and loss is: 0.00041413880535401404\n",
      "Iteration is: 10094 and loss is: 0.00041397911263629794\n",
      "Iteration is: 10095 and loss is: 0.00041382061317563057\n",
      "Iteration is: 10096 and loss is: 0.0004136569914408028\n",
      "Iteration is: 10097 and loss is: 0.0004135012277401984\n",
      "Iteration is: 10098 and loss is: 0.0004133414477109909\n",
      "Iteration is: 10099 and loss is: 0.0004131823079660535\n",
      "Iteration is: 10100 and loss is: 0.00041302788304165006\n",
      "Iteration is: 10101 and loss is: 0.00041287249769084156\n",
      "Iteration is: 10102 and loss is: 0.0004127119027543813\n",
      "Iteration is: 10103 and loss is: 0.0004125527339056134\n",
      "Iteration is: 10104 and loss is: 0.0004123988328501582\n",
      "Iteration is: 10105 and loss is: 0.000412243593018502\n",
      "Iteration is: 10106 and loss is: 0.00041208809125237167\n",
      "Iteration is: 10107 and loss is: 0.00041193090146407485\n",
      "Iteration is: 10108 and loss is: 0.00041177798993885517\n",
      "Iteration is: 10109 and loss is: 0.0004116263589821756\n",
      "Iteration is: 10110 and loss is: 0.0004114708281122148\n",
      "Iteration is: 10111 and loss is: 0.00041131809120997787\n",
      "Iteration is: 10112 and loss is: 0.0004111646267119795\n",
      "Iteration is: 10113 and loss is: 0.00041101413080468774\n",
      "Iteration is: 10114 and loss is: 0.00041086116107180715\n",
      "Iteration is: 10115 and loss is: 0.0004107087734155357\n",
      "Iteration is: 10116 and loss is: 0.0004105562111362815\n",
      "Iteration is: 10117 and loss is: 0.0004104030958842486\n",
      "Iteration is: 10118 and loss is: 0.0004102558013983071\n",
      "Iteration is: 10119 and loss is: 0.0004101040540263057\n",
      "Iteration is: 10120 and loss is: 0.00040995312156155705\n",
      "Iteration is: 10121 and loss is: 0.000409803818911314\n",
      "Iteration is: 10122 and loss is: 0.0004096560296602547\n",
      "Iteration is: 10123 and loss is: 0.0004095059703104198\n",
      "Iteration is: 10124 and loss is: 0.00040935821016319096\n",
      "Iteration is: 10125 and loss is: 0.0004092106828466058\n",
      "Iteration is: 10126 and loss is: 0.00040906271897256374\n",
      "Iteration is: 10127 and loss is: 0.0004089142312295735\n",
      "Iteration is: 10128 and loss is: 0.0004087694105692208\n",
      "Iteration is: 10129 and loss is: 0.00040862063178792596\n",
      "Iteration is: 10130 and loss is: 0.0004084748507011682\n",
      "Iteration is: 10131 and loss is: 0.00040832965169101954\n",
      "Iteration is: 10132 and loss is: 0.0004081816296093166\n",
      "Iteration is: 10133 and loss is: 0.000408036052249372\n",
      "Iteration is: 10134 and loss is: 0.0004078914935234934\n",
      "Iteration is: 10135 and loss is: 0.0004077472258359194\n",
      "Iteration is: 10136 and loss is: 0.00040760065894573927\n",
      "Iteration is: 10137 and loss is: 0.0004074554890394211\n",
      "Iteration is: 10138 and loss is: 0.0004073158488608897\n",
      "Iteration is: 10139 and loss is: 0.0004071682633366436\n",
      "Iteration is: 10140 and loss is: 0.0004070255090482533\n",
      "Iteration is: 10141 and loss is: 0.0004068800189998001\n",
      "Iteration is: 10142 and loss is: 0.0004067375557497144\n",
      "Iteration is: 10143 and loss is: 0.0004065953544341028\n",
      "Iteration is: 10144 and loss is: 0.00040645216358825564\n",
      "Iteration is: 10145 and loss is: 0.00040631109732203186\n",
      "Iteration is: 10146 and loss is: 0.00040616909973323345\n",
      "Iteration is: 10147 and loss is: 0.00040602750959806144\n",
      "Iteration is: 10148 and loss is: 0.00040588737465441227\n",
      "Iteration is: 10149 and loss is: 0.0004057458136230707\n",
      "Iteration is: 10150 and loss is: 0.000405604689149186\n",
      "Iteration is: 10151 and loss is: 0.00040546501986682415\n",
      "Iteration is: 10152 and loss is: 0.0004053266311530024\n",
      "Iteration is: 10153 and loss is: 0.00040518390596844256\n",
      "Iteration is: 10154 and loss is: 0.0004050439747516066\n",
      "Iteration is: 10155 and loss is: 0.0004049047129228711\n",
      "Iteration is: 10156 and loss is: 0.0004047667607665062\n",
      "Iteration is: 10157 and loss is: 0.00040462781907990575\n",
      "Iteration is: 10158 and loss is: 0.00040449044900014997\n",
      "Iteration is: 10159 and loss is: 0.0004043511871714145\n",
      "Iteration is: 10160 and loss is: 0.00040421256562694907\n",
      "Iteration is: 10161 and loss is: 0.000404072692617774\n",
      "Iteration is: 10162 and loss is: 0.0004039347404614091\n",
      "Iteration is: 10163 and loss is: 0.0004037977196276188\n",
      "Iteration is: 10164 and loss is: 0.00040366046596318483\n",
      "Iteration is: 10165 and loss is: 0.00040352478390559554\n",
      "Iteration is: 10166 and loss is: 0.00040338802500627935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 10167 and loss is: 0.0004032515862490982\n",
      "Iteration is: 10168 and loss is: 0.0004031181742902845\n",
      "Iteration is: 10169 and loss is: 0.00040297990199178457\n",
      "Iteration is: 10170 and loss is: 0.0004028455005027354\n",
      "Iteration is: 10171 and loss is: 0.0004027095274068415\n",
      "Iteration is: 10172 and loss is: 0.00040257605724036694\n",
      "Iteration is: 10173 and loss is: 0.00040243961848318577\n",
      "Iteration is: 10174 and loss is: 0.0004023039946332574\n",
      "Iteration is: 10175 and loss is: 0.0004021689237561077\n",
      "Iteration is: 10176 and loss is: 0.00040203501703217626\n",
      "Iteration is: 10177 and loss is: 0.0004019000334665179\n",
      "Iteration is: 10178 and loss is: 0.0004017661849502474\n",
      "Iteration is: 10179 and loss is: 0.00040163181256502867\n",
      "Iteration is: 10180 and loss is: 0.0004014970036223531\n",
      "Iteration is: 10181 and loss is: 0.0004013647558167577\n",
      "Iteration is: 10182 and loss is: 0.0004012325662188232\n",
      "Iteration is: 10183 and loss is: 0.0004011007258668542\n",
      "Iteration is: 10184 and loss is: 0.0004009678668808192\n",
      "Iteration is: 10185 and loss is: 0.0004008340765722096\n",
      "Iteration is: 10186 and loss is: 0.00040070360410027206\n",
      "Iteration is: 10187 and loss is: 0.0004005702503491193\n",
      "Iteration is: 10188 and loss is: 0.0004004373331554234\n",
      "Iteration is: 10189 and loss is: 0.0004003057547379285\n",
      "Iteration is: 10190 and loss is: 0.0004001728957518935\n",
      "Iteration is: 10191 and loss is: 0.00040004312177188694\n",
      "Iteration is: 10192 and loss is: 0.00039991174708120525\n",
      "Iteration is: 10193 and loss is: 0.00039977888809517026\n",
      "Iteration is: 10194 and loss is: 0.00039965013274922967\n",
      "Iteration is: 10195 and loss is: 0.0003995206789113581\n",
      "Iteration is: 10196 and loss is: 0.00039939014823175967\n",
      "Iteration is: 10197 and loss is: 0.00039925926830619574\n",
      "Iteration is: 10198 and loss is: 0.00039912995998747647\n",
      "Iteration is: 10199 and loss is: 0.00039899919647723436\n",
      "Iteration is: 10200 and loss is: 0.00039886924787424505\n",
      "Iteration is: 10201 and loss is: 0.0003987410746049136\n",
      "Iteration is: 10202 and loss is: 0.000398611300624907\n",
      "Iteration is: 10203 and loss is: 0.0003984810318797827\n",
      "Iteration is: 10204 and loss is: 0.00039835507050156593\n",
      "Iteration is: 10205 and loss is: 0.0003982251510024071\n",
      "Iteration is: 10206 and loss is: 0.00039809587178751826\n",
      "Iteration is: 10207 and loss is: 0.00039796909550204873\n",
      "Iteration is: 10208 and loss is: 0.0003978415043093264\n",
      "Iteration is: 10209 and loss is: 0.0003977125743404031\n",
      "Iteration is: 10210 and loss is: 0.0003975835279561579\n",
      "Iteration is: 10211 and loss is: 0.00039745410322211683\n",
      "Iteration is: 10212 and loss is: 0.0003973288694396615\n",
      "Iteration is: 10213 and loss is: 0.0003972009290009737\n",
      "Iteration is: 10214 and loss is: 0.00039707537507638335\n",
      "Iteration is: 10215 and loss is: 0.00039694886072538793\n",
      "Iteration is: 10216 and loss is: 0.0003968197270296514\n",
      "Iteration is: 10217 and loss is: 0.00039669309626333416\n",
      "Iteration is: 10218 and loss is: 0.0003965698415413499\n",
      "Iteration is: 10219 and loss is: 0.00039644172647967935\n",
      "Iteration is: 10220 and loss is: 0.000396314833778888\n",
      "Iteration is: 10221 and loss is: 0.0003961892216466367\n",
      "Iteration is: 10222 and loss is: 0.00039606294012628496\n",
      "Iteration is: 10223 and loss is: 0.00039593857945874333\n",
      "Iteration is: 10224 and loss is: 0.00039581116288900375\n",
      "Iteration is: 10225 and loss is: 0.0003956858708988875\n",
      "Iteration is: 10226 and loss is: 0.00039556267438456416\n",
      "Iteration is: 10227 and loss is: 0.00039543677121400833\n",
      "Iteration is: 10228 and loss is: 0.0003953133709728718\n",
      "Iteration is: 10229 and loss is: 0.0003951885737478733\n",
      "Iteration is: 10230 and loss is: 0.00039506435859948397\n",
      "Iteration is: 10231 and loss is: 0.00039493918302468956\n",
      "Iteration is: 10232 and loss is: 0.0003948142984881997\n",
      "Iteration is: 10233 and loss is: 0.0003946891811210662\n",
      "Iteration is: 10234 and loss is: 0.00039456458762288094\n",
      "Iteration is: 10235 and loss is: 0.0003944417694583535\n",
      "Iteration is: 10236 and loss is: 0.0003943186020478606\n",
      "Iteration is: 10237 and loss is: 0.0003941960749216378\n",
      "Iteration is: 10238 and loss is: 0.0003940727619919926\n",
      "Iteration is: 10239 and loss is: 0.00039394947816617787\n",
      "Iteration is: 10240 and loss is: 0.00039382808608934283\n",
      "Iteration is: 10241 and loss is: 0.00039370113518089056\n",
      "Iteration is: 10242 and loss is: 0.00039357980131171644\n",
      "Iteration is: 10243 and loss is: 0.00039345669210888445\n",
      "Iteration is: 10244 and loss is: 0.0003933338448405266\n",
      "Iteration is: 10245 and loss is: 0.00039321219082921743\n",
      "Iteration is: 10246 and loss is: 0.00039308835403062403\n",
      "Iteration is: 10247 and loss is: 0.00039296632166951895\n",
      "Iteration is: 10248 and loss is: 0.0003928456862922758\n",
      "Iteration is: 10249 and loss is: 0.00039272435242310166\n",
      "Iteration is: 10250 and loss is: 0.00039260246558114886\n",
      "Iteration is: 10251 and loss is: 0.000392481277231127\n",
      "Iteration is: 10252 and loss is: 0.00039235898293554783\n",
      "Iteration is: 10253 and loss is: 0.00039223788189701736\n",
      "Iteration is: 10254 and loss is: 0.0003921171010006219\n",
      "Iteration is: 10255 and loss is: 0.0003919950977433473\n",
      "Iteration is: 10256 and loss is: 0.0003918742877431214\n",
      "Iteration is: 10257 and loss is: 0.0003917542635463178\n",
      "Iteration is: 10258 and loss is: 0.00039163377368822694\n",
      "Iteration is: 10259 and loss is: 0.0003915119741577655\n",
      "Iteration is: 10260 and loss is: 0.0003913920954801142\n",
      "Iteration is: 10261 and loss is: 0.000391271518310532\n",
      "Iteration is: 10262 and loss is: 0.0003911511739715934\n",
      "Iteration is: 10263 and loss is: 0.0003910290834028274\n",
      "Iteration is: 10264 and loss is: 0.00039090937934815884\n",
      "Iteration is: 10265 and loss is: 0.0003907903446815908\n",
      "Iteration is: 10266 and loss is: 0.00039067023317329586\n",
      "Iteration is: 10267 and loss is: 0.00039055279921740294\n",
      "Iteration is: 10268 and loss is: 0.0003904325421899557\n",
      "Iteration is: 10269 and loss is: 0.0003903136239387095\n",
      "Iteration is: 10270 and loss is: 0.0003901947638951242\n",
      "Iteration is: 10271 and loss is: 0.0003900739538948983\n",
      "Iteration is: 10272 and loss is: 0.0003899578587152064\n",
      "Iteration is: 10273 and loss is: 0.0003898364957422018\n",
      "Iteration is: 10274 and loss is: 0.0003897189744748175\n",
      "Iteration is: 10275 and loss is: 0.0003896010748576373\n",
      "Iteration is: 10276 and loss is: 0.00038948305882513523\n",
      "Iteration is: 10277 and loss is: 0.0003893636167049408\n",
      "Iteration is: 10278 and loss is: 0.00038924632826820016\n",
      "Iteration is: 10279 and loss is: 0.000389129389077425\n",
      "Iteration is: 10280 and loss is: 0.00038900828803889453\n",
      "Iteration is: 10281 and loss is: 0.0003888940846081823\n",
      "Iteration is: 10282 and loss is: 0.00038877487531863153\n",
      "Iteration is: 10283 and loss is: 0.0003886553749907762\n",
      "Iteration is: 10284 and loss is: 0.00038853572914376855\n",
      "Iteration is: 10285 and loss is: 0.0003884205361828208\n",
      "Iteration is: 10286 and loss is: 0.00038830243283882737\n",
      "Iteration is: 10287 and loss is: 0.00038818776374682784\n",
      "Iteration is: 10288 and loss is: 0.0003880687872879207\n",
      "Iteration is: 10289 and loss is: 0.0003879508003592491\n",
      "Iteration is: 10290 and loss is: 0.00038783473428338766\n",
      "Iteration is: 10291 and loss is: 0.00038771863910369575\n",
      "Iteration is: 10292 and loss is: 0.00038760050665587187\n",
      "Iteration is: 10293 and loss is: 0.0003874845278915018\n",
      "Iteration is: 10294 and loss is: 0.00038736831629648805\n",
      "Iteration is: 10295 and loss is: 0.00038725309423170984\n",
      "Iteration is: 10296 and loss is: 0.0003871336521115154\n",
      "Iteration is: 10297 and loss is: 0.000387020583730191\n",
      "Iteration is: 10298 and loss is: 0.00038690370274707675\n",
      "Iteration is: 10299 and loss is: 0.0003867840860038996\n",
      "Iteration is: 10300 and loss is: 0.0003866712504532188\n",
      "Iteration is: 10301 and loss is: 0.0003865557664539665\n",
      "Iteration is: 10302 and loss is: 0.0003864387981593609\n",
      "Iteration is: 10303 and loss is: 0.0003863240999635309\n",
      "Iteration is: 10304 and loss is: 0.000386208324925974\n",
      "Iteration is: 10305 and loss is: 0.0003860941797029227\n",
      "Iteration is: 10306 and loss is: 0.00038597924867644906\n",
      "Iteration is: 10307 and loss is: 0.00038586207665503025\n",
      "Iteration is: 10308 and loss is: 0.0003857463598251343\n",
      "Iteration is: 10309 and loss is: 0.0003856340772472322\n",
      "Iteration is: 10310 and loss is: 0.0003855152754113078\n",
      "Iteration is: 10311 and loss is: 0.0003854024107567966\n",
      "Iteration is: 10312 and loss is: 0.00038528896402567625\n",
      "Iteration is: 10313 and loss is: 0.00038517353823408484\n",
      "Iteration is: 10314 and loss is: 0.0003850613720715046\n",
      "Iteration is: 10315 and loss is: 0.00038494571344926953\n",
      "Iteration is: 10316 and loss is: 0.00038483133539557457\n",
      "Iteration is: 10317 and loss is: 0.0003847152693197131\n",
      "Iteration is: 10318 and loss is: 0.0003846011240966618\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 10319 and loss is: 0.0003844881721306592\n",
      "Iteration is: 10320 and loss is: 0.0003843730373773724\n",
      "Iteration is: 10321 and loss is: 0.0003842609585262835\n",
      "Iteration is: 10322 and loss is: 0.00038414739537984133\n",
      "Iteration is: 10323 and loss is: 0.0003840315039269626\n",
      "Iteration is: 10324 and loss is: 0.00038391846464946866\n",
      "Iteration is: 10325 and loss is: 0.0003838060365524143\n",
      "Iteration is: 10326 and loss is: 0.00038369273534044623\n",
      "Iteration is: 10327 and loss is: 0.00038357952143996954\n",
      "Iteration is: 10328 and loss is: 0.0003834661329165101\n",
      "Iteration is: 10329 and loss is: 0.00038335309363901615\n",
      "Iteration is: 10330 and loss is: 0.0003832421498373151\n",
      "Iteration is: 10331 and loss is: 0.0003831288777291775\n",
      "Iteration is: 10332 and loss is: 0.0003830165369436145\n",
      "Iteration is: 10333 and loss is: 0.0003829069319181144\n",
      "Iteration is: 10334 and loss is: 0.00038279625005088747\n",
      "Iteration is: 10335 and loss is: 0.00038268661592155695\n",
      "Iteration is: 10336 and loss is: 0.0003825783496722579\n",
      "Iteration is: 10337 and loss is: 0.00038247439078986645\n",
      "Iteration is: 10338 and loss is: 0.00038237235276028514\n",
      "Iteration is: 10339 and loss is: 0.00038227770710363984\n",
      "Iteration is: 10340 and loss is: 0.00038218958070501685\n",
      "Iteration is: 10341 and loss is: 0.0003821140853688121\n",
      "Iteration is: 10342 and loss is: 0.0003820571000687778\n",
      "Iteration is: 10343 and loss is: 0.0003820290439762175\n",
      "Iteration is: 10344 and loss is: 0.00038204353768378496\n",
      "Iteration is: 10345 and loss is: 0.00038213006337173283\n",
      "Iteration is: 10346 and loss is: 0.0003823210718110204\n",
      "Iteration is: 10347 and loss is: 0.0003826877800747752\n",
      "Iteration is: 10348 and loss is: 0.00038333062548190355\n",
      "Iteration is: 10349 and loss is: 0.0003844141319859773\n",
      "Iteration is: 10350 and loss is: 0.0003862184821628034\n",
      "Iteration is: 10351 and loss is: 0.00038919862709008157\n",
      "Iteration is: 10352 and loss is: 0.00039409109740518034\n",
      "Iteration is: 10353 and loss is: 0.00040217727655544877\n",
      "Iteration is: 10354 and loss is: 0.00041546052671037614\n",
      "Iteration is: 10355 and loss is: 0.00043750792974606156\n",
      "Iteration is: 10356 and loss is: 0.00047370861284434795\n",
      "Iteration is: 10357 and loss is: 0.0005339094204828143\n",
      "Iteration is: 10358 and loss is: 0.0006312388577498496\n",
      "Iteration is: 10359 and loss is: 0.0007901659118942916\n",
      "Iteration is: 10360 and loss is: 0.0010302963200956583\n",
      "Iteration is: 10361 and loss is: 0.001389760640449822\n",
      "Iteration is: 10362 and loss is: 0.0018001479329541326\n",
      "Iteration is: 10363 and loss is: 0.0021709417924284935\n",
      "Iteration is: 10364 and loss is: 0.002036652760580182\n",
      "Iteration is: 10365 and loss is: 0.0012245390098541975\n",
      "Iteration is: 10366 and loss is: 0.00048240594333037734\n",
      "Iteration is: 10367 and loss is: 0.000617348006926477\n",
      "Iteration is: 10368 and loss is: 0.0010722645092755556\n",
      "Iteration is: 10369 and loss is: 0.0008027796284295619\n",
      "Iteration is: 10370 and loss is: 0.00045347571722231805\n",
      "Iteration is: 10371 and loss is: 0.0007284221355803311\n",
      "Iteration is: 10372 and loss is: 0.0007475552847608924\n",
      "Iteration is: 10373 and loss is: 0.00047971977619454265\n",
      "Iteration is: 10374 and loss is: 0.0006339414976537228\n",
      "Iteration is: 10375 and loss is: 0.0006572821876034141\n",
      "Iteration is: 10376 and loss is: 0.0004574330523610115\n",
      "Iteration is: 10377 and loss is: 0.000556682120077312\n",
      "Iteration is: 10378 and loss is: 0.0005940746632404625\n",
      "Iteration is: 10379 and loss is: 0.0004460196942090988\n",
      "Iteration is: 10380 and loss is: 0.00047294923570007086\n",
      "Iteration is: 10381 and loss is: 0.0005578089039772749\n",
      "Iteration is: 10382 and loss is: 0.00048149589565582573\n",
      "Iteration is: 10383 and loss is: 0.0004173596389591694\n",
      "Iteration is: 10384 and loss is: 0.00048654002603143454\n",
      "Iteration is: 10385 and loss is: 0.0005266007501631975\n",
      "Iteration is: 10386 and loss is: 0.0004607994924299419\n",
      "Iteration is: 10387 and loss is: 0.00041999469976872206\n",
      "Iteration is: 10388 and loss is: 0.00046277648652903736\n",
      "Iteration is: 10389 and loss is: 0.0004903723602183163\n",
      "Iteration is: 10390 and loss is: 0.00044875568710267544\n",
      "Iteration is: 10391 and loss is: 0.0004143164260312915\n",
      "Iteration is: 10392 and loss is: 0.0004380934697110206\n",
      "Iteration is: 10393 and loss is: 0.0004560271045193076\n",
      "Iteration is: 10394 and loss is: 0.0004272395744919777\n",
      "Iteration is: 10395 and loss is: 0.0004116943455301225\n",
      "Iteration is: 10396 and loss is: 0.0004329054499976337\n",
      "Iteration is: 10397 and loss is: 0.00043588364496827126\n",
      "Iteration is: 10398 and loss is: 0.00041512426105327904\n",
      "Iteration is: 10399 and loss is: 0.00041555814095772803\n",
      "Iteration is: 10400 and loss is: 0.00042961110011674464\n",
      "Iteration is: 10401 and loss is: 0.0004218141548335552\n",
      "Iteration is: 10402 and loss is: 0.0004096698248758912\n",
      "Iteration is: 10403 and loss is: 0.0004160557291470468\n",
      "Iteration is: 10404 and loss is: 0.00042162317549809813\n",
      "Iteration is: 10405 and loss is: 0.00041241126018576324\n",
      "Iteration is: 10406 and loss is: 0.0004072064475622028\n",
      "Iteration is: 10407 and loss is: 0.0004135159542784095\n",
      "Iteration is: 10408 and loss is: 0.0004157465591561049\n",
      "Iteration is: 10409 and loss is: 0.000409265689086169\n",
      "Iteration is: 10410 and loss is: 0.0004064304812345654\n",
      "Iteration is: 10411 and loss is: 0.0004106466076336801\n",
      "Iteration is: 10412 and loss is: 0.0004120356170460582\n",
      "Iteration is: 10413 and loss is: 0.00040751011692918837\n",
      "Iteration is: 10414 and loss is: 0.0004052139993291348\n",
      "Iteration is: 10415 and loss is: 0.0004076790064573288\n",
      "Iteration is: 10416 and loss is: 0.00040861923480406404\n",
      "Iteration is: 10417 and loss is: 0.000405709957703948\n",
      "Iteration is: 10418 and loss is: 0.00040417767013423145\n",
      "Iteration is: 10419 and loss is: 0.000405805476475507\n",
      "Iteration is: 10420 and loss is: 0.00040640265797264874\n",
      "Iteration is: 10421 and loss is: 0.0004044569213874638\n",
      "Iteration is: 10422 and loss is: 0.00040345091838389635\n",
      "Iteration is: 10423 and loss is: 0.0004044777888339013\n",
      "Iteration is: 10424 and loss is: 0.0004046924295835197\n",
      "Iteration is: 10425 and loss is: 0.000403307203669101\n",
      "Iteration is: 10426 and loss is: 0.0004025737871415913\n",
      "Iteration is: 10427 and loss is: 0.00040319329127669334\n",
      "Iteration is: 10428 and loss is: 0.00040335406083613634\n",
      "Iteration is: 10429 and loss is: 0.00040243612602353096\n",
      "Iteration is: 10430 and loss is: 0.00040186080150306225\n",
      "Iteration is: 10431 and loss is: 0.0004021783825010061\n",
      "Iteration is: 10432 and loss is: 0.0004023026558570564\n",
      "Iteration is: 10433 and loss is: 0.00040168670238927007\n",
      "Iteration is: 10434 and loss is: 0.0004011542478110641\n",
      "Iteration is: 10435 and loss is: 0.00040123623330146074\n",
      "Iteration is: 10436 and loss is: 0.00040132261347025633\n",
      "Iteration is: 10437 and loss is: 0.00040093250572681427\n",
      "Iteration is: 10438 and loss is: 0.0004004947259090841\n",
      "Iteration is: 10439 and loss is: 0.00040045991772785783\n",
      "Iteration is: 10440 and loss is: 0.0004005140799563378\n",
      "Iteration is: 10441 and loss is: 0.0004002679488621652\n",
      "Iteration is: 10442 and loss is: 0.00039990892400965095\n",
      "Iteration is: 10443 and loss is: 0.00039978354470804334\n",
      "Iteration is: 10444 and loss is: 0.00039977574488148093\n",
      "Iteration is: 10445 and loss is: 0.0003996000159531832\n",
      "Iteration is: 10446 and loss is: 0.00039930533966980875\n",
      "Iteration is: 10447 and loss is: 0.00039913749787956476\n",
      "Iteration is: 10448 and loss is: 0.0003990901750512421\n",
      "Iteration is: 10449 and loss is: 0.0003989723918493837\n",
      "Iteration is: 10450 and loss is: 0.0003987432864960283\n",
      "Iteration is: 10451 and loss is: 0.0003985628136433661\n",
      "Iteration is: 10452 and loss is: 0.00039847721927799284\n",
      "Iteration is: 10453 and loss is: 0.0003983761998824775\n",
      "Iteration is: 10454 and loss is: 0.00039819302037358284\n",
      "Iteration is: 10455 and loss is: 0.00039800541708245873\n",
      "Iteration is: 10456 and loss is: 0.0003978876629844308\n",
      "Iteration is: 10457 and loss is: 0.00039778894279152155\n",
      "Iteration is: 10458 and loss is: 0.0003976387088187039\n",
      "Iteration is: 10459 and loss is: 0.0003974678402300924\n",
      "Iteration is: 10460 and loss is: 0.00039733140147291124\n",
      "Iteration is: 10461 and loss is: 0.00039722968358546495\n",
      "Iteration is: 10462 and loss is: 0.000397101859562099\n",
      "Iteration is: 10463 and loss is: 0.00039694691076874733\n",
      "Iteration is: 10464 and loss is: 0.0003968042437918484\n",
      "Iteration is: 10465 and loss is: 0.00039668683893978596\n",
      "Iteration is: 10466 and loss is: 0.0003965686773881316\n",
      "Iteration is: 10467 and loss is: 0.00039643084164708853\n",
      "Iteration is: 10468 and loss is: 0.0003962889313697815\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 10469 and loss is: 0.00039616195135749876\n",
      "Iteration is: 10470 and loss is: 0.0003960466419812292\n",
      "Iteration is: 10471 and loss is: 0.00039592100074514747\n",
      "Iteration is: 10472 and loss is: 0.0003957847657147795\n",
      "Iteration is: 10473 and loss is: 0.0003956556611228734\n",
      "Iteration is: 10474 and loss is: 0.0003955361316911876\n",
      "Iteration is: 10475 and loss is: 0.0003954149433411658\n",
      "Iteration is: 10476 and loss is: 0.0003952884580940008\n",
      "Iteration is: 10477 and loss is: 0.00039516075048595667\n",
      "Iteration is: 10478 and loss is: 0.00039503700099885464\n",
      "Iteration is: 10479 and loss is: 0.0003949191013816744\n",
      "Iteration is: 10480 and loss is: 0.0003948005323763937\n",
      "Iteration is: 10481 and loss is: 0.00039467253373004496\n",
      "Iteration is: 10482 and loss is: 0.0003945491916965693\n",
      "Iteration is: 10483 and loss is: 0.00039443137939088047\n",
      "Iteration is: 10484 and loss is: 0.00039431144250556827\n",
      "Iteration is: 10485 and loss is: 0.0003941938339266926\n",
      "Iteration is: 10486 and loss is: 0.00039406941505149007\n",
      "Iteration is: 10487 and loss is: 0.00039394930354319513\n",
      "Iteration is: 10488 and loss is: 0.0003938322770409286\n",
      "Iteration is: 10489 and loss is: 0.00039371527964249253\n",
      "Iteration is: 10490 and loss is: 0.0003935962449759245\n",
      "Iteration is: 10491 and loss is: 0.00039347767597064376\n",
      "Iteration is: 10492 and loss is: 0.0003933597472496331\n",
      "Iteration is: 10493 and loss is: 0.00039324245881289244\n",
      "Iteration is: 10494 and loss is: 0.000393126392737031\n",
      "Iteration is: 10495 and loss is: 0.00039300983189605176\n",
      "Iteration is: 10496 and loss is: 0.000392890942748636\n",
      "Iteration is: 10497 and loss is: 0.00039277703035622835\n",
      "Iteration is: 10498 and loss is: 0.00039266145904548466\n",
      "Iteration is: 10499 and loss is: 0.0003925476339645684\n",
      "Iteration is: 10500 and loss is: 0.0003924307820852846\n",
      "Iteration is: 10501 and loss is: 0.0003923176846001297\n",
      "Iteration is: 10502 and loss is: 0.00039220217149704695\n",
      "Iteration is: 10503 and loss is: 0.0003920884628314525\n",
      "Iteration is: 10504 and loss is: 0.0003919736482203007\n",
      "Iteration is: 10505 and loss is: 0.0003918587462976575\n",
      "Iteration is: 10506 and loss is: 0.0003917456488125026\n",
      "Iteration is: 10507 and loss is: 0.000391633395338431\n",
      "Iteration is: 10508 and loss is: 0.0003915189881809056\n",
      "Iteration is: 10509 and loss is: 0.00039140734588727355\n",
      "Iteration is: 10510 and loss is: 0.00039129401557147503\n",
      "Iteration is: 10511 and loss is: 0.00039118455606512725\n",
      "Iteration is: 10512 and loss is: 0.0003910712548531592\n",
      "Iteration is: 10513 and loss is: 0.0003909576334990561\n",
      "Iteration is: 10514 and loss is: 0.0003908479120582342\n",
      "Iteration is: 10515 and loss is: 0.0003907368518412113\n",
      "Iteration is: 10516 and loss is: 0.00039062544237822294\n",
      "Iteration is: 10517 and loss is: 0.00039051382918842137\n",
      "Iteration is: 10518 and loss is: 0.000390402739867568\n",
      "Iteration is: 10519 and loss is: 0.00039029354229569435\n",
      "Iteration is: 10520 and loss is: 0.00039018262759782374\n",
      "Iteration is: 10521 and loss is: 0.00039007473969832063\n",
      "Iteration is: 10522 and loss is: 0.0003899608855135739\n",
      "Iteration is: 10523 and loss is: 0.0003898523864336312\n",
      "Iteration is: 10524 and loss is: 0.00038974243216216564\n",
      "Iteration is: 10525 and loss is: 0.0003896344278473407\n",
      "Iteration is: 10526 and loss is: 0.0003895237168762833\n",
      "Iteration is: 10527 and loss is: 0.00038941524690017104\n",
      "Iteration is: 10528 and loss is: 0.00038930674782022834\n",
      "Iteration is: 10529 and loss is: 0.0003891982196364552\n",
      "Iteration is: 10530 and loss is: 0.0003890884399879724\n",
      "Iteration is: 10531 and loss is: 0.00038897997001186013\n",
      "Iteration is: 10532 and loss is: 0.0003888722276315093\n",
      "Iteration is: 10533 and loss is: 0.0003887646016664803\n",
      "Iteration is: 10534 and loss is: 0.00038865674287080765\n",
      "Iteration is: 10535 and loss is: 0.0003885496989823878\n",
      "Iteration is: 10536 and loss is: 0.0003884409088641405\n",
      "Iteration is: 10537 and loss is: 0.00038833290454931557\n",
      "Iteration is: 10538 and loss is: 0.00038822548231109977\n",
      "Iteration is: 10539 and loss is: 0.00038811907870694995\n",
      "Iteration is: 10540 and loss is: 0.00038801110349595547\n",
      "Iteration is: 10541 and loss is: 0.00038790376856923103\n",
      "Iteration is: 10542 and loss is: 0.00038779707392677665\n",
      "Iteration is: 10543 and loss is: 0.00038769247476011515\n",
      "Iteration is: 10544 and loss is: 0.00038758578011766076\n",
      "Iteration is: 10545 and loss is: 0.00038747861981391907\n",
      "Iteration is: 10546 and loss is: 0.0003873725072480738\n",
      "Iteration is: 10547 and loss is: 0.0003872664528898895\n",
      "Iteration is: 10548 and loss is: 0.0003871593507938087\n",
      "Iteration is: 10549 and loss is: 0.00038705364568158984\n",
      "Iteration is: 10550 and loss is: 0.0003869487263727933\n",
      "Iteration is: 10551 and loss is: 0.0003868433414027095\n",
      "Iteration is: 10552 and loss is: 0.0003867371706292033\n",
      "Iteration is: 10553 and loss is: 0.00038663059240207076\n",
      "Iteration is: 10554 and loss is: 0.00038652517832815647\n",
      "Iteration is: 10555 and loss is: 0.0003864206373691559\n",
      "Iteration is: 10556 and loss is: 0.00038631539791822433\n",
      "Iteration is: 10557 and loss is: 0.00038621266139671206\n",
      "Iteration is: 10558 and loss is: 0.0003861068107653409\n",
      "Iteration is: 10559 and loss is: 0.00038600125117227435\n",
      "Iteration is: 10560 and loss is: 0.0003858986310660839\n",
      "Iteration is: 10561 and loss is: 0.0003857935080304742\n",
      "Iteration is: 10562 and loss is: 0.00038568879244849086\n",
      "Iteration is: 10563 and loss is: 0.00038558320375159383\n",
      "Iteration is: 10564 and loss is: 0.00038548134034499526\n",
      "Iteration is: 10565 and loss is: 0.000385376566555351\n",
      "Iteration is: 10566 and loss is: 0.0003852715599350631\n",
      "Iteration is: 10567 and loss is: 0.00038516888162121177\n",
      "Iteration is: 10568 and loss is: 0.0003850640496239066\n",
      "Iteration is: 10569 and loss is: 0.0003849612839985639\n",
      "Iteration is: 10570 and loss is: 0.00038485703407786787\n",
      "Iteration is: 10571 and loss is: 0.0003847539483103901\n",
      "Iteration is: 10572 and loss is: 0.00038464870885945857\n",
      "Iteration is: 10573 and loss is: 0.00038454795139841735\n",
      "Iteration is: 10574 and loss is: 0.00038444390520453453\n",
      "Iteration is: 10575 and loss is: 0.00038433860754594207\n",
      "Iteration is: 10576 and loss is: 0.0003842362202703953\n",
      "Iteration is: 10577 and loss is: 0.0003841347643174231\n",
      "Iteration is: 10578 and loss is: 0.0003840313293039799\n",
      "Iteration is: 10579 and loss is: 0.00038392707938328385\n",
      "Iteration is: 10580 and loss is: 0.0003838252741843462\n",
      "Iteration is: 10581 and loss is: 0.0003837249823845923\n",
      "Iteration is: 10582 and loss is: 0.0003836206451524049\n",
      "Iteration is: 10583 and loss is: 0.00038351837429217994\n",
      "Iteration is: 10584 and loss is: 0.00038341476465575397\n",
      "Iteration is: 10585 and loss is: 0.0003833139198832214\n",
      "Iteration is: 10586 and loss is: 0.00038321141619235277\n",
      "Iteration is: 10587 and loss is: 0.0003831105714198202\n",
      "Iteration is: 10588 and loss is: 0.0003830054774880409\n",
      "Iteration is: 10589 and loss is: 0.0003829020424745977\n",
      "Iteration is: 10590 and loss is: 0.00038280204171314836\n",
      "Iteration is: 10591 and loss is: 0.00038270000368356705\n",
      "Iteration is: 10592 and loss is: 0.0003825976455118507\n",
      "Iteration is: 10593 and loss is: 0.00038249680073931813\n",
      "Iteration is: 10594 and loss is: 0.00038239621790125966\n",
      "Iteration is: 10595 and loss is: 0.00038229365600273013\n",
      "Iteration is: 10596 and loss is: 0.0003821915597654879\n",
      "Iteration is: 10597 and loss is: 0.000382091267965734\n",
      "Iteration is: 10598 and loss is: 0.00038198695983737707\n",
      "Iteration is: 10599 and loss is: 0.0003818872501142323\n",
      "Iteration is: 10600 and loss is: 0.0003817848046310246\n",
      "Iteration is: 10601 and loss is: 0.0003816850367002189\n",
      "Iteration is: 10602 and loss is: 0.00038158599636517465\n",
      "Iteration is: 10603 and loss is: 0.00038148084422573447\n",
      "Iteration is: 10604 and loss is: 0.0003813806688413024\n",
      "Iteration is: 10605 and loss is: 0.0003812813083641231\n",
      "Iteration is: 10606 and loss is: 0.00038118078373372555\n",
      "Iteration is: 10607 and loss is: 0.0003810795897152275\n",
      "Iteration is: 10608 and loss is: 0.00038097609649412334\n",
      "Iteration is: 10609 and loss is: 0.0003808768233284354\n",
      "Iteration is: 10610 and loss is: 0.00038077638600952923\n",
      "Iteration is: 10611 and loss is: 0.000380675308406353\n",
      "Iteration is: 10612 and loss is: 0.0003805748710874468\n",
      "Iteration is: 10613 and loss is: 0.0003804751322604716\n",
      "Iteration is: 10614 and loss is: 0.0003803741419687867\n",
      "Iteration is: 10615 and loss is: 0.0003802734718192369\n",
      "Iteration is: 10616 and loss is: 0.0003801722778007388\n",
      "Iteration is: 10617 and loss is: 0.0003800744889304042\n",
      "Iteration is: 10618 and loss is: 0.0003799763508141041\n",
      "Iteration is: 10619 and loss is: 0.00037987460382282734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 10620 and loss is: 0.0003797733224928379\n",
      "Iteration is: 10621 and loss is: 0.00037967186653986573\n",
      "Iteration is: 10622 and loss is: 0.00037957291351631284\n",
      "Iteration is: 10623 and loss is: 0.00037947308737784624\n",
      "Iteration is: 10624 and loss is: 0.0003793736104853451\n",
      "Iteration is: 10625 and loss is: 0.0003792734642047435\n",
      "Iteration is: 10626 and loss is: 0.0003791729686781764\n",
      "Iteration is: 10627 and loss is: 0.00037907512160018086\n",
      "Iteration is: 10628 and loss is: 0.0003789719194173813\n",
      "Iteration is: 10629 and loss is: 0.0003788729663938284\n",
      "Iteration is: 10630 and loss is: 0.0003787730820477009\n",
      "Iteration is: 10631 and loss is: 0.00037867692299187183\n",
      "Iteration is: 10632 and loss is: 0.00037857628194615245\n",
      "Iteration is: 10633 and loss is: 0.00037847424391657114\n",
      "Iteration is: 10634 and loss is: 0.00037837581476196647\n",
      "Iteration is: 10635 and loss is: 0.00037827587220817804\n",
      "Iteration is: 10636 and loss is: 0.00037817886914126575\n",
      "Iteration is: 10637 and loss is: 0.0003780791303142905\n",
      "Iteration is: 10638 and loss is: 0.0003779781691264361\n",
      "Iteration is: 10639 and loss is: 0.00037788006011396646\n",
      "Iteration is: 10640 and loss is: 0.0003777800884563476\n",
      "Iteration is: 10641 and loss is: 0.00037768023321405053\n",
      "Iteration is: 10642 and loss is: 0.0003775832010433078\n",
      "Iteration is: 10643 and loss is: 0.0003774816286750138\n",
      "Iteration is: 10644 and loss is: 0.0003773847420234233\n",
      "Iteration is: 10645 and loss is: 0.0003772829077206552\n",
      "Iteration is: 10646 and loss is: 0.00037718506064265966\n",
      "Iteration is: 10647 and loss is: 0.0003770861658267677\n",
      "Iteration is: 10648 and loss is: 0.00037698750384151936\n",
      "Iteration is: 10649 and loss is: 0.000376889540348202\n",
      "Iteration is: 10650 and loss is: 0.0003767893649637699\n",
      "Iteration is: 10651 and loss is: 0.00037669378798455\n",
      "Iteration is: 10652 and loss is: 0.00037659419467672706\n",
      "Iteration is: 10653 and loss is: 0.0003764941939152777\n",
      "Iteration is: 10654 and loss is: 0.00037639663787558675\n",
      "Iteration is: 10655 and loss is: 0.0003762965206988156\n",
      "Iteration is: 10656 and loss is: 0.0003761990228667855\n",
      "Iteration is: 10657 and loss is: 0.0003761007101275027\n",
      "Iteration is: 10658 and loss is: 0.00037600003997795284\n",
      "Iteration is: 10659 and loss is: 0.00037590073770843446\n",
      "Iteration is: 10660 and loss is: 0.0003758046659640968\n",
      "Iteration is: 10661 and loss is: 0.00037570693530142307\n",
      "Iteration is: 10662 and loss is: 0.00037560772034339607\n",
      "Iteration is: 10663 and loss is: 0.00037551080458797514\n",
      "Iteration is: 10664 and loss is: 0.0003754108620341867\n",
      "Iteration is: 10665 and loss is: 0.00037531170528382063\n",
      "Iteration is: 10666 and loss is: 0.00037521455669775605\n",
      "Iteration is: 10667 and loss is: 0.00037511656410060823\n",
      "Iteration is: 10668 and loss is: 0.00037501909537240863\n",
      "Iteration is: 10669 and loss is: 0.0003749200259335339\n",
      "Iteration is: 10670 and loss is: 0.00037482334300875664\n",
      "Iteration is: 10671 and loss is: 0.000374723895220086\n",
      "Iteration is: 10672 and loss is: 0.0003746274742297828\n",
      "Iteration is: 10673 and loss is: 0.00037452857941389084\n",
      "Iteration is: 10674 and loss is: 0.0003744312562048435\n",
      "Iteration is: 10675 and loss is: 0.00037433276884257793\n",
      "Iteration is: 10676 and loss is: 0.000374234834453091\n",
      "Iteration is: 10677 and loss is: 0.0003741372493095696\n",
      "Iteration is: 10678 and loss is: 0.00037403986789286137\n",
      "Iteration is: 10679 and loss is: 0.00037394146784208715\n",
      "Iteration is: 10680 and loss is: 0.000373844028217718\n",
      "Iteration is: 10681 and loss is: 0.00037374760722741485\n",
      "Iteration is: 10682 and loss is: 0.000373648275854066\n",
      "Iteration is: 10683 and loss is: 0.0003735517675522715\n",
      "Iteration is: 10684 and loss is: 0.0003734540368895978\n",
      "Iteration is: 10685 and loss is: 0.00037335598608478904\n",
      "Iteration is: 10686 and loss is: 0.00037325909943319857\n",
      "Iteration is: 10687 and loss is: 0.00037316104862838984\n",
      "Iteration is: 10688 and loss is: 0.00037306291051208973\n",
      "Iteration is: 10689 and loss is: 0.00037296535447239876\n",
      "Iteration is: 10690 and loss is: 0.0003728679730556905\n",
      "Iteration is: 10691 and loss is: 0.00037276887451298535\n",
      "Iteration is: 10692 and loss is: 0.0003726730355992913\n",
      "Iteration is: 10693 and loss is: 0.0003725747228600085\n",
      "Iteration is: 10694 and loss is: 0.0003724789130501449\n",
      "Iteration is: 10695 and loss is: 0.0003723811823874712\n",
      "Iteration is: 10696 and loss is: 0.00037228415021672845\n",
      "Iteration is: 10697 and loss is: 0.00037218662328086793\n",
      "Iteration is: 10698 and loss is: 0.00037208874709904194\n",
      "Iteration is: 10699 and loss is: 0.00037199148209765553\n",
      "Iteration is: 10700 and loss is: 0.0003718978841789067\n",
      "Iteration is: 10701 and loss is: 0.00037179706851020455\n",
      "Iteration is: 10702 and loss is: 0.0003716999781318009\n",
      "Iteration is: 10703 and loss is: 0.0003716055362019688\n",
      "Iteration is: 10704 and loss is: 0.00037150722346268594\n",
      "Iteration is: 10705 and loss is: 0.0003714100457727909\n",
      "Iteration is: 10706 and loss is: 0.00037131388671696186\n",
      "Iteration is: 10707 and loss is: 0.0003712167090270668\n",
      "Iteration is: 10708 and loss is: 0.0003711195895448327\n",
      "Iteration is: 10709 and loss is: 0.00037102311034686863\n",
      "Iteration is: 10710 and loss is: 0.00037092494312673807\n",
      "Iteration is: 10711 and loss is: 0.00037082761991769075\n",
      "Iteration is: 10712 and loss is: 0.00037073087878525257\n",
      "Iteration is: 10713 and loss is: 0.0003706358256749809\n",
      "Iteration is: 10714 and loss is: 0.00037053838605061173\n",
      "Iteration is: 10715 and loss is: 0.00037044164491817355\n",
      "Iteration is: 10716 and loss is: 0.00037034577690064907\n",
      "Iteration is: 10717 and loss is: 0.00037024810444563627\n",
      "Iteration is: 10718 and loss is: 0.00037015098496340215\n",
      "Iteration is: 10719 and loss is: 0.00037005532067269087\n",
      "Iteration is: 10720 and loss is: 0.0003699572989717126\n",
      "Iteration is: 10721 and loss is: 0.00036986195482313633\n",
      "Iteration is: 10722 and loss is: 0.0003697667270898819\n",
      "Iteration is: 10723 and loss is: 0.0003696693747770041\n",
      "Iteration is: 10724 and loss is: 0.00036957146949134767\n",
      "Iteration is: 10725 and loss is: 0.0003694785700645298\n",
      "Iteration is: 10726 and loss is: 0.0003693804028443992\n",
      "Iteration is: 10727 and loss is: 0.00036928613553754985\n",
      "Iteration is: 10728 and loss is: 0.0003691904421430081\n",
      "Iteration is: 10729 and loss is: 0.0003690960584208369\n",
      "Iteration is: 10730 and loss is: 0.0003690042649395764\n",
      "Iteration is: 10731 and loss is: 0.000368913053534925\n",
      "Iteration is: 10732 and loss is: 0.0003688223077915609\n",
      "Iteration is: 10733 and loss is: 0.00036873691715300083\n",
      "Iteration is: 10734 and loss is: 0.0003686564741656184\n",
      "Iteration is: 10735 and loss is: 0.00036858522798866034\n",
      "Iteration is: 10736 and loss is: 0.0003685260599013418\n",
      "Iteration is: 10737 and loss is: 0.00036848869058303535\n",
      "Iteration is: 10738 and loss is: 0.00036848802119493484\n",
      "Iteration is: 10739 and loss is: 0.0003685368865262717\n",
      "Iteration is: 10740 and loss is: 0.00036867655580863357\n",
      "Iteration is: 10741 and loss is: 0.0003689565055537969\n",
      "Iteration is: 10742 and loss is: 0.00036946951877325773\n",
      "Iteration is: 10743 and loss is: 0.000370361958630383\n",
      "Iteration is: 10744 and loss is: 0.0003718955267686397\n",
      "Iteration is: 10745 and loss is: 0.0003744619316421449\n",
      "Iteration is: 10746 and loss is: 0.0003788335307035595\n",
      "Iteration is: 10747 and loss is: 0.0003860948490910232\n",
      "Iteration is: 10748 and loss is: 0.0003985133080277592\n",
      "Iteration is: 10749 and loss is: 0.000418960495153442\n",
      "Iteration is: 10750 and loss is: 0.0004541891103144735\n",
      "Iteration is: 10751 and loss is: 0.0005106787430122495\n",
      "Iteration is: 10752 and loss is: 0.000607224297709763\n",
      "Iteration is: 10753 and loss is: 0.0007496359758079052\n",
      "Iteration is: 10754 and loss is: 0.0009770216420292854\n",
      "Iteration is: 10755 and loss is: 0.0012302336981520057\n",
      "Iteration is: 10756 and loss is: 0.0015190724516287446\n",
      "Iteration is: 10757 and loss is: 0.001511929789558053\n",
      "Iteration is: 10758 and loss is: 0.0012077575083822012\n",
      "Iteration is: 10759 and loss is: 0.0006296352948993444\n",
      "Iteration is: 10760 and loss is: 0.00042850832687690854\n",
      "Iteration is: 10761 and loss is: 0.0007111188024282455\n",
      "Iteration is: 10762 and loss is: 0.0008536201785318553\n",
      "Iteration is: 10763 and loss is: 0.0006086152861826122\n",
      "Iteration is: 10764 and loss is: 0.00041318510193377733\n",
      "Iteration is: 10765 and loss is: 0.0005587273044511676\n",
      "Iteration is: 10766 and loss is: 0.0007330924272537231\n",
      "Iteration is: 10767 and loss is: 0.0006458749412558973\n",
      "Iteration is: 10768 and loss is: 0.0004600526299327612\n",
      "Iteration is: 10769 and loss is: 0.000385774445021525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 10770 and loss is: 0.0004658034013118595\n",
      "Iteration is: 10771 and loss is: 0.0006064175395295024\n",
      "Iteration is: 10772 and loss is: 0.0006983245257288218\n",
      "Iteration is: 10773 and loss is: 0.0007243663421832025\n",
      "Iteration is: 10774 and loss is: 0.0006718235672451556\n",
      "Iteration is: 10775 and loss is: 0.0005864726263098419\n",
      "Iteration is: 10776 and loss is: 0.0004814717685803771\n",
      "Iteration is: 10777 and loss is: 0.0004036752216052264\n",
      "Iteration is: 10778 and loss is: 0.00037496761069633067\n",
      "Iteration is: 10779 and loss is: 0.0003960503963753581\n",
      "Iteration is: 10780 and loss is: 0.0004401477053761482\n",
      "Iteration is: 10781 and loss is: 0.00046804628800600767\n",
      "Iteration is: 10782 and loss is: 0.00046263361582532525\n",
      "Iteration is: 10783 and loss is: 0.0004285565228201449\n",
      "Iteration is: 10784 and loss is: 0.00039159267907962203\n",
      "Iteration is: 10785 and loss is: 0.00037246046122163534\n",
      "Iteration is: 10786 and loss is: 0.0003775527002289891\n",
      "Iteration is: 10787 and loss is: 0.0003971003752667457\n",
      "Iteration is: 10788 and loss is: 0.00041835970478132367\n",
      "Iteration is: 10789 and loss is: 0.00043392908992245793\n",
      "Iteration is: 10790 and loss is: 0.0004393647541292012\n",
      "Iteration is: 10791 and loss is: 0.00043855042895302176\n",
      "Iteration is: 10792 and loss is: 0.00043141015339642763\n",
      "Iteration is: 10793 and loss is: 0.00042201942414976656\n",
      "Iteration is: 10794 and loss is: 0.00041061732918024063\n",
      "Iteration is: 10795 and loss is: 0.00039969879435375333\n",
      "Iteration is: 10796 and loss is: 0.00038883741945028305\n",
      "Iteration is: 10797 and loss is: 0.0003797504468820989\n",
      "Iteration is: 10798 and loss is: 0.0003727034491021186\n",
      "Iteration is: 10799 and loss is: 0.000368102133506909\n",
      "Iteration is: 10800 and loss is: 0.0003660219954326749\n",
      "Iteration is: 10801 and loss is: 0.00036595785059034824\n",
      "Iteration is: 10802 and loss is: 0.00036732113221660256\n",
      "Iteration is: 10803 and loss is: 0.0003697602660395205\n",
      "Iteration is: 10804 and loss is: 0.0003729147429112345\n",
      "Iteration is: 10805 and loss is: 0.00037659486406482756\n",
      "Iteration is: 10806 and loss is: 0.0003812569775618613\n",
      "Iteration is: 10807 and loss is: 0.00038696418050676584\n",
      "Iteration is: 10808 and loss is: 0.0003947309742216021\n",
      "Iteration is: 10809 and loss is: 0.0004048470873385668\n",
      "Iteration is: 10810 and loss is: 0.0004192869528196752\n",
      "Iteration is: 10811 and loss is: 0.00043787845061160624\n",
      "Iteration is: 10812 and loss is: 0.00046432891394943\n",
      "Iteration is: 10813 and loss is: 0.0004960225196555257\n",
      "Iteration is: 10814 and loss is: 0.000538702355697751\n",
      "Iteration is: 10815 and loss is: 0.0005820824881084263\n",
      "Iteration is: 10816 and loss is: 0.0006333526689559221\n",
      "Iteration is: 10817 and loss is: 0.000665806932374835\n",
      "Iteration is: 10818 and loss is: 0.0006877956329844892\n",
      "Iteration is: 10819 and loss is: 0.0006615216843783855\n",
      "Iteration is: 10820 and loss is: 0.0006072557880543172\n",
      "Iteration is: 10821 and loss is: 0.0005170546937733889\n",
      "Iteration is: 10822 and loss is: 0.00043294805800542235\n",
      "Iteration is: 10823 and loss is: 0.00037929503014311194\n",
      "Iteration is: 10824 and loss is: 0.00037123134825378656\n",
      "Iteration is: 10825 and loss is: 0.00039975944673642516\n",
      "Iteration is: 10826 and loss is: 0.0004448631079867482\n",
      "Iteration is: 10827 and loss is: 0.0004942064988426864\n",
      "Iteration is: 10828 and loss is: 0.0005390170263126493\n",
      "Iteration is: 10829 and loss is: 0.000592568889260292\n",
      "Iteration is: 10830 and loss is: 0.0006500834715552628\n",
      "Iteration is: 10831 and loss is: 0.0007355316774919629\n",
      "Iteration is: 10832 and loss is: 0.0008207024657167494\n",
      "Iteration is: 10833 and loss is: 0.0009207982220686972\n",
      "Iteration is: 10834 and loss is: 0.0009369789622724056\n",
      "Iteration is: 10835 and loss is: 0.0008710677502676845\n",
      "Iteration is: 10836 and loss is: 0.0006619427585974336\n",
      "Iteration is: 10837 and loss is: 0.00045478015090338886\n",
      "Iteration is: 10838 and loss is: 0.000382467289455235\n",
      "Iteration is: 10839 and loss is: 0.00047137890942394733\n",
      "Iteration is: 10840 and loss is: 0.000577932340092957\n",
      "Iteration is: 10841 and loss is: 0.0005549235502257943\n",
      "Iteration is: 10842 and loss is: 0.000447015801910311\n",
      "Iteration is: 10843 and loss is: 0.0003815755480900407\n",
      "Iteration is: 10844 and loss is: 0.00041801814222708344\n",
      "Iteration is: 10845 and loss is: 0.0004888593102805316\n",
      "Iteration is: 10846 and loss is: 0.0005047424929216504\n",
      "Iteration is: 10847 and loss is: 0.0004609361640177667\n",
      "Iteration is: 10848 and loss is: 0.0003996239975094795\n",
      "Iteration is: 10849 and loss is: 0.00036943203303962946\n",
      "Iteration is: 10850 and loss is: 0.000380637648049742\n",
      "Iteration is: 10851 and loss is: 0.00041789805982261896\n",
      "Iteration is: 10852 and loss is: 0.0004651848576031625\n",
      "Iteration is: 10853 and loss is: 0.0005124303861521184\n",
      "Iteration is: 10854 and loss is: 0.0005677802837453783\n",
      "Iteration is: 10855 and loss is: 0.0006271189777180552\n",
      "Iteration is: 10856 and loss is: 0.0007066348334774375\n",
      "Iteration is: 10857 and loss is: 0.0007730266079306602\n",
      "Iteration is: 10858 and loss is: 0.0008248176891356707\n",
      "Iteration is: 10859 and loss is: 0.0007775234989821911\n",
      "Iteration is: 10860 and loss is: 0.0006520189926959574\n",
      "Iteration is: 10861 and loss is: 0.0004759480943903327\n",
      "Iteration is: 10862 and loss is: 0.00037945766234770417\n",
      "Iteration is: 10863 and loss is: 0.0004151752800680697\n",
      "Iteration is: 10864 and loss is: 0.0005069304606877267\n",
      "Iteration is: 10865 and loss is: 0.0005369953578338027\n",
      "Iteration is: 10866 and loss is: 0.00046876503620296717\n",
      "Iteration is: 10867 and loss is: 0.00039191151154227555\n",
      "Iteration is: 10868 and loss is: 0.0003828566404990852\n",
      "Iteration is: 10869 and loss is: 0.0004314779653213918\n",
      "Iteration is: 10870 and loss is: 0.00047372159315273166\n",
      "Iteration is: 10871 and loss is: 0.0004657299432437867\n",
      "Iteration is: 10872 and loss is: 0.0004245454038027674\n",
      "Iteration is: 10873 and loss is: 0.0003825265448540449\n",
      "Iteration is: 10874 and loss is: 0.00036489672493189573\n",
      "Iteration is: 10875 and loss is: 0.00037383270682767034\n",
      "Iteration is: 10876 and loss is: 0.00040062516927719116\n",
      "Iteration is: 10877 and loss is: 0.0004390237736515701\n",
      "Iteration is: 10878 and loss is: 0.00048681246698834\n",
      "Iteration is: 10879 and loss is: 0.0005550251808017492\n",
      "Iteration is: 10880 and loss is: 0.0006459815194830298\n",
      "Iteration is: 10881 and loss is: 0.0007822893676348031\n",
      "Iteration is: 10882 and loss is: 0.0009220826905220747\n",
      "Iteration is: 10883 and loss is: 0.0010548477293923497\n",
      "Iteration is: 10884 and loss is: 0.0010101094376295805\n",
      "Iteration is: 10885 and loss is: 0.0008044001879170537\n",
      "Iteration is: 10886 and loss is: 0.000500379828736186\n",
      "Iteration is: 10887 and loss is: 0.0003871493972837925\n",
      "Iteration is: 10888 and loss is: 0.0005191948730498552\n",
      "Iteration is: 10889 and loss is: 0.0006412372458726168\n",
      "Iteration is: 10890 and loss is: 0.0005700836190953851\n",
      "Iteration is: 10891 and loss is: 0.000416276918258518\n",
      "Iteration is: 10892 and loss is: 0.0003972848935518414\n",
      "Iteration is: 10893 and loss is: 0.0004994302289560437\n",
      "Iteration is: 10894 and loss is: 0.0005607801140286028\n",
      "Iteration is: 10895 and loss is: 0.0005225989734753966\n",
      "Iteration is: 10896 and loss is: 0.00043254875345155597\n",
      "Iteration is: 10897 and loss is: 0.000374262104742229\n",
      "Iteration is: 10898 and loss is: 0.0003757018130272627\n",
      "Iteration is: 10899 and loss is: 0.00042412837501615286\n",
      "Iteration is: 10900 and loss is: 0.0004942434024997056\n",
      "Iteration is: 10901 and loss is: 0.0005650088423863053\n",
      "Iteration is: 10902 and loss is: 0.0006461196462623775\n",
      "Iteration is: 10903 and loss is: 0.0007247062749229372\n",
      "Iteration is: 10904 and loss is: 0.0008169276406988502\n",
      "Iteration is: 10905 and loss is: 0.0008605713373981416\n",
      "Iteration is: 10906 and loss is: 0.0008505797595717013\n",
      "Iteration is: 10907 and loss is: 0.0007047636900097132\n",
      "Iteration is: 10908 and loss is: 0.0005110002239234746\n",
      "Iteration is: 10909 and loss is: 0.0003829071647487581\n",
      "Iteration is: 10910 and loss is: 0.0004109620349481702\n",
      "Iteration is: 10911 and loss is: 0.0005174874095246196\n",
      "Iteration is: 10912 and loss is: 0.0005451637553051114\n",
      "Iteration is: 10913 and loss is: 0.0004665975284297019\n",
      "Iteration is: 10914 and loss is: 0.0003846489707939327\n",
      "Iteration is: 10915 and loss is: 0.0003939545713365078\n",
      "Iteration is: 10916 and loss is: 0.00045757656334899366\n",
      "Iteration is: 10917 and loss is: 0.00048434437485411763\n",
      "Iteration is: 10918 and loss is: 0.000452266976935789\n",
      "Iteration is: 10919 and loss is: 0.000396912160795182\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 10920 and loss is: 0.0003671037848107517\n",
      "Iteration is: 10921 and loss is: 0.0003765872388612479\n",
      "Iteration is: 10922 and loss is: 0.0004104188992641866\n",
      "Iteration is: 10923 and loss is: 0.00044851345592178404\n",
      "Iteration is: 10924 and loss is: 0.00047853472642600536\n",
      "Iteration is: 10925 and loss is: 0.0005062182899564505\n",
      "Iteration is: 10926 and loss is: 0.0005298969335854053\n",
      "Iteration is: 10927 and loss is: 0.0005627576028928161\n",
      "Iteration is: 10928 and loss is: 0.0005960649577900767\n",
      "Iteration is: 10929 and loss is: 0.0006387335597537458\n",
      "Iteration is: 10930 and loss is: 0.000659313693176955\n",
      "Iteration is: 10931 and loss is: 0.0006590645643882453\n",
      "Iteration is: 10932 and loss is: 0.0005991413490846753\n",
      "Iteration is: 10933 and loss is: 0.0005074483924545348\n",
      "Iteration is: 10934 and loss is: 0.00041214912198483944\n",
      "Iteration is: 10935 and loss is: 0.0003676902852021158\n",
      "Iteration is: 10936 and loss is: 0.00038750606472603977\n",
      "Iteration is: 10937 and loss is: 0.00043833954259753227\n",
      "Iteration is: 10938 and loss is: 0.0004727219929918647\n",
      "Iteration is: 10939 and loss is: 0.000461839692434296\n",
      "Iteration is: 10940 and loss is: 0.00042246223893016577\n",
      "Iteration is: 10941 and loss is: 0.0003811175120063126\n",
      "Iteration is: 10942 and loss is: 0.0003614060115069151\n",
      "Iteration is: 10943 and loss is: 0.000366720138117671\n",
      "Iteration is: 10944 and loss is: 0.00038877513725310564\n",
      "Iteration is: 10945 and loss is: 0.00041984202107414603\n",
      "Iteration is: 10946 and loss is: 0.00045672105625271797\n",
      "Iteration is: 10947 and loss is: 0.0005089269834570587\n",
      "Iteration is: 10948 and loss is: 0.0005829422734677792\n",
      "Iteration is: 10949 and loss is: 0.0007100472575984895\n",
      "Iteration is: 10950 and loss is: 0.0008898153901100159\n",
      "Iteration is: 10951 and loss is: 0.0011662831529974937\n",
      "Iteration is: 10952 and loss is: 0.0013873763382434845\n",
      "Iteration is: 10953 and loss is: 0.0014867105055600405\n",
      "Iteration is: 10954 and loss is: 0.0011113575892522931\n",
      "Iteration is: 10955 and loss is: 0.0005972517537884414\n",
      "Iteration is: 10956 and loss is: 0.0004251450882293284\n",
      "Iteration is: 10957 and loss is: 0.0006972020491957664\n",
      "Iteration is: 10958 and loss is: 0.0008553931256756186\n",
      "Iteration is: 10959 and loss is: 0.0006182151846587658\n",
      "Iteration is: 10960 and loss is: 0.0003999765031039715\n",
      "Iteration is: 10961 and loss is: 0.0005024266429245472\n",
      "Iteration is: 10962 and loss is: 0.0006912435055710375\n",
      "Iteration is: 10963 and loss is: 0.0007268860936164856\n",
      "Iteration is: 10964 and loss is: 0.0005886680446565151\n",
      "Iteration is: 10965 and loss is: 0.0004453173896763474\n",
      "Iteration is: 10966 and loss is: 0.00037882965989410877\n",
      "Iteration is: 10967 and loss is: 0.000388443935662508\n",
      "Iteration is: 10968 and loss is: 0.0004736132104881108\n",
      "Iteration is: 10969 and loss is: 0.0006063674227334559\n",
      "Iteration is: 10970 and loss is: 0.0007794917328283191\n",
      "Iteration is: 10971 and loss is: 0.0009773680940270424\n",
      "Iteration is: 10972 and loss is: 0.0011946912854909897\n",
      "Iteration is: 10973 and loss is: 0.0012354172067716718\n",
      "Iteration is: 10974 and loss is: 0.0010567575227469206\n",
      "Iteration is: 10975 and loss is: 0.0006237328052520752\n",
      "Iteration is: 10976 and loss is: 0.00038532886537723243\n",
      "Iteration is: 10977 and loss is: 0.0005407987628132105\n",
      "Iteration is: 10978 and loss is: 0.0007263303268700838\n",
      "Iteration is: 10979 and loss is: 0.0006180726923048496\n",
      "Iteration is: 10980 and loss is: 0.0004196704539936036\n",
      "Iteration is: 10981 and loss is: 0.00043923367047682405\n",
      "Iteration is: 10982 and loss is: 0.0005860003875568509\n",
      "Iteration is: 10983 and loss is: 0.0006034710677340627\n",
      "Iteration is: 10984 and loss is: 0.00048751902068033814\n",
      "Iteration is: 10985 and loss is: 0.0003825714811682701\n",
      "Iteration is: 10986 and loss is: 0.0003826387401204556\n",
      "Iteration is: 10987 and loss is: 0.00046149344416335225\n",
      "Iteration is: 10988 and loss is: 0.0005497298552654684\n",
      "Iteration is: 10989 and loss is: 0.000637556950096041\n",
      "Iteration is: 10990 and loss is: 0.0006957203149795532\n",
      "Iteration is: 10991 and loss is: 0.000737697584554553\n",
      "Iteration is: 10992 and loss is: 0.0007377828005701303\n",
      "Iteration is: 10993 and loss is: 0.0007059272611513734\n",
      "Iteration is: 10994 and loss is: 0.0006052901735529304\n",
      "Iteration is: 10995 and loss is: 0.000479126931168139\n",
      "Iteration is: 10996 and loss is: 0.0003826522151939571\n",
      "Iteration is: 10997 and loss is: 0.0003719718661159277\n",
      "Iteration is: 10998 and loss is: 0.0004272218211553991\n",
      "Iteration is: 10999 and loss is: 0.0004790868842974305\n",
      "Iteration is: 11000 and loss is: 0.0004715546383522451\n",
      "Iteration is: 11001 and loss is: 0.0004136959614697844\n",
      "Iteration is: 11002 and loss is: 0.00037006725324317813\n",
      "Iteration is: 11003 and loss is: 0.0003745613503269851\n",
      "Iteration is: 11004 and loss is: 0.00040895852725952864\n",
      "Iteration is: 11005 and loss is: 0.000432036817073822\n",
      "Iteration is: 11006 and loss is: 0.0004220198024995625\n",
      "Iteration is: 11007 and loss is: 0.0003921383759006858\n",
      "Iteration is: 11008 and loss is: 0.0003654633183032274\n",
      "Iteration is: 11009 and loss is: 0.0003583710640668869\n",
      "Iteration is: 11010 and loss is: 0.00036776141496375203\n",
      "Iteration is: 11011 and loss is: 0.00038590910844504833\n",
      "Iteration is: 11012 and loss is: 0.00040846384945325553\n",
      "Iteration is: 11013 and loss is: 0.00043270160676911473\n",
      "Iteration is: 11014 and loss is: 0.00046428668429143727\n",
      "Iteration is: 11015 and loss is: 0.0005058475653640926\n",
      "Iteration is: 11016 and loss is: 0.0005728853866457939\n",
      "Iteration is: 11017 and loss is: 0.0006610139971598983\n",
      "Iteration is: 11018 and loss is: 0.0007889845874160528\n",
      "Iteration is: 11019 and loss is: 0.000898495374713093\n",
      "Iteration is: 11020 and loss is: 0.0009750977624207735\n",
      "Iteration is: 11021 and loss is: 0.0008790760766714811\n",
      "Iteration is: 11022 and loss is: 0.0006682018283754587\n",
      "Iteration is: 11023 and loss is: 0.00043611531145870686\n",
      "Iteration is: 11024 and loss is: 0.0003827370819635689\n",
      "Iteration is: 11025 and loss is: 0.000502182578202337\n",
      "Iteration is: 11026 and loss is: 0.0005961574497632682\n",
      "Iteration is: 11027 and loss is: 0.00054500054102391\n",
      "Iteration is: 11028 and loss is: 0.0004167075967416167\n",
      "Iteration is: 11029 and loss is: 0.0003671811427921057\n",
      "Iteration is: 11030 and loss is: 0.00042708870023489\n",
      "Iteration is: 11031 and loss is: 0.0005066356388852\n",
      "Iteration is: 11032 and loss is: 0.0005330609274096787\n",
      "Iteration is: 11033 and loss is: 0.000496723223477602\n",
      "Iteration is: 11034 and loss is: 0.0004404311184771359\n",
      "Iteration is: 11035 and loss is: 0.00038739657611586154\n",
      "Iteration is: 11036 and loss is: 0.0003578368341550231\n",
      "Iteration is: 11037 and loss is: 0.00035612049396149814\n",
      "Iteration is: 11038 and loss is: 0.0003746979054994881\n",
      "Iteration is: 11039 and loss is: 0.0004084547399543226\n",
      "Iteration is: 11040 and loss is: 0.00046110956463962793\n",
      "Iteration is: 11041 and loss is: 0.0005459085805341601\n",
      "Iteration is: 11042 and loss is: 0.0006664112443104386\n",
      "Iteration is: 11043 and loss is: 0.0008494009380228817\n",
      "Iteration is: 11044 and loss is: 0.001034709275700152\n",
      "Iteration is: 11045 and loss is: 0.0011991633800789714\n",
      "Iteration is: 11046 and loss is: 0.001105503411963582\n",
      "Iteration is: 11047 and loss is: 0.0008118389523588121\n",
      "Iteration is: 11048 and loss is: 0.0004663368163164705\n",
      "Iteration is: 11049 and loss is: 0.0004170290194451809\n",
      "Iteration is: 11050 and loss is: 0.0006171322893351316\n",
      "Iteration is: 11051 and loss is: 0.0007080159848555923\n",
      "Iteration is: 11052 and loss is: 0.0005644640768878162\n",
      "Iteration is: 11053 and loss is: 0.0003932510153390467\n",
      "Iteration is: 11054 and loss is: 0.0004012886784039438\n",
      "Iteration is: 11055 and loss is: 0.0005397074855864048\n",
      "Iteration is: 11056 and loss is: 0.0006331622716970742\n",
      "Iteration is: 11057 and loss is: 0.0006200195057317615\n",
      "Iteration is: 11058 and loss is: 0.0005421936511993408\n",
      "Iteration is: 11059 and loss is: 0.00046705245040357113\n",
      "Iteration is: 11060 and loss is: 0.00039851799374446273\n",
      "Iteration is: 11061 and loss is: 0.00035960524110123515\n",
      "Iteration is: 11062 and loss is: 0.00035902694799005985\n",
      "Iteration is: 11063 and loss is: 0.0003800281265284866\n",
      "Iteration is: 11064 and loss is: 0.0004126455751247704\n",
      "Iteration is: 11065 and loss is: 0.0004661442362703383\n",
      "Iteration is: 11066 and loss is: 0.000550381257198751\n",
      "Iteration is: 11067 and loss is: 0.0006508507067337632\n",
      "Iteration is: 11068 and loss is: 0.0007761720335111022\n",
      "Iteration is: 11069 and loss is: 0.0008666961221024394\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 11070 and loss is: 0.000899511156603694\n",
      "Iteration is: 11071 and loss is: 0.0007671887869946659\n",
      "Iteration is: 11072 and loss is: 0.0005615716800093651\n",
      "Iteration is: 11073 and loss is: 0.0003968093660660088\n",
      "Iteration is: 11074 and loss is: 0.0003947168297600001\n",
      "Iteration is: 11075 and loss is: 0.0005040100077167153\n",
      "Iteration is: 11076 and loss is: 0.0005655169952660799\n",
      "Iteration is: 11077 and loss is: 0.0005088192410767078\n",
      "Iteration is: 11078 and loss is: 0.00040117703611031175\n",
      "Iteration is: 11079 and loss is: 0.00036269164411351085\n",
      "Iteration is: 11080 and loss is: 0.0004146010032854974\n",
      "Iteration is: 11081 and loss is: 0.0004814726416952908\n",
      "Iteration is: 11082 and loss is: 0.0005054349894635379\n",
      "Iteration is: 11083 and loss is: 0.0004837412270717323\n",
      "Iteration is: 11084 and loss is: 0.0004455698071978986\n",
      "Iteration is: 11085 and loss is: 0.0004002662608399987\n",
      "Iteration is: 11086 and loss is: 0.0003662852686829865\n",
      "Iteration is: 11087 and loss is: 0.0003525775973685086\n",
      "Iteration is: 11088 and loss is: 0.0003535975993145257\n",
      "Iteration is: 11089 and loss is: 0.0003620046190917492\n",
      "Iteration is: 11090 and loss is: 0.0003807318862527609\n",
      "Iteration is: 11091 and loss is: 0.0004159643140155822\n",
      "Iteration is: 11092 and loss is: 0.00047135038767009974\n",
      "Iteration is: 11093 and loss is: 0.0005637353751808405\n",
      "Iteration is: 11094 and loss is: 0.0006992352427914739\n",
      "Iteration is: 11095 and loss is: 0.000899982696864754\n",
      "Iteration is: 11096 and loss is: 0.0010772106470540166\n",
      "Iteration is: 11097 and loss is: 0.001202913699671626\n",
      "Iteration is: 11098 and loss is: 0.0010473312577232718\n",
      "Iteration is: 11099 and loss is: 0.0007217470556497574\n",
      "Iteration is: 11100 and loss is: 0.00042656948789954185\n",
      "Iteration is: 11101 and loss is: 0.00043917878065258265\n",
      "Iteration is: 11102 and loss is: 0.0006444361060857773\n",
      "Iteration is: 11103 and loss is: 0.0007078303024172783\n",
      "Iteration is: 11104 and loss is: 0.0005545055028051138\n",
      "Iteration is: 11105 and loss is: 0.00038700998993590474\n",
      "Iteration is: 11106 and loss is: 0.00038352736737579107\n",
      "Iteration is: 11107 and loss is: 0.0005092754727229476\n",
      "Iteration is: 11108 and loss is: 0.0006268095457926393\n",
      "Iteration is: 11109 and loss is: 0.0006761924596503377\n",
      "Iteration is: 11110 and loss is: 0.0006657502381131053\n",
      "Iteration is: 11111 and loss is: 0.0006451095105148852\n",
      "Iteration is: 11112 and loss is: 0.0005893384804949164\n",
      "Iteration is: 11113 and loss is: 0.0005350830615498126\n",
      "Iteration is: 11114 and loss is: 0.0004852903075516224\n",
      "Iteration is: 11115 and loss is: 0.0004445828963071108\n",
      "Iteration is: 11116 and loss is: 0.00040292658377438784\n",
      "Iteration is: 11117 and loss is: 0.00037082284688949585\n",
      "Iteration is: 11118 and loss is: 0.0003561491903383285\n",
      "Iteration is: 11119 and loss is: 0.00035206181928515434\n",
      "Iteration is: 11120 and loss is: 0.00035232360824011266\n",
      "Iteration is: 11121 and loss is: 0.00036115050897933543\n",
      "Iteration is: 11122 and loss is: 0.0003767134912777692\n",
      "Iteration is: 11123 and loss is: 0.00039269361877813935\n",
      "Iteration is: 11124 and loss is: 0.0004107411950826645\n",
      "Iteration is: 11125 and loss is: 0.00043501975596882403\n",
      "Iteration is: 11126 and loss is: 0.0004728565982077271\n",
      "Iteration is: 11127 and loss is: 0.0005232234252616763\n",
      "Iteration is: 11128 and loss is: 0.0006056303391233087\n",
      "Iteration is: 11129 and loss is: 0.0007181005785241723\n",
      "Iteration is: 11130 and loss is: 0.0008905482827685773\n",
      "Iteration is: 11131 and loss is: 0.001052298117429018\n",
      "Iteration is: 11132 and loss is: 0.0012075642589479685\n",
      "Iteration is: 11133 and loss is: 0.0011379190254956484\n",
      "Iteration is: 11134 and loss is: 0.0008860413217917085\n",
      "Iteration is: 11135 and loss is: 0.0005226858193054795\n",
      "Iteration is: 11136 and loss is: 0.0003868508501909673\n",
      "Iteration is: 11137 and loss is: 0.0005385554977692664\n",
      "Iteration is: 11138 and loss is: 0.000698376796208322\n",
      "Iteration is: 11139 and loss is: 0.0006512848194688559\n",
      "Iteration is: 11140 and loss is: 0.00046598902554251254\n",
      "Iteration is: 11141 and loss is: 0.0003602640936151147\n",
      "Iteration is: 11142 and loss is: 0.0004162380937486887\n",
      "Iteration is: 11143 and loss is: 0.0005484121502377093\n",
      "Iteration is: 11144 and loss is: 0.000647018663585186\n",
      "Iteration is: 11145 and loss is: 0.0006745278369635344\n",
      "Iteration is: 11146 and loss is: 0.0006809114711359143\n",
      "Iteration is: 11147 and loss is: 0.000651773065328598\n",
      "Iteration is: 11148 and loss is: 0.00061104103224352\n",
      "Iteration is: 11149 and loss is: 0.0005503082647919655\n",
      "Iteration is: 11150 and loss is: 0.0004990121815353632\n",
      "Iteration is: 11151 and loss is: 0.0004461487988010049\n",
      "Iteration is: 11152 and loss is: 0.0003951734397560358\n",
      "Iteration is: 11153 and loss is: 0.00035927313729189336\n",
      "Iteration is: 11154 and loss is: 0.0003495132550597191\n",
      "Iteration is: 11155 and loss is: 0.0003566595260053873\n",
      "Iteration is: 11156 and loss is: 0.0003706970892380923\n",
      "Iteration is: 11157 and loss is: 0.0003905668272636831\n",
      "Iteration is: 11158 and loss is: 0.0004133112670388073\n",
      "Iteration is: 11159 and loss is: 0.00043528806418180466\n",
      "Iteration is: 11160 and loss is: 0.00045353290624916553\n",
      "Iteration is: 11161 and loss is: 0.00048030991456471384\n",
      "Iteration is: 11162 and loss is: 0.0005151404766365886\n",
      "Iteration is: 11163 and loss is: 0.0005694959545508027\n",
      "Iteration is: 11164 and loss is: 0.000638519530184567\n",
      "Iteration is: 11165 and loss is: 0.0007484309608116746\n",
      "Iteration is: 11166 and loss is: 0.0008606794872321188\n",
      "Iteration is: 11167 and loss is: 0.0009904623730108142\n",
      "Iteration is: 11168 and loss is: 0.0010090407449752092\n",
      "Iteration is: 11169 and loss is: 0.0009245047112926841\n",
      "Iteration is: 11170 and loss is: 0.0006707793800160289\n",
      "Iteration is: 11171 and loss is: 0.0004366115899756551\n",
      "Iteration is: 11172 and loss is: 0.0003768037131521851\n",
      "Iteration is: 11173 and loss is: 0.0004912705044262111\n",
      "Iteration is: 11174 and loss is: 0.0006081273313611746\n",
      "Iteration is: 11175 and loss is: 0.0005817992496304214\n",
      "Iteration is: 11176 and loss is: 0.0004565669223666191\n",
      "Iteration is: 11177 and loss is: 0.0003612746368162334\n",
      "Iteration is: 11178 and loss is: 0.00037032848922535777\n",
      "Iteration is: 11179 and loss is: 0.0004498444905038923\n",
      "Iteration is: 11180 and loss is: 0.0005264693754725158\n",
      "Iteration is: 11181 and loss is: 0.0005734978476539254\n",
      "Iteration is: 11182 and loss is: 0.0005926156882196665\n",
      "Iteration is: 11183 and loss is: 0.0006091017858125269\n",
      "Iteration is: 11184 and loss is: 0.0006040745065547526\n",
      "Iteration is: 11185 and loss is: 0.0006035517435520887\n",
      "Iteration is: 11186 and loss is: 0.0005965251475572586\n",
      "Iteration is: 11187 and loss is: 0.0005947863683104515\n",
      "Iteration is: 11188 and loss is: 0.0005692938575521111\n",
      "Iteration is: 11189 and loss is: 0.0005304559017531574\n",
      "Iteration is: 11190 and loss is: 0.0004732549423351884\n",
      "Iteration is: 11191 and loss is: 0.0004164927522651851\n",
      "Iteration is: 11192 and loss is: 0.0003688533033709973\n",
      "Iteration is: 11193 and loss is: 0.00034657452488318086\n",
      "Iteration is: 11194 and loss is: 0.0003525950014591217\n",
      "Iteration is: 11195 and loss is: 0.00037572975270450115\n",
      "Iteration is: 11196 and loss is: 0.0004018883628305048\n",
      "Iteration is: 11197 and loss is: 0.0004219125839881599\n",
      "Iteration is: 11198 and loss is: 0.00043716724030673504\n",
      "Iteration is: 11199 and loss is: 0.00044579326640814543\n",
      "Iteration is: 11200 and loss is: 0.0004571016179397702\n",
      "Iteration is: 11201 and loss is: 0.00047469951095990837\n",
      "Iteration is: 11202 and loss is: 0.0005145781906321645\n",
      "Iteration is: 11203 and loss is: 0.0005790398572571576\n",
      "Iteration is: 11204 and loss is: 0.0007003300706855953\n",
      "Iteration is: 11205 and loss is: 0.000875862839166075\n",
      "Iteration is: 11206 and loss is: 0.0011583396699279547\n",
      "Iteration is: 11207 and loss is: 0.0014047881122678518\n",
      "Iteration is: 11208 and loss is: 0.0015712394379079342\n",
      "Iteration is: 11209 and loss is: 0.0012610562844201922\n",
      "Iteration is: 11210 and loss is: 0.0007357846479862928\n",
      "Iteration is: 11211 and loss is: 0.00042752703302539885\n",
      "Iteration is: 11212 and loss is: 0.0005980844725854695\n",
      "Iteration is: 11213 and loss is: 0.000855008140206337\n",
      "Iteration is: 11214 and loss is: 0.0007944218232296407\n",
      "Iteration is: 11215 and loss is: 0.0005128469783812761\n",
      "Iteration is: 11216 and loss is: 0.00038918963400647044\n",
      "Iteration is: 11217 and loss is: 0.0004900167696177959\n",
      "Iteration is: 11218 and loss is: 0.000689413514919579\n",
      "Iteration is: 11219 and loss is: 0.0008374308818019927\n",
      "Iteration is: 11220 and loss is: 0.0008713355055078864\n",
      "Iteration is: 11221 and loss is: 0.0008742013014853001\n",
      "Iteration is: 11222 and loss is: 0.0009640575153753161\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 11223 and loss is: 0.0009911389788612723\n",
      "Iteration is: 11224 and loss is: 0.0009645420359447598\n",
      "Iteration is: 11225 and loss is: 0.0008050656761042774\n",
      "Iteration is: 11226 and loss is: 0.0006023382302373648\n",
      "Iteration is: 11227 and loss is: 0.0004473357112146914\n",
      "Iteration is: 11228 and loss is: 0.0004005312221124768\n",
      "Iteration is: 11229 and loss is: 0.0004726026672869921\n",
      "Iteration is: 11230 and loss is: 0.0005637279828079045\n",
      "Iteration is: 11231 and loss is: 0.0005291893612593412\n",
      "Iteration is: 11232 and loss is: 0.0004198212700430304\n",
      "Iteration is: 11233 and loss is: 0.00036095044924877584\n",
      "Iteration is: 11234 and loss is: 0.0004073522868566215\n",
      "Iteration is: 11235 and loss is: 0.0004846386145800352\n",
      "Iteration is: 11236 and loss is: 0.0004871294950135052\n",
      "Iteration is: 11237 and loss is: 0.0004401375772431493\n",
      "Iteration is: 11238 and loss is: 0.0004112006863579154\n",
      "Iteration is: 11239 and loss is: 0.0003907809150405228\n",
      "Iteration is: 11240 and loss is: 0.00036327732959762216\n",
      "Iteration is: 11241 and loss is: 0.00034598109778016806\n",
      "Iteration is: 11242 and loss is: 0.00035584651050157845\n",
      "Iteration is: 11243 and loss is: 0.00038660928839817643\n",
      "Iteration is: 11244 and loss is: 0.0004177924420218915\n",
      "Iteration is: 11245 and loss is: 0.0004621129482984543\n",
      "Iteration is: 11246 and loss is: 0.0005477462545968592\n",
      "Iteration is: 11247 and loss is: 0.0006888669449836016\n",
      "Iteration is: 11248 and loss is: 0.000850495183840394\n",
      "Iteration is: 11249 and loss is: 0.001030697370879352\n",
      "Iteration is: 11250 and loss is: 0.0010810456005856395\n",
      "Iteration is: 11251 and loss is: 0.0009766037110239267\n",
      "Iteration is: 11252 and loss is: 0.0006505937199108303\n",
      "Iteration is: 11253 and loss is: 0.0004034886951558292\n",
      "Iteration is: 11254 and loss is: 0.00042270938865840435\n",
      "Iteration is: 11255 and loss is: 0.0005826090928167105\n",
      "Iteration is: 11256 and loss is: 0.0006387088797055185\n",
      "Iteration is: 11257 and loss is: 0.0005358104826882482\n",
      "Iteration is: 11258 and loss is: 0.00041097792563959956\n",
      "Iteration is: 11259 and loss is: 0.00036892248317599297\n",
      "Iteration is: 11260 and loss is: 0.0004255218373145908\n",
      "Iteration is: 11261 and loss is: 0.0005354495369829237\n",
      "Iteration is: 11262 and loss is: 0.0006116611184552312\n",
      "Iteration is: 11263 and loss is: 0.0006279320223256946\n",
      "Iteration is: 11264 and loss is: 0.000630377558991313\n",
      "Iteration is: 11265 and loss is: 0.0006764174904674292\n",
      "Iteration is: 11266 and loss is: 0.0007257828838191926\n",
      "Iteration is: 11267 and loss is: 0.0007798096048645675\n",
      "Iteration is: 11268 and loss is: 0.000800362671725452\n",
      "Iteration is: 11269 and loss is: 0.0008123171282932162\n",
      "Iteration is: 11270 and loss is: 0.0007299765711650252\n",
      "Iteration is: 11271 and loss is: 0.0005692866398021579\n",
      "Iteration is: 11272 and loss is: 0.0004037141043227166\n",
      "Iteration is: 11273 and loss is: 0.00035214656963944435\n",
      "Iteration is: 11274 and loss is: 0.00042614032281562686\n",
      "Iteration is: 11275 and loss is: 0.0005160211003385484\n",
      "Iteration is: 11276 and loss is: 0.0005172679666429758\n",
      "Iteration is: 11277 and loss is: 0.0004356717108748853\n",
      "Iteration is: 11278 and loss is: 0.0003604053636081517\n",
      "Iteration is: 11279 and loss is: 0.0003516606811899692\n",
      "Iteration is: 11280 and loss is: 0.00039709312841296196\n",
      "Iteration is: 11281 and loss is: 0.0004480146453715861\n",
      "Iteration is: 11282 and loss is: 0.00047089948202483356\n",
      "Iteration is: 11283 and loss is: 0.00046744674909859896\n",
      "Iteration is: 11284 and loss is: 0.00044225758756510913\n",
      "Iteration is: 11285 and loss is: 0.00041273870738223195\n",
      "Iteration is: 11286 and loss is: 0.00038443945231847465\n",
      "Iteration is: 11287 and loss is: 0.00036500158603303134\n",
      "Iteration is: 11288 and loss is: 0.0003532905247993767\n",
      "Iteration is: 11289 and loss is: 0.0003464254841674119\n",
      "Iteration is: 11290 and loss is: 0.0003417902044020593\n",
      "Iteration is: 11291 and loss is: 0.0003393555525690317\n",
      "Iteration is: 11292 and loss is: 0.00033875470398925245\n",
      "Iteration is: 11293 and loss is: 0.00033916495158337057\n",
      "Iteration is: 11294 and loss is: 0.00034036283614113927\n",
      "Iteration is: 11295 and loss is: 0.00034345046151429415\n",
      "Iteration is: 11296 and loss is: 0.0003498492296785116\n",
      "Iteration is: 11297 and loss is: 0.00036126968916505575\n",
      "Iteration is: 11298 and loss is: 0.00037942035123705864\n",
      "Iteration is: 11299 and loss is: 0.0004106732667423785\n",
      "Iteration is: 11300 and loss is: 0.0004618534876499325\n",
      "Iteration is: 11301 and loss is: 0.0005537953693419695\n",
      "Iteration is: 11302 and loss is: 0.0006993149872869253\n",
      "Iteration is: 11303 and loss is: 0.0009594063740223646\n",
      "Iteration is: 11304 and loss is: 0.001302208169363439\n",
      "Iteration is: 11305 and loss is: 0.0017897624056786299\n",
      "Iteration is: 11306 and loss is: 0.0019664678256958723\n",
      "Iteration is: 11307 and loss is: 0.0017377230105921626\n",
      "Iteration is: 11308 and loss is: 0.0009293454932048917\n",
      "Iteration is: 11309 and loss is: 0.0004899176419712603\n",
      "Iteration is: 11310 and loss is: 0.0007792960968799889\n",
      "Iteration is: 11311 and loss is: 0.0011462288675829768\n",
      "Iteration is: 11312 and loss is: 0.0009812036296352744\n",
      "Iteration is: 11313 and loss is: 0.0005848829168826342\n",
      "Iteration is: 11314 and loss is: 0.0004555421764962375\n",
      "Iteration is: 11315 and loss is: 0.0006299257511273026\n",
      "Iteration is: 11316 and loss is: 0.0008444412378594279\n",
      "Iteration is: 11317 and loss is: 0.001022237935103476\n",
      "Iteration is: 11318 and loss is: 0.0010270504280924797\n",
      "Iteration is: 11319 and loss is: 0.0009397646645084023\n",
      "Iteration is: 11320 and loss is: 0.0009751798934303224\n",
      "Iteration is: 11321 and loss is: 0.001080631511285901\n",
      "Iteration is: 11322 and loss is: 0.0010193423368036747\n",
      "Iteration is: 11323 and loss is: 0.0008128755143843591\n",
      "Iteration is: 11324 and loss is: 0.0005459047970362008\n",
      "Iteration is: 11325 and loss is: 0.0004505456308834255\n",
      "Iteration is: 11326 and loss is: 0.0004912747535854578\n",
      "Iteration is: 11327 and loss is: 0.0004908116534352303\n",
      "Iteration is: 11328 and loss is: 0.0005130628123879433\n",
      "Iteration is: 11329 and loss is: 0.0005315663293004036\n",
      "Iteration is: 11330 and loss is: 0.0004505167598836124\n",
      "Iteration is: 11331 and loss is: 0.0003922651521861553\n",
      "Iteration is: 11332 and loss is: 0.00040762388380244374\n",
      "Iteration is: 11333 and loss is: 0.0004652219940908253\n",
      "Iteration is: 11334 and loss is: 0.0004802970215678215\n",
      "Iteration is: 11335 and loss is: 0.0003949542879126966\n",
      "Iteration is: 11336 and loss is: 0.0003597406321205199\n",
      "Iteration is: 11337 and loss is: 0.0003970851539634168\n",
      "Iteration is: 11338 and loss is: 0.00039873761124908924\n",
      "Iteration is: 11339 and loss is: 0.0003696042695082724\n",
      "Iteration is: 11340 and loss is: 0.00034816510742530227\n",
      "Iteration is: 11341 and loss is: 0.0003612510918173939\n",
      "Iteration is: 11342 and loss is: 0.0003889665240421891\n",
      "Iteration is: 11343 and loss is: 0.00038475426845252514\n",
      "Iteration is: 11344 and loss is: 0.0003747180453501642\n",
      "Iteration is: 11345 and loss is: 0.0004074338939972222\n",
      "Iteration is: 11346 and loss is: 0.00045632763067260385\n",
      "Iteration is: 11347 and loss is: 0.0004947552806697786\n",
      "Iteration is: 11348 and loss is: 0.0005204315530136228\n",
      "Iteration is: 11349 and loss is: 0.00053390022367239\n",
      "Iteration is: 11350 and loss is: 0.0005465552676469088\n",
      "Iteration is: 11351 and loss is: 0.0005228944355621934\n",
      "Iteration is: 11352 and loss is: 0.0004736474365927279\n",
      "Iteration is: 11353 and loss is: 0.0004180579271633178\n",
      "Iteration is: 11354 and loss is: 0.00037983356742188334\n",
      "Iteration is: 11355 and loss is: 0.0003527180524542928\n",
      "Iteration is: 11356 and loss is: 0.00033743950189091265\n",
      "Iteration is: 11357 and loss is: 0.0003366207820363343\n",
      "Iteration is: 11358 and loss is: 0.0003548049135133624\n",
      "Iteration is: 11359 and loss is: 0.000388042361009866\n",
      "Iteration is: 11360 and loss is: 0.0004339262959547341\n",
      "Iteration is: 11361 and loss is: 0.0005216316203586757\n",
      "Iteration is: 11362 and loss is: 0.0007020048215053976\n",
      "Iteration is: 11363 and loss is: 0.0010902388021349907\n",
      "Iteration is: 11364 and loss is: 0.0017422509845346212\n",
      "Iteration is: 11365 and loss is: 0.0028178903739899397\n",
      "Iteration is: 11366 and loss is: 0.0031821783632040024\n",
      "Iteration is: 11367 and loss is: 0.0024276012554764748\n",
      "Iteration is: 11368 and loss is: 0.0010107543785125017\n",
      "Iteration is: 11369 and loss is: 0.0009501543827354908\n",
      "Iteration is: 11370 and loss is: 0.0021640320774167776\n",
      "Iteration is: 11371 and loss is: 0.004077507648617029\n",
      "Iteration is: 11372 and loss is: 0.00474962592124939\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 11373 and loss is: 0.004375948570668697\n",
      "Iteration is: 11374 and loss is: 0.001363999443128705\n",
      "Iteration is: 11375 and loss is: 0.0011687639635056257\n",
      "Iteration is: 11376 and loss is: 0.005128282587975264\n",
      "Iteration is: 11377 and loss is: 0.014089050702750683\n",
      "Iteration is: 11378 and loss is: 0.018521182239055634\n",
      "Iteration is: 11379 and loss is: 0.006580531131476164\n",
      "Iteration is: 11380 and loss is: 0.007230869960039854\n",
      "Iteration is: 11381 and loss is: 0.01889622211456299\n",
      "Iteration is: 11382 and loss is: 0.04144672304391861\n",
      "Iteration is: 11383 and loss is: 0.027890194207429886\n",
      "Iteration is: 11384 and loss is: 0.008043221198022366\n",
      "Iteration is: 11385 and loss is: 0.03156684711575508\n",
      "Iteration is: 11386 and loss is: 0.021303843706846237\n",
      "Iteration is: 11387 and loss is: 0.019566282629966736\n",
      "Iteration is: 11388 and loss is: 0.013221841305494308\n",
      "Iteration is: 11389 and loss is: 0.012297851964831352\n",
      "Iteration is: 11390 and loss is: 0.008722027763724327\n",
      "Iteration is: 11391 and loss is: 0.01206996664404869\n",
      "Iteration is: 11392 and loss is: 0.005742683075368404\n",
      "Iteration is: 11393 and loss is: 0.013361144810914993\n",
      "Iteration is: 11394 and loss is: 0.005691708996891975\n",
      "Iteration is: 11395 and loss is: 0.009247517213225365\n",
      "Iteration is: 11396 and loss is: 0.004119797609746456\n",
      "Iteration is: 11397 and loss is: 0.00886033196002245\n",
      "Iteration is: 11398 and loss is: 0.002598684513941407\n",
      "Iteration is: 11399 and loss is: 0.010008181445300579\n",
      "Iteration is: 11400 and loss is: 0.004308989737182856\n",
      "Iteration is: 11401 and loss is: 0.006327151786535978\n",
      "Iteration is: 11402 and loss is: 0.002886374481022358\n",
      "Iteration is: 11403 and loss is: 0.00548362173140049\n",
      "Iteration is: 11404 and loss is: 0.0036423788405954838\n",
      "Iteration is: 11405 and loss is: 0.0025160033255815506\n",
      "Iteration is: 11406 and loss is: 0.004146520048379898\n",
      "Iteration is: 11407 and loss is: 0.001964201685041189\n",
      "Iteration is: 11408 and loss is: 0.0031533504370599985\n",
      "Iteration is: 11409 and loss is: 0.0019124785903841257\n",
      "Iteration is: 11410 and loss is: 0.002509316662326455\n",
      "Iteration is: 11411 and loss is: 0.0018345490097999573\n",
      "Iteration is: 11412 and loss is: 0.0021674460731446743\n",
      "Iteration is: 11413 and loss is: 0.0017422942910343409\n",
      "Iteration is: 11414 and loss is: 0.0015782967675477266\n",
      "Iteration is: 11415 and loss is: 0.002045708941295743\n",
      "Iteration is: 11416 and loss is: 0.0010835579596459866\n",
      "Iteration is: 11417 and loss is: 0.0019175687339156866\n",
      "Iteration is: 11418 and loss is: 0.0011479606619104743\n",
      "Iteration is: 11419 and loss is: 0.001394050195813179\n",
      "Iteration is: 11420 and loss is: 0.001198735204525292\n",
      "Iteration is: 11421 and loss is: 0.0012158261379227042\n",
      "Iteration is: 11422 and loss is: 0.0011201121378690004\n",
      "Iteration is: 11423 and loss is: 0.0010243769502267241\n",
      "Iteration is: 11424 and loss is: 0.0011073208879679441\n",
      "Iteration is: 11425 and loss is: 0.0008553441148251295\n",
      "Iteration is: 11426 and loss is: 0.0010381508618593216\n",
      "Iteration is: 11427 and loss is: 0.0007645790465176105\n",
      "Iteration is: 11428 and loss is: 0.0009101375471800566\n",
      "Iteration is: 11429 and loss is: 0.0007225449080578983\n",
      "Iteration is: 11430 and loss is: 0.000794397434219718\n",
      "Iteration is: 11431 and loss is: 0.0007123360992409289\n",
      "Iteration is: 11432 and loss is: 0.0006976682925596833\n",
      "Iteration is: 11433 and loss is: 0.0007311546942219138\n",
      "Iteration is: 11434 and loss is: 0.0006001793080940843\n",
      "Iteration is: 11435 and loss is: 0.0007191847544163465\n",
      "Iteration is: 11436 and loss is: 0.0005598297575488687\n",
      "Iteration is: 11437 and loss is: 0.0006392592331394553\n",
      "Iteration is: 11438 and loss is: 0.0005849454319104552\n",
      "Iteration is: 11439 and loss is: 0.0005467641167342663\n",
      "Iteration is: 11440 and loss is: 0.0005988595075905323\n",
      "Iteration is: 11441 and loss is: 0.0004989209701307118\n",
      "Iteration is: 11442 and loss is: 0.0005578291602432728\n",
      "Iteration is: 11443 and loss is: 0.000500048219691962\n",
      "Iteration is: 11444 and loss is: 0.0005060359835624695\n",
      "Iteration is: 11445 and loss is: 0.0005003202240914106\n",
      "Iteration is: 11446 and loss is: 0.0004844031354878098\n",
      "Iteration is: 11447 and loss is: 0.00047313011600635946\n",
      "Iteration is: 11448 and loss is: 0.00047703832387924194\n",
      "Iteration is: 11449 and loss is: 0.0004553350736387074\n",
      "Iteration is: 11450 and loss is: 0.0004573409678414464\n",
      "Iteration is: 11451 and loss is: 0.0004538787470664829\n",
      "Iteration is: 11452 and loss is: 0.00043185410322621465\n",
      "Iteration is: 11453 and loss is: 0.00044797995360568166\n",
      "Iteration is: 11454 and loss is: 0.00042217870941385627\n",
      "Iteration is: 11455 and loss is: 0.0004309884971007705\n",
      "Iteration is: 11456 and loss is: 0.0004216166562400758\n",
      "Iteration is: 11457 and loss is: 0.00041833307477645576\n",
      "Iteration is: 11458 and loss is: 0.00041432399302721024\n",
      "Iteration is: 11459 and loss is: 0.00041374063584953547\n",
      "Iteration is: 11460 and loss is: 0.0004043781664222479\n",
      "Iteration is: 11461 and loss is: 0.00040614092722535133\n",
      "Iteration is: 11462 and loss is: 0.0004021122877020389\n",
      "Iteration is: 11463 and loss is: 0.00039414840284734964\n",
      "Iteration is: 11464 and loss is: 0.00039925670716911554\n",
      "Iteration is: 11465 and loss is: 0.00038933742325752974\n",
      "Iteration is: 11466 and loss is: 0.0003907670616172254\n",
      "Iteration is: 11467 and loss is: 0.0003876577829942107\n",
      "Iteration is: 11468 and loss is: 0.00038634252268821\n",
      "Iteration is: 11469 and loss is: 0.0003818392870016396\n",
      "Iteration is: 11470 and loss is: 0.00038339963066391647\n",
      "Iteration is: 11471 and loss is: 0.0003790047485381365\n",
      "Iteration is: 11472 and loss is: 0.00037723928107880056\n",
      "Iteration is: 11473 and loss is: 0.0003773376811295748\n",
      "Iteration is: 11474 and loss is: 0.00037414132384583354\n",
      "Iteration is: 11475 and loss is: 0.00037285807775333524\n",
      "Iteration is: 11476 and loss is: 0.00037169712595641613\n",
      "Iteration is: 11477 and loss is: 0.0003705751441884786\n",
      "Iteration is: 11478 and loss is: 0.0003678141802083701\n",
      "Iteration is: 11479 and loss is: 0.00036823618574999273\n",
      "Iteration is: 11480 and loss is: 0.0003660639631561935\n",
      "Iteration is: 11481 and loss is: 0.00036499928683042526\n",
      "Iteration is: 11482 and loss is: 0.0003639691276475787\n",
      "Iteration is: 11483 and loss is: 0.0003633328014984727\n",
      "Iteration is: 11484 and loss is: 0.0003614861343521625\n",
      "Iteration is: 11485 and loss is: 0.00036110414657741785\n",
      "Iteration is: 11486 and loss is: 0.00036010300391353667\n",
      "Iteration is: 11487 and loss is: 0.00035887493868358433\n",
      "Iteration is: 11488 and loss is: 0.00035809556720778346\n",
      "Iteration is: 11489 and loss is: 0.000357415818143636\n",
      "Iteration is: 11490 and loss is: 0.0003564456128515303\n",
      "Iteration is: 11491 and loss is: 0.0003555465955287218\n",
      "Iteration is: 11492 and loss is: 0.0003551694971974939\n",
      "Iteration is: 11493 and loss is: 0.00035411756834946573\n",
      "Iteration is: 11494 and loss is: 0.00035350158577784896\n",
      "Iteration is: 11495 and loss is: 0.0003527706430759281\n",
      "Iteration is: 11496 and loss is: 0.0003522453480400145\n",
      "Iteration is: 11497 and loss is: 0.00035130261676386\n",
      "Iteration is: 11498 and loss is: 0.0003508803201839328\n",
      "Iteration is: 11499 and loss is: 0.00035019629285670817\n",
      "Iteration is: 11500 and loss is: 0.0003495839482638985\n",
      "Iteration is: 11501 and loss is: 0.0003489591763354838\n",
      "Iteration is: 11502 and loss is: 0.0003485028864815831\n",
      "Iteration is: 11503 and loss is: 0.000347886118106544\n",
      "Iteration is: 11504 and loss is: 0.00034730846527963877\n",
      "Iteration is: 11505 and loss is: 0.0003468543873168528\n",
      "Iteration is: 11506 and loss is: 0.00034631032031029463\n",
      "Iteration is: 11507 and loss is: 0.00034579390194267035\n",
      "Iteration is: 11508 and loss is: 0.00034529855474829674\n",
      "Iteration is: 11509 and loss is: 0.0003448774805292487\n",
      "Iteration is: 11510 and loss is: 0.00034434505505487323\n",
      "Iteration is: 11511 and loss is: 0.00034391143708489835\n",
      "Iteration is: 11512 and loss is: 0.0003434646059758961\n",
      "Iteration is: 11513 and loss is: 0.00034304254222661257\n",
      "Iteration is: 11514 and loss is: 0.0003425750182941556\n",
      "Iteration is: 11515 and loss is: 0.0003421863657422364\n",
      "Iteration is: 11516 and loss is: 0.0003417705884203315\n",
      "Iteration is: 11517 and loss is: 0.00034136249450966716\n",
      "Iteration is: 11518 and loss is: 0.00034095969749614596\n",
      "Iteration is: 11519 and loss is: 0.0003405947354622185\n",
      "Iteration is: 11520 and loss is: 0.0003402017173357308\n",
      "Iteration is: 11521 and loss is: 0.0003398266853764653\n",
      "Iteration is: 11522 and loss is: 0.0003394608502276242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 11523 and loss is: 0.0003391108475625515\n",
      "Iteration is: 11524 and loss is: 0.0003387517645023763\n",
      "Iteration is: 11525 and loss is: 0.0003384053707122803\n",
      "Iteration is: 11526 and loss is: 0.00033807108411565423\n",
      "Iteration is: 11527 and loss is: 0.00033773790346458554\n",
      "Iteration is: 11528 and loss is: 0.00033740565413609147\n",
      "Iteration is: 11529 and loss is: 0.00033708184491842985\n",
      "Iteration is: 11530 and loss is: 0.00033677241299301386\n",
      "Iteration is: 11531 and loss is: 0.00033645378425717354\n",
      "Iteration is: 11532 and loss is: 0.00033614938729442656\n",
      "Iteration is: 11533 and loss is: 0.00033584877382963896\n",
      "Iteration is: 11534 and loss is: 0.0003355533699505031\n",
      "Iteration is: 11535 and loss is: 0.0003352567146066576\n",
      "Iteration is: 11536 and loss is: 0.0003349699836689979\n",
      "Iteration is: 11537 and loss is: 0.000334685028064996\n",
      "Iteration is: 11538 and loss is: 0.0003344088909216225\n",
      "Iteration is: 11539 and loss is: 0.00033413164783269167\n",
      "Iteration is: 11540 and loss is: 0.000333861680701375\n",
      "Iteration is: 11541 and loss is: 0.00033359398366883397\n",
      "Iteration is: 11542 and loss is: 0.00033333018654957414\n",
      "Iteration is: 11543 and loss is: 0.00033307087142020464\n",
      "Iteration is: 11544 and loss is: 0.0003328177845105529\n",
      "Iteration is: 11545 and loss is: 0.00033256481401622295\n",
      "Iteration is: 11546 and loss is: 0.0003323139389976859\n",
      "Iteration is: 11547 and loss is: 0.0003320709220133722\n",
      "Iteration is: 11548 and loss is: 0.00033183000050485134\n",
      "Iteration is: 11549 and loss is: 0.0003315872745588422\n",
      "Iteration is: 11550 and loss is: 0.00033135374542325735\n",
      "Iteration is: 11551 and loss is: 0.0003311208274681121\n",
      "Iteration is: 11552 and loss is: 0.0003308907907921821\n",
      "Iteration is: 11553 and loss is: 0.0003306652361061424\n",
      "Iteration is: 11554 and loss is: 0.0003304401761852205\n",
      "Iteration is: 11555 and loss is: 0.0003302215482108295\n",
      "Iteration is: 11556 and loss is: 0.00033000053372234106\n",
      "Iteration is: 11557 and loss is: 0.0003297863877378404\n",
      "Iteration is: 11558 and loss is: 0.000329572067130357\n",
      "Iteration is: 11559 and loss is: 0.00032936118077486753\n",
      "Iteration is: 11560 and loss is: 0.00032915364135988057\n",
      "Iteration is: 11561 and loss is: 0.00032894668402150273\n",
      "Iteration is: 11562 and loss is: 0.0003287445870228112\n",
      "Iteration is: 11563 and loss is: 0.0003285433631390333\n",
      "Iteration is: 11564 and loss is: 0.0003283448750153184\n",
      "Iteration is: 11565 and loss is: 0.00032814720179885626\n",
      "Iteration is: 11566 and loss is: 0.00032795313745737076\n",
      "Iteration is: 11567 and loss is: 0.0003277595096733421\n",
      "Iteration is: 11568 and loss is: 0.0003275701019447297\n",
      "Iteration is: 11569 and loss is: 0.0003273822949267924\n",
      "Iteration is: 11570 and loss is: 0.00032719477894715965\n",
      "Iteration is: 11571 and loss is: 0.00032700865995138884\n",
      "Iteration is: 11572 and loss is: 0.0003268247237429023\n",
      "Iteration is: 11573 and loss is: 0.0003266447747591883\n",
      "Iteration is: 11574 and loss is: 0.00032646654290147126\n",
      "Iteration is: 11575 and loss is: 0.00032628895132802427\n",
      "Iteration is: 11576 and loss is: 0.0003261114179622382\n",
      "Iteration is: 11577 and loss is: 0.00032593656214885414\n",
      "Iteration is: 11578 and loss is: 0.0003257638600189239\n",
      "Iteration is: 11579 and loss is: 0.0003255957562942058\n",
      "Iteration is: 11580 and loss is: 0.0003254236071370542\n",
      "Iteration is: 11581 and loss is: 0.0003252584137953818\n",
      "Iteration is: 11582 and loss is: 0.0003250914451200515\n",
      "Iteration is: 11583 and loss is: 0.00032492494210600853\n",
      "Iteration is: 11584 and loss is: 0.0003247607091907412\n",
      "Iteration is: 11585 and loss is: 0.0003245975240133703\n",
      "Iteration is: 11586 and loss is: 0.0003244374820496887\n",
      "Iteration is: 11587 and loss is: 0.00032427971018478274\n",
      "Iteration is: 11588 and loss is: 0.0003241209778934717\n",
      "Iteration is: 11589 and loss is: 0.0003239609650336206\n",
      "Iteration is: 11590 and loss is: 0.00032380782067775726\n",
      "Iteration is: 11591 and loss is: 0.00032365196966566145\n",
      "Iteration is: 11592 and loss is: 0.0003234989126212895\n",
      "Iteration is: 11593 and loss is: 0.00032334798015654087\n",
      "Iteration is: 11594 and loss is: 0.0003231952141504735\n",
      "Iteration is: 11595 and loss is: 0.0003230463189538568\n",
      "Iteration is: 11596 and loss is: 0.0003228976856917143\n",
      "Iteration is: 11597 and loss is: 0.0003227506240364164\n",
      "Iteration is: 11598 and loss is: 0.0003226020489819348\n",
      "Iteration is: 11599 and loss is: 0.0003224584215786308\n",
      "Iteration is: 11600 and loss is: 0.0003223132516723126\n",
      "Iteration is: 11601 and loss is: 0.00032216962426900864\n",
      "Iteration is: 11602 and loss is: 0.0003220263752155006\n",
      "Iteration is: 11603 and loss is: 0.000321884173899889\n",
      "Iteration is: 11604 and loss is: 0.00032174226362258196\n",
      "Iteration is: 11605 and loss is: 0.0003216044860891998\n",
      "Iteration is: 11606 and loss is: 0.0003214624885004014\n",
      "Iteration is: 11607 and loss is: 0.0003213253221474588\n",
      "Iteration is: 11608 and loss is: 0.00032118894159793854\n",
      "Iteration is: 11609 and loss is: 0.00032105357968248427\n",
      "Iteration is: 11610 and loss is: 0.00032091879984363914\n",
      "Iteration is: 11611 and loss is: 0.00032078003278002143\n",
      "Iteration is: 11612 and loss is: 0.00032064717379398644\n",
      "Iteration is: 11613 and loss is: 0.000320515624480322\n",
      "Iteration is: 11614 and loss is: 0.0003203819505870342\n",
      "Iteration is: 11615 and loss is: 0.0003202516818419099\n",
      "Iteration is: 11616 and loss is: 0.000320120103424415\n",
      "Iteration is: 11617 and loss is: 0.0003199904749635607\n",
      "Iteration is: 11618 and loss is: 0.0003198611084371805\n",
      "Iteration is: 11619 and loss is: 0.0003197334590367973\n",
      "Iteration is: 11620 and loss is: 0.000319604849210009\n",
      "Iteration is: 11621 and loss is: 0.000319475686410442\n",
      "Iteration is: 11622 and loss is: 0.0003193503653164953\n",
      "Iteration is: 11623 and loss is: 0.0003192245203536004\n",
      "Iteration is: 11624 and loss is: 0.0003190985880792141\n",
      "Iteration is: 11625 and loss is: 0.00031897437293082476\n",
      "Iteration is: 11626 and loss is: 0.00031885108910501003\n",
      "Iteration is: 11627 and loss is: 0.00031872832914814353\n",
      "Iteration is: 11628 and loss is: 0.0003186038520652801\n",
      "Iteration is: 11629 and loss is: 0.00031848138314671814\n",
      "Iteration is: 11630 and loss is: 0.0003183600783813745\n",
      "Iteration is: 11631 and loss is: 0.0003182399086654186\n",
      "Iteration is: 11632 and loss is: 0.0003181201172992587\n",
      "Iteration is: 11633 and loss is: 0.00031799846328794956\n",
      "Iteration is: 11634 and loss is: 0.00031787861371412873\n",
      "Iteration is: 11635 and loss is: 0.00031775946263223886\n",
      "Iteration is: 11636 and loss is: 0.00031764080631546676\n",
      "Iteration is: 11637 and loss is: 0.0003175226738676429\n",
      "Iteration is: 11638 and loss is: 0.0003174046869389713\n",
      "Iteration is: 11639 and loss is: 0.00031729001784697175\n",
      "Iteration is: 11640 and loss is: 0.00031717141973786056\n",
      "Iteration is: 11641 and loss is: 0.00031705613946542144\n",
      "Iteration is: 11642 and loss is: 0.00031694158678874373\n",
      "Iteration is: 11643 and loss is: 0.00031682520057074726\n",
      "Iteration is: 11644 and loss is: 0.0003167122195009142\n",
      "Iteration is: 11645 and loss is: 0.0003165975504089147\n",
      "Iteration is: 11646 and loss is: 0.0003164809022564441\n",
      "Iteration is: 11647 and loss is: 0.0003163715882692486\n",
      "Iteration is: 11648 and loss is: 0.00031625881092622876\n",
      "Iteration is: 11649 and loss is: 0.00031614548061043024\n",
      "Iteration is: 11650 and loss is: 0.00031603212119080126\n",
      "Iteration is: 11651 and loss is: 0.0003159226616844535\n",
      "Iteration is: 11652 and loss is: 0.00031581020448356867\n",
      "Iteration is: 11653 and loss is: 0.00031570220016874373\n",
      "Iteration is: 11654 and loss is: 0.00031559099443256855\n",
      "Iteration is: 11655 and loss is: 0.0003154809819534421\n",
      "Iteration is: 11656 and loss is: 0.0003153677098453045\n",
      "Iteration is: 11657 and loss is: 0.00031526191742159426\n",
      "Iteration is: 11658 and loss is: 0.00031515181763097644\n",
      "Iteration is: 11659 and loss is: 0.0003150461125187576\n",
      "Iteration is: 11660 and loss is: 0.00031493642018176615\n",
      "Iteration is: 11661 and loss is: 0.000314827891997993\n",
      "Iteration is: 11662 and loss is: 0.00031472230330109596\n",
      "Iteration is: 11663 and loss is: 0.00031461400794796646\n",
      "Iteration is: 11664 and loss is: 0.00031450827373191714\n",
      "Iteration is: 11665 and loss is: 0.0003144020738545805\n",
      "Iteration is: 11666 and loss is: 0.0003142951463814825\n",
      "Iteration is: 11667 and loss is: 0.00031419191509485245\n",
      "Iteration is: 11668 and loss is: 0.0003140846674796194\n",
      "Iteration is: 11669 and loss is: 0.0003139817272312939\n",
      "Iteration is: 11670 and loss is: 0.00031387642957270145\n",
      "Iteration is: 11671 and loss is: 0.00031377089908346534\n",
      "Iteration is: 11672 and loss is: 0.00031366600887849927\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 11673 and loss is: 0.0003135627193842083\n",
      "Iteration is: 11674 and loss is: 0.00031346053583547473\n",
      "Iteration is: 11675 and loss is: 0.00031335753737948835\n",
      "Iteration is: 11676 and loss is: 0.0003132568672299385\n",
      "Iteration is: 11677 and loss is: 0.0003131526173092425\n",
      "Iteration is: 11678 and loss is: 0.00031305127777159214\n",
      "Iteration is: 11679 and loss is: 0.0003129482502117753\n",
      "Iteration is: 11680 and loss is: 0.00031284536817111075\n",
      "Iteration is: 11681 and loss is: 0.00031274621142074466\n",
      "Iteration is: 11682 and loss is: 0.0003126448718830943\n",
      "Iteration is: 11683 and loss is: 0.0003125432413071394\n",
      "Iteration is: 11684 and loss is: 0.00031244251294992864\n",
      "Iteration is: 11685 and loss is: 0.00031234329799190164\n",
      "Iteration is: 11686 and loss is: 0.0003122434136457741\n",
      "Iteration is: 11687 and loss is: 0.0003121434710919857\n",
      "Iteration is: 11688 and loss is: 0.0003120439941994846\n",
      "Iteration is: 11689 and loss is: 0.0003119425382465124\n",
      "Iteration is: 11690 and loss is: 0.0003118454769719392\n",
      "Iteration is: 11691 and loss is: 0.0003117474843747914\n",
      "Iteration is: 11692 and loss is: 0.00031165056861937046\n",
      "Iteration is: 11693 and loss is: 0.0003115509753115475\n",
      "Iteration is: 11694 and loss is: 0.00031145394314080477\n",
      "Iteration is: 11695 and loss is: 0.000311354553559795\n",
      "Iteration is: 11696 and loss is: 0.00031125685200095177\n",
      "Iteration is: 11697 and loss is: 0.0003111603145953268\n",
      "Iteration is: 11698 and loss is: 0.00031106246751733124\n",
      "Iteration is: 11699 and loss is: 0.000310966424876824\n",
      "Iteration is: 11700 and loss is: 0.0003108681121375412\n",
      "Iteration is: 11701 and loss is: 0.0003107752127107233\n",
      "Iteration is: 11702 and loss is: 0.0003106780059169978\n",
      "Iteration is: 11703 and loss is: 0.00031058277818374336\n",
      "Iteration is: 11704 and loss is: 0.00031048664823174477\n",
      "Iteration is: 11705 and loss is: 0.00031039080931805074\n",
      "Iteration is: 11706 and loss is: 0.0003102960763499141\n",
      "Iteration is: 11707 and loss is: 0.0003102014306932688\n",
      "Iteration is: 11708 and loss is: 0.0003101069887634367\n",
      "Iteration is: 11709 and loss is: 0.0003100118483416736\n",
      "Iteration is: 11710 and loss is: 0.00030991557287052274\n",
      "Iteration is: 11711 and loss is: 0.00030982407042756677\n",
      "Iteration is: 11712 and loss is: 0.00030972762033343315\n",
      "Iteration is: 11713 and loss is: 0.00030963440076448023\n",
      "Iteration is: 11714 and loss is: 0.00030954310204833746\n",
      "Iteration is: 11715 and loss is: 0.00030944825266487896\n",
      "Iteration is: 11716 and loss is: 0.0003093551495112479\n",
      "Iteration is: 11717 and loss is: 0.0003092631231993437\n",
      "Iteration is: 11718 and loss is: 0.00030916856485418975\n",
      "Iteration is: 11719 and loss is: 0.00030907714972272515\n",
      "Iteration is: 11720 and loss is: 0.00030898404656909406\n",
      "Iteration is: 11721 and loss is: 0.0003088932135142386\n",
      "Iteration is: 11722 and loss is: 0.0003087990335188806\n",
      "Iteration is: 11723 and loss is: 0.00030870793852955103\n",
      "Iteration is: 11724 and loss is: 0.00030861611594446003\n",
      "Iteration is: 11725 and loss is: 0.0003085233038291335\n",
      "Iteration is: 11726 and loss is: 0.0003084329655393958\n",
      "Iteration is: 11727 and loss is: 0.00030834120116196573\n",
      "Iteration is: 11728 and loss is: 0.0003082498151343316\n",
      "Iteration is: 11729 and loss is: 0.0003081605245824903\n",
      "Iteration is: 11730 and loss is: 0.0003080684400629252\n",
      "Iteration is: 11731 and loss is: 0.0003079768503084779\n",
      "Iteration is: 11732 and loss is: 0.0003078887821175158\n",
      "Iteration is: 11733 and loss is: 0.0003077969013247639\n",
      "Iteration is: 11734 and loss is: 0.00030770848388783634\n",
      "Iteration is: 11735 and loss is: 0.00030761887319386005\n",
      "Iteration is: 11736 and loss is: 0.0003075295826420188\n",
      "Iteration is: 11737 and loss is: 0.00030743860406801105\n",
      "Iteration is: 11738 and loss is: 0.0003073488478548825\n",
      "Iteration is: 11739 and loss is: 0.00030726028489880264\n",
      "Iteration is: 11740 and loss is: 0.0003071713144890964\n",
      "Iteration is: 11741 and loss is: 0.0003070821985602379\n",
      "Iteration is: 11742 and loss is: 0.00030699055059812963\n",
      "Iteration is: 11743 and loss is: 0.0003069023077841848\n",
      "Iteration is: 11744 and loss is: 0.0003068152582272887\n",
      "Iteration is: 11745 and loss is: 0.0003067271027248353\n",
      "Iteration is: 11746 and loss is: 0.0003066374338231981\n",
      "Iteration is: 11747 and loss is: 0.00030655114096589386\n",
      "Iteration is: 11748 and loss is: 0.0003064603661186993\n",
      "Iteration is: 11749 and loss is: 0.0003063733456656337\n",
      "Iteration is: 11750 and loss is: 0.00030628431704826653\n",
      "Iteration is: 11751 and loss is: 0.00030619866447523236\n",
      "Iteration is: 11752 and loss is: 0.000306110770907253\n",
      "Iteration is: 11753 and loss is: 0.00030602322658523917\n",
      "Iteration is: 11754 and loss is: 0.0003059367008972913\n",
      "Iteration is: 11755 and loss is: 0.0003058497095480561\n",
      "Iteration is: 11756 and loss is: 0.0003057618741877377\n",
      "Iteration is: 11757 and loss is: 0.0003056744462810457\n",
      "Iteration is: 11758 and loss is: 0.0003055892593692988\n",
      "Iteration is: 11759 and loss is: 0.0003055022389162332\n",
      "Iteration is: 11760 and loss is: 0.0003054139669984579\n",
      "Iteration is: 11761 and loss is: 0.00030533073004335165\n",
      "Iteration is: 11762 and loss is: 0.0003052430402021855\n",
      "Iteration is: 11763 and loss is: 0.00030515645630657673\n",
      "Iteration is: 11764 and loss is: 0.000305071531329304\n",
      "Iteration is: 11765 and loss is: 0.00030498570413328707\n",
      "Iteration is: 11766 and loss is: 0.00030490008066408336\n",
      "Iteration is: 11767 and loss is: 0.0003048148355446756\n",
      "Iteration is: 11768 and loss is: 0.00030473017250187695\n",
      "Iteration is: 11769 and loss is: 0.0003046433557756245\n",
      "Iteration is: 11770 and loss is: 0.0003045579942408949\n",
      "Iteration is: 11771 and loss is: 0.00030447368044406176\n",
      "Iteration is: 11772 and loss is: 0.0003043877659365535\n",
      "Iteration is: 11773 and loss is: 0.0003043052274733782\n",
      "Iteration is: 11774 and loss is: 0.0003042187599930912\n",
      "Iteration is: 11775 and loss is: 0.00030413561034947634\n",
      "Iteration is: 11776 and loss is: 0.0003040481242351234\n",
      "Iteration is: 11777 and loss is: 0.00030396648799069226\n",
      "Iteration is: 11778 and loss is: 0.0003038807772099972\n",
      "Iteration is: 11779 and loss is: 0.00030379524105228484\n",
      "Iteration is: 11780 and loss is: 0.0003037096466869116\n",
      "Iteration is: 11781 and loss is: 0.00030362667166627944\n",
      "Iteration is: 11782 and loss is: 0.00030354392947629094\n",
      "Iteration is: 11783 and loss is: 0.0003034593537449837\n",
      "Iteration is: 11784 and loss is: 0.000303376727970317\n",
      "Iteration is: 11785 and loss is: 0.0003032903769053519\n",
      "Iteration is: 11786 and loss is: 0.000303209584672004\n",
      "Iteration is: 11787 and loss is: 0.000303124834317714\n",
      "Iteration is: 11788 and loss is: 0.00030304177198559046\n",
      "Iteration is: 11789 and loss is: 0.000302960688713938\n",
      "Iteration is: 11790 and loss is: 0.00030287489062175155\n",
      "Iteration is: 11791 and loss is: 0.00030279220663942397\n",
      "Iteration is: 11792 and loss is: 0.0003027109196409583\n",
      "Iteration is: 11793 and loss is: 0.00030262654763646424\n",
      "Iteration is: 11794 and loss is: 0.00030254386365413666\n",
      "Iteration is: 11795 and loss is: 0.00030246295500546694\n",
      "Iteration is: 11796 and loss is: 0.00030237852479331195\n",
      "Iteration is: 11797 and loss is: 0.0003022978489752859\n",
      "Iteration is: 11798 and loss is: 0.00030221298220567405\n",
      "Iteration is: 11799 and loss is: 0.00030213029822334647\n",
      "Iteration is: 11800 and loss is: 0.00030204906943254173\n",
      "Iteration is: 11801 and loss is: 0.00030197080923244357\n",
      "Iteration is: 11802 and loss is: 0.0003018862335011363\n",
      "Iteration is: 11803 and loss is: 0.0003018051793333143\n",
      "Iteration is: 11804 and loss is: 0.0003017220296896994\n",
      "Iteration is: 11805 and loss is: 0.00030163946212269366\n",
      "Iteration is: 11806 and loss is: 0.0003015586989931762\n",
      "Iteration is: 11807 and loss is: 0.00030147493816912174\n",
      "Iteration is: 11808 and loss is: 0.00030139650334604084\n",
      "Iteration is: 11809 and loss is: 0.0003013136447407305\n",
      "Iteration is: 11810 and loss is: 0.00030123250326141715\n",
      "Iteration is: 11811 and loss is: 0.0003011514199897647\n",
      "Iteration is: 11812 and loss is: 0.0003010686195921153\n",
      "Iteration is: 11813 and loss is: 0.00030098806018941104\n",
      "Iteration is: 11814 and loss is: 0.0003009102074429393\n",
      "Iteration is: 11815 and loss is: 0.0003008281928487122\n",
      "Iteration is: 11816 and loss is: 0.0003007467894349247\n",
      "Iteration is: 11817 and loss is: 0.00030066477484069765\n",
      "Iteration is: 11818 and loss is: 0.0003005846810992807\n",
      "Iteration is: 11819 and loss is: 0.00030050307395868003\n",
      "Iteration is: 11820 and loss is: 0.0003004219033755362\n",
      "Iteration is: 11821 and loss is: 0.0003003414603881538\n",
      "Iteration is: 11822 and loss is: 0.0003002611920237541\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 11823 and loss is: 0.0003001814766321331\n",
      "Iteration is: 11824 and loss is: 0.0003001013246830553\n",
      "Iteration is: 11825 and loss is: 0.000300021463772282\n",
      "Iteration is: 11826 and loss is: 0.0002999411663040519\n",
      "Iteration is: 11827 and loss is: 0.000299860694212839\n",
      "Iteration is: 11828 and loss is: 0.00029978103702887893\n",
      "Iteration is: 11829 and loss is: 0.00029970071045681834\n",
      "Iteration is: 11830 and loss is: 0.0002996200928464532\n",
      "Iteration is: 11831 and loss is: 0.00029954151250422\n",
      "Iteration is: 11832 and loss is: 0.00029946150607429445\n",
      "Iteration is: 11833 and loss is: 0.00029938307125121355\n",
      "Iteration is: 11834 and loss is: 0.0002993029193021357\n",
      "Iteration is: 11835 and loss is: 0.00029922372777946293\n",
      "Iteration is: 11836 and loss is: 0.0002991432265844196\n",
      "Iteration is: 11837 and loss is: 0.00029906287090852857\n",
      "Iteration is: 11838 and loss is: 0.00029898423235863447\n",
      "Iteration is: 11839 and loss is: 0.00029890425503253937\n",
      "Iteration is: 11840 and loss is: 0.0002988255873788148\n",
      "Iteration is: 11841 and loss is: 0.0002987467451021075\n",
      "Iteration is: 11842 and loss is: 0.0002986677282024175\n",
      "Iteration is: 11843 and loss is: 0.00029858824564144015\n",
      "Iteration is: 11844 and loss is: 0.00029850949067622423\n",
      "Iteration is: 11845 and loss is: 0.00029843050288036466\n",
      "Iteration is: 11846 and loss is: 0.0002983511658385396\n",
      "Iteration is: 11847 and loss is: 0.00029827392427250743\n",
      "Iteration is: 11848 and loss is: 0.00029819374321959913\n",
      "Iteration is: 11849 and loss is: 0.0002981179568450898\n",
      "Iteration is: 11850 and loss is: 0.0002980356803163886\n",
      "Iteration is: 11851 and loss is: 0.000297959748422727\n",
      "Iteration is: 11852 and loss is: 0.00029788154643028975\n",
      "Iteration is: 11853 and loss is: 0.0002978018601424992\n",
      "Iteration is: 11854 and loss is: 0.0002977243857458234\n",
      "Iteration is: 11855 and loss is: 0.0002976462128572166\n",
      "Iteration is: 11856 and loss is: 0.00029756693402305245\n",
      "Iteration is: 11857 and loss is: 0.00029748823726549745\n",
      "Iteration is: 11858 and loss is: 0.0002974116359837353\n",
      "Iteration is: 11859 and loss is: 0.0002973348891828209\n",
      "Iteration is: 11860 and loss is: 0.00029725575586780906\n",
      "Iteration is: 11861 and loss is: 0.00029717839788645506\n",
      "Iteration is: 11862 and loss is: 0.0002971008070744574\n",
      "Iteration is: 11863 and loss is: 0.0002970235364045948\n",
      "Iteration is: 11864 and loss is: 0.000296944344881922\n",
      "Iteration is: 11865 and loss is: 0.0002968698681797832\n",
      "Iteration is: 11866 and loss is: 0.00029678959981538355\n",
      "Iteration is: 11867 and loss is: 0.00029671404627151787\n",
      "Iteration is: 11868 and loss is: 0.00029663846362382174\n",
      "Iteration is: 11869 and loss is: 0.0002965638996101916\n",
      "Iteration is: 11870 and loss is: 0.0002964898303616792\n",
      "Iteration is: 11871 and loss is: 0.0002964178565889597\n",
      "Iteration is: 11872 and loss is: 0.0002963499864563346\n",
      "Iteration is: 11873 and loss is: 0.0002962847938761115\n",
      "Iteration is: 11874 and loss is: 0.0002962284197565168\n",
      "Iteration is: 11875 and loss is: 0.0002961806603707373\n",
      "Iteration is: 11876 and loss is: 0.00029615190578624606\n",
      "Iteration is: 11877 and loss is: 0.00029615324456244707\n",
      "Iteration is: 11878 and loss is: 0.0002961956197395921\n",
      "Iteration is: 11879 and loss is: 0.0002963162260130048\n",
      "Iteration is: 11880 and loss is: 0.000296553538646549\n",
      "Iteration is: 11881 and loss is: 0.0002969884080812335\n",
      "Iteration is: 11882 and loss is: 0.00029775392613373697\n",
      "Iteration is: 11883 and loss is: 0.0002990576613228768\n",
      "Iteration is: 11884 and loss is: 0.00030128401704132557\n",
      "Iteration is: 11885 and loss is: 0.0003050172235816717\n",
      "Iteration is: 11886 and loss is: 0.0003114081046078354\n",
      "Iteration is: 11887 and loss is: 0.000322047621011734\n",
      "Iteration is: 11888 and loss is: 0.00034040038008242846\n",
      "Iteration is: 11889 and loss is: 0.0003705636481754482\n",
      "Iteration is: 11890 and loss is: 0.00042271814891137183\n",
      "Iteration is: 11891 and loss is: 0.0005047323065809906\n",
      "Iteration is: 11892 and loss is: 0.0006427258485928178\n",
      "Iteration is: 11893 and loss is: 0.0008308857213705778\n",
      "Iteration is: 11894 and loss is: 0.0011056131916120648\n",
      "Iteration is: 11895 and loss is: 0.0013182894326746464\n",
      "Iteration is: 11896 and loss is: 0.0014269034145399928\n",
      "Iteration is: 11897 and loss is: 0.0010903788497671485\n",
      "Iteration is: 11898 and loss is: 0.0005881026736460626\n",
      "Iteration is: 11899 and loss is: 0.00035167072201147676\n",
      "Iteration is: 11900 and loss is: 0.000591105839703232\n",
      "Iteration is: 11901 and loss is: 0.0007898081094026566\n",
      "Iteration is: 11902 and loss is: 0.0005666863871738315\n",
      "Iteration is: 11903 and loss is: 0.00034362805308774114\n",
      "Iteration is: 11904 and loss is: 0.00043866122723557055\n",
      "Iteration is: 11905 and loss is: 0.0006217178306542337\n",
      "Iteration is: 11906 and loss is: 0.0006307156290858984\n",
      "Iteration is: 11907 and loss is: 0.0004637403762899339\n",
      "Iteration is: 11908 and loss is: 0.0003284321282990277\n",
      "Iteration is: 11909 and loss is: 0.0003217895282432437\n",
      "Iteration is: 11910 and loss is: 0.000422680372139439\n",
      "Iteration is: 11911 and loss is: 0.0005718128522858024\n",
      "Iteration is: 11912 and loss is: 0.0007139666704460979\n",
      "Iteration is: 11913 and loss is: 0.0008498546085320413\n",
      "Iteration is: 11914 and loss is: 0.0009019176359288394\n",
      "Iteration is: 11915 and loss is: 0.0008526936871930957\n",
      "Iteration is: 11916 and loss is: 0.0006223912932910025\n",
      "Iteration is: 11917 and loss is: 0.00038375650183297694\n",
      "Iteration is: 11918 and loss is: 0.00032624375307932496\n",
      "Iteration is: 11919 and loss is: 0.00045610597589984536\n",
      "Iteration is: 11920 and loss is: 0.0005416111089289188\n",
      "Iteration is: 11921 and loss is: 0.00044335119309835136\n",
      "Iteration is: 11922 and loss is: 0.00033247220562770963\n",
      "Iteration is: 11923 and loss is: 0.00036181695759296417\n",
      "Iteration is: 11924 and loss is: 0.00044568092562258244\n",
      "Iteration is: 11925 and loss is: 0.00044006630196236074\n",
      "Iteration is: 11926 and loss is: 0.0003552387352101505\n",
      "Iteration is: 11927 and loss is: 0.00031327176839113235\n",
      "Iteration is: 11928 and loss is: 0.000349504582118243\n",
      "Iteration is: 11929 and loss is: 0.00040247105062007904\n",
      "Iteration is: 11930 and loss is: 0.0004178463132120669\n",
      "Iteration is: 11931 and loss is: 0.00038928294088691473\n",
      "Iteration is: 11932 and loss is: 0.0003488380752969533\n",
      "Iteration is: 11933 and loss is: 0.0003174488083459437\n",
      "Iteration is: 11934 and loss is: 0.0003039193688891828\n",
      "Iteration is: 11935 and loss is: 0.00030541859450750053\n",
      "Iteration is: 11936 and loss is: 0.0003178984625265002\n",
      "Iteration is: 11937 and loss is: 0.00033751659793779254\n",
      "Iteration is: 11938 and loss is: 0.0003595890593715012\n",
      "Iteration is: 11939 and loss is: 0.00038029986899346113\n",
      "Iteration is: 11940 and loss is: 0.0003896921407431364\n",
      "Iteration is: 11941 and loss is: 0.00038496963679790497\n",
      "Iteration is: 11942 and loss is: 0.0003627621044870466\n",
      "Iteration is: 11943 and loss is: 0.00033412614720873535\n",
      "Iteration is: 11944 and loss is: 0.0003095332649536431\n",
      "Iteration is: 11945 and loss is: 0.000298700004350394\n",
      "Iteration is: 11946 and loss is: 0.00030231167329475284\n",
      "Iteration is: 11947 and loss is: 0.00031524323276244104\n",
      "Iteration is: 11948 and loss is: 0.00033239510958082974\n",
      "Iteration is: 11949 and loss is: 0.00035053043393418193\n",
      "Iteration is: 11950 and loss is: 0.00037151528522372246\n",
      "Iteration is: 11951 and loss is: 0.0003956692526116967\n",
      "Iteration is: 11952 and loss is: 0.0004297934065107256\n",
      "Iteration is: 11953 and loss is: 0.00047081633238121867\n",
      "Iteration is: 11954 and loss is: 0.0005254826974123716\n",
      "Iteration is: 11955 and loss is: 0.0005719150649383664\n",
      "Iteration is: 11956 and loss is: 0.0006095921853557229\n",
      "Iteration is: 11957 and loss is: 0.000592628144659102\n",
      "Iteration is: 11958 and loss is: 0.0005336229223757982\n",
      "Iteration is: 11959 and loss is: 0.00043044734047725797\n",
      "Iteration is: 11960 and loss is: 0.000341059232596308\n",
      "Iteration is: 11961 and loss is: 0.0003025338228326291\n",
      "Iteration is: 11962 and loss is: 0.000322677253279835\n",
      "Iteration is: 11963 and loss is: 0.0003754463978111744\n",
      "Iteration is: 11964 and loss is: 0.0004277495900169015\n",
      "Iteration is: 11965 and loss is: 0.00047083149547688663\n",
      "Iteration is: 11966 and loss is: 0.0004978671204298735\n",
      "Iteration is: 11967 and loss is: 0.0005284395883791149\n",
      "Iteration is: 11968 and loss is: 0.0005546780303120613\n",
      "Iteration is: 11969 and loss is: 0.0005931657506152987\n",
      "Iteration is: 11970 and loss is: 0.000614927732385695\n",
      "Iteration is: 11971 and loss is: 0.0006256821216084063\n",
      "Iteration is: 11972 and loss is: 0.0005786544643342495\n",
      "Iteration is: 11973 and loss is: 0.0004952809540554881\n",
      "Iteration is: 11974 and loss is: 0.0003876137488987297\n",
      "Iteration is: 11975 and loss is: 0.0003151439595967531\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 11976 and loss is: 0.0003064756456296891\n",
      "Iteration is: 11977 and loss is: 0.00034922160557471216\n",
      "Iteration is: 11978 and loss is: 0.0004053573065903038\n",
      "Iteration is: 11979 and loss is: 0.00044026703108102083\n",
      "Iteration is: 11980 and loss is: 0.0004533097380772233\n",
      "Iteration is: 11981 and loss is: 0.0004442272474989295\n",
      "Iteration is: 11982 and loss is: 0.00043240468949079514\n",
      "Iteration is: 11983 and loss is: 0.00041729662916623056\n",
      "Iteration is: 11984 and loss is: 0.0004083482490386814\n",
      "Iteration is: 11985 and loss is: 0.00040000019362196326\n",
      "Iteration is: 11986 and loss is: 0.00039591858512721956\n",
      "Iteration is: 11987 and loss is: 0.00038918614154681563\n",
      "Iteration is: 11988 and loss is: 0.00038217907422222197\n",
      "Iteration is: 11989 and loss is: 0.00036947435000911355\n",
      "Iteration is: 11990 and loss is: 0.0003547650121618062\n",
      "Iteration is: 11991 and loss is: 0.0003371749189682305\n",
      "Iteration is: 11992 and loss is: 0.000321116327540949\n",
      "Iteration is: 11993 and loss is: 0.0003076516150031239\n",
      "Iteration is: 11994 and loss is: 0.00029835448367521167\n",
      "Iteration is: 11995 and loss is: 0.00029288537916727364\n",
      "Iteration is: 11996 and loss is: 0.0002905122237280011\n",
      "Iteration is: 11997 and loss is: 0.0002902699925471097\n",
      "Iteration is: 11998 and loss is: 0.000291504489723593\n",
      "Iteration is: 11999 and loss is: 0.00029410223942250013\n",
      "Iteration is: 12000 and loss is: 0.0002985214814543724\n",
      "Iteration is: 12001 and loss is: 0.00030599458841606975\n",
      "Iteration is: 12002 and loss is: 0.00031828336068429053\n",
      "Iteration is: 12003 and loss is: 0.0003388927725609392\n",
      "Iteration is: 12004 and loss is: 0.0003707946452777833\n",
      "Iteration is: 12005 and loss is: 0.00042192733963020146\n",
      "Iteration is: 12006 and loss is: 0.0004932858282700181\n",
      "Iteration is: 12007 and loss is: 0.0006008418276906013\n",
      "Iteration is: 12008 and loss is: 0.000724715762771666\n",
      "Iteration is: 12009 and loss is: 0.0008838632493279874\n",
      "Iteration is: 12010 and loss is: 0.0009774246718734503\n",
      "Iteration is: 12011 and loss is: 0.0010055098682641983\n",
      "Iteration is: 12012 and loss is: 0.0008193376706913114\n",
      "Iteration is: 12013 and loss is: 0.000547829142306\n",
      "Iteration is: 12014 and loss is: 0.0003393766819499433\n",
      "Iteration is: 12015 and loss is: 0.0003586684470064938\n",
      "Iteration is: 12016 and loss is: 0.0005157549167051911\n",
      "Iteration is: 12017 and loss is: 0.000580198597162962\n",
      "Iteration is: 12018 and loss is: 0.000491526210680604\n",
      "Iteration is: 12019 and loss is: 0.0003515366988722235\n",
      "Iteration is: 12020 and loss is: 0.0003119929460808635\n",
      "Iteration is: 12021 and loss is: 0.000381986319553107\n",
      "Iteration is: 12022 and loss is: 0.0004702894948422909\n",
      "Iteration is: 12023 and loss is: 0.0005139884306117892\n",
      "Iteration is: 12024 and loss is: 0.0004933260497637093\n",
      "Iteration is: 12025 and loss is: 0.0004489021375775337\n",
      "Iteration is: 12026 and loss is: 0.0003965187934227288\n",
      "Iteration is: 12027 and loss is: 0.00035663062590174377\n",
      "Iteration is: 12028 and loss is: 0.00032810601987876\n",
      "Iteration is: 12029 and loss is: 0.00031014083651825786\n",
      "Iteration is: 12030 and loss is: 0.0002994572860188782\n",
      "Iteration is: 12031 and loss is: 0.0002938732213806361\n",
      "Iteration is: 12032 and loss is: 0.00029163615545257926\n",
      "Iteration is: 12033 and loss is: 0.00029214128153398633\n",
      "Iteration is: 12034 and loss is: 0.00029472849564626813\n",
      "Iteration is: 12035 and loss is: 0.0002984072780236602\n",
      "Iteration is: 12036 and loss is: 0.00030272910953499377\n",
      "Iteration is: 12037 and loss is: 0.00030739596695639193\n",
      "Iteration is: 12038 and loss is: 0.0003125328221358359\n",
      "Iteration is: 12039 and loss is: 0.0003181852225679904\n",
      "Iteration is: 12040 and loss is: 0.00032612698851153255\n",
      "Iteration is: 12041 and loss is: 0.0003371948841959238\n",
      "Iteration is: 12042 and loss is: 0.000355318421497941\n",
      "Iteration is: 12043 and loss is: 0.0003830079804174602\n",
      "Iteration is: 12044 and loss is: 0.0004306191694922745\n",
      "Iteration is: 12045 and loss is: 0.0005024755373597145\n",
      "Iteration is: 12046 and loss is: 0.0006226613186299801\n",
      "Iteration is: 12047 and loss is: 0.0007784916087985039\n",
      "Iteration is: 12048 and loss is: 0.0009976213332265615\n",
      "Iteration is: 12049 and loss is: 0.0011452181497588754\n",
      "Iteration is: 12050 and loss is: 0.001193780335597694\n",
      "Iteration is: 12051 and loss is: 0.0009102616459131241\n",
      "Iteration is: 12052 and loss is: 0.0005237850127741694\n",
      "Iteration is: 12053 and loss is: 0.00033201591577380896\n",
      "Iteration is: 12054 and loss is: 0.0004894369631074369\n",
      "Iteration is: 12055 and loss is: 0.0006795129738748074\n",
      "Iteration is: 12056 and loss is: 0.0005842308164574206\n",
      "Iteration is: 12057 and loss is: 0.00037594075547531247\n",
      "Iteration is: 12058 and loss is: 0.0003229128196835518\n",
      "Iteration is: 12059 and loss is: 0.00044615514343604445\n",
      "Iteration is: 12060 and loss is: 0.0005798633210361004\n",
      "Iteration is: 12061 and loss is: 0.0005922990385442972\n",
      "Iteration is: 12062 and loss is: 0.0005218962323851883\n",
      "Iteration is: 12063 and loss is: 0.00041824905201792717\n",
      "Iteration is: 12064 and loss is: 0.0003392871003597975\n",
      "Iteration is: 12065 and loss is: 0.0003000519354827702\n",
      "Iteration is: 12066 and loss is: 0.00029978438396938145\n",
      "Iteration is: 12067 and loss is: 0.0003265792038291693\n",
      "Iteration is: 12068 and loss is: 0.0003748855961021036\n",
      "Iteration is: 12069 and loss is: 0.000446784048108384\n",
      "Iteration is: 12070 and loss is: 0.0005356652545742691\n",
      "Iteration is: 12071 and loss is: 0.0006406446918845177\n",
      "Iteration is: 12072 and loss is: 0.0007053274312056601\n",
      "Iteration is: 12073 and loss is: 0.0007113796309567988\n",
      "Iteration is: 12074 and loss is: 0.000594057550188154\n",
      "Iteration is: 12075 and loss is: 0.0004315690021030605\n",
      "Iteration is: 12076 and loss is: 0.0003181550418958068\n",
      "Iteration is: 12077 and loss is: 0.0003313565975986421\n",
      "Iteration is: 12078 and loss is: 0.0004214392101857811\n",
      "Iteration is: 12079 and loss is: 0.0004708708147518337\n",
      "Iteration is: 12080 and loss is: 0.0004344872140791267\n",
      "Iteration is: 12081 and loss is: 0.000350460089975968\n",
      "Iteration is: 12082 and loss is: 0.000302832864690572\n",
      "Iteration is: 12083 and loss is: 0.0003188739065080881\n",
      "Iteration is: 12084 and loss is: 0.00036881142295897007\n",
      "Iteration is: 12085 and loss is: 0.0004170652537140995\n",
      "Iteration is: 12086 and loss is: 0.00044513243483379483\n",
      "Iteration is: 12087 and loss is: 0.0004663883300963789\n",
      "Iteration is: 12088 and loss is: 0.00048245597281493247\n",
      "Iteration is: 12089 and loss is: 0.00051485700532794\n",
      "Iteration is: 12090 and loss is: 0.0005578146665357053\n",
      "Iteration is: 12091 and loss is: 0.0006288557779043913\n",
      "Iteration is: 12092 and loss is: 0.0006920176674611866\n",
      "Iteration is: 12093 and loss is: 0.0007465836824849248\n",
      "Iteration is: 12094 and loss is: 0.0007050287676975131\n",
      "Iteration is: 12095 and loss is: 0.0005867286236025393\n",
      "Iteration is: 12096 and loss is: 0.00041296216659247875\n",
      "Iteration is: 12097 and loss is: 0.0003114928840659559\n",
      "Iteration is: 12098 and loss is: 0.00033709860872477293\n",
      "Iteration is: 12099 and loss is: 0.0004276406252756715\n",
      "Iteration is: 12100 and loss is: 0.00047399249160662293\n",
      "Iteration is: 12101 and loss is: 0.00042390363523736596\n",
      "Iteration is: 12102 and loss is: 0.000341900740750134\n",
      "Iteration is: 12103 and loss is: 0.00030054873786866665\n",
      "Iteration is: 12104 and loss is: 0.0003219525679014623\n",
      "Iteration is: 12105 and loss is: 0.0003740654792636633\n",
      "Iteration is: 12106 and loss is: 0.0004186092992313206\n",
      "Iteration is: 12107 and loss is: 0.00044920164509676397\n",
      "Iteration is: 12108 and loss is: 0.0004649835464078933\n",
      "Iteration is: 12109 and loss is: 0.00048790406435728073\n",
      "Iteration is: 12110 and loss is: 0.0005172780947759748\n",
      "Iteration is: 12111 and loss is: 0.0005735806771554053\n",
      "Iteration is: 12112 and loss is: 0.0006388783222064376\n",
      "Iteration is: 12113 and loss is: 0.000724310870282352\n",
      "Iteration is: 12114 and loss is: 0.0007565007545053959\n",
      "Iteration is: 12115 and loss is: 0.0007277934928424656\n",
      "Iteration is: 12116 and loss is: 0.0005752280703745782\n",
      "Iteration is: 12117 and loss is: 0.0004000784538220614\n",
      "Iteration is: 12118 and loss is: 0.00030770263401791453\n",
      "Iteration is: 12119 and loss is: 0.00035278493305668235\n",
      "Iteration is: 12120 and loss is: 0.00045099930139258504\n",
      "Iteration is: 12121 and loss is: 0.00047502602683380246\n",
      "Iteration is: 12122 and loss is: 0.00041030155261978507\n",
      "Iteration is: 12123 and loss is: 0.0003255871415603906\n",
      "Iteration is: 12124 and loss is: 0.0003018403076566756\n",
      "Iteration is: 12125 and loss is: 0.00034193863393738866\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 12126 and loss is: 0.0003973531420342624\n",
      "Iteration is: 12127 and loss is: 0.0004341108724474907\n",
      "Iteration is: 12128 and loss is: 0.00044134745257906616\n",
      "Iteration is: 12129 and loss is: 0.00043949703103862703\n",
      "Iteration is: 12130 and loss is: 0.00043252704199403524\n",
      "Iteration is: 12131 and loss is: 0.0004374232667032629\n",
      "Iteration is: 12132 and loss is: 0.0004515462787821889\n",
      "Iteration is: 12133 and loss is: 0.00048558914568275213\n",
      "Iteration is: 12134 and loss is: 0.0005282219499349594\n",
      "Iteration is: 12135 and loss is: 0.0005872930632904172\n",
      "Iteration is: 12136 and loss is: 0.0006224954267963767\n",
      "Iteration is: 12137 and loss is: 0.0006301497342064977\n",
      "Iteration is: 12138 and loss is: 0.0005608053179457784\n",
      "Iteration is: 12139 and loss is: 0.00045306998072192073\n",
      "Iteration is: 12140 and loss is: 0.00034411795786581933\n",
      "Iteration is: 12141 and loss is: 0.00029910128796473145\n",
      "Iteration is: 12142 and loss is: 0.0003290523309260607\n",
      "Iteration is: 12143 and loss is: 0.00039113560342229903\n",
      "Iteration is: 12144 and loss is: 0.0004342833999544382\n",
      "Iteration is: 12145 and loss is: 0.00042874677455984056\n",
      "Iteration is: 12146 and loss is: 0.00039395433850586414\n",
      "Iteration is: 12147 and loss is: 0.00034809194039553404\n",
      "Iteration is: 12148 and loss is: 0.000312652817228809\n",
      "Iteration is: 12149 and loss is: 0.00029210178763605654\n",
      "Iteration is: 12150 and loss is: 0.0002851199824362993\n",
      "Iteration is: 12151 and loss is: 0.00028722413117066026\n",
      "Iteration is: 12152 and loss is: 0.00029582518618553877\n",
      "Iteration is: 12153 and loss is: 0.00031286050216294825\n",
      "Iteration is: 12154 and loss is: 0.0003451969241723418\n",
      "Iteration is: 12155 and loss is: 0.00040799734415486455\n",
      "Iteration is: 12156 and loss is: 0.0005168052157387137\n",
      "Iteration is: 12157 and loss is: 0.00070423714350909\n",
      "Iteration is: 12158 and loss is: 0.000938437704462558\n",
      "Iteration is: 12159 and loss is: 0.0012151772389188409\n",
      "Iteration is: 12160 and loss is: 0.001260455697774887\n",
      "Iteration is: 12161 and loss is: 0.0010491777211427689\n",
      "Iteration is: 12162 and loss is: 0.0005664871423505247\n",
      "Iteration is: 12163 and loss is: 0.00034465116914361715\n",
      "Iteration is: 12164 and loss is: 0.0005431700265035033\n",
      "Iteration is: 12165 and loss is: 0.0007401874172501266\n",
      "Iteration is: 12166 and loss is: 0.0006288703298196197\n",
      "Iteration is: 12167 and loss is: 0.0003791353083215654\n",
      "Iteration is: 12168 and loss is: 0.0003208257840014994\n",
      "Iteration is: 12169 and loss is: 0.00046957057202234864\n",
      "Iteration is: 12170 and loss is: 0.0006478037685155869\n",
      "Iteration is: 12171 and loss is: 0.0007583816768601537\n",
      "Iteration is: 12172 and loss is: 0.000772348081227392\n",
      "Iteration is: 12173 and loss is: 0.0007824198110029101\n",
      "Iteration is: 12174 and loss is: 0.0007430936675518751\n",
      "Iteration is: 12175 and loss is: 0.0006851854268461466\n",
      "Iteration is: 12176 and loss is: 0.000571034790482372\n",
      "Iteration is: 12177 and loss is: 0.0004473619628697634\n",
      "Iteration is: 12178 and loss is: 0.00034063454950228333\n",
      "Iteration is: 12179 and loss is: 0.0002985520113725215\n",
      "Iteration is: 12180 and loss is: 0.0003347753663547337\n",
      "Iteration is: 12181 and loss is: 0.00040086492663249373\n",
      "Iteration is: 12182 and loss is: 0.00042859476525336504\n",
      "Iteration is: 12183 and loss is: 0.0003956235887017101\n",
      "Iteration is: 12184 and loss is: 0.00034096246236003935\n",
      "Iteration is: 12185 and loss is: 0.00030212639831006527\n",
      "Iteration is: 12186 and loss is: 0.00029642839217558503\n",
      "Iteration is: 12187 and loss is: 0.00031841889722272754\n",
      "Iteration is: 12188 and loss is: 0.000354626594344154\n",
      "Iteration is: 12189 and loss is: 0.0003933468833565712\n",
      "Iteration is: 12190 and loss is: 0.0004285863251425326\n",
      "Iteration is: 12191 and loss is: 0.0004763604956679046\n",
      "Iteration is: 12192 and loss is: 0.0005386607954278588\n",
      "Iteration is: 12193 and loss is: 0.0006386605091392994\n",
      "Iteration is: 12194 and loss is: 0.0007524919346906245\n",
      "Iteration is: 12195 and loss is: 0.0008955204393714666\n",
      "Iteration is: 12196 and loss is: 0.0009369708131998777\n",
      "Iteration is: 12197 and loss is: 0.0008568580378778279\n",
      "Iteration is: 12198 and loss is: 0.0005872106994502246\n",
      "Iteration is: 12199 and loss is: 0.00035647410550154746\n",
      "Iteration is: 12200 and loss is: 0.0003372094070073217\n",
      "Iteration is: 12201 and loss is: 0.0004794126725755632\n",
      "Iteration is: 12202 and loss is: 0.0005602106684818864\n",
      "Iteration is: 12203 and loss is: 0.00046718341764062643\n",
      "Iteration is: 12204 and loss is: 0.00033745652763172984\n",
      "Iteration is: 12205 and loss is: 0.00030924699967727065\n",
      "Iteration is: 12206 and loss is: 0.0003855862596537918\n",
      "Iteration is: 12207 and loss is: 0.00047696969704702497\n",
      "Iteration is: 12208 and loss is: 0.0005056139780208468\n",
      "Iteration is: 12209 and loss is: 0.00048756456817500293\n",
      "Iteration is: 12210 and loss is: 0.0004490436695050448\n",
      "Iteration is: 12211 and loss is: 0.0004175894137006253\n",
      "Iteration is: 12212 and loss is: 0.0003855852410197258\n",
      "Iteration is: 12213 and loss is: 0.0003634195018094033\n",
      "Iteration is: 12214 and loss is: 0.000352017639670521\n",
      "Iteration is: 12215 and loss is: 0.00034811143996194005\n",
      "Iteration is: 12216 and loss is: 0.0003430375945754349\n",
      "Iteration is: 12217 and loss is: 0.0003411784127820283\n",
      "Iteration is: 12218 and loss is: 0.00034176683402620256\n",
      "Iteration is: 12219 and loss is: 0.00034110958222299814\n",
      "Iteration is: 12220 and loss is: 0.00033526861807331443\n",
      "Iteration is: 12221 and loss is: 0.00032924432889558375\n",
      "Iteration is: 12222 and loss is: 0.0003232281596865505\n",
      "Iteration is: 12223 and loss is: 0.00031698064412921667\n",
      "Iteration is: 12224 and loss is: 0.0003101793408859521\n",
      "Iteration is: 12225 and loss is: 0.0003071842947974801\n",
      "Iteration is: 12226 and loss is: 0.0003083073243033141\n",
      "Iteration is: 12227 and loss is: 0.00031359525746665895\n",
      "Iteration is: 12228 and loss is: 0.0003248907742090523\n",
      "Iteration is: 12229 and loss is: 0.00035013098386116326\n",
      "Iteration is: 12230 and loss is: 0.0003978556196670979\n",
      "Iteration is: 12231 and loss is: 0.0004905432579107583\n",
      "Iteration is: 12232 and loss is: 0.0006470207590609789\n",
      "Iteration is: 12233 and loss is: 0.0009286790736950934\n",
      "Iteration is: 12234 and loss is: 0.0012796495575457811\n",
      "Iteration is: 12235 and loss is: 0.0017053888877853751\n",
      "Iteration is: 12236 and loss is: 0.0016732232179492712\n",
      "Iteration is: 12237 and loss is: 0.0011986494064331055\n",
      "Iteration is: 12238 and loss is: 0.0005137142725288868\n",
      "Iteration is: 12239 and loss is: 0.0004975526826456189\n",
      "Iteration is: 12240 and loss is: 0.0009420327842235565\n",
      "Iteration is: 12241 and loss is: 0.0009775354992598295\n",
      "Iteration is: 12242 and loss is: 0.0005272388807497919\n",
      "Iteration is: 12243 and loss is: 0.00035321939503774047\n",
      "Iteration is: 12244 and loss is: 0.0005943446885794401\n",
      "Iteration is: 12245 and loss is: 0.0008734511211514473\n",
      "Iteration is: 12246 and loss is: 0.0009336185175925493\n",
      "Iteration is: 12247 and loss is: 0.0008203411707654595\n",
      "Iteration is: 12248 and loss is: 0.0006158872274681926\n",
      "Iteration is: 12249 and loss is: 0.0004841713234782219\n",
      "Iteration is: 12250 and loss is: 0.0003767114249058068\n",
      "Iteration is: 12251 and loss is: 0.00031376542756333947\n",
      "Iteration is: 12252 and loss is: 0.0003044258919544518\n",
      "Iteration is: 12253 and loss is: 0.0003287282888777554\n",
      "Iteration is: 12254 and loss is: 0.00039109980571083724\n",
      "Iteration is: 12255 and loss is: 0.0004284914757590741\n",
      "Iteration is: 12256 and loss is: 0.0004582494148053229\n",
      "Iteration is: 12257 and loss is: 0.0004647845635190606\n",
      "Iteration is: 12258 and loss is: 0.0004382412589620799\n",
      "Iteration is: 12259 and loss is: 0.0003852916997857392\n",
      "Iteration is: 12260 and loss is: 0.00033009573235176504\n",
      "Iteration is: 12261 and loss is: 0.00029743899358436465\n",
      "Iteration is: 12262 and loss is: 0.0002949670306406915\n",
      "Iteration is: 12263 and loss is: 0.00030520252767018974\n",
      "Iteration is: 12264 and loss is: 0.0003305462305434048\n",
      "Iteration is: 12265 and loss is: 0.0003665049443952739\n",
      "Iteration is: 12266 and loss is: 0.0004005427472293377\n",
      "Iteration is: 12267 and loss is: 0.00044928526040166616\n",
      "Iteration is: 12268 and loss is: 0.0005104877636767924\n",
      "Iteration is: 12269 and loss is: 0.0006138831377029419\n",
      "Iteration is: 12270 and loss is: 0.0007416749140247703\n",
      "Iteration is: 12271 and loss is: 0.0009167181560769677\n",
      "Iteration is: 12272 and loss is: 0.0010177390649914742\n",
      "Iteration is: 12273 and loss is: 0.0010164473205804825\n",
      "Iteration is: 12274 and loss is: 0.0007560736266896129\n",
      "Iteration is: 12275 and loss is: 0.00044555560452863574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 12276 and loss is: 0.0003206706023775041\n",
      "Iteration is: 12277 and loss is: 0.00045205032802186906\n",
      "Iteration is: 12278 and loss is: 0.0005960942944511771\n",
      "Iteration is: 12279 and loss is: 0.000535801867954433\n",
      "Iteration is: 12280 and loss is: 0.00038281542947515845\n",
      "Iteration is: 12281 and loss is: 0.0003100568428635597\n",
      "Iteration is: 12282 and loss is: 0.00036214382271282375\n",
      "Iteration is: 12283 and loss is: 0.00047301437007263303\n",
      "Iteration is: 12284 and loss is: 0.0005430968012660742\n",
      "Iteration is: 12285 and loss is: 0.0005441148532554507\n",
      "Iteration is: 12286 and loss is: 0.0005031049367971718\n",
      "Iteration is: 12287 and loss is: 0.0004766342754010111\n",
      "Iteration is: 12288 and loss is: 0.0004548300348687917\n",
      "Iteration is: 12289 and loss is: 0.00043612337321974337\n",
      "Iteration is: 12290 and loss is: 0.00041385143413208425\n",
      "Iteration is: 12291 and loss is: 0.0004056243342347443\n",
      "Iteration is: 12292 and loss is: 0.00039785238914191723\n",
      "Iteration is: 12293 and loss is: 0.00037984506343491375\n",
      "Iteration is: 12294 and loss is: 0.00035089021548628807\n",
      "Iteration is: 12295 and loss is: 0.00032461347291246057\n",
      "Iteration is: 12296 and loss is: 0.00030451067141257226\n",
      "Iteration is: 12297 and loss is: 0.0002895172219723463\n",
      "Iteration is: 12298 and loss is: 0.00027996778953820467\n",
      "Iteration is: 12299 and loss is: 0.00028118869522586465\n",
      "Iteration is: 12300 and loss is: 0.0002912679919973016\n",
      "Iteration is: 12301 and loss is: 0.00030559749575331807\n",
      "Iteration is: 12302 and loss is: 0.00032784283393993974\n",
      "Iteration is: 12303 and loss is: 0.00036682095378637314\n",
      "Iteration is: 12304 and loss is: 0.00044100783998146653\n",
      "Iteration is: 12305 and loss is: 0.0005678749294020236\n",
      "Iteration is: 12306 and loss is: 0.0008024721755646169\n",
      "Iteration is: 12307 and loss is: 0.0011301079066470265\n",
      "Iteration is: 12308 and loss is: 0.0015812786296010017\n",
      "Iteration is: 12309 and loss is: 0.001707109622657299\n",
      "Iteration is: 12310 and loss is: 0.0014058246742933989\n",
      "Iteration is: 12311 and loss is: 0.0006545002688653767\n",
      "Iteration is: 12312 and loss is: 0.0004267655895091593\n",
      "Iteration is: 12313 and loss is: 0.0008350203279405832\n",
      "Iteration is: 12314 and loss is: 0.001020984724164009\n",
      "Iteration is: 12315 and loss is: 0.000646734144538641\n",
      "Iteration is: 12316 and loss is: 0.00036332494346424937\n",
      "Iteration is: 12317 and loss is: 0.0005134557140991092\n",
      "Iteration is: 12318 and loss is: 0.0007743228925392032\n",
      "Iteration is: 12319 and loss is: 0.0008795091416686773\n",
      "Iteration is: 12320 and loss is: 0.0008497062954120338\n",
      "Iteration is: 12321 and loss is: 0.000658892560750246\n",
      "Iteration is: 12322 and loss is: 0.0004941128427162766\n",
      "Iteration is: 12323 and loss is: 0.0004076760378666222\n",
      "Iteration is: 12324 and loss is: 0.0003521550970617682\n",
      "Iteration is: 12325 and loss is: 0.0003227244014851749\n",
      "Iteration is: 12326 and loss is: 0.00029657449340447783\n",
      "Iteration is: 12327 and loss is: 0.00032495823688805103\n",
      "Iteration is: 12328 and loss is: 0.0003787620516959578\n",
      "Iteration is: 12329 and loss is: 0.000395838898839429\n",
      "Iteration is: 12330 and loss is: 0.0003965949290432036\n",
      "Iteration is: 12331 and loss is: 0.0003981547197327018\n",
      "Iteration is: 12332 and loss is: 0.00038404783117584884\n",
      "Iteration is: 12333 and loss is: 0.000353796174749732\n",
      "Iteration is: 12334 and loss is: 0.00031079421751201153\n",
      "Iteration is: 12335 and loss is: 0.0002873850171454251\n",
      "Iteration is: 12336 and loss is: 0.0002917874662671238\n",
      "Iteration is: 12337 and loss is: 0.0002934266231022775\n",
      "Iteration is: 12338 and loss is: 0.00030035022064112127\n",
      "Iteration is: 12339 and loss is: 0.0003177328617312014\n",
      "Iteration is: 12340 and loss is: 0.00035199173726141453\n",
      "Iteration is: 12341 and loss is: 0.000404742662794888\n",
      "Iteration is: 12342 and loss is: 0.0004821541951969266\n",
      "Iteration is: 12343 and loss is: 0.0006073513650335371\n",
      "Iteration is: 12344 and loss is: 0.0008235801942646503\n",
      "Iteration is: 12345 and loss is: 0.0010566929122433066\n",
      "Iteration is: 12346 and loss is: 0.0012946841306984425\n",
      "Iteration is: 12347 and loss is: 0.0012153927236795425\n",
      "Iteration is: 12348 and loss is: 0.0008593326201662421\n",
      "Iteration is: 12349 and loss is: 0.00041961611714214087\n",
      "Iteration is: 12350 and loss is: 0.0003802675928454846\n",
      "Iteration is: 12351 and loss is: 0.0006441371515393257\n",
      "Iteration is: 12352 and loss is: 0.0007214260986074805\n",
      "Iteration is: 12353 and loss is: 0.0005151326186023653\n",
      "Iteration is: 12354 and loss is: 0.00033401488326489925\n",
      "Iteration is: 12355 and loss is: 0.0003675030602607876\n",
      "Iteration is: 12356 and loss is: 0.0005158594576641917\n",
      "Iteration is: 12357 and loss is: 0.0006536872242577374\n",
      "Iteration is: 12358 and loss is: 0.000726697442587465\n",
      "Iteration is: 12359 and loss is: 0.0007012474816292524\n",
      "Iteration is: 12360 and loss is: 0.0006759363459423184\n",
      "Iteration is: 12361 and loss is: 0.0006466491613537073\n",
      "Iteration is: 12362 and loss is: 0.0006252411403693259\n",
      "Iteration is: 12363 and loss is: 0.0005510984919965267\n",
      "Iteration is: 12364 and loss is: 0.0004410972469486296\n",
      "Iteration is: 12365 and loss is: 0.0003478552680462599\n",
      "Iteration is: 12366 and loss is: 0.00030994205735623837\n",
      "Iteration is: 12367 and loss is: 0.0003119683242402971\n",
      "Iteration is: 12368 and loss is: 0.00034005861380137503\n",
      "Iteration is: 12369 and loss is: 0.00037479703314602375\n",
      "Iteration is: 12370 and loss is: 0.000385854160413146\n",
      "Iteration is: 12371 and loss is: 0.0003626123652793467\n",
      "Iteration is: 12372 and loss is: 0.0003162844222970307\n",
      "Iteration is: 12373 and loss is: 0.0002838033251464367\n",
      "Iteration is: 12374 and loss is: 0.00028305043815635145\n",
      "Iteration is: 12375 and loss is: 0.0002954416850116104\n",
      "Iteration is: 12376 and loss is: 0.000309991417452693\n",
      "Iteration is: 12377 and loss is: 0.00033481878926977515\n",
      "Iteration is: 12378 and loss is: 0.00038742064498364925\n",
      "Iteration is: 12379 and loss is: 0.00048096722457557917\n",
      "Iteration is: 12380 and loss is: 0.0006385953747667372\n",
      "Iteration is: 12381 and loss is: 0.0008635749109089375\n",
      "Iteration is: 12382 and loss is: 0.0011890066089108586\n",
      "Iteration is: 12383 and loss is: 0.0013601364335045218\n",
      "Iteration is: 12384 and loss is: 0.001275920425541699\n",
      "Iteration is: 12385 and loss is: 0.0007488947012461722\n",
      "Iteration is: 12386 and loss is: 0.0003721265238709748\n",
      "Iteration is: 12387 and loss is: 0.0004984774277545512\n",
      "Iteration is: 12388 and loss is: 0.0007510354043915868\n",
      "Iteration is: 12389 and loss is: 0.000693384266924113\n",
      "Iteration is: 12390 and loss is: 0.00044583631097339094\n",
      "Iteration is: 12391 and loss is: 0.00035069286241196096\n",
      "Iteration is: 12392 and loss is: 0.0004692295042332262\n",
      "Iteration is: 12393 and loss is: 0.0006182185024954379\n",
      "Iteration is: 12394 and loss is: 0.0007151874015107751\n",
      "Iteration is: 12395 and loss is: 0.0007028650725260377\n",
      "Iteration is: 12396 and loss is: 0.0006268030847422779\n",
      "Iteration is: 12397 and loss is: 0.0005934365326538682\n",
      "Iteration is: 12398 and loss is: 0.0006265039555728436\n",
      "Iteration is: 12399 and loss is: 0.0006212398875504732\n",
      "Iteration is: 12400 and loss is: 0.000569493044167757\n",
      "Iteration is: 12401 and loss is: 0.00047330540837720037\n",
      "Iteration is: 12402 and loss is: 0.0003950783866457641\n",
      "Iteration is: 12403 and loss is: 0.00034942413913086057\n",
      "Iteration is: 12404 and loss is: 0.00031707307789474726\n",
      "Iteration is: 12405 and loss is: 0.0003252041933592409\n",
      "Iteration is: 12406 and loss is: 0.0003676686610560864\n",
      "Iteration is: 12407 and loss is: 0.0003923708572983742\n",
      "Iteration is: 12408 and loss is: 0.00036969484062865376\n",
      "Iteration is: 12409 and loss is: 0.00031347584445029497\n",
      "Iteration is: 12410 and loss is: 0.00028129472048021853\n",
      "Iteration is: 12411 and loss is: 0.0002956398529931903\n",
      "Iteration is: 12412 and loss is: 0.00032067447318695486\n",
      "Iteration is: 12413 and loss is: 0.0003359116381034255\n",
      "Iteration is: 12414 and loss is: 0.0003560715413186699\n",
      "Iteration is: 12415 and loss is: 0.00039932492654770613\n",
      "Iteration is: 12416 and loss is: 0.0004827476805076003\n",
      "Iteration is: 12417 and loss is: 0.0006042191525921226\n",
      "Iteration is: 12418 and loss is: 0.0007969133439473808\n",
      "Iteration is: 12419 and loss is: 0.0010301166912540793\n",
      "Iteration is: 12420 and loss is: 0.0012873347150161862\n",
      "Iteration is: 12421 and loss is: 0.0012161554768681526\n",
      "Iteration is: 12422 and loss is: 0.0008478951058350503\n",
      "Iteration is: 12423 and loss is: 0.00041500356746837497\n",
      "Iteration is: 12424 and loss is: 0.0004040376516059041\n",
      "Iteration is: 12425 and loss is: 0.0006664711399935186\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 12426 and loss is: 0.0007170652970671654\n",
      "Iteration is: 12427 and loss is: 0.0005040263058617711\n",
      "Iteration is: 12428 and loss is: 0.00034313672222197056\n",
      "Iteration is: 12429 and loss is: 0.00038432920700870454\n",
      "Iteration is: 12430 and loss is: 0.0005339380586519837\n",
      "Iteration is: 12431 and loss is: 0.0006589427357539535\n",
      "Iteration is: 12432 and loss is: 0.0007097837515175343\n",
      "Iteration is: 12433 and loss is: 0.0006680718506686389\n",
      "Iteration is: 12434 and loss is: 0.0006400658749043941\n",
      "Iteration is: 12435 and loss is: 0.00065285072196275\n",
      "Iteration is: 12436 and loss is: 0.0006809592014178634\n",
      "Iteration is: 12437 and loss is: 0.0006306344293989241\n",
      "Iteration is: 12438 and loss is: 0.0005157527048140764\n",
      "Iteration is: 12439 and loss is: 0.000391499197576195\n",
      "Iteration is: 12440 and loss is: 0.00033355288906022906\n",
      "Iteration is: 12441 and loss is: 0.0003322986885905266\n",
      "Iteration is: 12442 and loss is: 0.0003511761606205255\n",
      "Iteration is: 12443 and loss is: 0.000379563047317788\n",
      "Iteration is: 12444 and loss is: 0.0003891625674441457\n",
      "Iteration is: 12445 and loss is: 0.0003614798770286143\n",
      "Iteration is: 12446 and loss is: 0.0003155139565933496\n",
      "Iteration is: 12447 and loss is: 0.0002873961057048291\n",
      "Iteration is: 12448 and loss is: 0.0003084027557633817\n",
      "Iteration is: 12449 and loss is: 0.0003487950307317078\n",
      "Iteration is: 12450 and loss is: 0.0003601021599024534\n",
      "Iteration is: 12451 and loss is: 0.0003479870792943984\n",
      "Iteration is: 12452 and loss is: 0.0003438819549046457\n",
      "Iteration is: 12453 and loss is: 0.00036143301986157894\n",
      "Iteration is: 12454 and loss is: 0.00039562516030855477\n",
      "Iteration is: 12455 and loss is: 0.0004332322278060019\n",
      "Iteration is: 12456 and loss is: 0.0004945407854393125\n",
      "Iteration is: 12457 and loss is: 0.0005934861255809665\n",
      "Iteration is: 12458 and loss is: 0.0007403722265735269\n",
      "Iteration is: 12459 and loss is: 0.0008368433336727321\n",
      "Iteration is: 12460 and loss is: 0.0008460604585707188\n",
      "Iteration is: 12461 and loss is: 0.0006686124252155423\n",
      "Iteration is: 12462 and loss is: 0.00043548515532165766\n",
      "Iteration is: 12463 and loss is: 0.00029955810168758035\n",
      "Iteration is: 12464 and loss is: 0.000355773838236928\n",
      "Iteration is: 12465 and loss is: 0.0004892850411124527\n",
      "Iteration is: 12466 and loss is: 0.0005185235058888793\n",
      "Iteration is: 12467 and loss is: 0.00042807840509340167\n",
      "Iteration is: 12468 and loss is: 0.0003192146250512451\n",
      "Iteration is: 12469 and loss is: 0.0002884201821871102\n",
      "Iteration is: 12470 and loss is: 0.0003361963026691228\n",
      "Iteration is: 12471 and loss is: 0.0004143744008615613\n",
      "Iteration is: 12472 and loss is: 0.0004856628947891295\n",
      "Iteration is: 12473 and loss is: 0.0005307176616042852\n",
      "Iteration is: 12474 and loss is: 0.0005824195686727762\n",
      "Iteration is: 12475 and loss is: 0.0006455656257458031\n",
      "Iteration is: 12476 and loss is: 0.0007631272892467678\n",
      "Iteration is: 12477 and loss is: 0.0008730535628274083\n",
      "Iteration is: 12478 and loss is: 0.0009641789365559816\n",
      "Iteration is: 12479 and loss is: 0.0008677940350025892\n",
      "Iteration is: 12480 and loss is: 0.00064188486430794\n",
      "Iteration is: 12481 and loss is: 0.00039148505311459303\n",
      "Iteration is: 12482 and loss is: 0.00033853156492114067\n",
      "Iteration is: 12483 and loss is: 0.000451825704658404\n",
      "Iteration is: 12484 and loss is: 0.0005212308606132865\n",
      "Iteration is: 12485 and loss is: 0.00047166168224066496\n",
      "Iteration is: 12486 and loss is: 0.000382214377168566\n",
      "Iteration is: 12487 and loss is: 0.0003323557903058827\n",
      "Iteration is: 12488 and loss is: 0.0003420131397433579\n",
      "Iteration is: 12489 and loss is: 0.00039964995812624693\n",
      "Iteration is: 12490 and loss is: 0.0004726117476820946\n",
      "Iteration is: 12491 and loss is: 0.0005026979488320649\n",
      "Iteration is: 12492 and loss is: 0.0004842377966269851\n",
      "Iteration is: 12493 and loss is: 0.00047082355013117194\n",
      "Iteration is: 12494 and loss is: 0.0005138317937962711\n",
      "Iteration is: 12495 and loss is: 0.0005956806126050651\n",
      "Iteration is: 12496 and loss is: 0.0006905797636136413\n",
      "Iteration is: 12497 and loss is: 0.0007368701626546681\n",
      "Iteration is: 12498 and loss is: 0.0007394608110189438\n",
      "Iteration is: 12499 and loss is: 0.0006336202495731413\n",
      "Iteration is: 12500 and loss is: 0.00047896901378408074\n",
      "Iteration is: 12501 and loss is: 0.0003538946039043367\n",
      "Iteration is: 12502 and loss is: 0.0003331245679873973\n",
      "Iteration is: 12503 and loss is: 0.0003984015784226358\n",
      "Iteration is: 12504 and loss is: 0.0004565127892419696\n",
      "Iteration is: 12505 and loss is: 0.0004446746315807104\n",
      "Iteration is: 12506 and loss is: 0.00037404760951176286\n",
      "Iteration is: 12507 and loss is: 0.000300651416182518\n",
      "Iteration is: 12508 and loss is: 0.0002930410555563867\n",
      "Iteration is: 12509 and loss is: 0.00034860282903537154\n",
      "Iteration is: 12510 and loss is: 0.00040944499778561294\n",
      "Iteration is: 12511 and loss is: 0.0004380363679956645\n",
      "Iteration is: 12512 and loss is: 0.00045106944162398577\n",
      "Iteration is: 12513 and loss is: 0.0004998017684556544\n",
      "Iteration is: 12514 and loss is: 0.000630864524282515\n",
      "Iteration is: 12515 and loss is: 0.0008250778773799539\n",
      "Iteration is: 12516 and loss is: 0.0010924417292699218\n",
      "Iteration is: 12517 and loss is: 0.0012568046804517508\n",
      "Iteration is: 12518 and loss is: 0.0012442386941984296\n",
      "Iteration is: 12519 and loss is: 0.0008334987796843052\n",
      "Iteration is: 12520 and loss is: 0.00046870793448761106\n",
      "Iteration is: 12521 and loss is: 0.00047827279195189476\n",
      "Iteration is: 12522 and loss is: 0.0006481441087089479\n",
      "Iteration is: 12523 and loss is: 0.0006499806768260896\n",
      "Iteration is: 12524 and loss is: 0.0005135473911650479\n",
      "Iteration is: 12525 and loss is: 0.00042515885434113443\n",
      "Iteration is: 12526 and loss is: 0.0004363943007774651\n",
      "Iteration is: 12527 and loss is: 0.00045975425746291876\n",
      "Iteration is: 12528 and loss is: 0.0005495853256434202\n",
      "Iteration is: 12529 and loss is: 0.0006714477785862982\n",
      "Iteration is: 12530 and loss is: 0.0006955118151381612\n",
      "Iteration is: 12531 and loss is: 0.0006944162887521088\n",
      "Iteration is: 12532 and loss is: 0.0008109518676064909\n",
      "Iteration is: 12533 and loss is: 0.001017422997392714\n",
      "Iteration is: 12534 and loss is: 0.0012471429072320461\n",
      "Iteration is: 12535 and loss is: 0.0011244427878409624\n",
      "Iteration is: 12536 and loss is: 0.0006810636259615421\n",
      "Iteration is: 12537 and loss is: 0.0003925892524421215\n",
      "Iteration is: 12538 and loss is: 0.0005477836821228266\n",
      "Iteration is: 12539 and loss is: 0.0006848578923381865\n",
      "Iteration is: 12540 and loss is: 0.0005732853896915913\n",
      "Iteration is: 12541 and loss is: 0.00039653119165450335\n",
      "Iteration is: 12542 and loss is: 0.0004127955180592835\n",
      "Iteration is: 12543 and loss is: 0.0005204456974752247\n",
      "Iteration is: 12544 and loss is: 0.00047404816723428667\n",
      "Iteration is: 12545 and loss is: 0.00040918533341027796\n",
      "Iteration is: 12546 and loss is: 0.00046063147601671517\n",
      "Iteration is: 12547 and loss is: 0.00043611772707663476\n",
      "Iteration is: 12548 and loss is: 0.0003552918788045645\n",
      "Iteration is: 12549 and loss is: 0.00032823250512592494\n",
      "Iteration is: 12550 and loss is: 0.00030753319151699543\n",
      "Iteration is: 12551 and loss is: 0.0003387362230569124\n",
      "Iteration is: 12552 and loss is: 0.00035715874400921166\n",
      "Iteration is: 12553 and loss is: 0.0003037945425603539\n",
      "Iteration is: 12554 and loss is: 0.0002771116851363331\n",
      "Iteration is: 12555 and loss is: 0.0002950306225102395\n",
      "Iteration is: 12556 and loss is: 0.0003049829392693937\n",
      "Iteration is: 12557 and loss is: 0.0003103661583736539\n",
      "Iteration is: 12558 and loss is: 0.0002956986427307129\n",
      "Iteration is: 12559 and loss is: 0.0002714759320951998\n",
      "Iteration is: 12560 and loss is: 0.00028356676921248436\n",
      "Iteration is: 12561 and loss is: 0.00029430584982037544\n",
      "Iteration is: 12562 and loss is: 0.00028356537222862244\n",
      "Iteration is: 12563 and loss is: 0.000274269754299894\n",
      "Iteration is: 12564 and loss is: 0.00026866773259826005\n",
      "Iteration is: 12565 and loss is: 0.0002705762453842908\n",
      "Iteration is: 12566 and loss is: 0.0002803916286211461\n",
      "Iteration is: 12567 and loss is: 0.0002780522918328643\n",
      "Iteration is: 12568 and loss is: 0.00026658078422769904\n",
      "Iteration is: 12569 and loss is: 0.00026609524502418935\n",
      "Iteration is: 12570 and loss is: 0.00026879875804297626\n",
      "Iteration is: 12571 and loss is: 0.00027318333741277456\n",
      "Iteration is: 12572 and loss is: 0.0002792964514810592\n",
      "Iteration is: 12573 and loss is: 0.00028152778395451605\n",
      "Iteration is: 12574 and loss is: 0.00029212108347564936\n",
      "Iteration is: 12575 and loss is: 0.00031926186056807637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 12576 and loss is: 0.0003643759700935334\n",
      "Iteration is: 12577 and loss is: 0.00044103909749537706\n",
      "Iteration is: 12578 and loss is: 0.000593427219428122\n",
      "Iteration is: 12579 and loss is: 0.0008570011705160141\n",
      "Iteration is: 12580 and loss is: 0.001376965781673789\n",
      "Iteration is: 12581 and loss is: 0.002026206348091364\n",
      "Iteration is: 12582 and loss is: 0.0027570228558033705\n",
      "Iteration is: 12583 and loss is: 0.0022430564276874065\n",
      "Iteration is: 12584 and loss is: 0.0010489656124264002\n",
      "Iteration is: 12585 and loss is: 0.0006488013314083219\n",
      "Iteration is: 12586 and loss is: 0.0013236186932772398\n",
      "Iteration is: 12587 and loss is: 0.0021911663934588432\n",
      "Iteration is: 12588 and loss is: 0.002464026678353548\n",
      "Iteration is: 12589 and loss is: 0.0011920821852982044\n",
      "Iteration is: 12590 and loss is: 0.0003832185175269842\n",
      "Iteration is: 12591 and loss is: 0.0010297740809619427\n",
      "Iteration is: 12592 and loss is: 0.002093244343996048\n",
      "Iteration is: 12593 and loss is: 0.003245323896408081\n",
      "Iteration is: 12594 and loss is: 0.005000783130526543\n",
      "Iteration is: 12595 and loss is: 0.004655934404581785\n",
      "Iteration is: 12596 and loss is: 0.0018522781319916248\n",
      "Iteration is: 12597 and loss is: 0.0017332634888589382\n",
      "Iteration is: 12598 and loss is: 0.004404151346534491\n",
      "Iteration is: 12599 and loss is: 0.00964068528264761\n",
      "Iteration is: 12600 and loss is: 0.01488574780523777\n",
      "Iteration is: 12601 and loss is: 0.012209709733724594\n",
      "Iteration is: 12602 and loss is: 0.005791027098894119\n",
      "Iteration is: 12603 and loss is: 0.0023450939916074276\n",
      "Iteration is: 12604 and loss is: 0.0028167946729809046\n",
      "Iteration is: 12605 and loss is: 0.003234040690585971\n",
      "Iteration is: 12606 and loss is: 0.0012029451318085194\n",
      "Iteration is: 12607 and loss is: 0.0036395713686943054\n",
      "Iteration is: 12608 and loss is: 0.007686644326895475\n",
      "Iteration is: 12609 and loss is: 0.010800966992974281\n",
      "Iteration is: 12610 and loss is: 0.0016619337256997824\n",
      "Iteration is: 12611 and loss is: 0.005589034408330917\n",
      "Iteration is: 12612 and loss is: 0.012593384832143784\n",
      "Iteration is: 12613 and loss is: 0.021155184134840965\n",
      "Iteration is: 12614 and loss is: 0.016593607142567635\n",
      "Iteration is: 12615 and loss is: 0.005579788237810135\n",
      "Iteration is: 12616 and loss is: 0.014790009707212448\n",
      "Iteration is: 12617 and loss is: 0.01667828857898712\n",
      "Iteration is: 12618 and loss is: 0.003995519131422043\n",
      "Iteration is: 12619 and loss is: 0.011344127357006073\n",
      "Iteration is: 12620 and loss is: 0.022511987015604973\n",
      "Iteration is: 12621 and loss is: 0.007794974371790886\n",
      "Iteration is: 12622 and loss is: 0.004461892414838076\n",
      "Iteration is: 12623 and loss is: 0.007005250547081232\n",
      "Iteration is: 12624 and loss is: 0.0034762523137032986\n",
      "Iteration is: 12625 and loss is: 0.005717849358916283\n",
      "Iteration is: 12626 and loss is: 0.004340563900768757\n",
      "Iteration is: 12627 and loss is: 0.005368579179048538\n",
      "Iteration is: 12628 and loss is: 0.005526837892830372\n",
      "Iteration is: 12629 and loss is: 0.0031962008215487003\n",
      "Iteration is: 12630 and loss is: 0.0053659845143556595\n",
      "Iteration is: 12631 and loss is: 0.0030486213508993387\n",
      "Iteration is: 12632 and loss is: 0.0034381793811917305\n",
      "Iteration is: 12633 and loss is: 0.003412755439057946\n",
      "Iteration is: 12634 and loss is: 0.0019457952585071325\n",
      "Iteration is: 12635 and loss is: 0.0029295412823557854\n",
      "Iteration is: 12636 and loss is: 0.002319390419870615\n",
      "Iteration is: 12637 and loss is: 0.0013083955273032188\n",
      "Iteration is: 12638 and loss is: 0.001773505238816142\n",
      "Iteration is: 12639 and loss is: 0.0016968497075140476\n",
      "Iteration is: 12640 and loss is: 0.0011593641247600317\n",
      "Iteration is: 12641 and loss is: 0.0014137803809717298\n",
      "Iteration is: 12642 and loss is: 0.0010579840745776892\n",
      "Iteration is: 12643 and loss is: 0.0012977409642189741\n",
      "Iteration is: 12644 and loss is: 0.0012664642417803407\n",
      "Iteration is: 12645 and loss is: 0.001074077794328332\n",
      "Iteration is: 12646 and loss is: 0.0012626637471839786\n",
      "Iteration is: 12647 and loss is: 0.0010495531605556607\n",
      "Iteration is: 12648 and loss is: 0.0010337193962186575\n",
      "Iteration is: 12649 and loss is: 0.0009929228108376265\n",
      "Iteration is: 12650 and loss is: 0.0007497244514524937\n",
      "Iteration is: 12651 and loss is: 0.0009050179505720735\n",
      "Iteration is: 12652 and loss is: 0.0005797706544399261\n",
      "Iteration is: 12653 and loss is: 0.0008193938992917538\n",
      "Iteration is: 12654 and loss is: 0.000513421487994492\n",
      "Iteration is: 12655 and loss is: 0.0006861808942630887\n",
      "Iteration is: 12656 and loss is: 0.0005094486987218261\n",
      "Iteration is: 12657 and loss is: 0.0005958123947493732\n",
      "Iteration is: 12658 and loss is: 0.0005542496801353991\n",
      "Iteration is: 12659 and loss is: 0.000480207905638963\n",
      "Iteration is: 12660 and loss is: 0.0005706276278942823\n",
      "Iteration is: 12661 and loss is: 0.00044101482490077615\n",
      "Iteration is: 12662 and loss is: 0.0005012303590774536\n",
      "Iteration is: 12663 and loss is: 0.0004890917334705591\n",
      "Iteration is: 12664 and loss is: 0.0004213947686366737\n",
      "Iteration is: 12665 and loss is: 0.00046475225826725364\n",
      "Iteration is: 12666 and loss is: 0.0004448811523616314\n",
      "Iteration is: 12667 and loss is: 0.00037070794496685266\n",
      "Iteration is: 12668 and loss is: 0.0004292770754545927\n",
      "Iteration is: 12669 and loss is: 0.00040126091334968805\n",
      "Iteration is: 12670 and loss is: 0.0003472376265563071\n",
      "Iteration is: 12671 and loss is: 0.00039084895979613066\n",
      "Iteration is: 12672 and loss is: 0.00035996813676320016\n",
      "Iteration is: 12673 and loss is: 0.0003414888633415103\n",
      "Iteration is: 12674 and loss is: 0.00035600646515376866\n",
      "Iteration is: 12675 and loss is: 0.0003348910831846297\n",
      "Iteration is: 12676 and loss is: 0.00032807327806949615\n",
      "Iteration is: 12677 and loss is: 0.0003437052946537733\n",
      "Iteration is: 12678 and loss is: 0.0003106322546955198\n",
      "Iteration is: 12679 and loss is: 0.0003137127496302128\n",
      "Iteration is: 12680 and loss is: 0.0003264342085458338\n",
      "Iteration is: 12681 and loss is: 0.0003013265086337924\n",
      "Iteration is: 12682 and loss is: 0.000300315412459895\n",
      "Iteration is: 12683 and loss is: 0.0003054054395761341\n",
      "Iteration is: 12684 and loss is: 0.0002923556021414697\n",
      "Iteration is: 12685 and loss is: 0.00029184529557824135\n",
      "Iteration is: 12686 and loss is: 0.0002914025099016726\n",
      "Iteration is: 12687 and loss is: 0.0002810835721902549\n",
      "Iteration is: 12688 and loss is: 0.00028587004635483027\n",
      "Iteration is: 12689 and loss is: 0.0002850288583431393\n",
      "Iteration is: 12690 and loss is: 0.000273851677775383\n",
      "Iteration is: 12691 and loss is: 0.0002779329661279917\n",
      "Iteration is: 12692 and loss is: 0.00027769076405093074\n",
      "Iteration is: 12693 and loss is: 0.0002711769484449178\n",
      "Iteration is: 12694 and loss is: 0.00027136679273098707\n",
      "Iteration is: 12695 and loss is: 0.00026995205553248525\n",
      "Iteration is: 12696 and loss is: 0.00026611582143232226\n",
      "Iteration is: 12697 and loss is: 0.0002671631227713078\n",
      "Iteration is: 12698 and loss is: 0.000266280141659081\n",
      "Iteration is: 12699 and loss is: 0.0002615090343169868\n",
      "Iteration is: 12700 and loss is: 0.0002621599123813212\n",
      "Iteration is: 12701 and loss is: 0.0002619099395815283\n",
      "Iteration is: 12702 and loss is: 0.00025953748263418674\n",
      "Iteration is: 12703 and loss is: 0.00025888546952046454\n",
      "Iteration is: 12704 and loss is: 0.000258184561971575\n",
      "Iteration is: 12705 and loss is: 0.0002561896981205791\n",
      "Iteration is: 12706 and loss is: 0.0002559292479418218\n",
      "Iteration is: 12707 and loss is: 0.0002558200212661177\n",
      "Iteration is: 12708 and loss is: 0.0002537884283810854\n",
      "Iteration is: 12709 and loss is: 0.0002532067592255771\n",
      "Iteration is: 12710 and loss is: 0.00025325868045911193\n",
      "Iteration is: 12711 and loss is: 0.00025191972963511944\n",
      "Iteration is: 12712 and loss is: 0.0002514603838790208\n",
      "Iteration is: 12713 and loss is: 0.00025106468820013106\n",
      "Iteration is: 12714 and loss is: 0.00024997693253681064\n",
      "Iteration is: 12715 and loss is: 0.00024940547882579267\n",
      "Iteration is: 12716 and loss is: 0.0002493929350748658\n",
      "Iteration is: 12717 and loss is: 0.00024849726469255984\n",
      "Iteration is: 12718 and loss is: 0.00024781000684015453\n",
      "Iteration is: 12719 and loss is: 0.000247554067755118\n",
      "Iteration is: 12720 and loss is: 0.0002469237078912556\n",
      "Iteration is: 12721 and loss is: 0.00024645013036206365\n",
      "Iteration is: 12722 and loss is: 0.0002462354605086148\n",
      "Iteration is: 12723 and loss is: 0.0002457018126733601\n",
      "Iteration is: 12724 and loss is: 0.00024513358948752284\n",
      "Iteration is: 12725 and loss is: 0.0002448633313179016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 12726 and loss is: 0.00024456059327349067\n",
      "Iteration is: 12727 and loss is: 0.00024406651209574193\n",
      "Iteration is: 12728 and loss is: 0.0002438075898680836\n",
      "Iteration is: 12729 and loss is: 0.0002434487541904673\n",
      "Iteration is: 12730 and loss is: 0.00024299151846207678\n",
      "Iteration is: 12731 and loss is: 0.0002427232830086723\n",
      "Iteration is: 12732 and loss is: 0.00024246292014140636\n",
      "Iteration is: 12733 and loss is: 0.00024208718969020993\n",
      "Iteration is: 12734 and loss is: 0.0002417810173938051\n",
      "Iteration is: 12735 and loss is: 0.0002415160124655813\n",
      "Iteration is: 12736 and loss is: 0.00024118155124597251\n",
      "Iteration is: 12737 and loss is: 0.0002408985747024417\n",
      "Iteration is: 12738 and loss is: 0.00024067916092462838\n",
      "Iteration is: 12739 and loss is: 0.0002403892285656184\n",
      "Iteration is: 12740 and loss is: 0.00024010104243643582\n",
      "Iteration is: 12741 and loss is: 0.0002398602373432368\n",
      "Iteration is: 12742 and loss is: 0.00023961263650562614\n",
      "Iteration is: 12743 and loss is: 0.00023934915952850133\n",
      "Iteration is: 12744 and loss is: 0.00023912869801279157\n",
      "Iteration is: 12745 and loss is: 0.0002389020228292793\n",
      "Iteration is: 12746 and loss is: 0.00023864686954766512\n",
      "Iteration is: 12747 and loss is: 0.0002384228428127244\n",
      "Iteration is: 12748 and loss is: 0.00023821595823392272\n",
      "Iteration is: 12749 and loss is: 0.00023799037444405258\n",
      "Iteration is: 12750 and loss is: 0.00023778146714903414\n",
      "Iteration is: 12751 and loss is: 0.00023758114548400044\n",
      "Iteration is: 12752 and loss is: 0.00023737194715067744\n",
      "Iteration is: 12753 and loss is: 0.00023716445139143616\n",
      "Iteration is: 12754 and loss is: 0.00023697360302321613\n",
      "Iteration is: 12755 and loss is: 0.00023678276920691133\n",
      "Iteration is: 12756 and loss is: 0.0002365847467444837\n",
      "Iteration is: 12757 and loss is: 0.0002364023675909266\n",
      "Iteration is: 12758 and loss is: 0.00023622090520802885\n",
      "Iteration is: 12759 and loss is: 0.00023603397130500525\n",
      "Iteration is: 12760 and loss is: 0.0002358575293328613\n",
      "Iteration is: 12761 and loss is: 0.00023568543838337064\n",
      "Iteration is: 12762 and loss is: 0.00023551523918285966\n",
      "Iteration is: 12763 and loss is: 0.0002353424351895228\n",
      "Iteration is: 12764 and loss is: 0.0002351782750338316\n",
      "Iteration is: 12765 and loss is: 0.0002350151480641216\n",
      "Iteration is: 12766 and loss is: 0.0002348479174543172\n",
      "Iteration is: 12767 and loss is: 0.00023468909785151482\n",
      "Iteration is: 12768 and loss is: 0.00023453275207430124\n",
      "Iteration is: 12769 and loss is: 0.0002343778032809496\n",
      "Iteration is: 12770 and loss is: 0.0002342232910450548\n",
      "Iteration is: 12771 and loss is: 0.00023407148546539247\n",
      "Iteration is: 12772 and loss is: 0.0002339239581488073\n",
      "Iteration is: 12773 and loss is: 0.0002337750920560211\n",
      "Iteration is: 12774 and loss is: 0.00023362990759778768\n",
      "Iteration is: 12775 and loss is: 0.00023348594550043344\n",
      "Iteration is: 12776 and loss is: 0.00023334627621807158\n",
      "Iteration is: 12777 and loss is: 0.00023320525360759348\n",
      "Iteration is: 12778 and loss is: 0.0002330660936422646\n",
      "Iteration is: 12779 and loss is: 0.00023292872356250882\n",
      "Iteration is: 12780 and loss is: 0.00023279254673980176\n",
      "Iteration is: 12781 and loss is: 0.00023265820345841348\n",
      "Iteration is: 12782 and loss is: 0.00023252557730302215\n",
      "Iteration is: 12783 and loss is: 0.00023239434813149273\n",
      "Iteration is: 12784 and loss is: 0.0002322631044080481\n",
      "Iteration is: 12785 and loss is: 0.00023213581880554557\n",
      "Iteration is: 12786 and loss is: 0.0002320096973562613\n",
      "Iteration is: 12787 and loss is: 0.00023188252816908062\n",
      "Iteration is: 12788 and loss is: 0.00023175752721726894\n",
      "Iteration is: 12789 and loss is: 0.00023163313744589686\n",
      "Iteration is: 12790 and loss is: 0.00023151010100264102\n",
      "Iteration is: 12791 and loss is: 0.00023138993128668517\n",
      "Iteration is: 12792 and loss is: 0.00023126680753193796\n",
      "Iteration is: 12793 and loss is: 0.0002311468415427953\n",
      "Iteration is: 12794 and loss is: 0.0002310316194780171\n",
      "Iteration is: 12795 and loss is: 0.0002309126139152795\n",
      "Iteration is: 12796 and loss is: 0.00023079721722751856\n",
      "Iteration is: 12797 and loss is: 0.00023068059817887843\n",
      "Iteration is: 12798 and loss is: 0.00023056801001075655\n",
      "Iteration is: 12799 and loss is: 0.00023045443231239915\n",
      "Iteration is: 12800 and loss is: 0.00023034104378893971\n",
      "Iteration is: 12801 and loss is: 0.00023022934328764677\n",
      "Iteration is: 12802 and loss is: 0.00023011679877527058\n",
      "Iteration is: 12803 and loss is: 0.0002300068299518898\n",
      "Iteration is: 12804 and loss is: 0.00022989936405792832\n",
      "Iteration is: 12805 and loss is: 0.0002297886967426166\n",
      "Iteration is: 12806 and loss is: 0.0002296797465533018\n",
      "Iteration is: 12807 and loss is: 0.00022957446344662458\n",
      "Iteration is: 12808 and loss is: 0.0002294669975526631\n",
      "Iteration is: 12809 and loss is: 0.0002293620491400361\n",
      "Iteration is: 12810 and loss is: 0.0002292550925631076\n",
      "Iteration is: 12811 and loss is: 0.00022915081353858113\n",
      "Iteration is: 12812 and loss is: 0.00022904938668943942\n",
      "Iteration is: 12813 and loss is: 0.0002289447729708627\n",
      "Iteration is: 12814 and loss is: 0.0002288428950123489\n",
      "Iteration is: 12815 and loss is: 0.00022874334536027163\n",
      "Iteration is: 12816 and loss is: 0.0002286400704178959\n",
      "Iteration is: 12817 and loss is: 0.00022854017151985317\n",
      "Iteration is: 12818 and loss is: 0.00022844150953460485\n",
      "Iteration is: 12819 and loss is: 0.00022833887487649918\n",
      "Iteration is: 12820 and loss is: 0.00022824073676019907\n",
      "Iteration is: 12821 and loss is: 0.00022814341355115175\n",
      "Iteration is: 12822 and loss is: 0.00022804716718383133\n",
      "Iteration is: 12823 and loss is: 0.0002279497857671231\n",
      "Iteration is: 12824 and loss is: 0.00022785176406614482\n",
      "Iteration is: 12825 and loss is: 0.00022775449906475842\n",
      "Iteration is: 12826 and loss is: 0.00022765880567021668\n",
      "Iteration is: 12827 and loss is: 0.0002275655569974333\n",
      "Iteration is: 12828 and loss is: 0.0002274702419526875\n",
      "Iteration is: 12829 and loss is: 0.00022737644030712545\n",
      "Iteration is: 12830 and loss is: 0.00022728173644281924\n",
      "Iteration is: 12831 and loss is: 0.0002271882549393922\n",
      "Iteration is: 12832 and loss is: 0.00022709553013555706\n",
      "Iteration is: 12833 and loss is: 0.00022700185945723206\n",
      "Iteration is: 12834 and loss is: 0.0002269122051075101\n",
      "Iteration is: 12835 and loss is: 0.00022681974223814905\n",
      "Iteration is: 12836 and loss is: 0.00022672780323773623\n",
      "Iteration is: 12837 and loss is: 0.0002266371448058635\n",
      "Iteration is: 12838 and loss is: 0.00022654730128124356\n",
      "Iteration is: 12839 and loss is: 0.0002264570794068277\n",
      "Iteration is: 12840 and loss is: 0.00022636805078946054\n",
      "Iteration is: 12841 and loss is: 0.0002262802008772269\n",
      "Iteration is: 12842 and loss is: 0.0002261900226585567\n",
      "Iteration is: 12843 and loss is: 0.00022610294399783015\n",
      "Iteration is: 12844 and loss is: 0.0002260134497191757\n",
      "Iteration is: 12845 and loss is: 0.00022592622553929687\n",
      "Iteration is: 12846 and loss is: 0.000225837851758115\n",
      "Iteration is: 12847 and loss is: 0.00022575075854547322\n",
      "Iteration is: 12848 and loss is: 0.0002256649313494563\n",
      "Iteration is: 12849 and loss is: 0.00022557986085303128\n",
      "Iteration is: 12850 and loss is: 0.00022549339337274432\n",
      "Iteration is: 12851 and loss is: 0.00022540608188137412\n",
      "Iteration is: 12852 and loss is: 0.00022532246657647192\n",
      "Iteration is: 12853 and loss is: 0.00022523774532601237\n",
      "Iteration is: 12854 and loss is: 0.00022515135060530156\n",
      "Iteration is: 12855 and loss is: 0.00022506891400553286\n",
      "Iteration is: 12856 and loss is: 0.00022498324688058347\n",
      "Iteration is: 12857 and loss is: 0.00022490089759230614\n",
      "Iteration is: 12858 and loss is: 0.00022481716587208211\n",
      "Iteration is: 12859 and loss is: 0.00022473465651273727\n",
      "Iteration is: 12860 and loss is: 0.00022465003712568432\n",
      "Iteration is: 12861 and loss is: 0.00022456840088125318\n",
      "Iteration is: 12862 and loss is: 0.00022448513482231647\n",
      "Iteration is: 12863 and loss is: 0.00022440365864895284\n",
      "Iteration is: 12864 and loss is: 0.0002243220224045217\n",
      "Iteration is: 12865 and loss is: 0.0002242400514660403\n",
      "Iteration is: 12866 and loss is: 0.00022416026331484318\n",
      "Iteration is: 12867 and loss is: 0.00022407813230529428\n",
      "Iteration is: 12868 and loss is: 0.00022399748559109867\n",
      "Iteration is: 12869 and loss is: 0.00022391695529222488\n",
      "Iteration is: 12870 and loss is: 0.00022383767645806074\n",
      "Iteration is: 12871 and loss is: 0.00022375682601705194\n",
      "Iteration is: 12872 and loss is: 0.0002236786240246147\n",
      "Iteration is: 12873 and loss is: 0.00022359615832101554\n",
      "Iteration is: 12874 and loss is: 0.0002235180581919849\n",
      "Iteration is: 12875 and loss is: 0.00022343985619954765\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 12876 and loss is: 0.00022336264373734593\n",
      "Iteration is: 12877 and loss is: 0.00022328237537294626\n",
      "Iteration is: 12878 and loss is: 0.00022320417338050902\n",
      "Iteration is: 12879 and loss is: 0.0002231258840765804\n",
      "Iteration is: 12880 and loss is: 0.00022304798767436296\n",
      "Iteration is: 12881 and loss is: 0.0002229708043159917\n",
      "Iteration is: 12882 and loss is: 0.0002228926750831306\n",
      "Iteration is: 12883 and loss is: 0.00022281687415670604\n",
      "Iteration is: 12884 and loss is: 0.0002227399090770632\n",
      "Iteration is: 12885 and loss is: 0.00022266281303018332\n",
      "Iteration is: 12886 and loss is: 0.00022258675016928464\n",
      "Iteration is: 12887 and loss is: 0.00022250990150496364\n",
      "Iteration is: 12888 and loss is: 0.00022243481362238526\n",
      "Iteration is: 12889 and loss is: 0.00022235990036278963\n",
      "Iteration is: 12890 and loss is: 0.0002222888870164752\n",
      "Iteration is: 12891 and loss is: 0.0002222177863586694\n",
      "Iteration is: 12892 and loss is: 0.00022215010540094227\n",
      "Iteration is: 12893 and loss is: 0.00022208821610547602\n",
      "Iteration is: 12894 and loss is: 0.00022203325352165848\n",
      "Iteration is: 12895 and loss is: 0.0002219919115304947\n",
      "Iteration is: 12896 and loss is: 0.0002219712478108704\n",
      "Iteration is: 12897 and loss is: 0.00022198690567165613\n",
      "Iteration is: 12898 and loss is: 0.00022206531139090657\n",
      "Iteration is: 12899 and loss is: 0.0002222428738605231\n",
      "Iteration is: 12900 and loss is: 0.00022260060359258205\n",
      "Iteration is: 12901 and loss is: 0.0002232587430626154\n",
      "Iteration is: 12902 and loss is: 0.00022443848138209432\n",
      "Iteration is: 12903 and loss is: 0.00022653589257970452\n",
      "Iteration is: 12904 and loss is: 0.00023020482331048697\n",
      "Iteration is: 12905 and loss is: 0.00023671890085097402\n",
      "Iteration is: 12906 and loss is: 0.0002480637631379068\n",
      "Iteration is: 12907 and loss is: 0.0002683273341972381\n",
      "Iteration is: 12908 and loss is: 0.0003032971580978483\n",
      "Iteration is: 12909 and loss is: 0.00036588619695976377\n",
      "Iteration is: 12910 and loss is: 0.00046989574912004173\n",
      "Iteration is: 12911 and loss is: 0.0006510466919280589\n",
      "Iteration is: 12912 and loss is: 0.0009127066005021334\n",
      "Iteration is: 12913 and loss is: 0.0013021794147789478\n",
      "Iteration is: 12914 and loss is: 0.0016041564522311091\n",
      "Iteration is: 12915 and loss is: 0.0016410737298429012\n",
      "Iteration is: 12916 and loss is: 0.001258208998478949\n",
      "Iteration is: 12917 and loss is: 0.0006776774534955621\n",
      "Iteration is: 12918 and loss is: 0.00039619897142983973\n",
      "Iteration is: 12919 and loss is: 0.00039522399310953915\n",
      "Iteration is: 12920 and loss is: 0.0006151529378257692\n",
      "Iteration is: 12921 and loss is: 0.0008718087337911129\n",
      "Iteration is: 12922 and loss is: 0.0006503938930109143\n",
      "Iteration is: 12923 and loss is: 0.00035064207622781396\n",
      "Iteration is: 12924 and loss is: 0.0003170282579958439\n",
      "Iteration is: 12925 and loss is: 0.0004669381305575371\n",
      "Iteration is: 12926 and loss is: 0.0005690147518180311\n",
      "Iteration is: 12927 and loss is: 0.0005275370785966516\n",
      "Iteration is: 12928 and loss is: 0.00040664480184204876\n",
      "Iteration is: 12929 and loss is: 0.0002840980305336416\n",
      "Iteration is: 12930 and loss is: 0.0002821977250277996\n",
      "Iteration is: 12931 and loss is: 0.00039728707633912563\n",
      "Iteration is: 12932 and loss is: 0.00046792259672656655\n",
      "Iteration is: 12933 and loss is: 0.00042451295303180814\n",
      "Iteration is: 12934 and loss is: 0.00033178937155753374\n",
      "Iteration is: 12935 and loss is: 0.00027309462893754244\n",
      "Iteration is: 12936 and loss is: 0.00027608493110165\n",
      "Iteration is: 12937 and loss is: 0.0003214712196495384\n",
      "Iteration is: 12938 and loss is: 0.0003651944571174681\n",
      "Iteration is: 12939 and loss is: 0.00035062484676018357\n",
      "Iteration is: 12940 and loss is: 0.0002976058458443731\n",
      "Iteration is: 12941 and loss is: 0.0002583923633210361\n",
      "Iteration is: 12942 and loss is: 0.0002623415202833712\n",
      "Iteration is: 12943 and loss is: 0.0002871813194360584\n",
      "Iteration is: 12944 and loss is: 0.0003060323651880026\n",
      "Iteration is: 12945 and loss is: 0.0003039250150322914\n",
      "Iteration is: 12946 and loss is: 0.00028105932869948447\n",
      "Iteration is: 12947 and loss is: 0.0002562720619607717\n",
      "Iteration is: 12948 and loss is: 0.0002516283420845866\n",
      "Iteration is: 12949 and loss is: 0.00026620487915351987\n",
      "Iteration is: 12950 and loss is: 0.0002790260477922857\n",
      "Iteration is: 12951 and loss is: 0.0002780132053885609\n",
      "Iteration is: 12952 and loss is: 0.0002683594648260623\n",
      "Iteration is: 12953 and loss is: 0.00025574397295713425\n",
      "Iteration is: 12954 and loss is: 0.0002493259671609849\n",
      "Iteration is: 12955 and loss is: 0.0002532314392738044\n",
      "Iteration is: 12956 and loss is: 0.0002616077836137265\n",
      "Iteration is: 12957 and loss is: 0.00026503094704821706\n",
      "Iteration is: 12958 and loss is: 0.00026026315754279494\n",
      "Iteration is: 12959 and loss is: 0.00025303990696556866\n",
      "Iteration is: 12960 and loss is: 0.00024874514201655984\n",
      "Iteration is: 12961 and loss is: 0.00024842392303980887\n",
      "Iteration is: 12962 and loss is: 0.00025159845245070755\n",
      "Iteration is: 12963 and loss is: 0.0002547619806136936\n",
      "Iteration is: 12964 and loss is: 0.0002539783017709851\n",
      "Iteration is: 12965 and loss is: 0.0002500730915926397\n",
      "Iteration is: 12966 and loss is: 0.00024674090673215687\n",
      "Iteration is: 12967 and loss is: 0.00024569049128331244\n",
      "Iteration is: 12968 and loss is: 0.00024651727289892733\n",
      "Iteration is: 12969 and loss is: 0.0002479715913068503\n",
      "Iteration is: 12970 and loss is: 0.0002487624587956816\n",
      "Iteration is: 12971 and loss is: 0.00024777400540187955\n",
      "Iteration is: 12972 and loss is: 0.00024556246353313327\n",
      "Iteration is: 12973 and loss is: 0.00024424592265859246\n",
      "Iteration is: 12974 and loss is: 0.00024431856581941247\n",
      "Iteration is: 12975 and loss is: 0.0002449004095979035\n",
      "Iteration is: 12976 and loss is: 0.0002454185741953552\n",
      "Iteration is: 12977 and loss is: 0.00024535052943974733\n",
      "Iteration is: 12978 and loss is: 0.00024450101773254573\n",
      "Iteration is: 12979 and loss is: 0.00024347321595996618\n",
      "Iteration is: 12980 and loss is: 0.00024298796779476106\n",
      "Iteration is: 12981 and loss is: 0.00024315124028362334\n",
      "Iteration is: 12982 and loss is: 0.0002434592170175165\n",
      "Iteration is: 12983 and loss is: 0.0002435894130030647\n",
      "Iteration is: 12984 and loss is: 0.00024343261611647904\n",
      "Iteration is: 12985 and loss is: 0.00024294266768265516\n",
      "Iteration is: 12986 and loss is: 0.00024235558521468192\n",
      "Iteration is: 12987 and loss is: 0.00024207688693422824\n",
      "Iteration is: 12988 and loss is: 0.00024209334515035152\n",
      "Iteration is: 12989 and loss is: 0.00024219723127316684\n",
      "Iteration is: 12990 and loss is: 0.00024223120999522507\n",
      "Iteration is: 12991 and loss is: 0.0002421190874883905\n",
      "Iteration is: 12992 and loss is: 0.0002418442745693028\n",
      "Iteration is: 12993 and loss is: 0.00024151240359060466\n",
      "Iteration is: 12994 and loss is: 0.00024129006487783045\n",
      "Iteration is: 12995 and loss is: 0.00024122769536916167\n",
      "Iteration is: 12996 and loss is: 0.0002412186877336353\n",
      "Iteration is: 12997 and loss is: 0.00024120695888996124\n",
      "Iteration is: 12998 and loss is: 0.00024114736879710108\n",
      "Iteration is: 12999 and loss is: 0.00024099909933283925\n",
      "Iteration is: 13000 and loss is: 0.00024080122238956392\n",
      "Iteration is: 13001 and loss is: 0.00024062948068603873\n",
      "Iteration is: 13002 and loss is: 0.0002405175328021869\n",
      "Iteration is: 13003 and loss is: 0.00024045139434747398\n",
      "Iteration is: 13004 and loss is: 0.0002404088736511767\n",
      "Iteration is: 13005 and loss is: 0.00024035919341258705\n",
      "Iteration is: 13006 and loss is: 0.0002402866375632584\n",
      "Iteration is: 13007 and loss is: 0.00024016902898438275\n",
      "Iteration is: 13008 and loss is: 0.00024004325678106397\n",
      "Iteration is: 13009 and loss is: 0.00023993078502826393\n",
      "Iteration is: 13010 and loss is: 0.00023983577557373792\n",
      "Iteration is: 13011 and loss is: 0.00023975930525921285\n",
      "Iteration is: 13012 and loss is: 0.00023970137408468872\n",
      "Iteration is: 13013 and loss is: 0.0002396410854998976\n",
      "Iteration is: 13014 and loss is: 0.00023956579389050603\n",
      "Iteration is: 13015 and loss is: 0.0002394808252574876\n",
      "Iteration is: 13016 and loss is: 0.00023938814410939813\n",
      "Iteration is: 13017 and loss is: 0.00023929518647491932\n",
      "Iteration is: 13018 and loss is: 0.00023920726380310953\n",
      "Iteration is: 13019 and loss is: 0.00023912786855362356\n",
      "Iteration is: 13020 and loss is: 0.00023906087153591216\n",
      "Iteration is: 13021 and loss is: 0.00023899524239823222\n",
      "Iteration is: 13022 and loss is: 0.0002389255096204579\n",
      "Iteration is: 13023 and loss is: 0.00023885536938905716\n",
      "Iteration is: 13024 and loss is: 0.00023878001957200468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 13025 and loss is: 0.00023869991127867252\n",
      "Iteration is: 13026 and loss is: 0.0002386197738815099\n",
      "Iteration is: 13027 and loss is: 0.0002385443076491356\n",
      "Iteration is: 13028 and loss is: 0.00023847197007853538\n",
      "Iteration is: 13029 and loss is: 0.00023840280482545495\n",
      "Iteration is: 13030 and loss is: 0.0002383344544796273\n",
      "Iteration is: 13031 and loss is: 0.00023826726828701794\n",
      "Iteration is: 13032 and loss is: 0.00023820000933483243\n",
      "Iteration is: 13033 and loss is: 0.00023813215375412256\n",
      "Iteration is: 13034 and loss is: 0.00023806135868653655\n",
      "Iteration is: 13035 and loss is: 0.0002379908983130008\n",
      "Iteration is: 13036 and loss is: 0.00023792099091224372\n",
      "Iteration is: 13037 and loss is: 0.000237850850680843\n",
      "Iteration is: 13038 and loss is: 0.00023778260219842196\n",
      "Iteration is: 13039 and loss is: 0.00023771589621901512\n",
      "Iteration is: 13040 and loss is: 0.00023765169316902757\n",
      "Iteration is: 13041 and loss is: 0.00023758417228236794\n",
      "Iteration is: 13042 and loss is: 0.00023752132256049663\n",
      "Iteration is: 13043 and loss is: 0.00023745431099087\n",
      "Iteration is: 13044 and loss is: 0.00023738804156892002\n",
      "Iteration is: 13045 and loss is: 0.00023732302361167967\n",
      "Iteration is: 13046 and loss is: 0.00023725746723357588\n",
      "Iteration is: 13047 and loss is: 0.00023719275486655533\n",
      "Iteration is: 13048 and loss is: 0.00023712617985438555\n",
      "Iteration is: 13049 and loss is: 0.00023706088541075587\n",
      "Iteration is: 13050 and loss is: 0.0002369983703829348\n",
      "Iteration is: 13051 and loss is: 0.00023693428374826908\n",
      "Iteration is: 13052 and loss is: 0.00023687214707024395\n",
      "Iteration is: 13053 and loss is: 0.00023680765298195183\n",
      "Iteration is: 13054 and loss is: 0.0002367447450524196\n",
      "Iteration is: 13055 and loss is: 0.00023668247740715742\n",
      "Iteration is: 13056 and loss is: 0.00023661827435716987\n",
      "Iteration is: 13057 and loss is: 0.00023655853874515742\n",
      "Iteration is: 13058 and loss is: 0.00023649458307772875\n",
      "Iteration is: 13059 and loss is: 0.00023643378517590463\n",
      "Iteration is: 13060 and loss is: 0.00023637176491320133\n",
      "Iteration is: 13061 and loss is: 0.00023630827490705997\n",
      "Iteration is: 13062 and loss is: 0.0002362470986554399\n",
      "Iteration is: 13063 and loss is: 0.0002361849619774148\n",
      "Iteration is: 13064 and loss is: 0.00023612345103174448\n",
      "Iteration is: 13065 and loss is: 0.00023606218746863306\n",
      "Iteration is: 13066 and loss is: 0.00023600293206982315\n",
      "Iteration is: 13067 and loss is: 0.00023594187223352492\n",
      "Iteration is: 13068 and loss is: 0.00023588085605297238\n",
      "Iteration is: 13069 and loss is: 0.0002358185447519645\n",
      "Iteration is: 13070 and loss is: 0.00023576030798722059\n",
      "Iteration is: 13071 and loss is: 0.00023569987388327718\n",
      "Iteration is: 13072 and loss is: 0.00023563773720525205\n",
      "Iteration is: 13073 and loss is: 0.00023557872918900102\n",
      "Iteration is: 13074 and loss is: 0.00023552072525490075\n",
      "Iteration is: 13075 and loss is: 0.0002354601165279746\n",
      "Iteration is: 13076 and loss is: 0.00023540186521131545\n",
      "Iteration is: 13077 and loss is: 0.00023534081992693245\n",
      "Iteration is: 13078 and loss is: 0.0002352813899051398\n",
      "Iteration is: 13079 and loss is: 0.0002352217852603644\n",
      "Iteration is: 13080 and loss is: 0.00023516503279097378\n",
      "Iteration is: 13081 and loss is: 0.00023510483151767403\n",
      "Iteration is: 13082 and loss is: 0.00023504614364355803\n",
      "Iteration is: 13083 and loss is: 0.00023498824157286435\n",
      "Iteration is: 13084 and loss is: 0.00023492808395531029\n",
      "Iteration is: 13085 and loss is: 0.00023487224825657904\n",
      "Iteration is: 13086 and loss is: 0.00023481168318539858\n",
      "Iteration is: 13087 and loss is: 0.00023475420312024653\n",
      "Iteration is: 13088 and loss is: 0.00023469436564482749\n",
      "Iteration is: 13089 and loss is: 0.00023463802062906325\n",
      "Iteration is: 13090 and loss is: 0.000234581355471164\n",
      "Iteration is: 13091 and loss is: 0.00023452320601791143\n",
      "Iteration is: 13092 and loss is: 0.00023446415434591472\n",
      "Iteration is: 13093 and loss is: 0.00023440724180545658\n",
      "Iteration is: 13094 and loss is: 0.00023434922331944108\n",
      "Iteration is: 13095 and loss is: 0.00023429503198713064\n",
      "Iteration is: 13096 and loss is: 0.00023423664970323443\n",
      "Iteration is: 13097 and loss is: 0.0002341804647585377\n",
      "Iteration is: 13098 and loss is: 0.000234125618590042\n",
      "Iteration is: 13099 and loss is: 0.00023407135449815542\n",
      "Iteration is: 13100 and loss is: 0.00023401889484375715\n",
      "Iteration is: 13101 and loss is: 0.00023396759934257716\n",
      "Iteration is: 13102 and loss is: 0.0002339197526453063\n",
      "Iteration is: 13103 and loss is: 0.0002338785125175491\n",
      "Iteration is: 13104 and loss is: 0.00023384115775115788\n",
      "Iteration is: 13105 and loss is: 0.00023381866049021482\n",
      "Iteration is: 13106 and loss is: 0.00023380879429169\n",
      "Iteration is: 13107 and loss is: 0.00023382923973258585\n",
      "Iteration is: 13108 and loss is: 0.00023389034322462976\n",
      "Iteration is: 13109 and loss is: 0.00023401570797432214\n",
      "Iteration is: 13110 and loss is: 0.0002342493098694831\n",
      "Iteration is: 13111 and loss is: 0.00023464577679987997\n",
      "Iteration is: 13112 and loss is: 0.000235326835536398\n",
      "Iteration is: 13113 and loss is: 0.00023644632892683148\n",
      "Iteration is: 13114 and loss is: 0.0002383228566031903\n",
      "Iteration is: 13115 and loss is: 0.00024138246953953058\n",
      "Iteration is: 13116 and loss is: 0.00024656648747622967\n",
      "Iteration is: 13117 and loss is: 0.0002549773489590734\n",
      "Iteration is: 13118 and loss is: 0.0002695003058761358\n",
      "Iteration is: 13119 and loss is: 0.000292876735329628\n",
      "Iteration is: 13120 and loss is: 0.00033425691071897745\n",
      "Iteration is: 13121 and loss is: 0.0003992027777712792\n",
      "Iteration is: 13122 and loss is: 0.000517279258929193\n",
      "Iteration is: 13123 and loss is: 0.0006906419293954968\n",
      "Iteration is: 13124 and loss is: 0.0010104775428771973\n",
      "Iteration is: 13125 and loss is: 0.0014045235002413392\n",
      "Iteration is: 13126 and loss is: 0.0020995168015360832\n",
      "Iteration is: 13127 and loss is: 0.002590900519862771\n",
      "Iteration is: 13128 and loss is: 0.0032171769998967648\n",
      "Iteration is: 13129 and loss is: 0.002699347212910652\n",
      "Iteration is: 13130 and loss is: 0.0018243142403662205\n",
      "Iteration is: 13131 and loss is: 0.0006405527819879353\n",
      "Iteration is: 13132 and loss is: 0.00033794084447436035\n",
      "Iteration is: 13133 and loss is: 0.000824833638034761\n",
      "Iteration is: 13134 and loss is: 0.00128254946321249\n",
      "Iteration is: 13135 and loss is: 0.0013258734252303839\n",
      "Iteration is: 13136 and loss is: 0.0006103790947236121\n",
      "Iteration is: 13137 and loss is: 0.0002730751293711364\n",
      "Iteration is: 13138 and loss is: 0.0005349238053895533\n",
      "Iteration is: 13139 and loss is: 0.0008411432499997318\n",
      "Iteration is: 13140 and loss is: 0.0009095652494579554\n",
      "Iteration is: 13141 and loss is: 0.0006775869987905025\n",
      "Iteration is: 13142 and loss is: 0.0004818619345314801\n",
      "Iteration is: 13143 and loss is: 0.0003504220221657306\n",
      "Iteration is: 13144 and loss is: 0.00029826993704773486\n",
      "Iteration is: 13145 and loss is: 0.0004084874235559255\n",
      "Iteration is: 13146 and loss is: 0.0005752776633016765\n",
      "Iteration is: 13147 and loss is: 0.0005603751051239669\n",
      "Iteration is: 13148 and loss is: 0.00040079312748275697\n",
      "Iteration is: 13149 and loss is: 0.00029749763780273497\n",
      "Iteration is: 13150 and loss is: 0.0003117490850854665\n",
      "Iteration is: 13151 and loss is: 0.0003290924069005996\n",
      "Iteration is: 13152 and loss is: 0.0003435804392211139\n",
      "Iteration is: 13153 and loss is: 0.00036738667404279113\n",
      "Iteration is: 13154 and loss is: 0.0003319007228128612\n",
      "Iteration is: 13155 and loss is: 0.0002516156528145075\n",
      "Iteration is: 13156 and loss is: 0.00025947217363864183\n",
      "Iteration is: 13157 and loss is: 0.00031803734600543976\n",
      "Iteration is: 13158 and loss is: 0.00031247411970980465\n",
      "Iteration is: 13159 and loss is: 0.00028163104434497654\n",
      "Iteration is: 13160 and loss is: 0.00028398886206559837\n",
      "Iteration is: 13161 and loss is: 0.0002894258068408817\n",
      "Iteration is: 13162 and loss is: 0.0002604233450256288\n",
      "Iteration is: 13163 and loss is: 0.00023911171592772007\n",
      "Iteration is: 13164 and loss is: 0.0002577629347797483\n",
      "Iteration is: 13165 and loss is: 0.0002737445174716413\n",
      "Iteration is: 13166 and loss is: 0.0002585880574770272\n",
      "Iteration is: 13167 and loss is: 0.00024730456061661243\n",
      "Iteration is: 13168 and loss is: 0.00025528683909215033\n",
      "Iteration is: 13169 and loss is: 0.00025312346406280994\n",
      "Iteration is: 13170 and loss is: 0.00023938124650157988\n",
      "Iteration is: 13171 and loss is: 0.00024026856408454478\n",
      "Iteration is: 13172 and loss is: 0.00025125290267169476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 13173 and loss is: 0.00024933728855103254\n",
      "Iteration is: 13174 and loss is: 0.00023916608188301325\n",
      "Iteration is: 13175 and loss is: 0.00023940848768688738\n",
      "Iteration is: 13176 and loss is: 0.00024388296878896654\n",
      "Iteration is: 13177 and loss is: 0.00023976282682269812\n",
      "Iteration is: 13178 and loss is: 0.00023404840612784028\n",
      "Iteration is: 13179 and loss is: 0.00023654465621802956\n",
      "Iteration is: 13180 and loss is: 0.00024060215218923986\n",
      "Iteration is: 13181 and loss is: 0.00023810735729057342\n",
      "Iteration is: 13182 and loss is: 0.00023451782180927694\n",
      "Iteration is: 13183 and loss is: 0.000235855724895373\n",
      "Iteration is: 13184 and loss is: 0.00023731404507998377\n",
      "Iteration is: 13185 and loss is: 0.0002345009706914425\n",
      "Iteration is: 13186 and loss is: 0.0002321629144717008\n",
      "Iteration is: 13187 and loss is: 0.00023357407189905643\n",
      "Iteration is: 13188 and loss is: 0.00023478377261199057\n",
      "Iteration is: 13189 and loss is: 0.0002333502343390137\n",
      "Iteration is: 13190 and loss is: 0.00023231089289765805\n",
      "Iteration is: 13191 and loss is: 0.0002333842421649024\n",
      "Iteration is: 13192 and loss is: 0.00023388222325593233\n",
      "Iteration is: 13193 and loss is: 0.00023248812067322433\n",
      "Iteration is: 13194 and loss is: 0.00023142536520026624\n",
      "Iteration is: 13195 and loss is: 0.00023184207384474576\n",
      "Iteration is: 13196 and loss is: 0.00023206142941489816\n",
      "Iteration is: 13197 and loss is: 0.0002312477445229888\n",
      "Iteration is: 13198 and loss is: 0.00023076632351148874\n",
      "Iteration is: 13199 and loss is: 0.00023128098109737039\n",
      "Iteration is: 13200 and loss is: 0.0002316132449777797\n",
      "Iteration is: 13201 and loss is: 0.00023112798226065934\n",
      "Iteration is: 13202 and loss is: 0.00023072486510500312\n",
      "Iteration is: 13203 and loss is: 0.00023089474416337907\n",
      "Iteration is: 13204 and loss is: 0.00023095223878044635\n",
      "Iteration is: 13205 and loss is: 0.00023051451717037708\n",
      "Iteration is: 13206 and loss is: 0.0002301477361470461\n",
      "Iteration is: 13207 and loss is: 0.0002302094071637839\n",
      "Iteration is: 13208 and loss is: 0.00023028580471873283\n",
      "Iteration is: 13209 and loss is: 0.0002300563792232424\n",
      "Iteration is: 13210 and loss is: 0.00022983041708357632\n",
      "Iteration is: 13211 and loss is: 0.00022987506235949695\n",
      "Iteration is: 13212 and loss is: 0.0002299583429703489\n",
      "Iteration is: 13213 and loss is: 0.00022983585949987173\n",
      "Iteration is: 13214 and loss is: 0.00022966031974647194\n",
      "Iteration is: 13215 and loss is: 0.00022964330855756998\n",
      "Iteration is: 13216 and loss is: 0.00022967325639910996\n",
      "Iteration is: 13217 and loss is: 0.00022956788598094136\n",
      "Iteration is: 13218 and loss is: 0.00022939947666600347\n",
      "Iteration is: 13219 and loss is: 0.0002293224970344454\n",
      "Iteration is: 13220 and loss is: 0.00022930836712475866\n",
      "Iteration is: 13221 and loss is: 0.000229226250667125\n",
      "Iteration is: 13222 and loss is: 0.00022909001563675702\n",
      "Iteration is: 13223 and loss is: 0.00022900442127138376\n",
      "Iteration is: 13224 and loss is: 0.00022897915914654732\n",
      "Iteration is: 13225 and loss is: 0.0002289203112013638\n",
      "Iteration is: 13226 and loss is: 0.00022881777840666473\n",
      "Iteration is: 13227 and loss is: 0.00022873349371366203\n",
      "Iteration is: 13228 and loss is: 0.00022869426175020635\n",
      "Iteration is: 13229 and loss is: 0.00022864772472530603\n",
      "Iteration is: 13230 and loss is: 0.00022857154544908553\n",
      "Iteration is: 13231 and loss is: 0.00022849306697025895\n",
      "Iteration is: 13232 and loss is: 0.00022844306658953428\n",
      "Iteration is: 13233 and loss is: 0.00022840118617750704\n",
      "Iteration is: 13234 and loss is: 0.00022833881666883826\n",
      "Iteration is: 13235 and loss is: 0.0002282659406773746\n",
      "Iteration is: 13236 and loss is: 0.00022820959566161036\n",
      "Iteration is: 13237 and loss is: 0.00022816541604697704\n",
      "Iteration is: 13238 and loss is: 0.00022811097733210772\n",
      "Iteration is: 13239 and loss is: 0.00022804780746810138\n",
      "Iteration is: 13240 and loss is: 0.00022798788268119097\n",
      "Iteration is: 13241 and loss is: 0.00022794175310991704\n",
      "Iteration is: 13242 and loss is: 0.00022789383365307003\n",
      "Iteration is: 13243 and loss is: 0.00022783927852287889\n",
      "Iteration is: 13244 and loss is: 0.0002277892199344933\n",
      "Iteration is: 13245 and loss is: 0.00022774840181227773\n",
      "Iteration is: 13246 and loss is: 0.0002277163730468601\n",
      "Iteration is: 13247 and loss is: 0.0002276872837683186\n",
      "Iteration is: 13248 and loss is: 0.0002276660525240004\n",
      "Iteration is: 13249 and loss is: 0.00022766628535464406\n",
      "Iteration is: 13250 and loss is: 0.0002276986197102815\n",
      "Iteration is: 13251 and loss is: 0.00022776651894673705\n",
      "Iteration is: 13252 and loss is: 0.00022788639762438834\n",
      "Iteration is: 13253 and loss is: 0.00022810083464719355\n",
      "Iteration is: 13254 and loss is: 0.0002284588699694723\n",
      "Iteration is: 13255 and loss is: 0.00022903687204234302\n",
      "Iteration is: 13256 and loss is: 0.0002299531188327819\n",
      "Iteration is: 13257 and loss is: 0.0002314156445208937\n",
      "Iteration is: 13258 and loss is: 0.00023373239673674107\n",
      "Iteration is: 13259 and loss is: 0.00023743216297589242\n",
      "Iteration is: 13260 and loss is: 0.0002433360496070236\n",
      "Iteration is: 13261 and loss is: 0.0002528242766857147\n",
      "Iteration is: 13262 and loss is: 0.0002680757315829396\n",
      "Iteration is: 13263 and loss is: 0.0002927980967797339\n",
      "Iteration is: 13264 and loss is: 0.0003325641737319529\n",
      "Iteration is: 13265 and loss is: 0.000396995572373271\n",
      "Iteration is: 13266 and loss is: 0.0004988415748812258\n",
      "Iteration is: 13267 and loss is: 0.0006589972763322294\n",
      "Iteration is: 13268 and loss is: 0.0008952479111030698\n",
      "Iteration is: 13269 and loss is: 0.0012254345929250121\n",
      "Iteration is: 13270 and loss is: 0.001610209234058857\n",
      "Iteration is: 13271 and loss is: 0.0019356244010850787\n",
      "Iteration is: 13272 and loss is: 0.001993287354707718\n",
      "Iteration is: 13273 and loss is: 0.00152747449465096\n",
      "Iteration is: 13274 and loss is: 0.0008903349516913295\n",
      "Iteration is: 13275 and loss is: 0.00039212388219311833\n",
      "Iteration is: 13276 and loss is: 0.00039494718657806516\n",
      "Iteration is: 13277 and loss is: 0.0007152380421757698\n",
      "Iteration is: 13278 and loss is: 0.0009211895521730185\n",
      "Iteration is: 13279 and loss is: 0.0008704030187800527\n",
      "Iteration is: 13280 and loss is: 0.0004631601623259485\n",
      "Iteration is: 13281 and loss is: 0.0002621064195409417\n",
      "Iteration is: 13282 and loss is: 0.0003582206554710865\n",
      "Iteration is: 13283 and loss is: 0.0005143695743754506\n",
      "Iteration is: 13284 and loss is: 0.000628143607173115\n",
      "Iteration is: 13285 and loss is: 0.0007328406209126115\n",
      "Iteration is: 13286 and loss is: 0.000955107738263905\n",
      "Iteration is: 13287 and loss is: 0.001234796247445047\n",
      "Iteration is: 13288 and loss is: 0.00123512907885015\n",
      "Iteration is: 13289 and loss is: 0.0009964266791939735\n",
      "Iteration is: 13290 and loss is: 0.0006543050403706729\n",
      "Iteration is: 13291 and loss is: 0.000507264630869031\n",
      "Iteration is: 13292 and loss is: 0.0004472109430935234\n",
      "Iteration is: 13293 and loss is: 0.00041391910053789616\n",
      "Iteration is: 13294 and loss is: 0.0005763038643635809\n",
      "Iteration is: 13295 and loss is: 0.0006452946690842509\n",
      "Iteration is: 13296 and loss is: 0.00037240347592160106\n",
      "Iteration is: 13297 and loss is: 0.0002883888082578778\n",
      "Iteration is: 13298 and loss is: 0.000429767940659076\n",
      "Iteration is: 13299 and loss is: 0.0004345700144767761\n",
      "Iteration is: 13300 and loss is: 0.00034995691385120153\n",
      "Iteration is: 13301 and loss is: 0.00037012185202911496\n",
      "Iteration is: 13302 and loss is: 0.00046746464795432985\n",
      "Iteration is: 13303 and loss is: 0.00046868479694239795\n",
      "Iteration is: 13304 and loss is: 0.0003740736283361912\n",
      "Iteration is: 13305 and loss is: 0.00033237942261621356\n",
      "Iteration is: 13306 and loss is: 0.0003565079823601991\n",
      "Iteration is: 13307 and loss is: 0.0003358139656484127\n",
      "Iteration is: 13308 and loss is: 0.00027270629652775824\n",
      "Iteration is: 13309 and loss is: 0.00024016716633923352\n",
      "Iteration is: 13310 and loss is: 0.00026349525433033705\n",
      "Iteration is: 13311 and loss is: 0.0002829190343618393\n",
      "Iteration is: 13312 and loss is: 0.00027071801014244556\n",
      "Iteration is: 13313 and loss is: 0.0002680038451217115\n",
      "Iteration is: 13314 and loss is: 0.00027810767642222345\n",
      "Iteration is: 13315 and loss is: 0.0002673485141713172\n",
      "Iteration is: 13316 and loss is: 0.00023999399854801595\n",
      "Iteration is: 13317 and loss is: 0.000232150501688011\n",
      "Iteration is: 13318 and loss is: 0.00024550469242967665\n",
      "Iteration is: 13319 and loss is: 0.00025560593348927796\n",
      "Iteration is: 13320 and loss is: 0.00025768112391233444\n",
      "Iteration is: 13321 and loss is: 0.0002654228010214865\n",
      "Iteration is: 13322 and loss is: 0.0002790052676573396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 13323 and loss is: 0.0002853377955034375\n",
      "Iteration is: 13324 and loss is: 0.00028227269649505615\n",
      "Iteration is: 13325 and loss is: 0.000276519131148234\n",
      "Iteration is: 13326 and loss is: 0.0002714319562073797\n",
      "Iteration is: 13327 and loss is: 0.00026247603818774223\n",
      "Iteration is: 13328 and loss is: 0.00024757266510277987\n",
      "Iteration is: 13329 and loss is: 0.0002360495418542996\n",
      "Iteration is: 13330 and loss is: 0.00023190845968201756\n",
      "Iteration is: 13331 and loss is: 0.00023063950357027352\n",
      "Iteration is: 13332 and loss is: 0.00022973259910941124\n",
      "Iteration is: 13333 and loss is: 0.00023192849766928703\n",
      "Iteration is: 13334 and loss is: 0.00023814909218344837\n",
      "Iteration is: 13335 and loss is: 0.000245009723585099\n",
      "Iteration is: 13336 and loss is: 0.0002500787959434092\n",
      "Iteration is: 13337 and loss is: 0.00025578989880159497\n",
      "Iteration is: 13338 and loss is: 0.00026473746402189136\n",
      "Iteration is: 13339 and loss is: 0.0002751261636149138\n",
      "Iteration is: 13340 and loss is: 0.00028594114701263607\n",
      "Iteration is: 13341 and loss is: 0.00029548932798206806\n",
      "Iteration is: 13342 and loss is: 0.0003070800448767841\n",
      "Iteration is: 13343 and loss is: 0.0003172734868712723\n",
      "Iteration is: 13344 and loss is: 0.0003245151601731777\n",
      "Iteration is: 13345 and loss is: 0.0003270289453212172\n",
      "Iteration is: 13346 and loss is: 0.0003279513621237129\n",
      "Iteration is: 13347 and loss is: 0.0003266283601988107\n",
      "Iteration is: 13348 and loss is: 0.0003271014429628849\n",
      "Iteration is: 13349 and loss is: 0.0003259041113778949\n",
      "Iteration is: 13350 and loss is: 0.00032866618130356073\n",
      "Iteration is: 13351 and loss is: 0.00033301018993370235\n",
      "Iteration is: 13352 and loss is: 0.0003426282783038914\n",
      "Iteration is: 13353 and loss is: 0.0003546738007571548\n",
      "Iteration is: 13354 and loss is: 0.00037264134152792394\n",
      "Iteration is: 13355 and loss is: 0.00039244344225153327\n",
      "Iteration is: 13356 and loss is: 0.00041905275429598987\n",
      "Iteration is: 13357 and loss is: 0.00044157315278425813\n",
      "Iteration is: 13358 and loss is: 0.00046708754962310195\n",
      "Iteration is: 13359 and loss is: 0.00048014894127845764\n",
      "Iteration is: 13360 and loss is: 0.0004880686174146831\n",
      "Iteration is: 13361 and loss is: 0.0004774520930368453\n",
      "Iteration is: 13362 and loss is: 0.0004565395356621593\n",
      "Iteration is: 13363 and loss is: 0.0004212314379401505\n",
      "Iteration is: 13364 and loss is: 0.00038357620360329747\n",
      "Iteration is: 13365 and loss is: 0.00034381437581032515\n",
      "Iteration is: 13366 and loss is: 0.00031085521914064884\n",
      "Iteration is: 13367 and loss is: 0.00028341749566607177\n",
      "Iteration is: 13368 and loss is: 0.00026403472293168306\n",
      "Iteration is: 13369 and loss is: 0.0002504568255972117\n",
      "Iteration is: 13370 and loss is: 0.00024181409389711916\n",
      "Iteration is: 13371 and loss is: 0.00023627968039363623\n",
      "Iteration is: 13372 and loss is: 0.0002328782866243273\n",
      "Iteration is: 13373 and loss is: 0.00023079894890543073\n",
      "Iteration is: 13374 and loss is: 0.00022967072436586022\n",
      "Iteration is: 13375 and loss is: 0.00022915011504665017\n",
      "Iteration is: 13376 and loss is: 0.00022915878798812628\n",
      "Iteration is: 13377 and loss is: 0.00022961449576541781\n",
      "Iteration is: 13378 and loss is: 0.00023059741943143308\n",
      "Iteration is: 13379 and loss is: 0.0002321570791536942\n",
      "Iteration is: 13380 and loss is: 0.0002345909015275538\n",
      "Iteration is: 13381 and loss is: 0.00023827444238122553\n",
      "Iteration is: 13382 and loss is: 0.0002441217948216945\n",
      "Iteration is: 13383 and loss is: 0.00025313941296190023\n",
      "Iteration is: 13384 and loss is: 0.00026780663756653666\n",
      "Iteration is: 13385 and loss is: 0.00029062828980386257\n",
      "Iteration is: 13386 and loss is: 0.00032842651125974953\n",
      "Iteration is: 13387 and loss is: 0.00038656932883895934\n",
      "Iteration is: 13388 and loss is: 0.0004819798923563212\n",
      "Iteration is: 13389 and loss is: 0.0006178977782838047\n",
      "Iteration is: 13390 and loss is: 0.0008250781102105975\n",
      "Iteration is: 13391 and loss is: 0.001056452514603734\n",
      "Iteration is: 13392 and loss is: 0.0013202594127506018\n",
      "Iteration is: 13393 and loss is: 0.0014043361879885197\n",
      "Iteration is: 13394 and loss is: 0.0012660932261496782\n",
      "Iteration is: 13395 and loss is: 0.0008687960216775537\n",
      "Iteration is: 13396 and loss is: 0.00045082042925059795\n",
      "Iteration is: 13397 and loss is: 0.0003057519788853824\n",
      "Iteration is: 13398 and loss is: 0.00042000290704891086\n",
      "Iteration is: 13399 and loss is: 0.0006216054316610098\n",
      "Iteration is: 13400 and loss is: 0.0007313680252991617\n",
      "Iteration is: 13401 and loss is: 0.0005645263590849936\n",
      "Iteration is: 13402 and loss is: 0.00034709018655121326\n",
      "Iteration is: 13403 and loss is: 0.00024094004766084254\n",
      "Iteration is: 13404 and loss is: 0.00030418485403060913\n",
      "Iteration is: 13405 and loss is: 0.0004199804097879678\n",
      "Iteration is: 13406 and loss is: 0.00048011160106398165\n",
      "Iteration is: 13407 and loss is: 0.0005836444324813783\n",
      "Iteration is: 13408 and loss is: 0.0008386104600504041\n",
      "Iteration is: 13409 and loss is: 0.001221807673573494\n",
      "Iteration is: 13410 and loss is: 0.0016533035086467862\n",
      "Iteration is: 13411 and loss is: 0.0016082143411040306\n",
      "Iteration is: 13412 and loss is: 0.0011730978731065989\n",
      "Iteration is: 13413 and loss is: 0.0007537517813034356\n",
      "Iteration is: 13414 and loss is: 0.0006563863717019558\n",
      "Iteration is: 13415 and loss is: 0.0006038383580744267\n",
      "Iteration is: 13416 and loss is: 0.0006982997292652726\n",
      "Iteration is: 13417 and loss is: 0.0007968173595145345\n",
      "Iteration is: 13418 and loss is: 0.0006092404946684837\n",
      "Iteration is: 13419 and loss is: 0.0003407378098927438\n",
      "Iteration is: 13420 and loss is: 0.00043438124703243375\n",
      "Iteration is: 13421 and loss is: 0.0005904627032577991\n",
      "Iteration is: 13422 and loss is: 0.0004903215449303389\n",
      "Iteration is: 13423 and loss is: 0.00034859770676121116\n",
      "Iteration is: 13424 and loss is: 0.0004155132919549942\n",
      "Iteration is: 13425 and loss is: 0.0006437041447497904\n",
      "Iteration is: 13426 and loss is: 0.000795375497546047\n",
      "Iteration is: 13427 and loss is: 0.0009636855684220791\n",
      "Iteration is: 13428 and loss is: 0.001567008555866778\n",
      "Iteration is: 13429 and loss is: 0.002450600964948535\n",
      "Iteration is: 13430 and loss is: 0.0029152054339647293\n",
      "Iteration is: 13431 and loss is: 0.0018327080179005861\n",
      "Iteration is: 13432 and loss is: 0.0005777598707936704\n",
      "Iteration is: 13433 and loss is: 0.0009131821570917964\n",
      "Iteration is: 13434 and loss is: 0.0016206398140639067\n",
      "Iteration is: 13435 and loss is: 0.0013821087777614594\n",
      "Iteration is: 13436 and loss is: 0.000731640204321593\n",
      "Iteration is: 13437 and loss is: 0.0004753549874294549\n",
      "Iteration is: 13438 and loss is: 0.001036652596667409\n",
      "Iteration is: 13439 and loss is: 0.0011452770559117198\n",
      "Iteration is: 13440 and loss is: 0.0004202543059363961\n",
      "Iteration is: 13441 and loss is: 0.0004469666164368391\n",
      "Iteration is: 13442 and loss is: 0.0007854180294089019\n",
      "Iteration is: 13443 and loss is: 0.0006401248974725604\n",
      "Iteration is: 13444 and loss is: 0.0006830975180491805\n",
      "Iteration is: 13445 and loss is: 0.0007578947697766125\n",
      "Iteration is: 13446 and loss is: 0.0008090554620139301\n",
      "Iteration is: 13447 and loss is: 0.0013092117151245475\n",
      "Iteration is: 13448 and loss is: 0.0020836712792515755\n",
      "Iteration is: 13449 and loss is: 0.003318877425044775\n",
      "Iteration is: 13450 and loss is: 0.0032231954392045736\n",
      "Iteration is: 13451 and loss is: 0.0013394128764048219\n",
      "Iteration is: 13452 and loss is: 0.0013873254647478461\n",
      "Iteration is: 13453 and loss is: 0.0033521950244903564\n",
      "Iteration is: 13454 and loss is: 0.0036460906267166138\n",
      "Iteration is: 13455 and loss is: 0.0021533784456551075\n",
      "Iteration is: 13456 and loss is: 0.0007879727054387331\n",
      "Iteration is: 13457 and loss is: 0.0012978175655007362\n",
      "Iteration is: 13458 and loss is: 0.0017981803975999355\n",
      "Iteration is: 13459 and loss is: 0.0011203751200810075\n",
      "Iteration is: 13460 and loss is: 0.00036118124262429774\n",
      "Iteration is: 13461 and loss is: 0.0010525245452299714\n",
      "Iteration is: 13462 and loss is: 0.0019426995422691107\n",
      "Iteration is: 13463 and loss is: 0.0030533848330378532\n",
      "Iteration is: 13464 and loss is: 0.003968392964452505\n",
      "Iteration is: 13465 and loss is: 0.0022827943321317434\n",
      "Iteration is: 13466 and loss is: 0.00045815901830792427\n",
      "Iteration is: 13467 and loss is: 0.0019760411232709885\n",
      "Iteration is: 13468 and loss is: 0.002222954761236906\n",
      "Iteration is: 13469 and loss is: 0.0016568915452808142\n",
      "Iteration is: 13470 and loss is: 0.001077296445146203\n",
      "Iteration is: 13471 and loss is: 0.0009766736766323447\n",
      "Iteration is: 13472 and loss is: 0.0012308454606682062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 13473 and loss is: 0.0012504562037065625\n",
      "Iteration is: 13474 and loss is: 0.0005666542565450072\n",
      "Iteration is: 13475 and loss is: 0.0008699578465893865\n",
      "Iteration is: 13476 and loss is: 0.0008619807194918394\n",
      "Iteration is: 13477 and loss is: 0.0003713988116942346\n",
      "Iteration is: 13478 and loss is: 0.0006934538250789046\n",
      "Iteration is: 13479 and loss is: 0.0008309982949867845\n",
      "Iteration is: 13480 and loss is: 0.001049934420734644\n",
      "Iteration is: 13481 and loss is: 0.002150261076167226\n",
      "Iteration is: 13482 and loss is: 0.0038025386165827513\n",
      "Iteration is: 13483 and loss is: 0.00592705886811018\n",
      "Iteration is: 13484 and loss is: 0.0051236581057310104\n",
      "Iteration is: 13485 and loss is: 0.0011479309760034084\n",
      "Iteration is: 13486 and loss is: 0.004164387937635183\n",
      "Iteration is: 13487 and loss is: 0.006888282485306263\n",
      "Iteration is: 13488 and loss is: 0.003898420138284564\n",
      "Iteration is: 13489 and loss is: 0.0009336700895801187\n",
      "Iteration is: 13490 and loss is: 0.0033234937582165003\n",
      "Iteration is: 13491 and loss is: 0.005064669996500015\n",
      "Iteration is: 13492 and loss is: 0.0018137693405151367\n",
      "Iteration is: 13493 and loss is: 0.0011849766597151756\n",
      "Iteration is: 13494 and loss is: 0.0024089617654681206\n",
      "Iteration is: 13495 and loss is: 0.0008697495795786381\n",
      "Iteration is: 13496 and loss is: 0.0010538343340158463\n",
      "Iteration is: 13497 and loss is: 0.0022754548117518425\n",
      "Iteration is: 13498 and loss is: 0.002236983273178339\n",
      "Iteration is: 13499 and loss is: 0.0018100118031725287\n",
      "Iteration is: 13500 and loss is: 0.0007175784558057785\n",
      "Iteration is: 13501 and loss is: 0.0007424534996971488\n",
      "Iteration is: 13502 and loss is: 0.0007901606149971485\n",
      "Iteration is: 13503 and loss is: 0.0010573477484285831\n",
      "Iteration is: 13504 and loss is: 0.0007411804981529713\n",
      "Iteration is: 13505 and loss is: 0.0003636251494754106\n",
      "Iteration is: 13506 and loss is: 0.0007640906842425466\n",
      "Iteration is: 13507 and loss is: 0.0005055661895312369\n",
      "Iteration is: 13508 and loss is: 0.0004050575662404299\n",
      "Iteration is: 13509 and loss is: 0.0006366759771481156\n",
      "Iteration is: 13510 and loss is: 0.0003452047531027347\n",
      "Iteration is: 13511 and loss is: 0.0005152465309947729\n",
      "Iteration is: 13512 and loss is: 0.0004755580157507211\n",
      "Iteration is: 13513 and loss is: 0.0003387239994481206\n",
      "Iteration is: 13514 and loss is: 0.0004831928526982665\n",
      "Iteration is: 13515 and loss is: 0.0003550421097315848\n",
      "Iteration is: 13516 and loss is: 0.00033676964812912047\n",
      "Iteration is: 13517 and loss is: 0.0004172373446635902\n",
      "Iteration is: 13518 and loss is: 0.0002863964473363012\n",
      "Iteration is: 13519 and loss is: 0.00033917295513674617\n",
      "Iteration is: 13520 and loss is: 0.0003540791803970933\n",
      "Iteration is: 13521 and loss is: 0.00029872983577661216\n",
      "Iteration is: 13522 and loss is: 0.00030601187609136105\n",
      "Iteration is: 13523 and loss is: 0.0003203519736416638\n",
      "Iteration is: 13524 and loss is: 0.00030579412123188376\n",
      "Iteration is: 13525 and loss is: 0.0002828411525115371\n",
      "Iteration is: 13526 and loss is: 0.00028905508224852383\n",
      "Iteration is: 13527 and loss is: 0.0002862655383069068\n",
      "Iteration is: 13528 and loss is: 0.00026166756288148463\n",
      "Iteration is: 13529 and loss is: 0.00028200389351695776\n",
      "Iteration is: 13530 and loss is: 0.000263829599134624\n",
      "Iteration is: 13531 and loss is: 0.00025090930284932256\n",
      "Iteration is: 13532 and loss is: 0.0002729434927459806\n",
      "Iteration is: 13533 and loss is: 0.0002582961751613766\n",
      "Iteration is: 13534 and loss is: 0.0002453494817018509\n",
      "Iteration is: 13535 and loss is: 0.00025727355387061834\n",
      "Iteration is: 13536 and loss is: 0.0002512128558009863\n",
      "Iteration is: 13537 and loss is: 0.0002442689728923142\n",
      "Iteration is: 13538 and loss is: 0.00024144456256181002\n",
      "Iteration is: 13539 and loss is: 0.0002461433468852192\n",
      "Iteration is: 13540 and loss is: 0.0002418424846837297\n",
      "Iteration is: 13541 and loss is: 0.000236816966207698\n",
      "Iteration is: 13542 and loss is: 0.0002415482304058969\n",
      "Iteration is: 13543 and loss is: 0.0002368123095948249\n",
      "Iteration is: 13544 and loss is: 0.00023260344460140914\n",
      "Iteration is: 13545 and loss is: 0.00023857681662775576\n",
      "Iteration is: 13546 and loss is: 0.00023197813425213099\n",
      "Iteration is: 13547 and loss is: 0.0002297613536939025\n",
      "Iteration is: 13548 and loss is: 0.00023338670143857598\n",
      "Iteration is: 13549 and loss is: 0.000231117955991067\n",
      "Iteration is: 13550 and loss is: 0.00022870348766446114\n",
      "Iteration is: 13551 and loss is: 0.00022912491112947464\n",
      "Iteration is: 13552 and loss is: 0.00022887038358021528\n",
      "Iteration is: 13553 and loss is: 0.00022793843527324498\n",
      "Iteration is: 13554 and loss is: 0.00022623551194556057\n",
      "Iteration is: 13555 and loss is: 0.00022722137509845197\n",
      "Iteration is: 13556 and loss is: 0.0002260156034026295\n",
      "Iteration is: 13557 and loss is: 0.0002250882680527866\n",
      "Iteration is: 13558 and loss is: 0.00022617813374381512\n",
      "Iteration is: 13559 and loss is: 0.00022492236166726798\n",
      "Iteration is: 13560 and loss is: 0.0002236232830910012\n",
      "Iteration is: 13561 and loss is: 0.00022460977197624743\n",
      "Iteration is: 13562 and loss is: 0.00022403078037314117\n",
      "Iteration is: 13563 and loss is: 0.00022320383868645877\n",
      "Iteration is: 13564 and loss is: 0.00022312837245408446\n",
      "Iteration is: 13565 and loss is: 0.00022303689911495894\n",
      "Iteration is: 13566 and loss is: 0.00022275579976849258\n",
      "Iteration is: 13567 and loss is: 0.00022236103541217744\n",
      "Iteration is: 13568 and loss is: 0.00022224773420020938\n",
      "Iteration is: 13569 and loss is: 0.0002220438327640295\n",
      "Iteration is: 13570 and loss is: 0.00022148582502268255\n",
      "Iteration is: 13571 and loss is: 0.00022157900093588978\n",
      "Iteration is: 13572 and loss is: 0.00022159838408697397\n",
      "Iteration is: 13573 and loss is: 0.00022099327179603279\n",
      "Iteration is: 13574 and loss is: 0.0002209294179920107\n",
      "Iteration is: 13575 and loss is: 0.000220941670704633\n",
      "Iteration is: 13576 and loss is: 0.00022062627249397337\n",
      "Iteration is: 13577 and loss is: 0.00022047720267437398\n",
      "Iteration is: 13578 and loss is: 0.00022037472808733582\n",
      "Iteration is: 13579 and loss is: 0.00022017730225343257\n",
      "Iteration is: 13580 and loss is: 0.00022003136109560728\n",
      "Iteration is: 13581 and loss is: 0.00021989672677591443\n",
      "Iteration is: 13582 and loss is: 0.00021986648789606988\n",
      "Iteration is: 13583 and loss is: 0.0002196541172452271\n",
      "Iteration is: 13584 and loss is: 0.00021946437482256442\n",
      "Iteration is: 13585 and loss is: 0.00021944560285191983\n",
      "Iteration is: 13586 and loss is: 0.00021931515948381275\n",
      "Iteration is: 13587 and loss is: 0.00021914963144809008\n",
      "Iteration is: 13588 and loss is: 0.00021909663337282836\n",
      "Iteration is: 13589 and loss is: 0.00021897249098401517\n",
      "Iteration is: 13590 and loss is: 0.00021883811859879643\n",
      "Iteration is: 13591 and loss is: 0.00021875396487303078\n",
      "Iteration is: 13592 and loss is: 0.00021866588213015348\n",
      "Iteration is: 13593 and loss is: 0.00021856214152649045\n",
      "Iteration is: 13594 and loss is: 0.00021844451839569956\n",
      "Iteration is: 13595 and loss is: 0.0002183508186135441\n",
      "Iteration is: 13596 and loss is: 0.0002182819735025987\n",
      "Iteration is: 13597 and loss is: 0.00021817981905769557\n",
      "Iteration is: 13598 and loss is: 0.000218080822378397\n",
      "Iteration is: 13599 and loss is: 0.000218015571590513\n",
      "Iteration is: 13600 and loss is: 0.00021791156905237585\n",
      "Iteration is: 13601 and loss is: 0.00021781810210086405\n",
      "Iteration is: 13602 and loss is: 0.00021775034838356078\n",
      "Iteration is: 13603 and loss is: 0.0002176693524233997\n",
      "Iteration is: 13604 and loss is: 0.00021758186630904675\n",
      "Iteration is: 13605 and loss is: 0.00021750167070422322\n",
      "Iteration is: 13606 and loss is: 0.00021742269746027887\n",
      "Iteration is: 13607 and loss is: 0.00021734816255047917\n",
      "Iteration is: 13608 and loss is: 0.0002172666136175394\n",
      "Iteration is: 13609 and loss is: 0.00021719085634686053\n",
      "Iteration is: 13610 and loss is: 0.0002171213272958994\n",
      "Iteration is: 13611 and loss is: 0.00021704283426515758\n",
      "Iteration is: 13612 and loss is: 0.00021696608746424317\n",
      "Iteration is: 13613 and loss is: 0.00021689827553927898\n",
      "Iteration is: 13614 and loss is: 0.00021682612714357674\n",
      "Iteration is: 13615 and loss is: 0.0002167530037695542\n",
      "Iteration is: 13616 and loss is: 0.0002166821068385616\n",
      "Iteration is: 13617 and loss is: 0.0002166139311157167\n",
      "Iteration is: 13618 and loss is: 0.00021654469310306013\n",
      "Iteration is: 13619 and loss is: 0.00021647478570230305\n",
      "Iteration is: 13620 and loss is: 0.00021640656632371247\n",
      "Iteration is: 13621 and loss is: 0.000216340966289863\n",
      "Iteration is: 13622 and loss is: 0.00021627340174745768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 13623 and loss is: 0.00021620342158712447\n",
      "Iteration is: 13624 and loss is: 0.00021613991702906787\n",
      "Iteration is: 13625 and loss is: 0.00021607412782032043\n",
      "Iteration is: 13626 and loss is: 0.0002160079311579466\n",
      "Iteration is: 13627 and loss is: 0.0002159416617359966\n",
      "Iteration is: 13628 and loss is: 0.0002158796414732933\n",
      "Iteration is: 13629 and loss is: 0.00021581625333055854\n",
      "Iteration is: 13630 and loss is: 0.00021575199207291007\n",
      "Iteration is: 13631 and loss is: 0.00021568778902292252\n",
      "Iteration is: 13632 and loss is: 0.00021562469191849232\n",
      "Iteration is: 13633 and loss is: 0.00021556144929490983\n",
      "Iteration is: 13634 and loss is: 0.00021549902157858014\n",
      "Iteration is: 13635 and loss is: 0.00021543737966567278\n",
      "Iteration is: 13636 and loss is: 0.0002153769601136446\n",
      "Iteration is: 13637 and loss is: 0.0002153141249436885\n",
      "Iteration is: 13638 and loss is: 0.00021525457850657403\n",
      "Iteration is: 13639 and loss is: 0.00021519292204175144\n",
      "Iteration is: 13640 and loss is: 0.00021513302635867149\n",
      "Iteration is: 13641 and loss is: 0.00021507262135855854\n",
      "Iteration is: 13642 and loss is: 0.00021501228911802173\n",
      "Iteration is: 13643 and loss is: 0.00021495566761586815\n",
      "Iteration is: 13644 and loss is: 0.00021489348728209734\n",
      "Iteration is: 13645 and loss is: 0.0002148343191947788\n",
      "Iteration is: 13646 and loss is: 0.0002147764025721699\n",
      "Iteration is: 13647 and loss is: 0.00021471883519552648\n",
      "Iteration is: 13648 and loss is: 0.00021465684403665364\n",
      "Iteration is: 13649 and loss is: 0.00021460035350173712\n",
      "Iteration is: 13650 and loss is: 0.00021454229135997593\n",
      "Iteration is: 13651 and loss is: 0.00021448374900501221\n",
      "Iteration is: 13652 and loss is: 0.0002144247409887612\n",
      "Iteration is: 13653 and loss is: 0.00021436833776533604\n",
      "Iteration is: 13654 and loss is: 0.00021431069762911648\n",
      "Iteration is: 13655 and loss is: 0.00021425267914310098\n",
      "Iteration is: 13656 and loss is: 0.0002141983713954687\n",
      "Iteration is: 13657 and loss is: 0.00021414109505712986\n",
      "Iteration is: 13658 and loss is: 0.00021408242173492908\n",
      "Iteration is: 13659 and loss is: 0.00021402686252258718\n",
      "Iteration is: 13660 and loss is: 0.0002139683929271996\n",
      "Iteration is: 13661 and loss is: 0.00021391425980255008\n",
      "Iteration is: 13662 and loss is: 0.00021385736181400716\n",
      "Iteration is: 13663 and loss is: 0.0002138022391591221\n",
      "Iteration is: 13664 and loss is: 0.00021374577772803605\n",
      "Iteration is: 13665 and loss is: 0.00021368997113313526\n",
      "Iteration is: 13666 and loss is: 0.00021363416453823447\n",
      "Iteration is: 13667 and loss is: 0.0002135795948561281\n",
      "Iteration is: 13668 and loss is: 0.00021352211479097605\n",
      "Iteration is: 13669 and loss is: 0.00021346844732761383\n",
      "Iteration is: 13670 and loss is: 0.0002134129754267633\n",
      "Iteration is: 13671 and loss is: 0.0002133580856025219\n",
      "Iteration is: 13672 and loss is: 0.0002133040688931942\n",
      "Iteration is: 13673 and loss is: 0.00021324906265363097\n",
      "Iteration is: 13674 and loss is: 0.00021319651568774134\n",
      "Iteration is: 13675 and loss is: 0.0002131402143277228\n",
      "Iteration is: 13676 and loss is: 0.00021308730356395245\n",
      "Iteration is: 13677 and loss is: 0.0002130330103682354\n",
      "Iteration is: 13678 and loss is: 0.00021297851344570518\n",
      "Iteration is: 13679 and loss is: 0.00021292458404786885\n",
      "Iteration is: 13680 and loss is: 0.00021287004346959293\n",
      "Iteration is: 13681 and loss is: 0.00021281848603393883\n",
      "Iteration is: 13682 and loss is: 0.00021276323241181672\n",
      "Iteration is: 13683 and loss is: 0.00021270816796459258\n",
      "Iteration is: 13684 and loss is: 0.0002126562176272273\n",
      "Iteration is: 13685 and loss is: 0.00021260228822939098\n",
      "Iteration is: 13686 and loss is: 0.00021254821331240237\n",
      "Iteration is: 13687 and loss is: 0.00021249469136819243\n",
      "Iteration is: 13688 and loss is: 0.00021244199888315052\n",
      "Iteration is: 13689 and loss is: 0.00021239178022369742\n",
      "Iteration is: 13690 and loss is: 0.00021233907318674028\n",
      "Iteration is: 13691 and loss is: 0.00021228677360340953\n",
      "Iteration is: 13692 and loss is: 0.0002122330479323864\n",
      "Iteration is: 13693 and loss is: 0.00021218063193373382\n",
      "Iteration is: 13694 and loss is: 0.00021212772116996348\n",
      "Iteration is: 13695 and loss is: 0.00021207300596870482\n",
      "Iteration is: 13696 and loss is: 0.00021202280186116695\n",
      "Iteration is: 13697 and loss is: 0.00021197264140937477\n",
      "Iteration is: 13698 and loss is: 0.00021191916312091053\n",
      "Iteration is: 13699 and loss is: 0.0002118666743626818\n",
      "Iteration is: 13700 and loss is: 0.00021181502961553633\n",
      "Iteration is: 13701 and loss is: 0.00021176118752919137\n",
      "Iteration is: 13702 and loss is: 0.00021170997933950275\n",
      "Iteration is: 13703 and loss is: 0.00021165887301322073\n",
      "Iteration is: 13704 and loss is: 0.00021160909091122448\n",
      "Iteration is: 13705 and loss is: 0.0002115547249559313\n",
      "Iteration is: 13706 and loss is: 0.00021150498650968075\n",
      "Iteration is: 13707 and loss is: 0.00021145159553270787\n",
      "Iteration is: 13708 and loss is: 0.00021140105673111975\n",
      "Iteration is: 13709 and loss is: 0.00021135031420271844\n",
      "Iteration is: 13710 and loss is: 0.00021129977540113032\n",
      "Iteration is: 13711 and loss is: 0.00021124767954461277\n",
      "Iteration is: 13712 and loss is: 0.00021119577286299318\n",
      "Iteration is: 13713 and loss is: 0.00021114537958055735\n",
      "Iteration is: 13714 and loss is: 0.0002110951900249347\n",
      "Iteration is: 13715 and loss is: 0.0002110437781084329\n",
      "Iteration is: 13716 and loss is: 0.00021099334117025137\n",
      "Iteration is: 13717 and loss is: 0.00021094136172905564\n",
      "Iteration is: 13718 and loss is: 0.00021089104120619595\n",
      "Iteration is: 13719 and loss is: 0.00021084019681438804\n",
      "Iteration is: 13720 and loss is: 0.00021078840654809028\n",
      "Iteration is: 13721 and loss is: 0.0002107382024405524\n",
      "Iteration is: 13722 and loss is: 0.0002106887404806912\n",
      "Iteration is: 13723 and loss is: 0.00021063786698505282\n",
      "Iteration is: 13724 and loss is: 0.0002105878375004977\n",
      "Iteration is: 13725 and loss is: 0.00021053770615253597\n",
      "Iteration is: 13726 and loss is: 0.0002104870800394565\n",
      "Iteration is: 13727 and loss is: 0.0002104372688336298\n",
      "Iteration is: 13728 and loss is: 0.00021038655540905893\n",
      "Iteration is: 13729 and loss is: 0.00021033592929597944\n",
      "Iteration is: 13730 and loss is: 0.00021028704941272736\n",
      "Iteration is: 13731 and loss is: 0.00021023709268774837\n",
      "Iteration is: 13732 and loss is: 0.00021018613188061863\n",
      "Iteration is: 13733 and loss is: 0.00021013656805735081\n",
      "Iteration is: 13734 and loss is: 0.00021008768817409873\n",
      "Iteration is: 13735 and loss is: 0.0002100376586895436\n",
      "Iteration is: 13736 and loss is: 0.0002099889243254438\n",
      "Iteration is: 13737 and loss is: 0.00020993742509745061\n",
      "Iteration is: 13738 and loss is: 0.00020988710457459092\n",
      "Iteration is: 13739 and loss is: 0.0002098385593853891\n",
      "Iteration is: 13740 and loss is: 0.00020978919928893447\n",
      "Iteration is: 13741 and loss is: 0.00020974120707251132\n",
      "Iteration is: 13742 and loss is: 0.00020968966418877244\n",
      "Iteration is: 13743 and loss is: 0.0002096406533382833\n",
      "Iteration is: 13744 and loss is: 0.00020959167159162462\n",
      "Iteration is: 13745 and loss is: 0.00020954234059900045\n",
      "Iteration is: 13746 and loss is: 0.00020949161262251437\n",
      "Iteration is: 13747 and loss is: 0.0002094436204060912\n",
      "Iteration is: 13748 and loss is: 0.00020939567184541374\n",
      "Iteration is: 13749 and loss is: 0.0002093453804263845\n",
      "Iteration is: 13750 and loss is: 0.0002092980721499771\n",
      "Iteration is: 13751 and loss is: 0.0002092482172884047\n",
      "Iteration is: 13752 and loss is: 0.0002091977366944775\n",
      "Iteration is: 13753 and loss is: 0.0002091487986035645\n",
      "Iteration is: 13754 and loss is: 0.00020910044258926064\n",
      "Iteration is: 13755 and loss is: 0.0002090515336021781\n",
      "Iteration is: 13756 and loss is: 0.00020900304662063718\n",
      "Iteration is: 13757 and loss is: 0.00020895323541481048\n",
      "Iteration is: 13758 and loss is: 0.0002089056361000985\n",
      "Iteration is: 13759 and loss is: 0.00020885717822238803\n",
      "Iteration is: 13760 and loss is: 0.0002088075561914593\n",
      "Iteration is: 13761 and loss is: 0.00020876017515547574\n",
      "Iteration is: 13762 and loss is: 0.00020871227025054395\n",
      "Iteration is: 13763 and loss is: 0.00020866302656941116\n",
      "Iteration is: 13764 and loss is: 0.000208616052987054\n",
      "Iteration is: 13765 and loss is: 0.00020857082563452423\n",
      "Iteration is: 13766 and loss is: 0.0002085227897623554\n",
      "Iteration is: 13767 and loss is: 0.00020847829000558704\n",
      "Iteration is: 13768 and loss is: 0.0002084355364786461\n",
      "Iteration is: 13769 and loss is: 0.00020839876378886402\n",
      "Iteration is: 13770 and loss is: 0.0002083648432744667\n",
      "Iteration is: 13771 and loss is: 0.00020834544557146728\n",
      "Iteration is: 13772 and loss is: 0.00020833926100749522\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 13773 and loss is: 0.00020836532348766923\n",
      "Iteration is: 13774 and loss is: 0.00020843930542469025\n",
      "Iteration is: 13775 and loss is: 0.00020859987125732005\n",
      "Iteration is: 13776 and loss is: 0.00020890511223115027\n",
      "Iteration is: 13777 and loss is: 0.00020946191216353327\n",
      "Iteration is: 13778 and loss is: 0.00021046356414444745\n",
      "Iteration is: 13779 and loss is: 0.00021222805662546307\n",
      "Iteration is: 13780 and loss is: 0.00021536790882237256\n",
      "Iteration is: 13781 and loss is: 0.0002208807854913175\n",
      "Iteration is: 13782 and loss is: 0.00023075516219250858\n",
      "Iteration is: 13783 and loss is: 0.00024808786110952497\n",
      "Iteration is: 13784 and loss is: 0.00027944735484197736\n",
      "Iteration is: 13785 and loss is: 0.00033382352557964623\n",
      "Iteration is: 13786 and loss is: 0.00043222022941336036\n",
      "Iteration is: 13787 and loss is: 0.0005941891577094793\n",
      "Iteration is: 13788 and loss is: 0.000875158584676683\n",
      "Iteration is: 13789 and loss is: 0.001260044053196907\n",
      "Iteration is: 13790 and loss is: 0.0017992113716900349\n",
      "Iteration is: 13791 and loss is: 0.0021148091182112694\n",
      "Iteration is: 13792 and loss is: 0.002034128177911043\n",
      "Iteration is: 13793 and loss is: 0.0013522668741643429\n",
      "Iteration is: 13794 and loss is: 0.0006436848780140281\n",
      "Iteration is: 13795 and loss is: 0.0005183738539926708\n",
      "Iteration is: 13796 and loss is: 0.0006384927546605468\n",
      "Iteration is: 13797 and loss is: 0.00082777114585042\n",
      "Iteration is: 13798 and loss is: 0.0009891524678096175\n",
      "Iteration is: 13799 and loss is: 0.0006056113634258509\n",
      "Iteration is: 13800 and loss is: 0.00027206013328395784\n",
      "Iteration is: 13801 and loss is: 0.0003771594201680273\n",
      "Iteration is: 13802 and loss is: 0.0006389548070728779\n",
      "Iteration is: 13803 and loss is: 0.0006749308668076992\n",
      "Iteration is: 13804 and loss is: 0.00040763747529126704\n",
      "Iteration is: 13805 and loss is: 0.0003912294050678611\n",
      "Iteration is: 13806 and loss is: 0.0007655004737898707\n",
      "Iteration is: 13807 and loss is: 0.0014803203521296382\n",
      "Iteration is: 13808 and loss is: 0.003059159033000469\n",
      "Iteration is: 13809 and loss is: 0.005298737436532974\n",
      "Iteration is: 13810 and loss is: 0.005519193131476641\n",
      "Iteration is: 13811 and loss is: 0.003049528691917658\n",
      "Iteration is: 13812 and loss is: 0.0038562752306461334\n",
      "Iteration is: 13813 and loss is: 0.008526834659278393\n",
      "Iteration is: 13814 and loss is: 0.013926709070801735\n",
      "Iteration is: 13815 and loss is: 0.009017985314130783\n",
      "Iteration is: 13816 and loss is: 0.002060959581285715\n",
      "Iteration is: 13817 and loss is: 0.003931189887225628\n",
      "Iteration is: 13818 and loss is: 0.011124920099973679\n",
      "Iteration is: 13819 and loss is: 0.02090916782617569\n",
      "Iteration is: 13820 and loss is: 0.03827688470482826\n",
      "Iteration is: 13821 and loss is: 0.044752322137355804\n",
      "Iteration is: 13822 and loss is: 0.014329029247164726\n",
      "Iteration is: 13823 and loss is: 0.024207429960370064\n",
      "Iteration is: 13824 and loss is: 0.024937722831964493\n",
      "Iteration is: 13825 and loss is: 0.023601360619068146\n",
      "Iteration is: 13826 and loss is: 0.03000703826546669\n",
      "Iteration is: 13827 and loss is: 0.03463922068476677\n",
      "Iteration is: 13828 and loss is: 0.03290010988712311\n",
      "Iteration is: 13829 and loss is: 0.040164679288864136\n",
      "Iteration is: 13830 and loss is: 0.028072867542505264\n",
      "Iteration is: 13831 and loss is: 0.03761717677116394\n",
      "Iteration is: 13832 and loss is: 0.02875591069459915\n",
      "Iteration is: 13833 and loss is: 0.03461079299449921\n",
      "Iteration is: 13834 and loss is: 0.024584101513028145\n",
      "Iteration is: 13835 and loss is: 0.027972543612122536\n",
      "Iteration is: 13836 and loss is: 0.02557310089468956\n",
      "Iteration is: 13837 and loss is: 0.018811283633112907\n",
      "Iteration is: 13838 and loss is: 0.017940275371074677\n",
      "Iteration is: 13839 and loss is: 0.01415199600160122\n",
      "Iteration is: 13840 and loss is: 0.013232802972197533\n",
      "Iteration is: 13841 and loss is: 0.009901728481054306\n",
      "Iteration is: 13842 and loss is: 0.010984745807945728\n",
      "Iteration is: 13843 and loss is: 0.007664061151444912\n",
      "Iteration is: 13844 and loss is: 0.009335467591881752\n",
      "Iteration is: 13845 and loss is: 0.005966219585388899\n",
      "Iteration is: 13846 and loss is: 0.005972358398139477\n",
      "Iteration is: 13847 and loss is: 0.005776602774858475\n",
      "Iteration is: 13848 and loss is: 0.005805331282317638\n",
      "Iteration is: 13849 and loss is: 0.005449336022138596\n",
      "Iteration is: 13850 and loss is: 0.005260910373181105\n",
      "Iteration is: 13851 and loss is: 0.0048812199383974075\n",
      "Iteration is: 13852 and loss is: 0.005134233273565769\n",
      "Iteration is: 13853 and loss is: 0.004617833532392979\n",
      "Iteration is: 13854 and loss is: 0.004875713028013706\n",
      "Iteration is: 13855 and loss is: 0.004248141776770353\n",
      "Iteration is: 13856 and loss is: 0.004631131887435913\n",
      "Iteration is: 13857 and loss is: 0.0035557816736400127\n",
      "Iteration is: 13858 and loss is: 0.003988166339695454\n",
      "Iteration is: 13859 and loss is: 0.0030926321633160114\n",
      "Iteration is: 13860 and loss is: 0.0033962982706725597\n",
      "Iteration is: 13861 and loss is: 0.0027840733528137207\n",
      "Iteration is: 13862 and loss is: 0.0026831957511603832\n",
      "Iteration is: 13863 and loss is: 0.0024496468249708414\n",
      "Iteration is: 13864 and loss is: 0.002170008607208729\n",
      "Iteration is: 13865 and loss is: 0.00214600283652544\n",
      "Iteration is: 13866 and loss is: 0.0019286805763840675\n",
      "Iteration is: 13867 and loss is: 0.0018082605674862862\n",
      "Iteration is: 13868 and loss is: 0.0018831309862434864\n",
      "Iteration is: 13869 and loss is: 0.0014955038204789162\n",
      "Iteration is: 13870 and loss is: 0.001702444744296372\n",
      "Iteration is: 13871 and loss is: 0.0013484342489391565\n",
      "Iteration is: 13872 and loss is: 0.0015480834990739822\n",
      "Iteration is: 13873 and loss is: 0.0013200577814131975\n",
      "Iteration is: 13874 and loss is: 0.001347106066532433\n",
      "Iteration is: 13875 and loss is: 0.001291300286538899\n",
      "Iteration is: 13876 and loss is: 0.001219120342284441\n",
      "Iteration is: 13877 and loss is: 0.0012419302947819233\n",
      "Iteration is: 13878 and loss is: 0.0011225896887481213\n",
      "Iteration is: 13879 and loss is: 0.0011729160323739052\n",
      "Iteration is: 13880 and loss is: 0.0010347719071432948\n",
      "Iteration is: 13881 and loss is: 0.0010727812768891454\n",
      "Iteration is: 13882 and loss is: 0.0009683519601821899\n",
      "Iteration is: 13883 and loss is: 0.000962603953666985\n",
      "Iteration is: 13884 and loss is: 0.0009150819387286901\n",
      "Iteration is: 13885 and loss is: 0.0008723639184609056\n",
      "Iteration is: 13886 and loss is: 0.0008657817961648107\n",
      "Iteration is: 13887 and loss is: 0.0007955176988616586\n",
      "Iteration is: 13888 and loss is: 0.000806972268037498\n",
      "Iteration is: 13889 and loss is: 0.0007349840016104281\n",
      "Iteration is: 13890 and loss is: 0.0007402829942293465\n",
      "Iteration is: 13891 and loss is: 0.0006969773676246405\n",
      "Iteration is: 13892 and loss is: 0.0006856416584923863\n",
      "Iteration is: 13893 and loss is: 0.0006638289778493345\n",
      "Iteration is: 13894 and loss is: 0.0006392034702003002\n",
      "Iteration is: 13895 and loss is: 0.0006313966005109251\n",
      "Iteration is: 13896 and loss is: 0.000601036474108696\n",
      "Iteration is: 13897 and loss is: 0.0005938785616308451\n",
      "Iteration is: 13898 and loss is: 0.0005667979130521417\n",
      "Iteration is: 13899 and loss is: 0.000559028354473412\n",
      "Iteration is: 13900 and loss is: 0.0005423108814284205\n",
      "Iteration is: 13901 and loss is: 0.0005293143331073225\n",
      "Iteration is: 13902 and loss is: 0.0005180392763577402\n",
      "Iteration is: 13903 and loss is: 0.0005000331439077854\n",
      "Iteration is: 13904 and loss is: 0.0004958559293299913\n",
      "Iteration is: 13905 and loss is: 0.00047972824540920556\n",
      "Iteration is: 13906 and loss is: 0.0004777664435096085\n",
      "Iteration is: 13907 and loss is: 0.0004630963667295873\n",
      "Iteration is: 13908 and loss is: 0.0004598625237122178\n",
      "Iteration is: 13909 and loss is: 0.0004469366976991296\n",
      "Iteration is: 13910 and loss is: 0.0004418237367644906\n",
      "Iteration is: 13911 and loss is: 0.0004316732520237565\n",
      "Iteration is: 13912 and loss is: 0.0004246277967467904\n",
      "Iteration is: 13913 and loss is: 0.0004171907203271985\n",
      "Iteration is: 13914 and loss is: 0.000411083921790123\n",
      "Iteration is: 13915 and loss is: 0.0004062680818606168\n",
      "Iteration is: 13916 and loss is: 0.0004002337227575481\n",
      "Iteration is: 13917 and loss is: 0.0003957945737056434\n",
      "Iteration is: 13918 and loss is: 0.0003905337071046233\n",
      "Iteration is: 13919 and loss is: 0.0003859417629428208\n",
      "Iteration is: 13920 and loss is: 0.00038165622390806675\n",
      "Iteration is: 13921 and loss is: 0.00037665365380235016\n",
      "Iteration is: 13922 and loss is: 0.0003729172167368233\n",
      "Iteration is: 13923 and loss is: 0.00036798365181311965\n",
      "Iteration is: 13924 and loss is: 0.0003641927905846387\n",
      "Iteration is: 13925 and loss is: 0.00035898940404877067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 13926 and loss is: 0.0003547167289070785\n",
      "Iteration is: 13927 and loss is: 0.00034996651811525226\n",
      "Iteration is: 13928 and loss is: 0.00034578790655359626\n",
      "Iteration is: 13929 and loss is: 0.00034169561695307493\n",
      "Iteration is: 13930 and loss is: 0.0003379469853825867\n",
      "Iteration is: 13931 and loss is: 0.0003341083647683263\n",
      "Iteration is: 13932 and loss is: 0.0003308047307655215\n",
      "Iteration is: 13933 and loss is: 0.00032718159491196275\n",
      "Iteration is: 13934 and loss is: 0.0003242243838030845\n",
      "Iteration is: 13935 and loss is: 0.00032067589927464724\n",
      "Iteration is: 13936 and loss is: 0.00031798132113181055\n",
      "Iteration is: 13937 and loss is: 0.0003147886600345373\n",
      "Iteration is: 13938 and loss is: 0.0003122230409644544\n",
      "Iteration is: 13939 and loss is: 0.00030932718073017895\n",
      "Iteration is: 13940 and loss is: 0.00030671790591441095\n",
      "Iteration is: 13941 and loss is: 0.0003040814190171659\n",
      "Iteration is: 13942 and loss is: 0.0003015950496774167\n",
      "Iteration is: 13943 and loss is: 0.00029919337248429656\n",
      "Iteration is: 13944 and loss is: 0.0002969109336845577\n",
      "Iteration is: 13945 and loss is: 0.0002947088214568794\n",
      "Iteration is: 13946 and loss is: 0.00029262714087963104\n",
      "Iteration is: 13947 and loss is: 0.0002905903966166079\n",
      "Iteration is: 13948 and loss is: 0.0002886399161070585\n",
      "Iteration is: 13949 and loss is: 0.00028676140937022865\n",
      "Iteration is: 13950 and loss is: 0.00028488622047007084\n",
      "Iteration is: 13951 and loss is: 0.00028316385578364134\n",
      "Iteration is: 13952 and loss is: 0.00028134946478530765\n",
      "Iteration is: 13953 and loss is: 0.00027967829373665154\n",
      "Iteration is: 13954 and loss is: 0.00027792935725301504\n",
      "Iteration is: 13955 and loss is: 0.00027629267424345016\n",
      "Iteration is: 13956 and loss is: 0.0002746452228166163\n",
      "Iteration is: 13957 and loss is: 0.00027306616539135575\n",
      "Iteration is: 13958 and loss is: 0.0002715139416977763\n",
      "Iteration is: 13959 and loss is: 0.0002700112236198038\n",
      "Iteration is: 13960 and loss is: 0.00026853225426748395\n",
      "Iteration is: 13961 and loss is: 0.000267097755568102\n",
      "Iteration is: 13962 and loss is: 0.00026566526503302157\n",
      "Iteration is: 13963 and loss is: 0.000264273548964411\n",
      "Iteration is: 13964 and loss is: 0.000262912071775645\n",
      "Iteration is: 13965 and loss is: 0.0002615746925584972\n",
      "Iteration is: 13966 and loss is: 0.0002602935128379613\n",
      "Iteration is: 13967 and loss is: 0.00025902094785124063\n",
      "Iteration is: 13968 and loss is: 0.00025780824944376945\n",
      "Iteration is: 13969 and loss is: 0.0002566161274444312\n",
      "Iteration is: 13970 and loss is: 0.00025547260884195566\n",
      "Iteration is: 13971 and loss is: 0.00025435819406993687\n",
      "Iteration is: 13972 and loss is: 0.0002532840589992702\n",
      "Iteration is: 13973 and loss is: 0.00025223736884072423\n",
      "Iteration is: 13974 and loss is: 0.0002512289793230593\n",
      "Iteration is: 13975 and loss is: 0.0002502455608919263\n",
      "Iteration is: 13976 and loss is: 0.00024929316714406013\n",
      "Iteration is: 13977 and loss is: 0.00024837383534759283\n",
      "Iteration is: 13978 and loss is: 0.0002474822977092117\n",
      "Iteration is: 13979 and loss is: 0.00024662609212100506\n",
      "Iteration is: 13980 and loss is: 0.00024579622549936175\n",
      "Iteration is: 13981 and loss is: 0.0002449968596920371\n",
      "Iteration is: 13982 and loss is: 0.00024422098067589104\n",
      "Iteration is: 13983 and loss is: 0.00024346800637431443\n",
      "Iteration is: 13984 and loss is: 0.00024274017778225243\n",
      "Iteration is: 13985 and loss is: 0.00024203435168601573\n",
      "Iteration is: 13986 and loss is: 0.00024134771956596524\n",
      "Iteration is: 13987 and loss is: 0.00024068406492006034\n",
      "Iteration is: 13988 and loss is: 0.00024004012811928988\n",
      "Iteration is: 13989 and loss is: 0.00023941276594996452\n",
      "Iteration is: 13990 and loss is: 0.00023880827939137816\n",
      "Iteration is: 13991 and loss is: 0.0002382156962994486\n",
      "Iteration is: 13992 and loss is: 0.00023764815705362707\n",
      "Iteration is: 13993 and loss is: 0.00023708990192972124\n",
      "Iteration is: 13994 and loss is: 0.0002365503751207143\n",
      "Iteration is: 13995 and loss is: 0.00023602746659889817\n",
      "Iteration is: 13996 and loss is: 0.00023551641788799316\n",
      "Iteration is: 13997 and loss is: 0.00023501802934333682\n",
      "Iteration is: 13998 and loss is: 0.0002345341199543327\n",
      "Iteration is: 13999 and loss is: 0.00023406338004861027\n",
      "Iteration is: 14000 and loss is: 0.00023360626073554158\n",
      "Iteration is: 14001 and loss is: 0.00023316290753427893\n",
      "Iteration is: 14002 and loss is: 0.00023272947873920202\n",
      "Iteration is: 14003 and loss is: 0.0002323031658306718\n",
      "Iteration is: 14004 and loss is: 0.0002318914921488613\n",
      "Iteration is: 14005 and loss is: 0.0002314877201570198\n",
      "Iteration is: 14006 and loss is: 0.00023109740868676454\n",
      "Iteration is: 14007 and loss is: 0.0002307150571141392\n",
      "Iteration is: 14008 and loss is: 0.00023034194600768387\n",
      "Iteration is: 14009 and loss is: 0.00022997755149845034\n",
      "Iteration is: 14010 and loss is: 0.00022962247021496296\n",
      "Iteration is: 14011 and loss is: 0.00022927523241378367\n",
      "Iteration is: 14012 and loss is: 0.00022893407731316984\n",
      "Iteration is: 14013 and loss is: 0.0002286036469740793\n",
      "Iteration is: 14014 and loss is: 0.00022827874636277556\n",
      "Iteration is: 14015 and loss is: 0.00022796480334363878\n",
      "Iteration is: 14016 and loss is: 0.00022765478934161365\n",
      "Iteration is: 14017 and loss is: 0.0002273526188218966\n",
      "Iteration is: 14018 and loss is: 0.00022705612354911864\n",
      "Iteration is: 14019 and loss is: 0.00022676491062156856\n",
      "Iteration is: 14020 and loss is: 0.00022648018784821033\n",
      "Iteration is: 14021 and loss is: 0.0002262035704916343\n",
      "Iteration is: 14022 and loss is: 0.00022593048925045878\n",
      "Iteration is: 14023 and loss is: 0.00022566330153495073\n",
      "Iteration is: 14024 and loss is: 0.00022540250211022794\n",
      "Iteration is: 14025 and loss is: 0.00022514647571370006\n",
      "Iteration is: 14026 and loss is: 0.00022489676484838128\n",
      "Iteration is: 14027 and loss is: 0.00022464906214736402\n",
      "Iteration is: 14028 and loss is: 0.0002244093338958919\n",
      "Iteration is: 14029 and loss is: 0.00022417434956878424\n",
      "Iteration is: 14030 and loss is: 0.00022393892868421972\n",
      "Iteration is: 14031 and loss is: 0.00022371183149516582\n",
      "Iteration is: 14032 and loss is: 0.00022348680067807436\n",
      "Iteration is: 14033 and loss is: 0.00022326663020066917\n",
      "Iteration is: 14034 and loss is: 0.00022305178572423756\n",
      "Iteration is: 14035 and loss is: 0.0002228397934231907\n",
      "Iteration is: 14036 and loss is: 0.00022263098799157888\n",
      "Iteration is: 14037 and loss is: 0.00022242643171921372\n",
      "Iteration is: 14038 and loss is: 0.00022222453844733536\n",
      "Iteration is: 14039 and loss is: 0.00022202647232916206\n",
      "Iteration is: 14040 and loss is: 0.0002218343724962324\n",
      "Iteration is: 14041 and loss is: 0.0002216440043412149\n",
      "Iteration is: 14042 and loss is: 0.00022145432012621313\n",
      "Iteration is: 14043 and loss is: 0.00022126725525595248\n",
      "Iteration is: 14044 and loss is: 0.00022108563280198723\n",
      "Iteration is: 14045 and loss is: 0.0002209070953540504\n",
      "Iteration is: 14046 and loss is: 0.00022072982392273843\n",
      "Iteration is: 14047 and loss is: 0.00022055860608816147\n",
      "Iteration is: 14048 and loss is: 0.00022038351744413376\n",
      "Iteration is: 14049 and loss is: 0.00022021721815690398\n",
      "Iteration is: 14050 and loss is: 0.00022004952188581228\n",
      "Iteration is: 14051 and loss is: 0.00021988549269735813\n",
      "Iteration is: 14052 and loss is: 0.0002197238791268319\n",
      "Iteration is: 14053 and loss is: 0.0002195658889831975\n",
      "Iteration is: 14054 and loss is: 0.0002194062399212271\n",
      "Iteration is: 14055 and loss is: 0.0002192539832321927\n",
      "Iteration is: 14056 and loss is: 0.00021910117357037961\n",
      "Iteration is: 14057 and loss is: 0.00021894898964092135\n",
      "Iteration is: 14058 and loss is: 0.0002188001381000504\n",
      "Iteration is: 14059 and loss is: 0.00021865381859242916\n",
      "Iteration is: 14060 and loss is: 0.00021851144265383482\n",
      "Iteration is: 14061 and loss is: 0.00021836535597685724\n",
      "Iteration is: 14062 and loss is: 0.00021822421695105731\n",
      "Iteration is: 14063 and loss is: 0.0002180855517508462\n",
      "Iteration is: 14064 and loss is: 0.0002179494476877153\n",
      "Iteration is: 14065 and loss is: 0.00021780867245979607\n",
      "Iteration is: 14066 and loss is: 0.00021767636644653976\n",
      "Iteration is: 14067 and loss is: 0.0002175422850996256\n",
      "Iteration is: 14068 and loss is: 0.0002174091205233708\n",
      "Iteration is: 14069 and loss is: 0.00021728016145061702\n",
      "Iteration is: 14070 and loss is: 0.0002171530795749277\n",
      "Iteration is: 14071 and loss is: 0.00021702589583583176\n",
      "Iteration is: 14072 and loss is: 0.0002169002837035805\n",
      "Iteration is: 14073 and loss is: 0.0002167741913581267\n",
      "Iteration is: 14074 and loss is: 0.00021665135864168406\n",
      "Iteration is: 14075 and loss is: 0.00021652804571203887\n",
      "Iteration is: 14076 and loss is: 0.00021640892373397946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 14077 and loss is: 0.00021628919057548046\n",
      "Iteration is: 14078 and loss is: 0.00021617099991999567\n",
      "Iteration is: 14079 and loss is: 0.00021605254733003676\n",
      "Iteration is: 14080 and loss is: 0.00021593864948954433\n",
      "Iteration is: 14081 and loss is: 0.0002158227434847504\n",
      "Iteration is: 14082 and loss is: 0.00021570829267147928\n",
      "Iteration is: 14083 and loss is: 0.00021559643209911883\n",
      "Iteration is: 14084 and loss is: 0.00021548510994762182\n",
      "Iteration is: 14085 and loss is: 0.00021537358406931162\n",
      "Iteration is: 14086 and loss is: 0.00021526566706597805\n",
      "Iteration is: 14087 and loss is: 0.0002151554508600384\n",
      "Iteration is: 14088 and loss is: 0.0002150481886928901\n",
      "Iteration is: 14089 and loss is: 0.00021493950043804944\n",
      "Iteration is: 14090 and loss is: 0.00021483487216755748\n",
      "Iteration is: 14091 and loss is: 0.00021472881780937314\n",
      "Iteration is: 14092 and loss is: 0.00021462520817294717\n",
      "Iteration is: 14093 and loss is: 0.00021452069631777704\n",
      "Iteration is: 14094 and loss is: 0.0002144191530533135\n",
      "Iteration is: 14095 and loss is: 0.00021431755158118904\n",
      "Iteration is: 14096 and loss is: 0.00021421595010906458\n",
      "Iteration is: 14097 and loss is: 0.00021411804482340813\n",
      "Iteration is: 14098 and loss is: 0.00021401894628070295\n",
      "Iteration is: 14099 and loss is: 0.00021392074995674193\n",
      "Iteration is: 14100 and loss is: 0.0002138229028787464\n",
      "Iteration is: 14101 and loss is: 0.00021372470655478537\n",
      "Iteration is: 14102 and loss is: 0.0002136295079253614\n",
      "Iteration is: 14103 and loss is: 0.0002135320974048227\n",
      "Iteration is: 14104 and loss is: 0.00021343861590139568\n",
      "Iteration is: 14105 and loss is: 0.00021334426128305495\n",
      "Iteration is: 14106 and loss is: 0.00021324961562640965\n",
      "Iteration is: 14107 and loss is: 0.00021315805497579277\n",
      "Iteration is: 14108 and loss is: 0.00021306454436853528\n",
      "Iteration is: 14109 and loss is: 0.00021297275088727474\n",
      "Iteration is: 14110 and loss is: 0.00021288127754814923\n",
      "Iteration is: 14111 and loss is: 0.00021278983331285417\n",
      "Iteration is: 14112 and loss is: 0.00021269926219247282\n",
      "Iteration is: 14113 and loss is: 0.00021261099027469754\n",
      "Iteration is: 14114 and loss is: 0.00021252353326417506\n",
      "Iteration is: 14115 and loss is: 0.00021243555238470435\n",
      "Iteration is: 14116 and loss is: 0.0002123466692864895\n",
      "Iteration is: 14117 and loss is: 0.0002122596197295934\n",
      "Iteration is: 14118 and loss is: 0.00021217457833699882\n",
      "Iteration is: 14119 and loss is: 0.00021208793623372912\n",
      "Iteration is: 14120 and loss is: 0.00021200161427259445\n",
      "Iteration is: 14121 and loss is: 0.00021191498672123998\n",
      "Iteration is: 14122 and loss is: 0.0002118319389410317\n",
      "Iteration is: 14123 and loss is: 0.00021174918219912797\n",
      "Iteration is: 14124 and loss is: 0.00021166287478990853\n",
      "Iteration is: 14125 and loss is: 0.00021158104937057942\n",
      "Iteration is: 14126 and loss is: 0.0002114996314048767\n",
      "Iteration is: 14127 and loss is: 0.00021141637989785522\n",
      "Iteration is: 14128 and loss is: 0.00021133723203092813\n",
      "Iteration is: 14129 and loss is: 0.00021125419880263507\n",
      "Iteration is: 14130 and loss is: 0.00021117375581525266\n",
      "Iteration is: 14131 and loss is: 0.00021109290537424386\n",
      "Iteration is: 14132 and loss is: 0.0002110143395839259\n",
      "Iteration is: 14133 and loss is: 0.0002109344204654917\n",
      "Iteration is: 14134 and loss is: 0.00021085375919938087\n",
      "Iteration is: 14135 and loss is: 0.00021077468409202993\n",
      "Iteration is: 14136 and loss is: 0.00021069744252599776\n",
      "Iteration is: 14137 and loss is: 0.00021061854204162955\n",
      "Iteration is: 14138 and loss is: 0.00021054467651993036\n",
      "Iteration is: 14139 and loss is: 0.00021046501933597028\n",
      "Iteration is: 14140 and loss is: 0.00021039045532234013\n",
      "Iteration is: 14141 and loss is: 0.00021031381038483232\n",
      "Iteration is: 14142 and loss is: 0.00021023821318522096\n",
      "Iteration is: 14143 and loss is: 0.00021016347454860806\n",
      "Iteration is: 14144 and loss is: 0.000210085985600017\n",
      "Iteration is: 14145 and loss is: 0.00021001174172852188\n",
      "Iteration is: 14146 and loss is: 0.0002099377306876704\n",
      "Iteration is: 14147 and loss is: 0.00020986318122595549\n",
      "Iteration is: 14148 and loss is: 0.00020979141118004918\n",
      "Iteration is: 14149 and loss is: 0.0002097164688166231\n",
      "Iteration is: 14150 and loss is: 0.00020964143914170563\n",
      "Iteration is: 14151 and loss is: 0.0002095686795655638\n",
      "Iteration is: 14152 and loss is: 0.0002094958326779306\n",
      "Iteration is: 14153 and loss is: 0.00020942496485076845\n",
      "Iteration is: 14154 and loss is: 0.00020935510110575706\n",
      "Iteration is: 14155 and loss is: 0.00020928197773173451\n",
      "Iteration is: 14156 and loss is: 0.00020921135728713125\n",
      "Iteration is: 14157 and loss is: 0.00020913957268930972\n",
      "Iteration is: 14158 and loss is: 0.0002090687776217237\n",
      "Iteration is: 14159 and loss is: 0.00020899932133033872\n",
      "Iteration is: 14160 and loss is: 0.00020892743486911058\n",
      "Iteration is: 14161 and loss is: 0.00020885816775262356\n",
      "Iteration is: 14162 and loss is: 0.00020879029761999846\n",
      "Iteration is: 14163 and loss is: 0.00020872242748737335\n",
      "Iteration is: 14164 and loss is: 0.00020865275291725993\n",
      "Iteration is: 14165 and loss is: 0.00020858350035268813\n",
      "Iteration is: 14166 and loss is: 0.00020851742010563612\n",
      "Iteration is: 14167 and loss is: 0.00020844752725679427\n",
      "Iteration is: 14168 and loss is: 0.00020837923511862755\n",
      "Iteration is: 14169 and loss is: 0.00020831228175666183\n",
      "Iteration is: 14170 and loss is: 0.00020824481907766312\n",
      "Iteration is: 14171 and loss is: 0.00020817809854634106\n",
      "Iteration is: 14172 and loss is: 0.00020811131980735809\n",
      "Iteration is: 14173 and loss is: 0.00020804518135264516\n",
      "Iteration is: 14174 and loss is: 0.00020797867910005152\n",
      "Iteration is: 14175 and loss is: 0.00020791366114281118\n",
      "Iteration is: 14176 and loss is: 0.00020784870139323175\n",
      "Iteration is: 14177 and loss is: 0.00020778161706402898\n",
      "Iteration is: 14178 and loss is: 0.00020771744311787188\n",
      "Iteration is: 14179 and loss is: 0.00020765297813341022\n",
      "Iteration is: 14180 and loss is: 0.00020758585014846176\n",
      "Iteration is: 14181 and loss is: 0.00020752311684191227\n",
      "Iteration is: 14182 and loss is: 0.00020745681831613183\n",
      "Iteration is: 14183 and loss is: 0.00020739386673085392\n",
      "Iteration is: 14184 and loss is: 0.00020732988195959479\n",
      "Iteration is: 14185 and loss is: 0.00020726679940707982\n",
      "Iteration is: 14186 and loss is: 0.00020720373140648007\n",
      "Iteration is: 14187 and loss is: 0.00020714144920930266\n",
      "Iteration is: 14188 and loss is: 0.00020707730436697602\n",
      "Iteration is: 14189 and loss is: 0.00020701551693491638\n",
      "Iteration is: 14190 and loss is: 0.00020695240527857095\n",
      "Iteration is: 14191 and loss is: 0.000206890152185224\n",
      "Iteration is: 14192 and loss is: 0.00020682907779701054\n",
      "Iteration is: 14193 and loss is: 0.00020676510757766664\n",
      "Iteration is: 14194 and loss is: 0.00020670417870860547\n",
      "Iteration is: 14195 and loss is: 0.00020664096518885344\n",
      "Iteration is: 14196 and loss is: 0.00020658195717260242\n",
      "Iteration is: 14197 and loss is: 0.00020652134844567627\n",
      "Iteration is: 14198 and loss is: 0.00020646059419959784\n",
      "Iteration is: 14199 and loss is: 0.00020639921422116458\n",
      "Iteration is: 14200 and loss is: 0.00020633722306229174\n",
      "Iteration is: 14201 and loss is: 0.00020627773483283818\n",
      "Iteration is: 14202 and loss is: 0.00020621731528081\n",
      "Iteration is: 14203 and loss is: 0.00020615675020962954\n",
      "Iteration is: 14204 and loss is: 0.0002060980477835983\n",
      "Iteration is: 14205 and loss is: 0.00020603742450475693\n",
      "Iteration is: 14206 and loss is: 0.00020597911498043686\n",
      "Iteration is: 14207 and loss is: 0.00020591969951055944\n",
      "Iteration is: 14208 and loss is: 0.0002058624231722206\n",
      "Iteration is: 14209 and loss is: 0.00020580120326485485\n",
      "Iteration is: 14210 and loss is: 0.0002057414094451815\n",
      "Iteration is: 14211 and loss is: 0.00020568424952216446\n",
      "Iteration is: 14212 and loss is: 0.00020562495046760887\n",
      "Iteration is: 14213 and loss is: 0.00020556844538077712\n",
      "Iteration is: 14214 and loss is: 0.00020550945191644132\n",
      "Iteration is: 14215 and loss is: 0.00020545018196571618\n",
      "Iteration is: 14216 and loss is: 0.00020539440447464585\n",
      "Iteration is: 14217 and loss is: 0.00020533721544779837\n",
      "Iteration is: 14218 and loss is: 0.00020527932792901993\n",
      "Iteration is: 14219 and loss is: 0.00020522080012597144\n",
      "Iteration is: 14220 and loss is: 0.00020516561926342547\n",
      "Iteration is: 14221 and loss is: 0.00020510872127488256\n",
      "Iteration is: 14222 and loss is: 0.0002050511830020696\n",
      "Iteration is: 14223 and loss is: 0.00020499540551099926\n",
      "Iteration is: 14224 and loss is: 0.0002049378672381863\n",
      "Iteration is: 14225 and loss is: 0.0002048811875283718\n",
      "Iteration is: 14226 and loss is: 0.00020482492982409894\n",
      "Iteration is: 14227 and loss is: 0.00020477038924582303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 14228 and loss is: 0.00020471311290748417\n",
      "Iteration is: 14229 and loss is: 0.00020465691341087222\n",
      "Iteration is: 14230 and loss is: 0.00020460138330236077\n",
      "Iteration is: 14231 and loss is: 0.00020454688637983054\n",
      "Iteration is: 14232 and loss is: 0.00020449020667001605\n",
      "Iteration is: 14233 and loss is: 0.00020443678658921272\n",
      "Iteration is: 14234 and loss is: 0.00020438354113139212\n",
      "Iteration is: 14235 and loss is: 0.00020432578457985073\n",
      "Iteration is: 14236 and loss is: 0.0002042707201326266\n",
      "Iteration is: 14237 and loss is: 0.00020421569934114814\n",
      "Iteration is: 14238 and loss is: 0.0002041624247794971\n",
      "Iteration is: 14239 and loss is: 0.00020410664728842676\n",
      "Iteration is: 14240 and loss is: 0.00020405212126206607\n",
      "Iteration is: 14241 and loss is: 0.00020399747882038355\n",
      "Iteration is: 14242 and loss is: 0.00020394488819874823\n",
      "Iteration is: 14243 and loss is: 0.00020389130804687738\n",
      "Iteration is: 14244 and loss is: 0.0002038362144958228\n",
      "Iteration is: 14245 and loss is: 0.00020378269255161285\n",
      "Iteration is: 14246 and loss is: 0.00020373045117594302\n",
      "Iteration is: 14247 and loss is: 0.00020367567776702344\n",
      "Iteration is: 14248 and loss is: 0.00020362384384498\n",
      "Iteration is: 14249 and loss is: 0.00020357005996629596\n",
      "Iteration is: 14250 and loss is: 0.00020351665443740785\n",
      "Iteration is: 14251 and loss is: 0.00020346487872302532\n",
      "Iteration is: 14252 and loss is: 0.00020341209892649204\n",
      "Iteration is: 14253 and loss is: 0.00020335885346867144\n",
      "Iteration is: 14254 and loss is: 0.0002033055352512747\n",
      "Iteration is: 14255 and loss is: 0.0002032519259955734\n",
      "Iteration is: 14256 and loss is: 0.0002031985786743462\n",
      "Iteration is: 14257 and loss is: 0.00020314627909101546\n",
      "Iteration is: 14258 and loss is: 0.0002030965406447649\n",
      "Iteration is: 14259 and loss is: 0.0002030436444329098\n",
      "Iteration is: 14260 and loss is: 0.00020299188327044249\n",
      "Iteration is: 14261 and loss is: 0.0002029394672717899\n",
      "Iteration is: 14262 and loss is: 0.00020288769155740738\n",
      "Iteration is: 14263 and loss is: 0.00020283754565753043\n",
      "Iteration is: 14264 and loss is: 0.0002027863811235875\n",
      "Iteration is: 14265 and loss is: 0.00020273367408663034\n",
      "Iteration is: 14266 and loss is: 0.00020268118532840163\n",
      "Iteration is: 14267 and loss is: 0.00020263077749405056\n",
      "Iteration is: 14268 and loss is: 0.00020258050062693655\n",
      "Iteration is: 14269 and loss is: 0.00020252828835509717\n",
      "Iteration is: 14270 and loss is: 0.0002024777204496786\n",
      "Iteration is: 14271 and loss is: 0.00020242799655534327\n",
      "Iteration is: 14272 and loss is: 0.00020237662829458714\n",
      "Iteration is: 14273 and loss is: 0.00020232581300660968\n",
      "Iteration is: 14274 and loss is: 0.0002022743719862774\n",
      "Iteration is: 14275 and loss is: 0.00020222290186211467\n",
      "Iteration is: 14276 and loss is: 0.00020217304700054228\n",
      "Iteration is: 14277 and loss is: 0.0002021243271883577\n",
      "Iteration is: 14278 and loss is: 0.0002020743559114635\n",
      "Iteration is: 14279 and loss is: 0.000202022900339216\n",
      "Iteration is: 14280 and loss is: 0.00020197387493681163\n",
      "Iteration is: 14281 and loss is: 0.00020192359806969762\n",
      "Iteration is: 14282 and loss is: 0.0002018737723119557\n",
      "Iteration is: 14283 and loss is: 0.00020182300067972392\n",
      "Iteration is: 14284 and loss is: 0.00020177233091089875\n",
      "Iteration is: 14285 and loss is: 0.0002017254155362025\n",
      "Iteration is: 14286 and loss is: 0.00020167502225376666\n",
      "Iteration is: 14287 and loss is: 0.00020162556029390544\n",
      "Iteration is: 14288 and loss is: 0.00020157592371106148\n",
      "Iteration is: 14289 and loss is: 0.00020152672368567437\n",
      "Iteration is: 14290 and loss is: 0.00020147675240878016\n",
      "Iteration is: 14291 and loss is: 0.0002014294732362032\n",
      "Iteration is: 14292 and loss is: 0.00020137912360951304\n",
      "Iteration is: 14293 and loss is: 0.00020132909412495792\n",
      "Iteration is: 14294 and loss is: 0.00020128169853705913\n",
      "Iteration is: 14295 and loss is: 0.0002012326440308243\n",
      "Iteration is: 14296 and loss is: 0.0002011826727539301\n",
      "Iteration is: 14297 and loss is: 0.00020113514619879425\n",
      "Iteration is: 14298 and loss is: 0.00020108622265979648\n",
      "Iteration is: 14299 and loss is: 0.00020103802671656013\n",
      "Iteration is: 14300 and loss is: 0.00020099047105759382\n",
      "Iteration is: 14301 and loss is: 0.00020094019419047982\n",
      "Iteration is: 14302 and loss is: 0.00020089096506126225\n",
      "Iteration is: 14303 and loss is: 0.00020084544667042792\n",
      "Iteration is: 14304 and loss is: 0.0002007950679399073\n",
      "Iteration is: 14305 and loss is: 0.00020074746862519532\n",
      "Iteration is: 14306 and loss is: 0.0002007011789828539\n",
      "Iteration is: 14307 and loss is: 0.00020065129501745105\n",
      "Iteration is: 14308 and loss is: 0.00020060344832018018\n",
      "Iteration is: 14309 and loss is: 0.0002005545684369281\n",
      "Iteration is: 14310 and loss is: 0.00020050760940648615\n",
      "Iteration is: 14311 and loss is: 0.0002004616690101102\n",
      "Iteration is: 14312 and loss is: 0.00020041372044943273\n",
      "Iteration is: 14313 and loss is: 0.00020036424393765628\n",
      "Iteration is: 14314 and loss is: 0.00020031830354128033\n",
      "Iteration is: 14315 and loss is: 0.00020027044229209423\n",
      "Iteration is: 14316 and loss is: 0.00020022274111397564\n",
      "Iteration is: 14317 and loss is: 0.00020017623319290578\n",
      "Iteration is: 14318 and loss is: 0.0002001298125833273\n",
      "Iteration is: 14319 and loss is: 0.00020008129649795592\n",
      "Iteration is: 14320 and loss is: 0.00020003507961519063\n",
      "Iteration is: 14321 and loss is: 0.00019998656352981925\n",
      "Iteration is: 14322 and loss is: 0.00019993999740108848\n",
      "Iteration is: 14323 and loss is: 0.00019989193242508918\n",
      "Iteration is: 14324 and loss is: 0.00019984784012194723\n",
      "Iteration is: 14325 and loss is: 0.0001997996587306261\n",
      "Iteration is: 14326 and loss is: 0.0001997530198423192\n",
      "Iteration is: 14327 and loss is: 0.00019970543507952243\n",
      "Iteration is: 14328 and loss is: 0.0001996593491639942\n",
      "Iteration is: 14329 and loss is: 0.00019961188081651926\n",
      "Iteration is: 14330 and loss is: 0.00019956621690653265\n",
      "Iteration is: 14331 and loss is: 0.00019951825379393995\n",
      "Iteration is: 14332 and loss is: 0.0001994724152609706\n",
      "Iteration is: 14333 and loss is: 0.00019942551443818957\n",
      "Iteration is: 14334 and loss is: 0.00019938024342991412\n",
      "Iteration is: 14335 and loss is: 0.00019933280418626964\n",
      "Iteration is: 14336 and loss is: 0.0001992876932490617\n",
      "Iteration is: 14337 and loss is: 0.00019923964282497764\n",
      "Iteration is: 14338 and loss is: 0.00019919467740692198\n",
      "Iteration is: 14339 and loss is: 0.0001991491299122572\n",
      "Iteration is: 14340 and loss is: 0.000199101836187765\n",
      "Iteration is: 14341 and loss is: 0.00019905510998796672\n",
      "Iteration is: 14342 and loss is: 0.0001990097516681999\n",
      "Iteration is: 14343 and loss is: 0.00019896289450116456\n",
      "Iteration is: 14344 and loss is: 0.00019891868578270078\n",
      "Iteration is: 14345 and loss is: 0.00019887066446244717\n",
      "Iteration is: 14346 and loss is: 0.00019882601918652654\n",
      "Iteration is: 14347 and loss is: 0.00019877910381183028\n",
      "Iteration is: 14348 and loss is: 0.00019873317796736956\n",
      "Iteration is: 14349 and loss is: 0.0001986906718229875\n",
      "Iteration is: 14350 and loss is: 0.0001986438874155283\n",
      "Iteration is: 14351 and loss is: 0.00019859783060383052\n",
      "Iteration is: 14352 and loss is: 0.00019855366554111242\n",
      "Iteration is: 14353 and loss is: 0.00019850638636853546\n",
      "Iteration is: 14354 and loss is: 0.00019846031500492245\n",
      "Iteration is: 14355 and loss is: 0.00019841772154904902\n",
      "Iteration is: 14356 and loss is: 0.00019837045692838728\n",
      "Iteration is: 14357 and loss is: 0.000198326917598024\n",
      "Iteration is: 14358 and loss is: 0.00019827918731607497\n",
      "Iteration is: 14359 and loss is: 0.00019823429465759546\n",
      "Iteration is: 14360 and loss is: 0.00019818739383481443\n",
      "Iteration is: 14361 and loss is: 0.00019814528059214354\n",
      "Iteration is: 14362 and loss is: 0.00019809887453448027\n",
      "Iteration is: 14363 and loss is: 0.00019805440388154238\n",
      "Iteration is: 14364 and loss is: 0.00019800770678557456\n",
      "Iteration is: 14365 and loss is: 0.00019796306150965393\n",
      "Iteration is: 14366 and loss is: 0.0001979176013264805\n",
      "Iteration is: 14367 and loss is: 0.00019787295605055988\n",
      "Iteration is: 14368 and loss is: 0.00019782703020609915\n",
      "Iteration is: 14369 and loss is: 0.0001977851497940719\n",
      "Iteration is: 14370 and loss is: 0.00019773955864366144\n",
      "Iteration is: 14371 and loss is: 0.00019769309437833726\n",
      "Iteration is: 14372 and loss is: 0.00019764751777984202\n",
      "Iteration is: 14373 and loss is: 0.00019760377472266555\n",
      "Iteration is: 14374 and loss is: 0.00019755991525016725\n",
      "Iteration is: 14375 and loss is: 0.00019751477520912886\n",
      "Iteration is: 14376 and loss is: 0.00019746892212424427\n",
      "Iteration is: 14377 and loss is: 0.00019742366566788405\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 14378 and loss is: 0.00019738043192774057\n",
      "Iteration is: 14379 and loss is: 0.00019733492808882147\n",
      "Iteration is: 14380 and loss is: 0.0001972915924852714\n",
      "Iteration is: 14381 and loss is: 0.00019724600133486092\n",
      "Iteration is: 14382 and loss is: 0.00019720286945812404\n",
      "Iteration is: 14383 and loss is: 0.00019715758389793336\n",
      "Iteration is: 14384 and loss is: 0.00019711221102625132\n",
      "Iteration is: 14385 and loss is: 0.00019706771126948297\n",
      "Iteration is: 14386 and loss is: 0.00019702303688973188\n",
      "Iteration is: 14387 and loss is: 0.00019697804236784577\n",
      "Iteration is: 14388 and loss is: 0.00019693498325068504\n",
      "Iteration is: 14389 and loss is: 0.00019689067266881466\n",
      "Iteration is: 14390 and loss is: 0.00019684561993926764\n",
      "Iteration is: 14391 and loss is: 0.00019680242985486984\n",
      "Iteration is: 14392 and loss is: 0.0001967595744645223\n",
      "Iteration is: 14393 and loss is: 0.00019671233894769102\n",
      "Iteration is: 14394 and loss is: 0.00019666840671561658\n",
      "Iteration is: 14395 and loss is: 0.00019662323757074773\n",
      "Iteration is: 14396 and loss is: 0.00019657929078675807\n",
      "Iteration is: 14397 and loss is: 0.0001965370902325958\n",
      "Iteration is: 14398 and loss is: 0.00019649200839921832\n",
      "Iteration is: 14399 and loss is: 0.00019644682470243424\n",
      "Iteration is: 14400 and loss is: 0.00019640402751974761\n",
      "Iteration is: 14401 and loss is: 0.0001963602553587407\n",
      "Iteration is: 14402 and loss is: 0.0001963153190445155\n",
      "Iteration is: 14403 and loss is: 0.00019627140136435628\n",
      "Iteration is: 14404 and loss is: 0.00019622617401182652\n",
      "Iteration is: 14405 and loss is: 0.00019618202350102365\n",
      "Iteration is: 14406 and loss is: 0.0001961385423783213\n",
      "Iteration is: 14407 and loss is: 0.00019609543960541487\n",
      "Iteration is: 14408 and loss is: 0.0001960511290235445\n",
      "Iteration is: 14409 and loss is: 0.0001960058871190995\n",
      "Iteration is: 14410 and loss is: 0.00019596278434619308\n",
      "Iteration is: 14411 and loss is: 0.00019591752788983285\n",
      "Iteration is: 14412 and loss is: 0.00019587448332458735\n",
      "Iteration is: 14413 and loss is: 0.00019582980894483626\n",
      "Iteration is: 14414 and loss is: 0.00019578734645619988\n",
      "Iteration is: 14415 and loss is: 0.00019574121688492596\n",
      "Iteration is: 14416 and loss is: 0.00019569878350012004\n",
      "Iteration is: 14417 and loss is: 0.0001956539781531319\n",
      "Iteration is: 14418 and loss is: 0.0001956124178832397\n",
      "Iteration is: 14419 and loss is: 0.0001955665647983551\n",
      "Iteration is: 14420 and loss is: 0.00019552306912373751\n",
      "Iteration is: 14421 and loss is: 0.0001954779145307839\n",
      "Iteration is: 14422 and loss is: 0.00019543454982340336\n",
      "Iteration is: 14423 and loss is: 0.00019539154891390353\n",
      "Iteration is: 14424 and loss is: 0.00019534601597115397\n",
      "Iteration is: 14425 and loss is: 0.00019530292775016278\n",
      "Iteration is: 14426 and loss is: 0.00019525917014107108\n",
      "Iteration is: 14427 and loss is: 0.00019521426293067634\n",
      "Iteration is: 14428 and loss is: 0.00019517060718499124\n",
      "Iteration is: 14429 and loss is: 0.00019512741710059345\n",
      "Iteration is: 14430 and loss is: 0.000195082655409351\n",
      "Iteration is: 14431 and loss is: 0.00019503996009007096\n",
      "Iteration is: 14432 and loss is: 0.0001949971483554691\n",
      "Iteration is: 14433 and loss is: 0.00019495042215567082\n",
      "Iteration is: 14434 and loss is: 0.00019490777049213648\n",
      "Iteration is: 14435 and loss is: 0.00019486580276861787\n",
      "Iteration is: 14436 and loss is: 0.00019481930939946324\n",
      "Iteration is: 14437 and loss is: 0.00019477601745165884\n",
      "Iteration is: 14438 and loss is: 0.00019473311840556562\n",
      "Iteration is: 14439 and loss is: 0.00019468623213469982\n",
      "Iteration is: 14440 and loss is: 0.00019464331853669137\n",
      "Iteration is: 14441 and loss is: 0.00019460023031570017\n",
      "Iteration is: 14442 and loss is: 0.00019455706933513284\n",
      "Iteration is: 14443 and loss is: 0.00019451123080216348\n",
      "Iteration is: 14444 and loss is: 0.00019446844817139208\n",
      "Iteration is: 14445 and loss is: 0.0001944244431797415\n",
      "Iteration is: 14446 and loss is: 0.00019437915761955082\n",
      "Iteration is: 14447 and loss is: 0.0001943342649610713\n",
      "Iteration is: 14448 and loss is: 0.00019429251551628113\n",
      "Iteration is: 14449 and loss is: 0.0001942468516062945\n",
      "Iteration is: 14450 and loss is: 0.0001942023664014414\n",
      "Iteration is: 14451 and loss is: 0.00019415709539316595\n",
      "Iteration is: 14452 and loss is: 0.00019411332323215902\n",
      "Iteration is: 14453 and loss is: 0.00019406984210945666\n",
      "Iteration is: 14454 and loss is: 0.00019402630277909338\n",
      "Iteration is: 14455 and loss is: 0.00019398215226829052\n",
      "Iteration is: 14456 and loss is: 0.00019393744878470898\n",
      "Iteration is: 14457 and loss is: 0.00019389198860153556\n",
      "Iteration is: 14458 and loss is: 0.00019384693587198853\n",
      "Iteration is: 14459 and loss is: 0.00019380325102247298\n",
      "Iteration is: 14460 and loss is: 0.00019375831470824778\n",
      "Iteration is: 14461 and loss is: 0.00019371356756892055\n",
      "Iteration is: 14462 and loss is: 0.0001936698390636593\n",
      "Iteration is: 14463 and loss is: 0.00019362311286386102\n",
      "Iteration is: 14464 and loss is: 0.00019358104327693582\n",
      "Iteration is: 14465 and loss is: 0.00019353504467289895\n",
      "Iteration is: 14466 and loss is: 0.00019348862406332046\n",
      "Iteration is: 14467 and loss is: 0.000193445710465312\n",
      "Iteration is: 14468 and loss is: 0.00019340201106388122\n",
      "Iteration is: 14469 and loss is: 0.0001933563908096403\n",
      "Iteration is: 14470 and loss is: 0.00019331029034219682\n",
      "Iteration is: 14471 and loss is: 0.00019326448091305792\n",
      "Iteration is: 14472 and loss is: 0.00019322181469760835\n",
      "Iteration is: 14473 and loss is: 0.00019317427359055728\n",
      "Iteration is: 14474 and loss is: 0.0001931282749865204\n",
      "Iteration is: 14475 and loss is: 0.00019308223272673786\n",
      "Iteration is: 14476 and loss is: 0.00019303540466353297\n",
      "Iteration is: 14477 and loss is: 0.00019299102132208645\n",
      "Iteration is: 14478 and loss is: 0.00019294352387078106\n",
      "Iteration is: 14479 and loss is: 0.00019289810734335333\n",
      "Iteration is: 14480 and loss is: 0.00019285304006189108\n",
      "Iteration is: 14481 and loss is: 0.00019280335982330143\n",
      "Iteration is: 14482 and loss is: 0.0001927583070937544\n",
      "Iteration is: 14483 and loss is: 0.00019271220662631094\n",
      "Iteration is: 14484 and loss is: 0.00019266371964477003\n",
      "Iteration is: 14485 and loss is: 0.0001926190743688494\n",
      "Iteration is: 14486 and loss is: 0.00019256914674770087\n",
      "Iteration is: 14487 and loss is: 0.00019252183847129345\n",
      "Iteration is: 14488 and loss is: 0.00019247407908551395\n",
      "Iteration is: 14489 and loss is: 0.00019242540292907506\n",
      "Iteration is: 14490 and loss is: 0.00019237579545006156\n",
      "Iteration is: 14491 and loss is: 0.0001923275412991643\n",
      "Iteration is: 14492 and loss is: 0.0001922772644320503\n",
      "Iteration is: 14493 and loss is: 0.00019222825358156115\n",
      "Iteration is: 14494 and loss is: 0.00019217707449570298\n",
      "Iteration is: 14495 and loss is: 0.00019212423649150878\n",
      "Iteration is: 14496 and loss is: 0.0001920723880175501\n",
      "Iteration is: 14497 and loss is: 0.00019201905524823815\n",
      "Iteration is: 14498 and loss is: 0.00019196435459889472\n",
      "Iteration is: 14499 and loss is: 0.00019190864986740053\n",
      "Iteration is: 14500 and loss is: 0.00019185211567673832\n",
      "Iteration is: 14501 and loss is: 0.00019179441733285785\n",
      "Iteration is: 14502 and loss is: 0.00019173497275914997\n",
      "Iteration is: 14503 and loss is: 0.00019167113350704312\n",
      "Iteration is: 14504 and loss is: 0.00019160860392730683\n",
      "Iteration is: 14505 and loss is: 0.00019153665925841779\n",
      "Iteration is: 14506 and loss is: 0.00019146452541463077\n",
      "Iteration is: 14507 and loss is: 0.00019138443167321384\n",
      "Iteration is: 14508 and loss is: 0.00019129784777760506\n",
      "Iteration is: 14509 and loss is: 0.000191199011169374\n",
      "Iteration is: 14510 and loss is: 0.0001910924620460719\n",
      "Iteration is: 14511 and loss is: 0.00019096318283118308\n",
      "Iteration is: 14512 and loss is: 0.0001908130361698568\n",
      "Iteration is: 14513 and loss is: 0.00019062909996137023\n",
      "Iteration is: 14514 and loss is: 0.00019039545441046357\n",
      "Iteration is: 14515 and loss is: 0.00019009587413165718\n",
      "Iteration is: 14516 and loss is: 0.0001897040056064725\n",
      "Iteration is: 14517 and loss is: 0.0001891823048936203\n",
      "Iteration is: 14518 and loss is: 0.00018848018953576684\n",
      "Iteration is: 14519 and loss is: 0.00018755183555185795\n",
      "Iteration is: 14520 and loss is: 0.00018637700122781098\n",
      "Iteration is: 14521 and loss is: 0.0001849994732765481\n",
      "Iteration is: 14522 and loss is: 0.00018357811495661736\n",
      "Iteration is: 14523 and loss is: 0.00018236221512779593\n",
      "Iteration is: 14524 and loss is: 0.0001815698342397809\n",
      "Iteration is: 14525 and loss is: 0.0001812060654629022\n",
      "Iteration is: 14526 and loss is: 0.00018108371295966208\n",
      "Iteration is: 14527 and loss is: 0.0001809284440241754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 14528 and loss is: 0.00018053254461847246\n",
      "Iteration is: 14529 and loss is: 0.0001798723969841376\n",
      "Iteration is: 14530 and loss is: 0.00017917409422807395\n",
      "Iteration is: 14531 and loss is: 0.00017883311375044286\n",
      "Iteration is: 14532 and loss is: 0.0001793261617422104\n",
      "Iteration is: 14533 and loss is: 0.00018166558584198356\n",
      "Iteration is: 14534 and loss is: 0.00018877160619013011\n",
      "Iteration is: 14535 and loss is: 0.00020910633611492813\n",
      "Iteration is: 14536 and loss is: 0.00026570900809019804\n",
      "Iteration is: 14537 and loss is: 0.00042675939039327204\n",
      "Iteration is: 14538 and loss is: 0.0008607762865722179\n",
      "Iteration is: 14539 and loss is: 0.0020450230222195387\n",
      "Iteration is: 14540 and loss is: 0.004500523675233126\n",
      "Iteration is: 14541 and loss is: 0.008130929432809353\n",
      "Iteration is: 14542 and loss is: 0.008541948162019253\n",
      "Iteration is: 14543 and loss is: 0.004111229907721281\n",
      "Iteration is: 14544 and loss is: 0.0024122390896081924\n",
      "Iteration is: 14545 and loss is: 0.006740855053067207\n",
      "Iteration is: 14546 and loss is: 0.008539054542779922\n",
      "Iteration is: 14547 and loss is: 0.004944449290633202\n",
      "Iteration is: 14548 and loss is: 0.001108324620872736\n",
      "Iteration is: 14549 and loss is: 0.006407379172742367\n",
      "Iteration is: 14550 and loss is: 0.005586754530668259\n",
      "Iteration is: 14551 and loss is: 0.0008606916526332498\n",
      "Iteration is: 14552 and loss is: 0.005268656648695469\n",
      "Iteration is: 14553 and loss is: 0.007558226119726896\n",
      "Iteration is: 14554 and loss is: 0.005160626955330372\n",
      "Iteration is: 14555 and loss is: 0.003894038265570998\n",
      "Iteration is: 14556 and loss is: 0.0023676129058003426\n",
      "Iteration is: 14557 and loss is: 0.0011947741732001305\n",
      "Iteration is: 14558 and loss is: 0.003054942237213254\n",
      "Iteration is: 14559 and loss is: 0.0015568853123113513\n",
      "Iteration is: 14560 and loss is: 0.0012929011136293411\n",
      "Iteration is: 14561 and loss is: 0.002274375408887863\n",
      "Iteration is: 14562 and loss is: 0.0004968590219505131\n",
      "Iteration is: 14563 and loss is: 0.0022028665989637375\n",
      "Iteration is: 14564 and loss is: 0.0007017493480816483\n",
      "Iteration is: 14565 and loss is: 0.0012511087115854025\n",
      "Iteration is: 14566 and loss is: 0.0011666221544146538\n",
      "Iteration is: 14567 and loss is: 0.0005184574984014034\n",
      "Iteration is: 14568 and loss is: 0.0013450803235173225\n",
      "Iteration is: 14569 and loss is: 0.00043461209861561656\n",
      "Iteration is: 14570 and loss is: 0.0009069930529221892\n",
      "Iteration is: 14571 and loss is: 0.0007091384613886476\n",
      "Iteration is: 14572 and loss is: 0.0005652057589031756\n",
      "Iteration is: 14573 and loss is: 0.0006871013902127743\n",
      "Iteration is: 14574 and loss is: 0.0005010801833122969\n",
      "Iteration is: 14575 and loss is: 0.0005233071278780699\n",
      "Iteration is: 14576 and loss is: 0.0006434458773583174\n",
      "Iteration is: 14577 and loss is: 0.0002836502972058952\n",
      "Iteration is: 14578 and loss is: 0.0006350508192554116\n",
      "Iteration is: 14579 and loss is: 0.00037822112790308893\n",
      "Iteration is: 14580 and loss is: 0.00045311579015105963\n",
      "Iteration is: 14581 and loss is: 0.0005146779003553092\n",
      "Iteration is: 14582 and loss is: 0.0002787614066619426\n",
      "Iteration is: 14583 and loss is: 0.00042286765528842807\n",
      "Iteration is: 14584 and loss is: 0.00029525061836466193\n",
      "Iteration is: 14585 and loss is: 0.0003328782331664115\n",
      "Iteration is: 14586 and loss is: 0.000365109124686569\n",
      "Iteration is: 14587 and loss is: 0.0002892371849156916\n",
      "Iteration is: 14588 and loss is: 0.00029874916072003543\n",
      "Iteration is: 14589 and loss is: 0.0002975576207973063\n",
      "Iteration is: 14590 and loss is: 0.00024102248426061124\n",
      "Iteration is: 14591 and loss is: 0.00031608238350600004\n",
      "Iteration is: 14592 and loss is: 0.0002513232175260782\n",
      "Iteration is: 14593 and loss is: 0.0002537897671572864\n",
      "Iteration is: 14594 and loss is: 0.00026274126139469445\n",
      "Iteration is: 14595 and loss is: 0.00020233402028679848\n",
      "Iteration is: 14596 and loss is: 0.0002626845380291343\n",
      "Iteration is: 14597 and loss is: 0.0002176212437916547\n",
      "Iteration is: 14598 and loss is: 0.00023636489640921354\n",
      "Iteration is: 14599 and loss is: 0.00023939096718095243\n",
      "Iteration is: 14600 and loss is: 0.0002144999452866614\n",
      "Iteration is: 14601 and loss is: 0.00022384947806131095\n",
      "Iteration is: 14602 and loss is: 0.00021067482884973288\n",
      "Iteration is: 14603 and loss is: 0.00019923178479075432\n",
      "Iteration is: 14604 and loss is: 0.0002081060374621302\n",
      "Iteration is: 14605 and loss is: 0.00019892657292075455\n",
      "Iteration is: 14606 and loss is: 0.00019885727670043707\n",
      "Iteration is: 14607 and loss is: 0.00020940371905453503\n",
      "Iteration is: 14608 and loss is: 0.00019148252613376826\n",
      "Iteration is: 14609 and loss is: 0.00020301886252127588\n",
      "Iteration is: 14610 and loss is: 0.0001921747752930969\n",
      "Iteration is: 14611 and loss is: 0.00018829177133738995\n",
      "Iteration is: 14612 and loss is: 0.00019307903130538762\n",
      "Iteration is: 14613 and loss is: 0.0001826459774747491\n",
      "Iteration is: 14614 and loss is: 0.00018783155246637762\n",
      "Iteration is: 14615 and loss is: 0.00018521702440921217\n",
      "Iteration is: 14616 and loss is: 0.00018319259106647223\n",
      "Iteration is: 14617 and loss is: 0.00018547695071902126\n",
      "Iteration is: 14618 and loss is: 0.00018422283756081015\n",
      "Iteration is: 14619 and loss is: 0.00018320043454878032\n",
      "Iteration is: 14620 and loss is: 0.00018536581774242222\n",
      "Iteration is: 14621 and loss is: 0.00018313754117116332\n",
      "Iteration is: 14622 and loss is: 0.0001828980603022501\n",
      "Iteration is: 14623 and loss is: 0.0001840755867306143\n",
      "Iteration is: 14624 and loss is: 0.00018046837067231536\n",
      "Iteration is: 14625 and loss is: 0.00018254309543408453\n",
      "Iteration is: 14626 and loss is: 0.0001807983499020338\n",
      "Iteration is: 14627 and loss is: 0.00017981926794163883\n",
      "Iteration is: 14628 and loss is: 0.00018106127390637994\n",
      "Iteration is: 14629 and loss is: 0.00017915311036631465\n",
      "Iteration is: 14630 and loss is: 0.00018003561126533896\n",
      "Iteration is: 14631 and loss is: 0.00018005634774453938\n",
      "Iteration is: 14632 and loss is: 0.00017988760373555124\n",
      "Iteration is: 14633 and loss is: 0.00018086643831338733\n",
      "Iteration is: 14634 and loss is: 0.00018168667156714946\n",
      "Iteration is: 14635 and loss is: 0.00018256125622428954\n",
      "Iteration is: 14636 and loss is: 0.00018473567615728825\n",
      "Iteration is: 14637 and loss is: 0.00018700733198784292\n",
      "Iteration is: 14638 and loss is: 0.00019006214279215783\n",
      "Iteration is: 14639 and loss is: 0.0001952510792762041\n",
      "Iteration is: 14640 and loss is: 0.0002009552699746564\n",
      "Iteration is: 14641 and loss is: 0.00021033771918155253\n",
      "Iteration is: 14642 and loss is: 0.00022259721299633384\n",
      "Iteration is: 14643 and loss is: 0.00024022077559493482\n",
      "Iteration is: 14644 and loss is: 0.0002640517195686698\n",
      "Iteration is: 14645 and loss is: 0.00029786917730234563\n",
      "Iteration is: 14646 and loss is: 0.00033815737697295845\n",
      "Iteration is: 14647 and loss is: 0.00039141540764831007\n",
      "Iteration is: 14648 and loss is: 0.0004370227688923478\n",
      "Iteration is: 14649 and loss is: 0.0004779680457431823\n",
      "Iteration is: 14650 and loss is: 0.00047137276851572096\n",
      "Iteration is: 14651 and loss is: 0.00041989475721493363\n",
      "Iteration is: 14652 and loss is: 0.0003220673534087837\n",
      "Iteration is: 14653 and loss is: 0.0002284267684444785\n",
      "Iteration is: 14654 and loss is: 0.00018306850688531995\n",
      "Iteration is: 14655 and loss is: 0.00019938180048484355\n",
      "Iteration is: 14656 and loss is: 0.00025160241057164967\n",
      "Iteration is: 14657 and loss is: 0.0003087587538175285\n",
      "Iteration is: 14658 and loss is: 0.00035937936627306044\n",
      "Iteration is: 14659 and loss is: 0.00039672941784374416\n",
      "Iteration is: 14660 and loss is: 0.00044063886161893606\n",
      "Iteration is: 14661 and loss is: 0.0004705785831902176\n",
      "Iteration is: 14662 and loss is: 0.0004975837073288858\n",
      "Iteration is: 14663 and loss is: 0.0004766983911395073\n",
      "Iteration is: 14664 and loss is: 0.0004111389280296862\n",
      "Iteration is: 14665 and loss is: 0.00030465383315458894\n",
      "Iteration is: 14666 and loss is: 0.0002128830528818071\n",
      "Iteration is: 14667 and loss is: 0.0001837594318203628\n",
      "Iteration is: 14668 and loss is: 0.0002192516840295866\n",
      "Iteration is: 14669 and loss is: 0.0002778532507363707\n",
      "Iteration is: 14670 and loss is: 0.00031748463516123593\n",
      "Iteration is: 14671 and loss is: 0.0003264553379267454\n",
      "Iteration is: 14672 and loss is: 0.00030972444801591337\n",
      "Iteration is: 14673 and loss is: 0.00028791456134058535\n",
      "Iteration is: 14674 and loss is: 0.0002641676110215485\n",
      "Iteration is: 14675 and loss is: 0.0002473719359841198\n",
      "Iteration is: 14676 and loss is: 0.00023373233852908015\n",
      "Iteration is: 14677 and loss is: 0.00022516862372867763\n",
      "Iteration is: 14678 and loss is: 0.00021848795586265624\n",
      "Iteration is: 14679 and loss is: 0.00021500789443962276\n",
      "Iteration is: 14680 and loss is: 0.00021266558906063437\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 14681 and loss is: 0.0002121785655617714\n",
      "Iteration is: 14682 and loss is: 0.0002116246469086036\n",
      "Iteration is: 14683 and loss is: 0.00021214773005340248\n",
      "Iteration is: 14684 and loss is: 0.00021256072795949876\n",
      "Iteration is: 14685 and loss is: 0.0002144966128980741\n",
      "Iteration is: 14686 and loss is: 0.00021733906760346144\n",
      "Iteration is: 14687 and loss is: 0.00022366837947629392\n",
      "Iteration is: 14688 and loss is: 0.00023314780264627188\n",
      "Iteration is: 14689 and loss is: 0.00025013211416080594\n",
      "Iteration is: 14690 and loss is: 0.00027447755564935505\n",
      "Iteration is: 14691 and loss is: 0.00031460291938856244\n",
      "Iteration is: 14692 and loss is: 0.0003669864381663501\n",
      "Iteration is: 14693 and loss is: 0.00044378850725479424\n",
      "Iteration is: 14694 and loss is: 0.0005191730451770127\n",
      "Iteration is: 14695 and loss is: 0.0005914150387980044\n",
      "Iteration is: 14696 and loss is: 0.0005886146682314575\n",
      "Iteration is: 14697 and loss is: 0.0004987596184946597\n",
      "Iteration is: 14698 and loss is: 0.00034131339634768665\n",
      "Iteration is: 14699 and loss is: 0.0002137998235411942\n",
      "Iteration is: 14700 and loss is: 0.0001941430091392249\n",
      "Iteration is: 14701 and loss is: 0.00026688064099289477\n",
      "Iteration is: 14702 and loss is: 0.0003464579058345407\n",
      "Iteration is: 14703 and loss is: 0.00036170496605336666\n",
      "Iteration is: 14704 and loss is: 0.000311930722091347\n",
      "Iteration is: 14705 and loss is: 0.00023898144718259573\n",
      "Iteration is: 14706 and loss is: 0.00018754151824396104\n",
      "Iteration is: 14707 and loss is: 0.00017467162979301065\n",
      "Iteration is: 14708 and loss is: 0.00019552072626538575\n",
      "Iteration is: 14709 and loss is: 0.00024409063917119056\n",
      "Iteration is: 14710 and loss is: 0.00032595021184533834\n",
      "Iteration is: 14711 and loss is: 0.0004724414611700922\n",
      "Iteration is: 14712 and loss is: 0.0007067426922731102\n",
      "Iteration is: 14713 and loss is: 0.0010896732565015554\n",
      "Iteration is: 14714 and loss is: 0.0014317729510366917\n",
      "Iteration is: 14715 and loss is: 0.0014554498484358191\n",
      "Iteration is: 14716 and loss is: 0.000913188443519175\n",
      "Iteration is: 14717 and loss is: 0.00031688768649473786\n",
      "Iteration is: 14718 and loss is: 0.00032816192833706737\n",
      "Iteration is: 14719 and loss is: 0.0006814323714934289\n",
      "Iteration is: 14720 and loss is: 0.000607976398896426\n",
      "Iteration is: 14721 and loss is: 0.0002933780779130757\n",
      "Iteration is: 14722 and loss is: 0.00040239215013571084\n",
      "Iteration is: 14723 and loss is: 0.0005712894489988685\n",
      "Iteration is: 14724 and loss is: 0.0003752910124603659\n",
      "Iteration is: 14725 and loss is: 0.00026333058485761285\n",
      "Iteration is: 14726 and loss is: 0.0004377860459499061\n",
      "Iteration is: 14727 and loss is: 0.00043100921902805567\n",
      "Iteration is: 14728 and loss is: 0.00023281660105567425\n",
      "Iteration is: 14729 and loss is: 0.0002576761762611568\n",
      "Iteration is: 14730 and loss is: 0.0003737035731319338\n",
      "Iteration is: 14731 and loss is: 0.0003608460247050971\n",
      "Iteration is: 14732 and loss is: 0.00029594628722406924\n",
      "Iteration is: 14733 and loss is: 0.0002233430277556181\n",
      "Iteration is: 14734 and loss is: 0.00022329601051751524\n",
      "Iteration is: 14735 and loss is: 0.0003119764442089945\n",
      "Iteration is: 14736 and loss is: 0.0003498571750242263\n",
      "Iteration is: 14737 and loss is: 0.0003318407980259508\n",
      "Iteration is: 14738 and loss is: 0.0003047846839763224\n",
      "Iteration is: 14739 and loss is: 0.00025601949892006814\n",
      "Iteration is: 14740 and loss is: 0.00022507994435727596\n",
      "Iteration is: 14741 and loss is: 0.0002184357144869864\n",
      "Iteration is: 14742 and loss is: 0.00020243291510269046\n",
      "Iteration is: 14743 and loss is: 0.00019690804765559733\n",
      "Iteration is: 14744 and loss is: 0.00020370393758639693\n",
      "Iteration is: 14745 and loss is: 0.00020153616787865758\n",
      "Iteration is: 14746 and loss is: 0.0002104197337757796\n",
      "Iteration is: 14747 and loss is: 0.00022825021005701274\n",
      "Iteration is: 14748 and loss is: 0.0002434623456792906\n",
      "Iteration is: 14749 and loss is: 0.0002691304252948612\n",
      "Iteration is: 14750 and loss is: 0.00030029757181182504\n",
      "Iteration is: 14751 and loss is: 0.0003227136912755668\n",
      "Iteration is: 14752 and loss is: 0.00035474568721838295\n",
      "Iteration is: 14753 and loss is: 0.0003793563228100538\n",
      "Iteration is: 14754 and loss is: 0.000403117184760049\n",
      "Iteration is: 14755 and loss is: 0.00041213707299903035\n",
      "Iteration is: 14756 and loss is: 0.00040841844747774303\n",
      "Iteration is: 14757 and loss is: 0.0003718872321769595\n",
      "Iteration is: 14758 and loss is: 0.00032287448993884027\n",
      "Iteration is: 14759 and loss is: 0.0002630601520650089\n",
      "Iteration is: 14760 and loss is: 0.00021211265993770212\n",
      "Iteration is: 14761 and loss is: 0.00018247791740577668\n",
      "Iteration is: 14762 and loss is: 0.00017348607070744038\n",
      "Iteration is: 14763 and loss is: 0.0001813193957787007\n",
      "Iteration is: 14764 and loss is: 0.00020550920453388244\n",
      "Iteration is: 14765 and loss is: 0.0002465679426677525\n",
      "Iteration is: 14766 and loss is: 0.000316420424496755\n",
      "Iteration is: 14767 and loss is: 0.0004520086804404855\n",
      "Iteration is: 14768 and loss is: 0.000675903691444546\n",
      "Iteration is: 14769 and loss is: 0.0010659866966307163\n",
      "Iteration is: 14770 and loss is: 0.0014555712696164846\n",
      "Iteration is: 14771 and loss is: 0.0015758428489789367\n",
      "Iteration is: 14772 and loss is: 0.001075995503924787\n",
      "Iteration is: 14773 and loss is: 0.0003961961483582854\n",
      "Iteration is: 14774 and loss is: 0.0002896357327699661\n",
      "Iteration is: 14775 and loss is: 0.000673547328915447\n",
      "Iteration is: 14776 and loss is: 0.0006743721896782517\n",
      "Iteration is: 14777 and loss is: 0.00033510589855723083\n",
      "Iteration is: 14778 and loss is: 0.000390837958548218\n",
      "Iteration is: 14779 and loss is: 0.0005860425299033523\n",
      "Iteration is: 14780 and loss is: 0.0004334174154791981\n",
      "Iteration is: 14781 and loss is: 0.0002652946568559855\n",
      "Iteration is: 14782 and loss is: 0.000420995318563655\n",
      "Iteration is: 14783 and loss is: 0.0004780993622262031\n",
      "Iteration is: 14784 and loss is: 0.00025836192071437836\n",
      "Iteration is: 14785 and loss is: 0.00024008376931305975\n",
      "Iteration is: 14786 and loss is: 0.0003756028600037098\n",
      "Iteration is: 14787 and loss is: 0.00037145125679671764\n",
      "Iteration is: 14788 and loss is: 0.00033272229484282434\n",
      "Iteration is: 14789 and loss is: 0.0002685130457393825\n",
      "Iteration is: 14790 and loss is: 0.00020130070333834738\n",
      "Iteration is: 14791 and loss is: 0.0002597791899461299\n",
      "Iteration is: 14792 and loss is: 0.00033793505281209946\n",
      "Iteration is: 14793 and loss is: 0.0003716329229064286\n",
      "Iteration is: 14794 and loss is: 0.0004347390204202384\n",
      "Iteration is: 14795 and loss is: 0.0004881831118836999\n",
      "Iteration is: 14796 and loss is: 0.0005571073852479458\n",
      "Iteration is: 14797 and loss is: 0.0006777406088076532\n",
      "Iteration is: 14798 and loss is: 0.0008450306486338377\n",
      "Iteration is: 14799 and loss is: 0.0008979838457889855\n",
      "Iteration is: 14800 and loss is: 0.0008077244856394827\n",
      "Iteration is: 14801 and loss is: 0.0005109870107844472\n",
      "Iteration is: 14802 and loss is: 0.0002276585146319121\n",
      "Iteration is: 14803 and loss is: 0.0002600907755549997\n",
      "Iteration is: 14804 and loss is: 0.00045957864494994283\n",
      "Iteration is: 14805 and loss is: 0.00045128626516088843\n",
      "Iteration is: 14806 and loss is: 0.0002597710117697716\n",
      "Iteration is: 14807 and loss is: 0.00022996956249698997\n",
      "Iteration is: 14808 and loss is: 0.00035965803544968367\n",
      "Iteration is: 14809 and loss is: 0.0003638723283074796\n",
      "Iteration is: 14810 and loss is: 0.00024112840765155852\n",
      "Iteration is: 14811 and loss is: 0.00020558976393658668\n",
      "Iteration is: 14812 and loss is: 0.00028638174990192056\n",
      "Iteration is: 14813 and loss is: 0.0003278639633208513\n",
      "Iteration is: 14814 and loss is: 0.0002654308336786926\n",
      "Iteration is: 14815 and loss is: 0.00019910602713935077\n",
      "Iteration is: 14816 and loss is: 0.00020699098240584135\n",
      "Iteration is: 14817 and loss is: 0.0002598990686237812\n",
      "Iteration is: 14818 and loss is: 0.00028528712573461235\n",
      "Iteration is: 14819 and loss is: 0.00025648431619629264\n",
      "Iteration is: 14820 and loss is: 0.0002093423972837627\n",
      "Iteration is: 14821 and loss is: 0.00018742635438684374\n",
      "Iteration is: 14822 and loss is: 0.00020198000129312277\n",
      "Iteration is: 14823 and loss is: 0.00023125301231630147\n",
      "Iteration is: 14824 and loss is: 0.0002506063901819289\n",
      "Iteration is: 14825 and loss is: 0.000252608151640743\n",
      "Iteration is: 14826 and loss is: 0.00024566048523411155\n",
      "Iteration is: 14827 and loss is: 0.00023950095055624843\n",
      "Iteration is: 14828 and loss is: 0.0002401372039457783\n",
      "Iteration is: 14829 and loss is: 0.0002546255709603429\n",
      "Iteration is: 14830 and loss is: 0.0002873785560950637\n",
      "Iteration is: 14831 and loss is: 0.0003591955464798957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 14832 and loss is: 0.00048012309707701206\n",
      "Iteration is: 14833 and loss is: 0.0006929794326424599\n",
      "Iteration is: 14834 and loss is: 0.0009341757977381349\n",
      "Iteration is: 14835 and loss is: 0.0011435096384957433\n",
      "Iteration is: 14836 and loss is: 0.0010462866630405188\n",
      "Iteration is: 14837 and loss is: 0.0006298348889686167\n",
      "Iteration is: 14838 and loss is: 0.0002451771288178861\n",
      "Iteration is: 14839 and loss is: 0.000295498437481001\n",
      "Iteration is: 14840 and loss is: 0.0005503874854184687\n",
      "Iteration is: 14841 and loss is: 0.0005080717382952571\n",
      "Iteration is: 14842 and loss is: 0.000273885321803391\n",
      "Iteration is: 14843 and loss is: 0.00026699621230363846\n",
      "Iteration is: 14844 and loss is: 0.00042674358701333404\n",
      "Iteration is: 14845 and loss is: 0.00039921706775203347\n",
      "Iteration is: 14846 and loss is: 0.00023416159092448652\n",
      "Iteration is: 14847 and loss is: 0.00022675099899061024\n",
      "Iteration is: 14848 and loss is: 0.0003408348129596561\n",
      "Iteration is: 14849 and loss is: 0.0003526411601342261\n",
      "Iteration is: 14850 and loss is: 0.00026850783615373075\n",
      "Iteration is: 14851 and loss is: 0.00020911896717734635\n",
      "Iteration is: 14852 and loss is: 0.00021910321083851159\n",
      "Iteration is: 14853 and loss is: 0.00027575905551202595\n",
      "Iteration is: 14854 and loss is: 0.00032073439797386527\n",
      "Iteration is: 14855 and loss is: 0.00030122161842882633\n",
      "Iteration is: 14856 and loss is: 0.00024435544037260115\n",
      "Iteration is: 14857 and loss is: 0.00020050288003403693\n",
      "Iteration is: 14858 and loss is: 0.00018112413818016648\n",
      "Iteration is: 14859 and loss is: 0.0001910617866087705\n",
      "Iteration is: 14860 and loss is: 0.00022823625477030873\n",
      "Iteration is: 14861 and loss is: 0.00028700235998257995\n",
      "Iteration is: 14862 and loss is: 0.00040169269777834415\n",
      "Iteration is: 14863 and loss is: 0.0006320724496617913\n",
      "Iteration is: 14864 and loss is: 0.001118900254368782\n",
      "Iteration is: 14865 and loss is: 0.0017952894559130073\n",
      "Iteration is: 14866 and loss is: 0.002425661776214838\n",
      "Iteration is: 14867 and loss is: 0.001907074125483632\n",
      "Iteration is: 14868 and loss is: 0.0006832035724073648\n",
      "Iteration is: 14869 and loss is: 0.0004873859870713204\n",
      "Iteration is: 14870 and loss is: 0.0010519642382860184\n",
      "Iteration is: 14871 and loss is: 0.0008196290000341833\n",
      "Iteration is: 14872 and loss is: 0.0005757996113970876\n",
      "Iteration is: 14873 and loss is: 0.0007673387881368399\n",
      "Iteration is: 14874 and loss is: 0.0007614500354975462\n",
      "Iteration is: 14875 and loss is: 0.0004612058983184397\n",
      "Iteration is: 14876 and loss is: 0.0005347174010239542\n",
      "Iteration is: 14877 and loss is: 0.0005235170247033238\n",
      "Iteration is: 14878 and loss is: 0.00041156591032631695\n",
      "Iteration is: 14879 and loss is: 0.00032391579588875175\n",
      "Iteration is: 14880 and loss is: 0.00035629814374260604\n",
      "Iteration is: 14881 and loss is: 0.00046363231376744807\n",
      "Iteration is: 14882 and loss is: 0.0002695299044717103\n",
      "Iteration is: 14883 and loss is: 0.0003048026701435447\n",
      "Iteration is: 14884 and loss is: 0.00030440304544754326\n",
      "Iteration is: 14885 and loss is: 0.00023823326046112925\n",
      "Iteration is: 14886 and loss is: 0.0003225234104320407\n",
      "Iteration is: 14887 and loss is: 0.0004234245861880481\n",
      "Iteration is: 14888 and loss is: 0.00059627799782902\n",
      "Iteration is: 14889 and loss is: 0.0010077109327539802\n",
      "Iteration is: 14890 and loss is: 0.001630447804927826\n",
      "Iteration is: 14891 and loss is: 0.00219299946911633\n",
      "Iteration is: 14892 and loss is: 0.0016871723346412182\n",
      "Iteration is: 14893 and loss is: 0.0005085632437840104\n",
      "Iteration is: 14894 and loss is: 0.00048066454473882914\n",
      "Iteration is: 14895 and loss is: 0.0010003815405070782\n",
      "Iteration is: 14896 and loss is: 0.000588305527344346\n",
      "Iteration is: 14897 and loss is: 0.0005948649486526847\n",
      "Iteration is: 14898 and loss is: 0.0007007734384387732\n",
      "Iteration is: 14899 and loss is: 0.0006242329254746437\n",
      "Iteration is: 14900 and loss is: 0.00048667562077753246\n",
      "Iteration is: 14901 and loss is: 0.0005589788197539747\n",
      "Iteration is: 14902 and loss is: 0.0004651904455386102\n",
      "Iteration is: 14903 and loss is: 0.0004048584378324449\n",
      "Iteration is: 14904 and loss is: 0.0003662147792056203\n",
      "Iteration is: 14905 and loss is: 0.00035073288017883897\n",
      "Iteration is: 14906 and loss is: 0.00038910540752112865\n",
      "Iteration is: 14907 and loss is: 0.00022762559819966555\n",
      "Iteration is: 14908 and loss is: 0.00033879780676215887\n",
      "Iteration is: 14909 and loss is: 0.00034071150003001094\n",
      "Iteration is: 14910 and loss is: 0.0002384573162999004\n",
      "Iteration is: 14911 and loss is: 0.0003313078195787966\n",
      "Iteration is: 14912 and loss is: 0.00029406213434413075\n",
      "Iteration is: 14913 and loss is: 0.00029088067822158337\n",
      "Iteration is: 14914 and loss is: 0.0003043909673579037\n",
      "Iteration is: 14915 and loss is: 0.0003481837047729641\n",
      "Iteration is: 14916 and loss is: 0.00036265028757043183\n",
      "Iteration is: 14917 and loss is: 0.00043190308497287333\n",
      "Iteration is: 14918 and loss is: 0.0005369592690840364\n",
      "Iteration is: 14919 and loss is: 0.0005879629170522094\n",
      "Iteration is: 14920 and loss is: 0.0006234265165403485\n",
      "Iteration is: 14921 and loss is: 0.0005352612934075296\n",
      "Iteration is: 14922 and loss is: 0.0002920466067735106\n",
      "Iteration is: 14923 and loss is: 0.0002127543557435274\n",
      "Iteration is: 14924 and loss is: 0.00030894161318428814\n",
      "Iteration is: 14925 and loss is: 0.00039259492768906057\n",
      "Iteration is: 14926 and loss is: 0.0003032808017451316\n",
      "Iteration is: 14927 and loss is: 0.000210988539038226\n",
      "Iteration is: 14928 and loss is: 0.00025576475309208035\n",
      "Iteration is: 14929 and loss is: 0.0003209934802725911\n",
      "Iteration is: 14930 and loss is: 0.0002721041673794389\n",
      "Iteration is: 14931 and loss is: 0.00020172077347524464\n",
      "Iteration is: 14932 and loss is: 0.00022355676628649235\n",
      "Iteration is: 14933 and loss is: 0.0002743069198913872\n",
      "Iteration is: 14934 and loss is: 0.0002616403216961771\n",
      "Iteration is: 14935 and loss is: 0.00021515901607926935\n",
      "Iteration is: 14936 and loss is: 0.0001899469643831253\n",
      "Iteration is: 14937 and loss is: 0.00021671281137969345\n",
      "Iteration is: 14938 and loss is: 0.0002469578175805509\n",
      "Iteration is: 14939 and loss is: 0.0002423469559289515\n",
      "Iteration is: 14940 and loss is: 0.00021672762522939593\n",
      "Iteration is: 14941 and loss is: 0.00019169101142324507\n",
      "Iteration is: 14942 and loss is: 0.00018733408069238067\n",
      "Iteration is: 14943 and loss is: 0.00019910366972908378\n",
      "Iteration is: 14944 and loss is: 0.00021758572256658226\n",
      "Iteration is: 14945 and loss is: 0.00022634665947407484\n",
      "Iteration is: 14946 and loss is: 0.00023184402380138636\n",
      "Iteration is: 14947 and loss is: 0.00023578520631417632\n",
      "Iteration is: 14948 and loss is: 0.000247189833316952\n",
      "Iteration is: 14949 and loss is: 0.0002768145059235394\n",
      "Iteration is: 14950 and loss is: 0.00034072139533236623\n",
      "Iteration is: 14951 and loss is: 0.000476341403555125\n",
      "Iteration is: 14952 and loss is: 0.0007053081644698977\n",
      "Iteration is: 14953 and loss is: 0.0011049865279346704\n",
      "Iteration is: 14954 and loss is: 0.0014690494863316417\n",
      "Iteration is: 14955 and loss is: 0.0015134674031287432\n",
      "Iteration is: 14956 and loss is: 0.0009109211387112737\n",
      "Iteration is: 14957 and loss is: 0.000306219095364213\n",
      "Iteration is: 14958 and loss is: 0.0003725977148860693\n",
      "Iteration is: 14959 and loss is: 0.000678587646689266\n",
      "Iteration is: 14960 and loss is: 0.0005430674063973129\n",
      "Iteration is: 14961 and loss is: 0.0003388548793736845\n",
      "Iteration is: 14962 and loss is: 0.00043023558100685477\n",
      "Iteration is: 14963 and loss is: 0.000518250570166856\n",
      "Iteration is: 14964 and loss is: 0.00039620258030481637\n",
      "Iteration is: 14965 and loss is: 0.00027908210176974535\n",
      "Iteration is: 14966 and loss is: 0.0004014054429717362\n",
      "Iteration is: 14967 and loss is: 0.0004338567377999425\n",
      "Iteration is: 14968 and loss is: 0.00023667464847676456\n",
      "Iteration is: 14969 and loss is: 0.0002447307633701712\n",
      "Iteration is: 14970 and loss is: 0.0003742892004083842\n",
      "Iteration is: 14971 and loss is: 0.00031776042305864394\n",
      "Iteration is: 14972 and loss is: 0.0002917265519499779\n",
      "Iteration is: 14973 and loss is: 0.0003116409352514893\n",
      "Iteration is: 14974 and loss is: 0.00022246173466555774\n",
      "Iteration is: 14975 and loss is: 0.00019019228057004511\n",
      "Iteration is: 14976 and loss is: 0.0002456815564073622\n",
      "Iteration is: 14977 and loss is: 0.0002650443057063967\n",
      "Iteration is: 14978 and loss is: 0.0003509190573822707\n",
      "Iteration is: 14979 and loss is: 0.0005767817492596805\n",
      "Iteration is: 14980 and loss is: 0.0009805684676393867\n",
      "Iteration is: 14981 and loss is: 0.0016581303207203746\n",
      "Iteration is: 14982 and loss is: 0.0026206837501376867\n",
      "Iteration is: 14983 and loss is: 0.0023527208250015974\n",
      "Iteration is: 14984 and loss is: 0.0009192582219839096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 14985 and loss is: 0.0007119076326489449\n",
      "Iteration is: 14986 and loss is: 0.0011320625199005008\n",
      "Iteration is: 14987 and loss is: 0.0010443513747304678\n",
      "Iteration is: 14988 and loss is: 0.000813374063000083\n",
      "Iteration is: 14989 and loss is: 0.0009614081354811788\n",
      "Iteration is: 14990 and loss is: 0.0011173372622579336\n",
      "Iteration is: 14991 and loss is: 0.0006228009006008506\n",
      "Iteration is: 14992 and loss is: 0.00064936961280182\n",
      "Iteration is: 14993 and loss is: 0.000823166745249182\n",
      "Iteration is: 14994 and loss is: 0.0005080963019281626\n",
      "Iteration is: 14995 and loss is: 0.00048226999933831394\n",
      "Iteration is: 14996 and loss is: 0.0004917140468023717\n",
      "Iteration is: 14997 and loss is: 0.0005642378819175065\n",
      "Iteration is: 14998 and loss is: 0.00035626254975795746\n",
      "Iteration is: 14999 and loss is: 0.0003011757507920265\n",
      "Iteration is: 15000 and loss is: 0.000538448803126812\n",
      "Iteration is: 15001 and loss is: 0.0005343948723748326\n",
      "Iteration is: 15002 and loss is: 0.0004519790527410805\n",
      "Iteration is: 15003 and loss is: 0.0005439852247945964\n",
      "Iteration is: 15004 and loss is: 0.0005991833750158548\n",
      "Iteration is: 15005 and loss is: 0.0006109154783189297\n",
      "Iteration is: 15006 and loss is: 0.0007408754900097847\n",
      "Iteration is: 15007 and loss is: 0.0008203949546441436\n",
      "Iteration is: 15008 and loss is: 0.0006890069344080985\n",
      "Iteration is: 15009 and loss is: 0.00040387240005657077\n",
      "Iteration is: 15010 and loss is: 0.00025686336448416114\n",
      "Iteration is: 15011 and loss is: 0.0002986277104355395\n",
      "Iteration is: 15012 and loss is: 0.0004573159967549145\n",
      "Iteration is: 15013 and loss is: 0.000360084290150553\n",
      "Iteration is: 15014 and loss is: 0.00022278739197645336\n",
      "Iteration is: 15015 and loss is: 0.0002682292542885989\n",
      "Iteration is: 15016 and loss is: 0.00034768664045259356\n",
      "Iteration is: 15017 and loss is: 0.0002784491516649723\n",
      "Iteration is: 15018 and loss is: 0.00023730151588097215\n",
      "Iteration is: 15019 and loss is: 0.00025087635731324553\n",
      "Iteration is: 15020 and loss is: 0.0002872181648854166\n",
      "Iteration is: 15021 and loss is: 0.0002554055245127529\n",
      "Iteration is: 15022 and loss is: 0.00020741450134664774\n",
      "Iteration is: 15023 and loss is: 0.00021340957027859986\n",
      "Iteration is: 15024 and loss is: 0.0002635739801917225\n",
      "Iteration is: 15025 and loss is: 0.000247513031354174\n",
      "Iteration is: 15026 and loss is: 0.0002091908681904897\n",
      "Iteration is: 15027 and loss is: 0.00020256114657968283\n",
      "Iteration is: 15028 and loss is: 0.00020494249474722892\n",
      "Iteration is: 15029 and loss is: 0.00021388084860518575\n",
      "Iteration is: 15030 and loss is: 0.00023226907069329172\n",
      "Iteration is: 15031 and loss is: 0.0002275542647112161\n",
      "Iteration is: 15032 and loss is: 0.0002053230709861964\n",
      "Iteration is: 15033 and loss is: 0.00019790865189861506\n",
      "Iteration is: 15034 and loss is: 0.00018574571004137397\n",
      "Iteration is: 15035 and loss is: 0.00017873018805403262\n",
      "Iteration is: 15036 and loss is: 0.0001780483580660075\n",
      "Iteration is: 15037 and loss is: 0.00017855918849818408\n",
      "Iteration is: 15038 and loss is: 0.00017236941494047642\n",
      "Iteration is: 15039 and loss is: 0.00017257363651879132\n",
      "Iteration is: 15040 and loss is: 0.00017340168415103108\n",
      "Iteration is: 15041 and loss is: 0.000170382802025415\n",
      "Iteration is: 15042 and loss is: 0.00016921562200877815\n",
      "Iteration is: 15043 and loss is: 0.0001705440372461453\n",
      "Iteration is: 15044 and loss is: 0.0001683749578660354\n",
      "Iteration is: 15045 and loss is: 0.00016720160783734173\n",
      "Iteration is: 15046 and loss is: 0.00016861186304595321\n",
      "Iteration is: 15047 and loss is: 0.0001665925228735432\n",
      "Iteration is: 15048 and loss is: 0.00016572502499911934\n",
      "Iteration is: 15049 and loss is: 0.00016627773584332317\n",
      "Iteration is: 15050 and loss is: 0.00016730591596569866\n",
      "Iteration is: 15051 and loss is: 0.00016907881945371628\n",
      "Iteration is: 15052 and loss is: 0.00017641656449995935\n",
      "Iteration is: 15053 and loss is: 0.00019228929886594415\n",
      "Iteration is: 15054 and loss is: 0.000227659591473639\n",
      "Iteration is: 15055 and loss is: 0.0003136712475679815\n",
      "Iteration is: 15056 and loss is: 0.0004989317385479808\n",
      "Iteration is: 15057 and loss is: 0.0009039986762218177\n",
      "Iteration is: 15058 and loss is: 0.001530695823021233\n",
      "Iteration is: 15059 and loss is: 0.002264107344672084\n",
      "Iteration is: 15060 and loss is: 0.002195104956626892\n",
      "Iteration is: 15061 and loss is: 0.0011718866880983114\n",
      "Iteration is: 15062 and loss is: 0.0004007309617009014\n",
      "Iteration is: 15063 and loss is: 0.00094212731346488\n",
      "Iteration is: 15064 and loss is: 0.0010200805263593793\n",
      "Iteration is: 15065 and loss is: 0.0007149071898311377\n",
      "Iteration is: 15066 and loss is: 0.0006859905552119017\n",
      "Iteration is: 15067 and loss is: 0.0008892970508895814\n",
      "Iteration is: 15068 and loss is: 0.0006998676108196378\n",
      "Iteration is: 15069 and loss is: 0.0005026115686632693\n",
      "Iteration is: 15070 and loss is: 0.0005948580801486969\n",
      "Iteration is: 15071 and loss is: 0.0006046622293069959\n",
      "Iteration is: 15072 and loss is: 0.00045589442015625536\n",
      "Iteration is: 15073 and loss is: 0.000282099936157465\n",
      "Iteration is: 15074 and loss is: 0.0005266061052680016\n",
      "Iteration is: 15075 and loss is: 0.0005146726034581661\n",
      "Iteration is: 15076 and loss is: 0.00034785320167429745\n",
      "Iteration is: 15077 and loss is: 0.0005856103962287307\n",
      "Iteration is: 15078 and loss is: 0.0006094615091569722\n",
      "Iteration is: 15079 and loss is: 0.000755173503421247\n",
      "Iteration is: 15080 and loss is: 0.0012771070469170809\n",
      "Iteration is: 15081 and loss is: 0.0019818234723061323\n",
      "Iteration is: 15082 and loss is: 0.002270602621138096\n",
      "Iteration is: 15083 and loss is: 0.0015465164324268699\n",
      "Iteration is: 15084 and loss is: 0.0003130722907371819\n",
      "Iteration is: 15085 and loss is: 0.0007026400417089462\n",
      "Iteration is: 15086 and loss is: 0.001070249592885375\n",
      "Iteration is: 15087 and loss is: 0.00047588342567905784\n",
      "Iteration is: 15088 and loss is: 0.0007110642036423087\n",
      "Iteration is: 15089 and loss is: 0.0008258195593953133\n",
      "Iteration is: 15090 and loss is: 0.0005444010021165013\n",
      "Iteration is: 15091 and loss is: 0.0005262223421595991\n",
      "Iteration is: 15092 and loss is: 0.0006492193788290024\n",
      "Iteration is: 15093 and loss is: 0.0004644810105673969\n",
      "Iteration is: 15094 and loss is: 0.00041408464312553406\n",
      "Iteration is: 15095 and loss is: 0.0004388892848510295\n",
      "Iteration is: 15096 and loss is: 0.0004174313507974148\n",
      "Iteration is: 15097 and loss is: 0.000365560787031427\n",
      "Iteration is: 15098 and loss is: 0.00026957091176882386\n",
      "Iteration is: 15099 and loss is: 0.0004225864540785551\n",
      "Iteration is: 15100 and loss is: 0.0003831282374449074\n",
      "Iteration is: 15101 and loss is: 0.00024733904865570366\n",
      "Iteration is: 15102 and loss is: 0.00039640715112909675\n",
      "Iteration is: 15103 and loss is: 0.00034480096655897796\n",
      "Iteration is: 15104 and loss is: 0.0003233726311009377\n",
      "Iteration is: 15105 and loss is: 0.00043003977043554187\n",
      "Iteration is: 15106 and loss is: 0.0005002286634407938\n",
      "Iteration is: 15107 and loss is: 0.000573629280552268\n",
      "Iteration is: 15108 and loss is: 0.0007096533081494272\n",
      "Iteration is: 15109 and loss is: 0.0007264863816089928\n",
      "Iteration is: 15110 and loss is: 0.0005491919000633061\n",
      "Iteration is: 15111 and loss is: 0.0003425677423365414\n",
      "Iteration is: 15112 and loss is: 0.0002703068603295833\n",
      "Iteration is: 15113 and loss is: 0.00027295536710880697\n",
      "Iteration is: 15114 and loss is: 0.0003632105654105544\n",
      "Iteration is: 15115 and loss is: 0.0003358105313964188\n",
      "Iteration is: 15116 and loss is: 0.00022448590607382357\n",
      "Iteration is: 15117 and loss is: 0.0002501948329154402\n",
      "Iteration is: 15118 and loss is: 0.00032204214949160814\n",
      "Iteration is: 15119 and loss is: 0.0002724381338339299\n",
      "Iteration is: 15120 and loss is: 0.00020106852753087878\n",
      "Iteration is: 15121 and loss is: 0.00022945000091567636\n",
      "Iteration is: 15122 and loss is: 0.00026550976326689124\n",
      "Iteration is: 15123 and loss is: 0.00024745427072048187\n",
      "Iteration is: 15124 and loss is: 0.0002219314919784665\n",
      "Iteration is: 15125 and loss is: 0.0002014213241636753\n",
      "Iteration is: 15126 and loss is: 0.00020112413039896637\n",
      "Iteration is: 15127 and loss is: 0.00023634915123693645\n",
      "Iteration is: 15128 and loss is: 0.00024595449212938547\n",
      "Iteration is: 15129 and loss is: 0.0002238962915726006\n",
      "Iteration is: 15130 and loss is: 0.00019836283172480762\n",
      "Iteration is: 15131 and loss is: 0.00018457464466337115\n",
      "Iteration is: 15132 and loss is: 0.00017212072270922363\n",
      "Iteration is: 15133 and loss is: 0.00018231611466035247\n",
      "Iteration is: 15134 and loss is: 0.0002038225211435929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 15135 and loss is: 0.0002365543768974021\n",
      "Iteration is: 15136 and loss is: 0.00031346286414191127\n",
      "Iteration is: 15137 and loss is: 0.0004978039069101214\n",
      "Iteration is: 15138 and loss is: 0.0008472460322082043\n",
      "Iteration is: 15139 and loss is: 0.0015154159627854824\n",
      "Iteration is: 15140 and loss is: 0.00210072030313313\n",
      "Iteration is: 15141 and loss is: 0.0018912861123681068\n",
      "Iteration is: 15142 and loss is: 0.000732570537365973\n",
      "Iteration is: 15143 and loss is: 0.00033188454108312726\n",
      "Iteration is: 15144 and loss is: 0.0008685016073286533\n",
      "Iteration is: 15145 and loss is: 0.0008203682955354452\n",
      "Iteration is: 15146 and loss is: 0.00046522615593858063\n",
      "Iteration is: 15147 and loss is: 0.000619405007455498\n",
      "Iteration is: 15148 and loss is: 0.0006821525748819113\n",
      "Iteration is: 15149 and loss is: 0.0004353821277618408\n",
      "Iteration is: 15150 and loss is: 0.0004312167293392122\n",
      "Iteration is: 15151 and loss is: 0.0004929769784212112\n",
      "Iteration is: 15152 and loss is: 0.00040433593676425517\n",
      "Iteration is: 15153 and loss is: 0.0002827576536219567\n",
      "Iteration is: 15154 and loss is: 0.0003278370713815093\n",
      "Iteration is: 15155 and loss is: 0.0004380911705084145\n",
      "Iteration is: 15156 and loss is: 0.0002918643585871905\n",
      "Iteration is: 15157 and loss is: 0.00022838427685201168\n",
      "Iteration is: 15158 and loss is: 0.0003208398702554405\n",
      "Iteration is: 15159 and loss is: 0.0002463336568325758\n",
      "Iteration is: 15160 and loss is: 0.0002667979570105672\n",
      "Iteration is: 15161 and loss is: 0.0004025416565127671\n",
      "Iteration is: 15162 and loss is: 0.0005766769172623754\n",
      "Iteration is: 15163 and loss is: 0.0009180559427477419\n",
      "Iteration is: 15164 and loss is: 0.0016074287705123425\n",
      "Iteration is: 15165 and loss is: 0.002130161039531231\n",
      "Iteration is: 15166 and loss is: 0.0018223919905722141\n",
      "Iteration is: 15167 and loss is: 0.0005631406093016267\n",
      "Iteration is: 15168 and loss is: 0.000385775143513456\n",
      "Iteration is: 15169 and loss is: 0.001032063621096313\n",
      "Iteration is: 15170 and loss is: 0.000650332192890346\n",
      "Iteration is: 15171 and loss is: 0.0004796176217496395\n",
      "Iteration is: 15172 and loss is: 0.0007855052826926112\n",
      "Iteration is: 15173 and loss is: 0.0006415833486244082\n",
      "Iteration is: 15174 and loss is: 0.0004215447697788477\n",
      "Iteration is: 15175 and loss is: 0.0005923107964918017\n",
      "Iteration is: 15176 and loss is: 0.0005419597728177905\n",
      "Iteration is: 15177 and loss is: 0.0003428230411373079\n",
      "Iteration is: 15178 and loss is: 0.0004200604453217238\n",
      "Iteration is: 15179 and loss is: 0.00043348001781851053\n",
      "Iteration is: 15180 and loss is: 0.00035371704143472016\n",
      "Iteration is: 15181 and loss is: 0.0002719223848544061\n",
      "Iteration is: 15182 and loss is: 0.00033671094570308924\n",
      "Iteration is: 15183 and loss is: 0.00041544524719938636\n",
      "Iteration is: 15184 and loss is: 0.000299068313324824\n",
      "Iteration is: 15185 and loss is: 0.00040874810656532645\n",
      "Iteration is: 15186 and loss is: 0.00047402671771124005\n",
      "Iteration is: 15187 and loss is: 0.0005073079955764115\n",
      "Iteration is: 15188 and loss is: 0.000724372046533972\n",
      "Iteration is: 15189 and loss is: 0.0009872696828097105\n",
      "Iteration is: 15190 and loss is: 0.001064094714820385\n",
      "Iteration is: 15191 and loss is: 0.0009150964906439185\n",
      "Iteration is: 15192 and loss is: 0.0004448139516171068\n",
      "Iteration is: 15193 and loss is: 0.00021695319446735084\n",
      "Iteration is: 15194 and loss is: 0.0004574432969093323\n",
      "Iteration is: 15195 and loss is: 0.0005166654009371996\n",
      "Iteration is: 15196 and loss is: 0.00028278454556129873\n",
      "Iteration is: 15197 and loss is: 0.0002996334224008024\n",
      "Iteration is: 15198 and loss is: 0.00040203527896665037\n",
      "Iteration is: 15199 and loss is: 0.000338457030011341\n",
      "Iteration is: 15200 and loss is: 0.0002918652899097651\n",
      "Iteration is: 15201 and loss is: 0.00029175373492762446\n",
      "Iteration is: 15202 and loss is: 0.00032428622944280505\n",
      "Iteration is: 15203 and loss is: 0.0002996755647473037\n",
      "Iteration is: 15204 and loss is: 0.0002219023008365184\n",
      "Iteration is: 15205 and loss is: 0.00025333589292131364\n",
      "Iteration is: 15206 and loss is: 0.0003076994908042252\n",
      "Iteration is: 15207 and loss is: 0.00023858455824665725\n",
      "Iteration is: 15208 and loss is: 0.00019987241830676794\n",
      "Iteration is: 15209 and loss is: 0.0002487772726453841\n",
      "Iteration is: 15210 and loss is: 0.0002438008232275024\n",
      "Iteration is: 15211 and loss is: 0.00022372975945472717\n",
      "Iteration is: 15212 and loss is: 0.00024684082018211484\n",
      "Iteration is: 15213 and loss is: 0.00021938534337095916\n",
      "Iteration is: 15214 and loss is: 0.00018271029694005847\n",
      "Iteration is: 15215 and loss is: 0.00019395390700083226\n",
      "Iteration is: 15216 and loss is: 0.0001933202875079587\n",
      "Iteration is: 15217 and loss is: 0.00017945337458513677\n",
      "Iteration is: 15218 and loss is: 0.0001948389399331063\n",
      "Iteration is: 15219 and loss is: 0.00021214279695414007\n",
      "Iteration is: 15220 and loss is: 0.0002314702869625762\n",
      "Iteration is: 15221 and loss is: 0.0002953131916001439\n",
      "Iteration is: 15222 and loss is: 0.00040144374361261725\n",
      "Iteration is: 15223 and loss is: 0.0005442223628051579\n",
      "Iteration is: 15224 and loss is: 0.0006800182163715363\n",
      "Iteration is: 15225 and loss is: 0.0007511537987738848\n",
      "Iteration is: 15226 and loss is: 0.0006205477402545512\n",
      "Iteration is: 15227 and loss is: 0.0003629983402788639\n",
      "Iteration is: 15228 and loss is: 0.00019301698193885386\n",
      "Iteration is: 15229 and loss is: 0.0002502606366761029\n",
      "Iteration is: 15230 and loss is: 0.0003900408046320081\n",
      "Iteration is: 15231 and loss is: 0.00037345479358918965\n",
      "Iteration is: 15232 and loss is: 0.00024066612240858376\n",
      "Iteration is: 15233 and loss is: 0.00019719678675755858\n",
      "Iteration is: 15234 and loss is: 0.0002786399272736162\n",
      "Iteration is: 15235 and loss is: 0.00032215542159974575\n",
      "Iteration is: 15236 and loss is: 0.00025514618027955294\n",
      "Iteration is: 15237 and loss is: 0.0001880956260720268\n",
      "Iteration is: 15238 and loss is: 0.0002054483920801431\n",
      "Iteration is: 15239 and loss is: 0.00026202882872894406\n",
      "Iteration is: 15240 and loss is: 0.0002721714263316244\n",
      "Iteration is: 15241 and loss is: 0.00022595700284000486\n",
      "Iteration is: 15242 and loss is: 0.00018611468840390444\n",
      "Iteration is: 15243 and loss is: 0.00018839555559679866\n",
      "Iteration is: 15244 and loss is: 0.00021587350056506693\n",
      "Iteration is: 15245 and loss is: 0.00023792008869349957\n",
      "Iteration is: 15246 and loss is: 0.00023505251738242805\n",
      "Iteration is: 15247 and loss is: 0.00020970829064026475\n",
      "Iteration is: 15248 and loss is: 0.00018319884839002043\n",
      "Iteration is: 15249 and loss is: 0.00017355293675791472\n",
      "Iteration is: 15250 and loss is: 0.00017925328575074673\n",
      "Iteration is: 15251 and loss is: 0.00019442959455773234\n",
      "Iteration is: 15252 and loss is: 0.0002157796116080135\n",
      "Iteration is: 15253 and loss is: 0.0002483382122591138\n",
      "Iteration is: 15254 and loss is: 0.0003093940613325685\n",
      "Iteration is: 15255 and loss is: 0.0004334503028076142\n",
      "Iteration is: 15256 and loss is: 0.0007199246319942176\n",
      "Iteration is: 15257 and loss is: 0.0012050180230289698\n",
      "Iteration is: 15258 and loss is: 0.0019448259845376015\n",
      "Iteration is: 15259 and loss is: 0.002182113006711006\n",
      "Iteration is: 15260 and loss is: 0.001400201697833836\n",
      "Iteration is: 15261 and loss is: 0.00042400474194437265\n",
      "Iteration is: 15262 and loss is: 0.0007668336620554328\n",
      "Iteration is: 15263 and loss is: 0.0010139906080439687\n",
      "Iteration is: 15264 and loss is: 0.0007595409988425672\n",
      "Iteration is: 15265 and loss is: 0.0006397434044629335\n",
      "Iteration is: 15266 and loss is: 0.0008239747257903218\n",
      "Iteration is: 15267 and loss is: 0.0007493029115721583\n",
      "Iteration is: 15268 and loss is: 0.00047231614007614553\n",
      "Iteration is: 15269 and loss is: 0.0005954580847173929\n",
      "Iteration is: 15270 and loss is: 0.0005681473412550986\n",
      "Iteration is: 15271 and loss is: 0.0004770774394273758\n",
      "Iteration is: 15272 and loss is: 0.0003284983686171472\n",
      "Iteration is: 15273 and loss is: 0.0004078927740920335\n",
      "Iteration is: 15274 and loss is: 0.0005737048340961337\n",
      "Iteration is: 15275 and loss is: 0.0003255023621022701\n",
      "Iteration is: 15276 and loss is: 0.00036047256435267627\n",
      "Iteration is: 15277 and loss is: 0.0004523308598436415\n",
      "Iteration is: 15278 and loss is: 0.00030118116410449147\n",
      "Iteration is: 15279 and loss is: 0.00036099873250350356\n",
      "Iteration is: 15280 and loss is: 0.0004599500971380621\n",
      "Iteration is: 15281 and loss is: 0.000527473574038595\n",
      "Iteration is: 15282 and loss is: 0.0007336527924053371\n",
      "Iteration is: 15283 and loss is: 0.0010125197004526854\n",
      "Iteration is: 15284 and loss is: 0.0012719458900392056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 15285 and loss is: 0.0010979619110003114\n",
      "Iteration is: 15286 and loss is: 0.0006121468031778932\n",
      "Iteration is: 15287 and loss is: 0.0004322896129451692\n",
      "Iteration is: 15288 and loss is: 0.00045331422006711364\n",
      "Iteration is: 15289 and loss is: 0.0006582103669643402\n",
      "Iteration is: 15290 and loss is: 0.0005966172320768237\n",
      "Iteration is: 15291 and loss is: 0.00034142870572395623\n",
      "Iteration is: 15292 and loss is: 0.000505759846419096\n",
      "Iteration is: 15293 and loss is: 0.0006170031847432256\n",
      "Iteration is: 15294 and loss is: 0.0003189426497556269\n",
      "Iteration is: 15295 and loss is: 0.0003485397610347718\n",
      "Iteration is: 15296 and loss is: 0.0004929045680910349\n",
      "Iteration is: 15297 and loss is: 0.0003719761734828353\n",
      "Iteration is: 15298 and loss is: 0.00022491763229481876\n",
      "Iteration is: 15299 and loss is: 0.0003384549345355481\n",
      "Iteration is: 15300 and loss is: 0.00035425295936875045\n",
      "Iteration is: 15301 and loss is: 0.0002975936804432422\n",
      "Iteration is: 15302 and loss is: 0.0003456936974544078\n",
      "Iteration is: 15303 and loss is: 0.00028450193349272013\n",
      "Iteration is: 15304 and loss is: 0.00019241697737015784\n",
      "Iteration is: 15305 and loss is: 0.00022265709412749857\n",
      "Iteration is: 15306 and loss is: 0.0002659480960574001\n",
      "Iteration is: 15307 and loss is: 0.0002924740838352591\n",
      "Iteration is: 15308 and loss is: 0.0004509258142206818\n",
      "Iteration is: 15309 and loss is: 0.00071943603688851\n",
      "Iteration is: 15310 and loss is: 0.0011987886391580105\n",
      "Iteration is: 15311 and loss is: 0.0020535574294626713\n",
      "Iteration is: 15312 and loss is: 0.0023836458567529917\n",
      "Iteration is: 15313 and loss is: 0.0013946801191195846\n",
      "Iteration is: 15314 and loss is: 0.00044533412437886\n",
      "Iteration is: 15315 and loss is: 0.0009624470258131623\n",
      "Iteration is: 15316 and loss is: 0.0009336608927696943\n",
      "Iteration is: 15317 and loss is: 0.0007641511037945747\n",
      "Iteration is: 15318 and loss is: 0.0007920180796645582\n",
      "Iteration is: 15319 and loss is: 0.0008976500248536468\n",
      "Iteration is: 15320 and loss is: 0.0006921705789864063\n",
      "Iteration is: 15321 and loss is: 0.0005177604616619647\n",
      "Iteration is: 15322 and loss is: 0.0006921630701981485\n",
      "Iteration is: 15323 and loss is: 0.0005126344622112811\n",
      "Iteration is: 15324 and loss is: 0.0004814682179130614\n",
      "Iteration is: 15325 and loss is: 0.0003883492317982018\n",
      "Iteration is: 15326 and loss is: 0.00044479541247710586\n",
      "Iteration is: 15327 and loss is: 0.0005567950429394841\n",
      "Iteration is: 15328 and loss is: 0.0002484405995346606\n",
      "Iteration is: 15329 and loss is: 0.00035856731119565666\n",
      "Iteration is: 15330 and loss is: 0.00036022806307300925\n",
      "Iteration is: 15331 and loss is: 0.000257544859778136\n",
      "Iteration is: 15332 and loss is: 0.0003804425068665296\n",
      "Iteration is: 15333 and loss is: 0.0004967956920154393\n",
      "Iteration is: 15334 and loss is: 0.0006590628181584179\n",
      "Iteration is: 15335 and loss is: 0.001040382543578744\n",
      "Iteration is: 15336 and loss is: 0.0014983470318838954\n",
      "Iteration is: 15337 and loss is: 0.0014372214209288359\n",
      "Iteration is: 15338 and loss is: 0.0007186927832663059\n",
      "Iteration is: 15339 and loss is: 0.0002261664194520563\n",
      "Iteration is: 15340 and loss is: 0.000706869934219867\n",
      "Iteration is: 15341 and loss is: 0.000767349440138787\n",
      "Iteration is: 15342 and loss is: 0.0003136532614007592\n",
      "Iteration is: 15343 and loss is: 0.0005031370092183352\n",
      "Iteration is: 15344 and loss is: 0.0006212303414940834\n",
      "Iteration is: 15345 and loss is: 0.0003798282123170793\n",
      "Iteration is: 15346 and loss is: 0.00039894841029308736\n",
      "Iteration is: 15347 and loss is: 0.0004842774069402367\n",
      "Iteration is: 15348 and loss is: 0.00041352020343765616\n",
      "Iteration is: 15349 and loss is: 0.00029065291164442897\n",
      "Iteration is: 15350 and loss is: 0.00035787024535238743\n",
      "Iteration is: 15351 and loss is: 0.00043088835082016885\n",
      "Iteration is: 15352 and loss is: 0.00029157617245800793\n",
      "Iteration is: 15353 and loss is: 0.00022389579680748284\n",
      "Iteration is: 15354 and loss is: 0.00036259196349419653\n",
      "Iteration is: 15355 and loss is: 0.00035695324186235666\n",
      "Iteration is: 15356 and loss is: 0.00026864471146836877\n",
      "Iteration is: 15357 and loss is: 0.0003058873116970062\n",
      "Iteration is: 15358 and loss is: 0.0002653211122378707\n",
      "Iteration is: 15359 and loss is: 0.00018277985509485006\n",
      "Iteration is: 15360 and loss is: 0.00022428741795010865\n",
      "Iteration is: 15361 and loss is: 0.000229334706091322\n",
      "Iteration is: 15362 and loss is: 0.0002044733555521816\n",
      "Iteration is: 15363 and loss is: 0.00024923341698013246\n",
      "Iteration is: 15364 and loss is: 0.00031277790549211204\n",
      "Iteration is: 15365 and loss is: 0.0003859911812469363\n",
      "Iteration is: 15366 and loss is: 0.0005300413467921317\n",
      "Iteration is: 15367 and loss is: 0.0006677717319689691\n",
      "Iteration is: 15368 and loss is: 0.0006800171686336398\n",
      "Iteration is: 15369 and loss is: 0.0005292054265737534\n",
      "Iteration is: 15370 and loss is: 0.0003242562524974346\n",
      "Iteration is: 15371 and loss is: 0.0002088201290462166\n",
      "Iteration is: 15372 and loss is: 0.00028338393894955516\n",
      "Iteration is: 15373 and loss is: 0.00040037589496932924\n",
      "Iteration is: 15374 and loss is: 0.00032276875572279096\n",
      "Iteration is: 15375 and loss is: 0.00020064495038241148\n",
      "Iteration is: 15376 and loss is: 0.0002548515622038394\n",
      "Iteration is: 15377 and loss is: 0.00033594854176044464\n",
      "Iteration is: 15378 and loss is: 0.0002746589307207614\n",
      "Iteration is: 15379 and loss is: 0.00019347266061231494\n",
      "Iteration is: 15380 and loss is: 0.00021139532327651978\n",
      "Iteration is: 15381 and loss is: 0.00026502847322262824\n",
      "Iteration is: 15382 and loss is: 0.0002722850476857275\n",
      "Iteration is: 15383 and loss is: 0.0002402604150120169\n",
      "Iteration is: 15384 and loss is: 0.0001931277511175722\n",
      "Iteration is: 15385 and loss is: 0.00018375895160716027\n",
      "Iteration is: 15386 and loss is: 0.00021993319387547672\n",
      "Iteration is: 15387 and loss is: 0.00024903513258323073\n",
      "Iteration is: 15388 and loss is: 0.0002443018020130694\n",
      "Iteration is: 15389 and loss is: 0.00022064661607146263\n",
      "Iteration is: 15390 and loss is: 0.00019249191973358393\n",
      "Iteration is: 15391 and loss is: 0.00016988182323984802\n",
      "Iteration is: 15392 and loss is: 0.00016518593474756926\n",
      "Iteration is: 15393 and loss is: 0.0001740837615216151\n",
      "Iteration is: 15394 and loss is: 0.00019178472575731575\n",
      "Iteration is: 15395 and loss is: 0.00023219487047754228\n",
      "Iteration is: 15396 and loss is: 0.00032867054687812924\n",
      "Iteration is: 15397 and loss is: 0.0005598277784883976\n",
      "Iteration is: 15398 and loss is: 0.0010218583047389984\n",
      "Iteration is: 15399 and loss is: 0.001891010208055377\n",
      "Iteration is: 15400 and loss is: 0.0024807651061564684\n",
      "Iteration is: 15401 and loss is: 0.001859423704445362\n",
      "Iteration is: 15402 and loss is: 0.0005027528386563063\n",
      "Iteration is: 15403 and loss is: 0.0007415965665131807\n",
      "Iteration is: 15404 and loss is: 0.0011658030562102795\n",
      "Iteration is: 15405 and loss is: 0.0007861500489525497\n",
      "Iteration is: 15406 and loss is: 0.0006639318307861686\n",
      "Iteration is: 15407 and loss is: 0.0009896750561892986\n",
      "Iteration is: 15408 and loss is: 0.000815371866337955\n",
      "Iteration is: 15409 and loss is: 0.00041805842192843556\n",
      "Iteration is: 15410 and loss is: 0.0007524132961407304\n",
      "Iteration is: 15411 and loss is: 0.0006286242860369384\n",
      "Iteration is: 15412 and loss is: 0.000426992162829265\n",
      "Iteration is: 15413 and loss is: 0.00044765783241018653\n",
      "Iteration is: 15414 and loss is: 0.00048689241521060467\n",
      "Iteration is: 15415 and loss is: 0.0006308841984719038\n",
      "Iteration is: 15416 and loss is: 0.00035014215973205864\n",
      "Iteration is: 15417 and loss is: 0.00028788542840629816\n",
      "Iteration is: 15418 and loss is: 0.0004080170765519142\n",
      "Iteration is: 15419 and loss is: 0.000266372284386307\n",
      "Iteration is: 15420 and loss is: 0.00042899054824374616\n",
      "Iteration is: 15421 and loss is: 0.000696976319886744\n",
      "Iteration is: 15422 and loss is: 0.0011228329967707396\n",
      "Iteration is: 15423 and loss is: 0.0018356813816353679\n",
      "Iteration is: 15424 and loss is: 0.002338043414056301\n",
      "Iteration is: 15425 and loss is: 0.0014316162560135126\n",
      "Iteration is: 15426 and loss is: 0.00039560857112519443\n",
      "Iteration is: 15427 and loss is: 0.0007761335582472384\n",
      "Iteration is: 15428 and loss is: 0.0010332781821489334\n",
      "Iteration is: 15429 and loss is: 0.0005915404763072729\n",
      "Iteration is: 15430 and loss is: 0.0006702590035274625\n",
      "Iteration is: 15431 and loss is: 0.001031953957863152\n",
      "Iteration is: 15432 and loss is: 0.0005821215454488993\n",
      "Iteration is: 15433 and loss is: 0.0005261271726340055\n",
      "Iteration is: 15434 and loss is: 0.0007718809647485614\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 15435 and loss is: 0.00048514886293560266\n",
      "Iteration is: 15436 and loss is: 0.0003698255750350654\n",
      "Iteration is: 15437 and loss is: 0.0005732554709538817\n",
      "Iteration is: 15438 and loss is: 0.00037753250217065215\n",
      "Iteration is: 15439 and loss is: 0.00024864455917850137\n",
      "Iteration is: 15440 and loss is: 0.00042064074659720063\n",
      "Iteration is: 15441 and loss is: 0.00047741722664795816\n",
      "Iteration is: 15442 and loss is: 0.0003537096781656146\n",
      "Iteration is: 15443 and loss is: 0.00023722379410173744\n",
      "Iteration is: 15444 and loss is: 0.00022336751862894744\n",
      "Iteration is: 15445 and loss is: 0.00032333313720300794\n",
      "Iteration is: 15446 and loss is: 0.0004916767938993871\n",
      "Iteration is: 15447 and loss is: 0.0008348986157216132\n",
      "Iteration is: 15448 and loss is: 0.0014342982321977615\n",
      "Iteration is: 15449 and loss is: 0.0019121732329949737\n",
      "Iteration is: 15450 and loss is: 0.0017201920272782445\n",
      "Iteration is: 15451 and loss is: 0.0005960991256870329\n",
      "Iteration is: 15452 and loss is: 0.00046914233826100826\n",
      "Iteration is: 15453 and loss is: 0.0008686678484082222\n",
      "Iteration is: 15454 and loss is: 0.0006469645886681974\n",
      "Iteration is: 15455 and loss is: 0.0006276044296100736\n",
      "Iteration is: 15456 and loss is: 0.0008001306559890509\n",
      "Iteration is: 15457 and loss is: 0.0006970334216021001\n",
      "Iteration is: 15458 and loss is: 0.0004309231007937342\n",
      "Iteration is: 15459 and loss is: 0.0006675187032669783\n",
      "Iteration is: 15460 and loss is: 0.0005840382073074579\n",
      "Iteration is: 15461 and loss is: 0.0003011788066942245\n",
      "Iteration is: 15462 and loss is: 0.00047905847895890474\n",
      "Iteration is: 15463 and loss is: 0.00042775593465194106\n",
      "Iteration is: 15464 and loss is: 0.00030862150015309453\n",
      "Iteration is: 15465 and loss is: 0.00030587997753173113\n",
      "Iteration is: 15466 and loss is: 0.00034717982634902\n",
      "Iteration is: 15467 and loss is: 0.00046460278099402785\n",
      "Iteration is: 15468 and loss is: 0.00039616995491087437\n",
      "Iteration is: 15469 and loss is: 0.00041095801861956716\n",
      "Iteration is: 15470 and loss is: 0.0004564104601740837\n",
      "Iteration is: 15471 and loss is: 0.00046951737022027373\n",
      "Iteration is: 15472 and loss is: 0.0005504308501258492\n",
      "Iteration is: 15473 and loss is: 0.0006573701975867152\n",
      "Iteration is: 15474 and loss is: 0.0006116069853305817\n",
      "Iteration is: 15475 and loss is: 0.00047579212696291506\n",
      "Iteration is: 15476 and loss is: 0.00026242094463668764\n",
      "Iteration is: 15477 and loss is: 0.00022152070596348494\n",
      "Iteration is: 15478 and loss is: 0.00034762060386128724\n",
      "Iteration is: 15479 and loss is: 0.0003581498167477548\n",
      "Iteration is: 15480 and loss is: 0.00024477049009874463\n",
      "Iteration is: 15481 and loss is: 0.00020115991355851293\n",
      "Iteration is: 15482 and loss is: 0.000289997027721256\n",
      "Iteration is: 15483 and loss is: 0.0002860872773453593\n",
      "Iteration is: 15484 and loss is: 0.00022033984714653343\n",
      "Iteration is: 15485 and loss is: 0.0002118809788953513\n",
      "Iteration is: 15486 and loss is: 0.0002571900258772075\n",
      "Iteration is: 15487 and loss is: 0.0002457812079228461\n",
      "Iteration is: 15488 and loss is: 0.0002020139218075201\n",
      "Iteration is: 15489 and loss is: 0.00018785230349749327\n",
      "Iteration is: 15490 and loss is: 0.00022714922670274973\n",
      "Iteration is: 15491 and loss is: 0.00023627873451914638\n",
      "Iteration is: 15492 and loss is: 0.0002090786147164181\n",
      "Iteration is: 15493 and loss is: 0.0001830466790124774\n",
      "Iteration is: 15494 and loss is: 0.00018635101150721312\n",
      "Iteration is: 15495 and loss is: 0.0001991551835089922\n",
      "Iteration is: 15496 and loss is: 0.0002096711250487715\n",
      "Iteration is: 15497 and loss is: 0.00021436091628856957\n",
      "Iteration is: 15498 and loss is: 0.00020105841394979507\n",
      "Iteration is: 15499 and loss is: 0.00018766385619528592\n",
      "Iteration is: 15500 and loss is: 0.00017839188512880355\n",
      "Iteration is: 15501 and loss is: 0.0001747938949847594\n",
      "Iteration is: 15502 and loss is: 0.0001694001111900434\n",
      "Iteration is: 15503 and loss is: 0.0001707294723019004\n",
      "Iteration is: 15504 and loss is: 0.00017199231660924852\n",
      "Iteration is: 15505 and loss is: 0.0001765604829415679\n",
      "Iteration is: 15506 and loss is: 0.00019064433581661433\n",
      "Iteration is: 15507 and loss is: 0.00021729599393438548\n",
      "Iteration is: 15508 and loss is: 0.00026676597190089524\n",
      "Iteration is: 15509 and loss is: 0.0003482764295767993\n",
      "Iteration is: 15510 and loss is: 0.0004911917494609952\n",
      "Iteration is: 15511 and loss is: 0.0006621176144108176\n",
      "Iteration is: 15512 and loss is: 0.0008562860894016922\n",
      "Iteration is: 15513 and loss is: 0.0008946240413933992\n",
      "Iteration is: 15514 and loss is: 0.0006886107148602605\n",
      "Iteration is: 15515 and loss is: 0.0003353811916895211\n",
      "Iteration is: 15516 and loss is: 0.00018445342720951885\n",
      "Iteration is: 15517 and loss is: 0.0003282112011220306\n",
      "Iteration is: 15518 and loss is: 0.00045323072117753327\n",
      "Iteration is: 15519 and loss is: 0.0003493097028695047\n",
      "Iteration is: 15520 and loss is: 0.00020838166528847069\n",
      "Iteration is: 15521 and loss is: 0.0002563168527558446\n",
      "Iteration is: 15522 and loss is: 0.0003572027198970318\n",
      "Iteration is: 15523 and loss is: 0.0003135311999358237\n",
      "Iteration is: 15524 and loss is: 0.00020130330813117325\n",
      "Iteration is: 15525 and loss is: 0.00018919905414804816\n",
      "Iteration is: 15526 and loss is: 0.00026953042834065855\n",
      "Iteration is: 15527 and loss is: 0.00030968140345066786\n",
      "Iteration is: 15528 and loss is: 0.00026992609491571784\n",
      "Iteration is: 15529 and loss is: 0.0002135510731022805\n",
      "Iteration is: 15530 and loss is: 0.00018242686928715557\n",
      "Iteration is: 15531 and loss is: 0.00018540493329055607\n",
      "Iteration is: 15532 and loss is: 0.00022063232609070837\n",
      "Iteration is: 15533 and loss is: 0.0002661932085175067\n",
      "Iteration is: 15534 and loss is: 0.00030125389457680285\n",
      "Iteration is: 15535 and loss is: 0.0003409380151424557\n",
      "Iteration is: 15536 and loss is: 0.00042442878475412726\n",
      "Iteration is: 15537 and loss is: 0.0005572477239184082\n",
      "Iteration is: 15538 and loss is: 0.0008207426872104406\n",
      "Iteration is: 15539 and loss is: 0.00113382819108665\n",
      "Iteration is: 15540 and loss is: 0.0013766380725428462\n",
      "Iteration is: 15541 and loss is: 0.0011132778599858284\n",
      "Iteration is: 15542 and loss is: 0.0005207284702919424\n",
      "Iteration is: 15543 and loss is: 0.00027400776161812246\n",
      "Iteration is: 15544 and loss is: 0.0005372017039917409\n",
      "Iteration is: 15545 and loss is: 0.00061212875880301\n",
      "Iteration is: 15546 and loss is: 0.0004281092551536858\n",
      "Iteration is: 15547 and loss is: 0.0003770396579056978\n",
      "Iteration is: 15548 and loss is: 0.0004543657414615154\n",
      "Iteration is: 15549 and loss is: 0.00046036834828555584\n",
      "Iteration is: 15550 and loss is: 0.0003305093268863857\n",
      "Iteration is: 15551 and loss is: 0.00029843748779967427\n",
      "Iteration is: 15552 and loss is: 0.0003833467490039766\n",
      "Iteration is: 15553 and loss is: 0.0003456722479313612\n",
      "Iteration is: 15554 and loss is: 0.00020954519277438521\n",
      "Iteration is: 15555 and loss is: 0.00024462188594043255\n",
      "Iteration is: 15556 and loss is: 0.00034673415939323604\n",
      "Iteration is: 15557 and loss is: 0.000272883044090122\n",
      "Iteration is: 15558 and loss is: 0.00022898460156284273\n",
      "Iteration is: 15559 and loss is: 0.0002886566217057407\n",
      "Iteration is: 15560 and loss is: 0.00026092585176229477\n",
      "Iteration is: 15561 and loss is: 0.0002146707847714424\n",
      "Iteration is: 15562 and loss is: 0.0002679255558177829\n",
      "Iteration is: 15563 and loss is: 0.0003269423614256084\n",
      "Iteration is: 15564 and loss is: 0.0003936087596230209\n",
      "Iteration is: 15565 and loss is: 0.0006284051924012601\n",
      "Iteration is: 15566 and loss is: 0.0011237309081479907\n",
      "Iteration is: 15567 and loss is: 0.0016083619557321072\n",
      "Iteration is: 15568 and loss is: 0.0016825920902192593\n",
      "Iteration is: 15569 and loss is: 0.0009626616956666112\n",
      "Iteration is: 15570 and loss is: 0.0003712504985742271\n",
      "Iteration is: 15571 and loss is: 0.0005821732920594513\n",
      "Iteration is: 15572 and loss is: 0.0007494955789297819\n",
      "Iteration is: 15573 and loss is: 0.0005545958410948515\n",
      "Iteration is: 15574 and loss is: 0.0004902167129330337\n",
      "Iteration is: 15575 and loss is: 0.0005518075777217746\n",
      "Iteration is: 15576 and loss is: 0.000497738947160542\n",
      "Iteration is: 15577 and loss is: 0.0003688852011691779\n",
      "Iteration is: 15578 and loss is: 0.000366675085388124\n",
      "Iteration is: 15579 and loss is: 0.00037109258119016886\n",
      "Iteration is: 15580 and loss is: 0.0003336570516694337\n",
      "Iteration is: 15581 and loss is: 0.0002439958625473082\n",
      "Iteration is: 15582 and loss is: 0.0002745754027273506\n",
      "Iteration is: 15583 and loss is: 0.0003618212940637022\n",
      "Iteration is: 15584 and loss is: 0.00022914158762432635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 15585 and loss is: 0.00023114454234018922\n",
      "Iteration is: 15586 and loss is: 0.00032982247648760676\n",
      "Iteration is: 15587 and loss is: 0.0002876406069844961\n",
      "Iteration is: 15588 and loss is: 0.0003322429838590324\n",
      "Iteration is: 15589 and loss is: 0.0005483751301653683\n",
      "Iteration is: 15590 and loss is: 0.0008829854778014123\n",
      "Iteration is: 15591 and loss is: 0.0014139980776235461\n",
      "Iteration is: 15592 and loss is: 0.0022110857535153627\n",
      "Iteration is: 15593 and loss is: 0.0019221565453335643\n",
      "Iteration is: 15594 and loss is: 0.000650244124699384\n",
      "Iteration is: 15595 and loss is: 0.0006214817985892296\n",
      "Iteration is: 15596 and loss is: 0.0010528645943850279\n",
      "Iteration is: 15597 and loss is: 0.0006970748072490096\n",
      "Iteration is: 15598 and loss is: 0.0007404440548270941\n",
      "Iteration is: 15599 and loss is: 0.0007536234334111214\n",
      "Iteration is: 15600 and loss is: 0.0008048145100474358\n",
      "Iteration is: 15601 and loss is: 0.0004886622191406786\n",
      "Iteration is: 15602 and loss is: 0.0005034104106016457\n",
      "Iteration is: 15603 and loss is: 0.0005517631652764976\n",
      "Iteration is: 15604 and loss is: 0.00033745606197044253\n",
      "Iteration is: 15605 and loss is: 0.0004426865198183805\n",
      "Iteration is: 15606 and loss is: 0.0003126361407339573\n",
      "Iteration is: 15607 and loss is: 0.0003148025134578347\n",
      "Iteration is: 15608 and loss is: 0.00044790413812734187\n",
      "Iteration is: 15609 and loss is: 0.0002587558119557798\n",
      "Iteration is: 15610 and loss is: 0.00044895612518303096\n",
      "Iteration is: 15611 and loss is: 0.0005345820100046694\n",
      "Iteration is: 15612 and loss is: 0.0008336645551025867\n",
      "Iteration is: 15613 and loss is: 0.0014625469921156764\n",
      "Iteration is: 15614 and loss is: 0.0024353377521038055\n",
      "Iteration is: 15615 and loss is: 0.002199603710323572\n",
      "Iteration is: 15616 and loss is: 0.0008324394002556801\n",
      "Iteration is: 15617 and loss is: 0.0006773825734853745\n",
      "Iteration is: 15618 and loss is: 0.0011409346479922533\n",
      "Iteration is: 15619 and loss is: 0.001090839970856905\n",
      "Iteration is: 15620 and loss is: 0.0005616792477667332\n",
      "Iteration is: 15621 and loss is: 0.0009945790516212583\n",
      "Iteration is: 15622 and loss is: 0.001074837171472609\n",
      "Iteration is: 15623 and loss is: 0.0006113703129813075\n",
      "Iteration is: 15624 and loss is: 0.0005162237212061882\n",
      "Iteration is: 15625 and loss is: 0.0007916386821307242\n",
      "Iteration is: 15626 and loss is: 0.0004353421973064542\n",
      "Iteration is: 15627 and loss is: 0.0003768260357901454\n",
      "Iteration is: 15628 and loss is: 0.0005470069008879364\n",
      "Iteration is: 15629 and loss is: 0.0004172367916908115\n",
      "Iteration is: 15630 and loss is: 0.0002384198596701026\n",
      "Iteration is: 15631 and loss is: 0.00042278834735043347\n",
      "Iteration is: 15632 and loss is: 0.0004710430803243071\n",
      "Iteration is: 15633 and loss is: 0.0005949786864221096\n",
      "Iteration is: 15634 and loss is: 0.000569585885386914\n",
      "Iteration is: 15635 and loss is: 0.0006197242764756083\n",
      "Iteration is: 15636 and loss is: 0.0007355302223004401\n",
      "Iteration is: 15637 and loss is: 0.0008390327566303313\n",
      "Iteration is: 15638 and loss is: 0.0008373652235604823\n",
      "Iteration is: 15639 and loss is: 0.0006401233258657157\n",
      "Iteration is: 15640 and loss is: 0.0003138435713481158\n",
      "Iteration is: 15641 and loss is: 0.00026928374427370727\n",
      "Iteration is: 15642 and loss is: 0.0004275197861716151\n",
      "Iteration is: 15643 and loss is: 0.0003937907167710364\n",
      "Iteration is: 15644 and loss is: 0.000286195456283167\n",
      "Iteration is: 15645 and loss is: 0.00027477688854560256\n",
      "Iteration is: 15646 and loss is: 0.0003740249667316675\n",
      "Iteration is: 15647 and loss is: 0.00030124030308797956\n",
      "Iteration is: 15648 and loss is: 0.0002288171963300556\n",
      "Iteration is: 15649 and loss is: 0.0003073839470744133\n",
      "Iteration is: 15650 and loss is: 0.0002839519875124097\n",
      "Iteration is: 15651 and loss is: 0.0002176408306695521\n",
      "Iteration is: 15652 and loss is: 0.0002209602389484644\n",
      "Iteration is: 15653 and loss is: 0.0002448904560878873\n",
      "Iteration is: 15654 and loss is: 0.00023929480812512338\n",
      "Iteration is: 15655 and loss is: 0.0001998593215830624\n",
      "Iteration is: 15656 and loss is: 0.00020018141367472708\n",
      "Iteration is: 15657 and loss is: 0.00022077036555856466\n",
      "Iteration is: 15658 and loss is: 0.00021155478316359222\n",
      "Iteration is: 15659 and loss is: 0.000219071313040331\n",
      "Iteration is: 15660 and loss is: 0.00021390238543972373\n",
      "Iteration is: 15661 and loss is: 0.00018752252799458802\n",
      "Iteration is: 15662 and loss is: 0.0001845084480009973\n",
      "Iteration is: 15663 and loss is: 0.0001830177498050034\n",
      "Iteration is: 15664 and loss is: 0.00017621030565351248\n",
      "Iteration is: 15665 and loss is: 0.0001741785672493279\n",
      "Iteration is: 15666 and loss is: 0.00017097890668082982\n",
      "Iteration is: 15667 and loss is: 0.00017036939971148968\n",
      "Iteration is: 15668 and loss is: 0.00016841935575939715\n",
      "Iteration is: 15669 and loss is: 0.00016969449643511325\n",
      "Iteration is: 15670 and loss is: 0.00017830156139098108\n",
      "Iteration is: 15671 and loss is: 0.00018018858099821955\n",
      "Iteration is: 15672 and loss is: 0.00018485062173567712\n",
      "Iteration is: 15673 and loss is: 0.00019592941680457443\n",
      "Iteration is: 15674 and loss is: 0.00020406482508406043\n",
      "Iteration is: 15675 and loss is: 0.00022304245794657618\n",
      "Iteration is: 15676 and loss is: 0.0002642020408529788\n",
      "Iteration is: 15677 and loss is: 0.0003338841488584876\n",
      "Iteration is: 15678 and loss is: 0.0004804408235941082\n",
      "Iteration is: 15679 and loss is: 0.0007135057239793241\n",
      "Iteration is: 15680 and loss is: 0.0010851698461920023\n",
      "Iteration is: 15681 and loss is: 0.0013312625233083963\n",
      "Iteration is: 15682 and loss is: 0.0011785316746681929\n",
      "Iteration is: 15683 and loss is: 0.0005636460846289992\n",
      "Iteration is: 15684 and loss is: 0.00023028832220006734\n",
      "Iteration is: 15685 and loss is: 0.0004558496584650129\n",
      "Iteration is: 15686 and loss is: 0.0005947827594354749\n",
      "Iteration is: 15687 and loss is: 0.00039908982580527663\n",
      "Iteration is: 15688 and loss is: 0.00030583166517317295\n",
      "Iteration is: 15689 and loss is: 0.00039471330819651484\n",
      "Iteration is: 15690 and loss is: 0.0004218306567054242\n",
      "Iteration is: 15691 and loss is: 0.00033186119981110096\n",
      "Iteration is: 15692 and loss is: 0.00025180098600685596\n",
      "Iteration is: 15693 and loss is: 0.0003228134009987116\n",
      "Iteration is: 15694 and loss is: 0.000366647494956851\n",
      "Iteration is: 15695 and loss is: 0.00024090120859909803\n",
      "Iteration is: 15696 and loss is: 0.0001943036331795156\n",
      "Iteration is: 15697 and loss is: 0.00029783949139527977\n",
      "Iteration is: 15698 and loss is: 0.00030574382981285453\n",
      "Iteration is: 15699 and loss is: 0.00023599791165906936\n",
      "Iteration is: 15700 and loss is: 0.00024915189715102315\n",
      "Iteration is: 15701 and loss is: 0.0002614622935652733\n",
      "Iteration is: 15702 and loss is: 0.0002000417880481109\n",
      "Iteration is: 15703 and loss is: 0.00019082840299233794\n",
      "Iteration is: 15704 and loss is: 0.00022641324903815985\n",
      "Iteration is: 15705 and loss is: 0.00022023299243301153\n",
      "Iteration is: 15706 and loss is: 0.00022773536329623312\n",
      "Iteration is: 15707 and loss is: 0.00031718998798169196\n",
      "Iteration is: 15708 and loss is: 0.00048425045679323375\n",
      "Iteration is: 15709 and loss is: 0.00073609733954072\n",
      "Iteration is: 15710 and loss is: 0.0011233612895011902\n",
      "Iteration is: 15711 and loss is: 0.0012930165976285934\n",
      "Iteration is: 15712 and loss is: 0.0009212163276970387\n",
      "Iteration is: 15713 and loss is: 0.00031381024746224284\n",
      "Iteration is: 15714 and loss is: 0.0003055859706364572\n",
      "Iteration is: 15715 and loss is: 0.0006676036864519119\n",
      "Iteration is: 15716 and loss is: 0.0005649703089147806\n",
      "Iteration is: 15717 and loss is: 0.00028213910991325974\n",
      "Iteration is: 15718 and loss is: 0.0003803088329732418\n",
      "Iteration is: 15719 and loss is: 0.0005076580564491451\n",
      "Iteration is: 15720 and loss is: 0.000355003634467721\n",
      "Iteration is: 15721 and loss is: 0.0002516601816751063\n",
      "Iteration is: 15722 and loss is: 0.0003252101014368236\n",
      "Iteration is: 15723 and loss is: 0.0003777440288104117\n",
      "Iteration is: 15724 and loss is: 0.0002809063589666039\n",
      "Iteration is: 15725 and loss is: 0.00018377468222752213\n",
      "Iteration is: 15726 and loss is: 0.00026342214550822973\n",
      "Iteration is: 15727 and loss is: 0.0003445454058237374\n",
      "Iteration is: 15728 and loss is: 0.00029041728703305125\n",
      "Iteration is: 15729 and loss is: 0.00024303002282977104\n",
      "Iteration is: 15730 and loss is: 0.00024786306312307715\n",
      "Iteration is: 15731 and loss is: 0.00020586338359862566\n",
      "Iteration is: 15732 and loss is: 0.0001634327636566013\n",
      "Iteration is: 15733 and loss is: 0.00018499646103009582\n",
      "Iteration is: 15734 and loss is: 0.00020067079458385706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 15735 and loss is: 0.00019728156621567905\n",
      "Iteration is: 15736 and loss is: 0.00024292254238389432\n",
      "Iteration is: 15737 and loss is: 0.0003732172481250018\n",
      "Iteration is: 15738 and loss is: 0.0006082885665819049\n",
      "Iteration is: 15739 and loss is: 0.0010564271360635757\n",
      "Iteration is: 15740 and loss is: 0.0014943404821678996\n",
      "Iteration is: 15741 and loss is: 0.0014952939236536622\n",
      "Iteration is: 15742 and loss is: 0.0007555776974186301\n",
      "Iteration is: 15743 and loss is: 0.0002949531772173941\n",
      "Iteration is: 15744 and loss is: 0.0005513489595614374\n",
      "Iteration is: 15745 and loss is: 0.0006137753953225911\n",
      "Iteration is: 15746 and loss is: 0.0004505047691054642\n",
      "Iteration is: 15747 and loss is: 0.0004823551280423999\n",
      "Iteration is: 15748 and loss is: 0.00042994707473553717\n",
      "Iteration is: 15749 and loss is: 0.0004139712546020746\n",
      "Iteration is: 15750 and loss is: 0.0003920343588106334\n",
      "Iteration is: 15751 and loss is: 0.0003034283290617168\n",
      "Iteration is: 15752 and loss is: 0.00029567224555648863\n",
      "Iteration is: 15753 and loss is: 0.0003569076070562005\n",
      "Iteration is: 15754 and loss is: 0.00028197673964314163\n",
      "Iteration is: 15755 and loss is: 0.00020654441323131323\n",
      "Iteration is: 15756 and loss is: 0.0003075118293054402\n",
      "Iteration is: 15757 and loss is: 0.00029312781407497823\n",
      "Iteration is: 15758 and loss is: 0.00018665158131625503\n",
      "Iteration is: 15759 and loss is: 0.0002538716944400221\n",
      "Iteration is: 15760 and loss is: 0.0003299838281236589\n",
      "Iteration is: 15761 and loss is: 0.0003549543907865882\n",
      "Iteration is: 15762 and loss is: 0.0005917721427977085\n",
      "Iteration is: 15763 and loss is: 0.0012090312084183097\n",
      "Iteration is: 15764 and loss is: 0.002066184300929308\n",
      "Iteration is: 15765 and loss is: 0.0027879406698048115\n",
      "Iteration is: 15766 and loss is: 0.0016362328315153718\n",
      "Iteration is: 15767 and loss is: 0.00034826784394681454\n",
      "Iteration is: 15768 and loss is: 0.0010488304542377591\n",
      "Iteration is: 15769 and loss is: 0.001146557042375207\n",
      "Iteration is: 15770 and loss is: 0.000665344181470573\n",
      "Iteration is: 15771 and loss is: 0.0006738824886269867\n",
      "Iteration is: 15772 and loss is: 0.0011220121523365378\n",
      "Iteration is: 15773 and loss is: 0.0008546161698177457\n",
      "Iteration is: 15774 and loss is: 0.00038825214141979814\n",
      "Iteration is: 15775 and loss is: 0.0006165991071611643\n",
      "Iteration is: 15776 and loss is: 0.0006015576655045152\n",
      "Iteration is: 15777 and loss is: 0.00027943309396505356\n",
      "Iteration is: 15778 and loss is: 0.0004064213717356324\n",
      "Iteration is: 15779 and loss is: 0.0004997174255549908\n",
      "Iteration is: 15780 and loss is: 0.0003993203572463244\n",
      "Iteration is: 15781 and loss is: 0.0003017041308339685\n",
      "Iteration is: 15782 and loss is: 0.0002160285657737404\n",
      "Iteration is: 15783 and loss is: 0.0003564156941138208\n",
      "Iteration is: 15784 and loss is: 0.0005953744403086603\n",
      "Iteration is: 15785 and loss is: 0.0010270290076732635\n",
      "Iteration is: 15786 and loss is: 0.0019180321833118796\n",
      "Iteration is: 15787 and loss is: 0.002907135058194399\n",
      "Iteration is: 15788 and loss is: 0.0030696808826178312\n",
      "Iteration is: 15789 and loss is: 0.0008698401506990194\n",
      "Iteration is: 15790 and loss is: 0.0010653630597516894\n",
      "Iteration is: 15791 and loss is: 0.0022035264410078526\n",
      "Iteration is: 15792 and loss is: 0.00156803906429559\n",
      "Iteration is: 15793 and loss is: 0.0006396493408828974\n",
      "Iteration is: 15794 and loss is: 0.0011625513434410095\n",
      "Iteration is: 15795 and loss is: 0.0017222240567207336\n",
      "Iteration is: 15796 and loss is: 0.0009771873010322452\n",
      "Iteration is: 15797 and loss is: 0.00048630376113578677\n",
      "Iteration is: 15798 and loss is: 0.0008861318929120898\n",
      "Iteration is: 15799 and loss is: 0.0005451132892630994\n",
      "Iteration is: 15800 and loss is: 0.00032822194043546915\n",
      "Iteration is: 15801 and loss is: 0.000790948630310595\n",
      "Iteration is: 15802 and loss is: 0.0007184804999269545\n",
      "Iteration is: 15803 and loss is: 0.00047726737102493644\n",
      "Iteration is: 15804 and loss is: 0.0003848737105727196\n",
      "Iteration is: 15805 and loss is: 0.00023795834567863494\n",
      "Iteration is: 15806 and loss is: 0.0003677738131955266\n",
      "Iteration is: 15807 and loss is: 0.000498464796692133\n",
      "Iteration is: 15808 and loss is: 0.0006479147123172879\n",
      "Iteration is: 15809 and loss is: 0.0009469898068346083\n",
      "Iteration is: 15810 and loss is: 0.0008502587443217635\n",
      "Iteration is: 15811 and loss is: 0.0005533549701794982\n",
      "Iteration is: 15812 and loss is: 0.0002901232219301164\n",
      "Iteration is: 15813 and loss is: 0.00033951224759221077\n",
      "Iteration is: 15814 and loss is: 0.0004702178994193673\n",
      "Iteration is: 15815 and loss is: 0.0003373179351910949\n",
      "Iteration is: 15816 and loss is: 0.00030416189110837877\n",
      "Iteration is: 15817 and loss is: 0.0003355815715622157\n",
      "Iteration is: 15818 and loss is: 0.00031100783962756395\n",
      "Iteration is: 15819 and loss is: 0.00032496204948984087\n",
      "Iteration is: 15820 and loss is: 0.0002570200012996793\n",
      "Iteration is: 15821 and loss is: 0.000273834855761379\n",
      "Iteration is: 15822 and loss is: 0.0003061362076550722\n",
      "Iteration is: 15823 and loss is: 0.00021028303308412433\n",
      "Iteration is: 15824 and loss is: 0.0002251428086310625\n",
      "Iteration is: 15825 and loss is: 0.0002550673088990152\n",
      "Iteration is: 15826 and loss is: 0.00022013859415892512\n",
      "Iteration is: 15827 and loss is: 0.00019997599883936346\n",
      "Iteration is: 15828 and loss is: 0.0002225106582045555\n",
      "Iteration is: 15829 and loss is: 0.00021621110499836504\n",
      "Iteration is: 15830 and loss is: 0.00018791673937812448\n",
      "Iteration is: 15831 and loss is: 0.00019845277711283416\n",
      "Iteration is: 15832 and loss is: 0.00021029390336479992\n",
      "Iteration is: 15833 and loss is: 0.00018460958381183445\n",
      "Iteration is: 15834 and loss is: 0.00017946660227607936\n",
      "Iteration is: 15835 and loss is: 0.00019862645422108471\n",
      "Iteration is: 15836 and loss is: 0.0001805029169190675\n",
      "Iteration is: 15837 and loss is: 0.0001724530739011243\n",
      "Iteration is: 15838 and loss is: 0.00018202309729531407\n",
      "Iteration is: 15839 and loss is: 0.00017814143211580813\n",
      "Iteration is: 15840 and loss is: 0.0001730593212414533\n",
      "Iteration is: 15841 and loss is: 0.0001838029274949804\n",
      "Iteration is: 15842 and loss is: 0.00019520893692970276\n",
      "Iteration is: 15843 and loss is: 0.0001961023372132331\n",
      "Iteration is: 15844 and loss is: 0.00019660271937027574\n",
      "Iteration is: 15845 and loss is: 0.00020146359747741371\n",
      "Iteration is: 15846 and loss is: 0.00020378838235046715\n",
      "Iteration is: 15847 and loss is: 0.00020094912906643003\n",
      "Iteration is: 15848 and loss is: 0.00021438830299302936\n",
      "Iteration is: 15849 and loss is: 0.0002353719319216907\n",
      "Iteration is: 15850 and loss is: 0.00026984253781847656\n",
      "Iteration is: 15851 and loss is: 0.0003354038926772773\n",
      "Iteration is: 15852 and loss is: 0.0004553031176328659\n",
      "Iteration is: 15853 and loss is: 0.0006067298236303031\n",
      "Iteration is: 15854 and loss is: 0.0008099926053546369\n",
      "Iteration is: 15855 and loss is: 0.000885556626599282\n",
      "Iteration is: 15856 and loss is: 0.0007210825569927692\n",
      "Iteration is: 15857 and loss is: 0.0003736036887858063\n",
      "Iteration is: 15858 and loss is: 0.00018407785682938993\n",
      "Iteration is: 15859 and loss is: 0.0002966111060231924\n",
      "Iteration is: 15860 and loss is: 0.0004411190457176417\n",
      "Iteration is: 15861 and loss is: 0.00037763279397040606\n",
      "Iteration is: 15862 and loss is: 0.0002271754201501608\n",
      "Iteration is: 15863 and loss is: 0.00022646784782409668\n",
      "Iteration is: 15864 and loss is: 0.0003298434894531965\n",
      "Iteration is: 15865 and loss is: 0.0003357102395966649\n",
      "Iteration is: 15866 and loss is: 0.00022827988141216338\n",
      "Iteration is: 15867 and loss is: 0.0001747681963024661\n",
      "Iteration is: 15868 and loss is: 0.00023574489750899374\n",
      "Iteration is: 15869 and loss is: 0.00029486208222806454\n",
      "Iteration is: 15870 and loss is: 0.0002650695387274027\n",
      "Iteration is: 15871 and loss is: 0.00020395802857819945\n",
      "Iteration is: 15872 and loss is: 0.00018479902064427733\n",
      "Iteration is: 15873 and loss is: 0.00018963622278533876\n",
      "Iteration is: 15874 and loss is: 0.00020297887385822833\n",
      "Iteration is: 15875 and loss is: 0.00023798404436092824\n",
      "Iteration is: 15876 and loss is: 0.00027519449940882623\n",
      "Iteration is: 15877 and loss is: 0.00030747117125429213\n",
      "Iteration is: 15878 and loss is: 0.0003773121861740947\n",
      "Iteration is: 15879 and loss is: 0.0005067420424893498\n",
      "Iteration is: 15880 and loss is: 0.0007619605166837573\n",
      "Iteration is: 15881 and loss is: 0.0010733401868492365\n",
      "Iteration is: 15882 and loss is: 0.0013447421370074153\n",
      "Iteration is: 15883 and loss is: 0.0011067654704675078\n",
      "Iteration is: 15884 and loss is: 0.0004971225280314684\n",
      "Iteration is: 15885 and loss is: 0.00028340218705125153\n",
      "Iteration is: 15886 and loss is: 0.0005438256775960326\n",
      "Iteration is: 15887 and loss is: 0.0005582195590250194\n",
      "Iteration is: 15888 and loss is: 0.00044194969814270735\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 15889 and loss is: 0.0004197423695586622\n",
      "Iteration is: 15890 and loss is: 0.0003826989559456706\n",
      "Iteration is: 15891 and loss is: 0.0004262510919943452\n",
      "Iteration is: 15892 and loss is: 0.0003908530925400555\n",
      "Iteration is: 15893 and loss is: 0.00027402525302022696\n",
      "Iteration is: 15894 and loss is: 0.00028162827948108315\n",
      "Iteration is: 15895 and loss is: 0.0003625283425208181\n",
      "Iteration is: 15896 and loss is: 0.00027527642669156194\n",
      "Iteration is: 15897 and loss is: 0.0001838856260292232\n",
      "Iteration is: 15898 and loss is: 0.0002930684422608465\n",
      "Iteration is: 15899 and loss is: 0.0002923054271377623\n",
      "Iteration is: 15900 and loss is: 0.0001811851980164647\n",
      "Iteration is: 15901 and loss is: 0.00021038297563791275\n",
      "Iteration is: 15902 and loss is: 0.00028111081337556243\n",
      "Iteration is: 15903 and loss is: 0.000258584797848016\n",
      "Iteration is: 15904 and loss is: 0.0002988397900480777\n",
      "Iteration is: 15905 and loss is: 0.0004944503889419138\n",
      "Iteration is: 15906 and loss is: 0.0008272508857771754\n",
      "Iteration is: 15907 and loss is: 0.0013505425304174423\n",
      "Iteration is: 15908 and loss is: 0.002047448419034481\n",
      "Iteration is: 15909 and loss is: 0.0017716967267915606\n",
      "Iteration is: 15910 and loss is: 0.0006090660463087261\n",
      "Iteration is: 15911 and loss is: 0.0005946643650531769\n",
      "Iteration is: 15912 and loss is: 0.0009765161667019129\n",
      "Iteration is: 15913 and loss is: 0.0006392282666638494\n",
      "Iteration is: 15914 and loss is: 0.0007356848800554872\n",
      "Iteration is: 15915 and loss is: 0.0006811541388742626\n",
      "Iteration is: 15916 and loss is: 0.0006603472866117954\n",
      "Iteration is: 15917 and loss is: 0.0005253981798887253\n",
      "Iteration is: 15918 and loss is: 0.00046673504402861\n",
      "Iteration is: 15919 and loss is: 0.0004939540522173047\n",
      "Iteration is: 15920 and loss is: 0.00031884817872196436\n",
      "Iteration is: 15921 and loss is: 0.00041903965757228434\n",
      "Iteration is: 15922 and loss is: 0.0003449614450801164\n",
      "Iteration is: 15923 and loss is: 0.00023420750221703202\n",
      "Iteration is: 15924 and loss is: 0.0004230014164932072\n",
      "Iteration is: 15925 and loss is: 0.00023157053510658443\n",
      "Iteration is: 15926 and loss is: 0.00029523574630729854\n",
      "Iteration is: 15927 and loss is: 0.0002865564019884914\n",
      "Iteration is: 15928 and loss is: 0.00027368066366761923\n",
      "Iteration is: 15929 and loss is: 0.00023834194871596992\n",
      "Iteration is: 15930 and loss is: 0.0002453539927955717\n",
      "Iteration is: 15931 and loss is: 0.00022865578648634255\n",
      "Iteration is: 15932 and loss is: 0.0002231600519735366\n",
      "Iteration is: 15933 and loss is: 0.00023733454872854054\n",
      "Iteration is: 15934 and loss is: 0.00020295327703934163\n",
      "Iteration is: 15935 and loss is: 0.00025570503203198314\n",
      "Iteration is: 15936 and loss is: 0.00019395581330172718\n",
      "Iteration is: 15937 and loss is: 0.00020584110461641103\n",
      "Iteration is: 15938 and loss is: 0.00020225666230544448\n",
      "Iteration is: 15939 and loss is: 0.00017918953381013125\n",
      "Iteration is: 15940 and loss is: 0.00017648664652369916\n",
      "Iteration is: 15941 and loss is: 0.00017876250785775483\n",
      "Iteration is: 15942 and loss is: 0.0001753020624164492\n",
      "Iteration is: 15943 and loss is: 0.00016335208783857524\n",
      "Iteration is: 15944 and loss is: 0.00017535447841510177\n",
      "Iteration is: 15945 and loss is: 0.00015813502250239253\n",
      "Iteration is: 15946 and loss is: 0.00016951971338130534\n",
      "Iteration is: 15947 and loss is: 0.00015589629765599966\n",
      "Iteration is: 15948 and loss is: 0.00015414695371873677\n",
      "Iteration is: 15949 and loss is: 0.00016200773825403303\n",
      "Iteration is: 15950 and loss is: 0.0001520014338893816\n",
      "Iteration is: 15951 and loss is: 0.00015456340042874217\n",
      "Iteration is: 15952 and loss is: 0.00015364385035354644\n",
      "Iteration is: 15953 and loss is: 0.00015462751616723835\n",
      "Iteration is: 15954 and loss is: 0.00014817774354014546\n",
      "Iteration is: 15955 and loss is: 0.00015538581646978855\n",
      "Iteration is: 15956 and loss is: 0.00015802503912709653\n",
      "Iteration is: 15957 and loss is: 0.0001688531192485243\n",
      "Iteration is: 15958 and loss is: 0.0002055800287052989\n",
      "Iteration is: 15959 and loss is: 0.0002878885134123266\n",
      "Iteration is: 15960 and loss is: 0.0005142730660736561\n",
      "Iteration is: 15961 and loss is: 0.000988830579444766\n",
      "Iteration is: 15962 and loss is: 0.0019110714783892035\n",
      "Iteration is: 15963 and loss is: 0.0026074738707393408\n",
      "Iteration is: 15964 and loss is: 0.00206844718195498\n",
      "Iteration is: 15965 and loss is: 0.000706506019923836\n",
      "Iteration is: 15966 and loss is: 0.0008181254379451275\n",
      "Iteration is: 15967 and loss is: 0.001090050209313631\n",
      "Iteration is: 15968 and loss is: 0.000884530134499073\n",
      "Iteration is: 15969 and loss is: 0.0007991628954187036\n",
      "Iteration is: 15970 and loss is: 0.0008295578300021589\n",
      "Iteration is: 15971 and loss is: 0.0007674512453377247\n",
      "Iteration is: 15972 and loss is: 0.0005063948919996619\n",
      "Iteration is: 15973 and loss is: 0.0006169440457597375\n",
      "Iteration is: 15974 and loss is: 0.00043238006765022874\n",
      "Iteration is: 15975 and loss is: 0.0003897370770573616\n",
      "Iteration is: 15976 and loss is: 0.000581011176109314\n",
      "Iteration is: 15977 and loss is: 0.00023289620003197342\n",
      "Iteration is: 15978 and loss is: 0.00034364094608463347\n",
      "Iteration is: 15979 and loss is: 0.00046175025636330247\n",
      "Iteration is: 15980 and loss is: 0.0002767308906186372\n",
      "Iteration is: 15981 and loss is: 0.0002959866833407432\n",
      "Iteration is: 15982 and loss is: 0.00038223291630856693\n",
      "Iteration is: 15983 and loss is: 0.00031280124676413834\n",
      "Iteration is: 15984 and loss is: 0.00033014584914781153\n",
      "Iteration is: 15985 and loss is: 0.0004297677078284323\n",
      "Iteration is: 15986 and loss is: 0.0005143344169482589\n",
      "Iteration is: 15987 and loss is: 0.0006520025781355798\n",
      "Iteration is: 15988 and loss is: 0.000583582790568471\n",
      "Iteration is: 15989 and loss is: 0.000533838989213109\n",
      "Iteration is: 15990 and loss is: 0.00039704726077616215\n",
      "Iteration is: 15991 and loss is: 0.00021858411491848528\n",
      "Iteration is: 15992 and loss is: 0.00035052455496042967\n",
      "Iteration is: 15993 and loss is: 0.0004739149007946253\n",
      "Iteration is: 15994 and loss is: 0.00034002968459390104\n",
      "Iteration is: 15995 and loss is: 0.00021493191889021546\n",
      "Iteration is: 15996 and loss is: 0.0003004239988513291\n",
      "Iteration is: 15997 and loss is: 0.0003519159508869052\n",
      "Iteration is: 15998 and loss is: 0.0003180738422088325\n",
      "Iteration is: 15999 and loss is: 0.00019589377916418016\n",
      "Iteration is: 16000 and loss is: 0.000223059905692935\n",
      "Iteration is: 16001 and loss is: 0.00025029643438756466\n",
      "Iteration is: 16002 and loss is: 0.0002831716265063733\n",
      "Iteration is: 16003 and loss is: 0.0002911899646278471\n",
      "Iteration is: 16004 and loss is: 0.0002692268753889948\n",
      "Iteration is: 16005 and loss is: 0.00021966840722598135\n",
      "Iteration is: 16006 and loss is: 0.0001684859744273126\n",
      "Iteration is: 16007 and loss is: 0.00017262130859307945\n",
      "Iteration is: 16008 and loss is: 0.00016051207785494626\n",
      "Iteration is: 16009 and loss is: 0.0001800243917386979\n",
      "Iteration is: 16010 and loss is: 0.00020914783817715943\n",
      "Iteration is: 16011 and loss is: 0.0003046425699722022\n",
      "Iteration is: 16012 and loss is: 0.0005233183037489653\n",
      "Iteration is: 16013 and loss is: 0.001074991887435317\n",
      "Iteration is: 16014 and loss is: 0.0020046327263116837\n",
      "Iteration is: 16015 and loss is: 0.0029177949763834476\n",
      "Iteration is: 16016 and loss is: 0.001983846537768841\n",
      "Iteration is: 16017 and loss is: 0.0006827430333942175\n",
      "Iteration is: 16018 and loss is: 0.0014206067426130176\n",
      "Iteration is: 16019 and loss is: 0.0016526104882359505\n",
      "Iteration is: 16020 and loss is: 0.0011336279567331076\n",
      "Iteration is: 16021 and loss is: 0.0007149107404984534\n",
      "Iteration is: 16022 and loss is: 0.0011393234599381685\n",
      "Iteration is: 16023 and loss is: 0.0012802780838683248\n",
      "Iteration is: 16024 and loss is: 0.0004720460856333375\n",
      "Iteration is: 16025 and loss is: 0.0005307081155478954\n",
      "Iteration is: 16026 and loss is: 0.0007269462221302092\n",
      "Iteration is: 16027 and loss is: 0.0003323506680317223\n",
      "Iteration is: 16028 and loss is: 0.0004921568906866014\n",
      "Iteration is: 16029 and loss is: 0.00039297601324506104\n",
      "Iteration is: 16030 and loss is: 0.00033884617732837796\n",
      "Iteration is: 16031 and loss is: 0.0007306436891667545\n",
      "Iteration is: 16032 and loss is: 0.0009093004046007991\n",
      "Iteration is: 16033 and loss is: 0.0017759890761226416\n",
      "Iteration is: 16034 and loss is: 0.004791646264493465\n",
      "Iteration is: 16035 and loss is: 0.011256871744990349\n",
      "Iteration is: 16036 and loss is: 0.014465298503637314\n",
      "Iteration is: 16037 and loss is: 0.0035822063218802214\n",
      "Iteration is: 16038 and loss is: 0.010445849969983101\n",
      "Iteration is: 16039 and loss is: 0.019730983301997185\n",
      "Iteration is: 16040 and loss is: 0.019403064623475075\n",
      "Iteration is: 16041 and loss is: 0.008047067560255527\n",
      "Iteration is: 16042 and loss is: 0.004635182674974203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 16043 and loss is: 0.01496681198477745\n",
      "Iteration is: 16044 and loss is: 0.007509995251893997\n",
      "Iteration is: 16045 and loss is: 0.003429824486374855\n",
      "Iteration is: 16046 and loss is: 0.008990742266178131\n",
      "Iteration is: 16047 and loss is: 0.0010610343888401985\n",
      "Iteration is: 16048 and loss is: 0.007296451833099127\n",
      "Iteration is: 16049 and loss is: 0.011105994693934917\n",
      "Iteration is: 16050 and loss is: 0.0035227476619184017\n",
      "Iteration is: 16051 and loss is: 0.002909304341301322\n",
      "Iteration is: 16052 and loss is: 0.006557516288012266\n",
      "Iteration is: 16053 and loss is: 0.0009889760985970497\n",
      "Iteration is: 16054 and loss is: 0.004850066266953945\n",
      "Iteration is: 16055 and loss is: 0.001155856647528708\n",
      "Iteration is: 16056 and loss is: 0.0039714789018034935\n",
      "Iteration is: 16057 and loss is: 0.0021354358177632093\n",
      "Iteration is: 16058 and loss is: 0.00223524053581059\n",
      "Iteration is: 16059 and loss is: 0.0016795231495052576\n",
      "Iteration is: 16060 and loss is: 0.0020378166809678078\n",
      "Iteration is: 16061 and loss is: 0.0014964198926463723\n",
      "Iteration is: 16062 and loss is: 0.001798164681531489\n",
      "Iteration is: 16063 and loss is: 0.0011120762210339308\n",
      "Iteration is: 16064 and loss is: 0.001535572111606598\n",
      "Iteration is: 16065 and loss is: 0.0009806343587115407\n",
      "Iteration is: 16066 and loss is: 0.001465602545067668\n",
      "Iteration is: 16067 and loss is: 0.0008260371396318078\n",
      "Iteration is: 16068 and loss is: 0.00112967099994421\n",
      "Iteration is: 16069 and loss is: 0.0008777041221037507\n",
      "Iteration is: 16070 and loss is: 0.0008169163484126329\n",
      "Iteration is: 16071 and loss is: 0.0008029439486563206\n",
      "Iteration is: 16072 and loss is: 0.0006946121575310826\n",
      "Iteration is: 16073 and loss is: 0.0006525022909045219\n",
      "Iteration is: 16074 and loss is: 0.000738123431801796\n",
      "Iteration is: 16075 and loss is: 0.000448526261607185\n",
      "Iteration is: 16076 and loss is: 0.0007051231805235147\n",
      "Iteration is: 16077 and loss is: 0.00038993358612060547\n",
      "Iteration is: 16078 and loss is: 0.0005960267735645175\n",
      "Iteration is: 16079 and loss is: 0.0004275795363355428\n",
      "Iteration is: 16080 and loss is: 0.0004334806581027806\n",
      "Iteration is: 16081 and loss is: 0.0004450692795217037\n",
      "Iteration is: 16082 and loss is: 0.00037112453719601035\n",
      "Iteration is: 16083 and loss is: 0.00039423772250302136\n",
      "Iteration is: 16084 and loss is: 0.0003603004734031856\n",
      "Iteration is: 16085 and loss is: 0.000324939435813576\n",
      "Iteration is: 16086 and loss is: 0.00035620469134300947\n",
      "Iteration is: 16087 and loss is: 0.00029251951491460204\n",
      "Iteration is: 16088 and loss is: 0.0003140761982649565\n",
      "Iteration is: 16089 and loss is: 0.00028031604597344995\n",
      "Iteration is: 16090 and loss is: 0.0002918246609624475\n",
      "Iteration is: 16091 and loss is: 0.0002626026689540595\n",
      "Iteration is: 16092 and loss is: 0.0002698143944144249\n",
      "Iteration is: 16093 and loss is: 0.00025815790286287665\n",
      "Iteration is: 16094 and loss is: 0.00023645180044695735\n",
      "Iteration is: 16095 and loss is: 0.00026597038959152997\n",
      "Iteration is: 16096 and loss is: 0.00020860997028648853\n",
      "Iteration is: 16097 and loss is: 0.0002521816932130605\n",
      "Iteration is: 16098 and loss is: 0.0002138028503395617\n",
      "Iteration is: 16099 and loss is: 0.00022476787853520364\n",
      "Iteration is: 16100 and loss is: 0.00021927751367911696\n",
      "Iteration is: 16101 and loss is: 0.00020959362154826522\n",
      "Iteration is: 16102 and loss is: 0.00021169785759411752\n",
      "Iteration is: 16103 and loss is: 0.00020923213742207736\n",
      "Iteration is: 16104 and loss is: 0.00020016924827359617\n",
      "Iteration is: 16105 and loss is: 0.00020441345986910164\n",
      "Iteration is: 16106 and loss is: 0.00019565435650292784\n",
      "Iteration is: 16107 and loss is: 0.00019864042405970395\n",
      "Iteration is: 16108 and loss is: 0.00019032909767702222\n",
      "Iteration is: 16109 and loss is: 0.00019514464656822383\n",
      "Iteration is: 16110 and loss is: 0.0001871845597634092\n",
      "Iteration is: 16111 and loss is: 0.0001896376343211159\n",
      "Iteration is: 16112 and loss is: 0.00018802820704877377\n",
      "Iteration is: 16113 and loss is: 0.00018301702220924199\n",
      "Iteration is: 16114 and loss is: 0.00018734729383140802\n",
      "Iteration is: 16115 and loss is: 0.00018176619778387249\n",
      "Iteration is: 16116 and loss is: 0.00018204674415756017\n",
      "Iteration is: 16117 and loss is: 0.00018202982028014958\n",
      "Iteration is: 16118 and loss is: 0.00017908369773067534\n",
      "Iteration is: 16119 and loss is: 0.00017974106594920158\n",
      "Iteration is: 16120 and loss is: 0.00017830260912887752\n",
      "Iteration is: 16121 and loss is: 0.00017724605277180672\n",
      "Iteration is: 16122 and loss is: 0.0001774953561834991\n",
      "Iteration is: 16123 and loss is: 0.0001758350117597729\n",
      "Iteration is: 16124 and loss is: 0.00017638533608987927\n",
      "Iteration is: 16125 and loss is: 0.00017424050020053983\n",
      "Iteration is: 16126 and loss is: 0.00017529798788018525\n",
      "Iteration is: 16127 and loss is: 0.00017372233560308814\n",
      "Iteration is: 16128 and loss is: 0.00017328860121779144\n",
      "Iteration is: 16129 and loss is: 0.00017354886222165078\n",
      "Iteration is: 16130 and loss is: 0.0001722088345559314\n",
      "Iteration is: 16131 and loss is: 0.00017231926904059947\n",
      "Iteration is: 16132 and loss is: 0.00017194612883031368\n",
      "Iteration is: 16133 and loss is: 0.00017115756054408848\n",
      "Iteration is: 16134 and loss is: 0.00017121413839049637\n",
      "Iteration is: 16135 and loss is: 0.0001706935145193711\n",
      "Iteration is: 16136 and loss is: 0.00017026811838150024\n",
      "Iteration is: 16137 and loss is: 0.00017012853641062975\n",
      "Iteration is: 16138 and loss is: 0.00016963930102065206\n",
      "Iteration is: 16139 and loss is: 0.00016958815103862435\n",
      "Iteration is: 16140 and loss is: 0.00016893146676011384\n",
      "Iteration is: 16141 and loss is: 0.00016904357471503317\n",
      "Iteration is: 16142 and loss is: 0.00016851889085955918\n",
      "Iteration is: 16143 and loss is: 0.0001682661968516186\n",
      "Iteration is: 16144 and loss is: 0.00016812791000120342\n",
      "Iteration is: 16145 and loss is: 0.0001677025284152478\n",
      "Iteration is: 16146 and loss is: 0.0001675086241448298\n",
      "Iteration is: 16147 and loss is: 0.0001672683865763247\n",
      "Iteration is: 16148 and loss is: 0.00016691337805241346\n",
      "Iteration is: 16149 and loss is: 0.00016664908616803586\n",
      "Iteration is: 16150 and loss is: 0.00016634062922094017\n",
      "Iteration is: 16151 and loss is: 0.00016592475003562868\n",
      "Iteration is: 16152 and loss is: 0.0001655540254432708\n",
      "Iteration is: 16153 and loss is: 0.00016493044677190483\n",
      "Iteration is: 16154 and loss is: 0.00016440504987258464\n",
      "Iteration is: 16155 and loss is: 0.00016341051377821714\n",
      "Iteration is: 16156 and loss is: 0.00016234688519034535\n",
      "Iteration is: 16157 and loss is: 0.0001609177270438522\n",
      "Iteration is: 16158 and loss is: 0.00015920666919555515\n",
      "Iteration is: 16159 and loss is: 0.00015741847164463252\n",
      "Iteration is: 16160 and loss is: 0.0001559411030029878\n",
      "Iteration is: 16161 and loss is: 0.00015497341519221663\n",
      "Iteration is: 16162 and loss is: 0.00015454353706445545\n",
      "Iteration is: 16163 and loss is: 0.00015437194088008255\n",
      "Iteration is: 16164 and loss is: 0.00015417661052197218\n",
      "Iteration is: 16165 and loss is: 0.00015369855100288987\n",
      "Iteration is: 16166 and loss is: 0.00015284185064956546\n",
      "Iteration is: 16167 and loss is: 0.00015192681166809052\n",
      "Iteration is: 16168 and loss is: 0.00015117529255803674\n",
      "Iteration is: 16169 and loss is: 0.00015076009731274098\n",
      "Iteration is: 16170 and loss is: 0.00015050606452859938\n",
      "Iteration is: 16171 and loss is: 0.00015028848429210484\n",
      "Iteration is: 16172 and loss is: 0.0001500430953456089\n",
      "Iteration is: 16173 and loss is: 0.00014963102876208723\n",
      "Iteration is: 16174 and loss is: 0.00014906327123753726\n",
      "Iteration is: 16175 and loss is: 0.0001484306703787297\n",
      "Iteration is: 16176 and loss is: 0.00014789222041144967\n",
      "Iteration is: 16177 and loss is: 0.00014749470574315637\n",
      "Iteration is: 16178 and loss is: 0.00014722613559570163\n",
      "Iteration is: 16179 and loss is: 0.00014700675092171878\n",
      "Iteration is: 16180 and loss is: 0.00014678563456982374\n",
      "Iteration is: 16181 and loss is: 0.00014649578952230513\n",
      "Iteration is: 16182 and loss is: 0.00014617296983487904\n",
      "Iteration is: 16183 and loss is: 0.00014588954218197614\n",
      "Iteration is: 16184 and loss is: 0.00014565294259227812\n",
      "Iteration is: 16185 and loss is: 0.00014547636965289712\n",
      "Iteration is: 16186 and loss is: 0.00014531608030665666\n",
      "Iteration is: 16187 and loss is: 0.0001451502466807142\n",
      "Iteration is: 16188 and loss is: 0.00014495123468805104\n",
      "Iteration is: 16189 and loss is: 0.00014472479233518243\n",
      "Iteration is: 16190 and loss is: 0.00014450534945353866\n",
      "Iteration is: 16191 and loss is: 0.00014431725139729679\n",
      "Iteration is: 16192 and loss is: 0.00014415454643312842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 16193 and loss is: 0.00014401850057765841\n",
      "Iteration is: 16194 and loss is: 0.00014389447460416704\n",
      "Iteration is: 16195 and loss is: 0.00014377418847288936\n",
      "Iteration is: 16196 and loss is: 0.0001436443708371371\n",
      "Iteration is: 16197 and loss is: 0.00014350321725942194\n",
      "Iteration is: 16198 and loss is: 0.00014336753520183265\n",
      "Iteration is: 16199 and loss is: 0.00014324451331049204\n",
      "Iteration is: 16200 and loss is: 0.0001431280397810042\n",
      "Iteration is: 16201 and loss is: 0.0001430255506420508\n",
      "Iteration is: 16202 and loss is: 0.0001429232652299106\n",
      "Iteration is: 16203 and loss is: 0.00014281441690400243\n",
      "Iteration is: 16204 and loss is: 0.00014269936946220696\n",
      "Iteration is: 16205 and loss is: 0.00014258318697102368\n",
      "Iteration is: 16206 and loss is: 0.00014247713261283934\n",
      "Iteration is: 16207 and loss is: 0.00014237879076972604\n",
      "Iteration is: 16208 and loss is: 0.00014229270163923502\n",
      "Iteration is: 16209 and loss is: 0.00014220669982023537\n",
      "Iteration is: 16210 and loss is: 0.0001421184279024601\n",
      "Iteration is: 16211 and loss is: 0.00014202440797816962\n",
      "Iteration is: 16212 and loss is: 0.0001419336476828903\n",
      "Iteration is: 16213 and loss is: 0.00014184348401613533\n",
      "Iteration is: 16214 and loss is: 0.00014175685646478087\n",
      "Iteration is: 16215 and loss is: 0.00014167380868457258\n",
      "Iteration is: 16216 and loss is: 0.00014159400598146021\n",
      "Iteration is: 16217 and loss is: 0.00014151165669318289\n",
      "Iteration is: 16218 and loss is: 0.0001414304133504629\n",
      "Iteration is: 16219 and loss is: 0.00014134618686512113\n",
      "Iteration is: 16220 and loss is: 0.00014126500173006207\n",
      "Iteration is: 16221 and loss is: 0.00014118652325123549\n",
      "Iteration is: 16222 and loss is: 0.0001411100383847952\n",
      "Iteration is: 16223 and loss is: 0.0001410355616826564\n",
      "Iteration is: 16224 and loss is: 0.00014096254017204046\n",
      "Iteration is: 16225 and loss is: 0.00014088978059589863\n",
      "Iteration is: 16226 and loss is: 0.00014081559493206441\n",
      "Iteration is: 16227 and loss is: 0.0001407424861099571\n",
      "Iteration is: 16228 and loss is: 0.00014067327720113099\n",
      "Iteration is: 16229 and loss is: 0.00014060011017136276\n",
      "Iteration is: 16230 and loss is: 0.00014053014456294477\n",
      "Iteration is: 16231 and loss is: 0.00014046212891116738\n",
      "Iteration is: 16232 and loss is: 0.00014039520465303212\n",
      "Iteration is: 16233 and loss is: 0.00014032756735105067\n",
      "Iteration is: 16234 and loss is: 0.00014026142889633775\n",
      "Iteration is: 16235 and loss is: 0.00014019141963217407\n",
      "Iteration is: 16236 and loss is: 0.00014012781321071088\n",
      "Iteration is: 16237 and loss is: 0.00014006324636284262\n",
      "Iteration is: 16238 and loss is: 0.00014000097871758044\n",
      "Iteration is: 16239 and loss is: 0.00013993728498462588\n",
      "Iteration is: 16240 and loss is: 0.00013987498823553324\n",
      "Iteration is: 16241 and loss is: 0.00013981273514218628\n",
      "Iteration is: 16242 and loss is: 0.00013974944886285812\n",
      "Iteration is: 16243 and loss is: 0.000139689160278067\n",
      "Iteration is: 16244 and loss is: 0.0001396282168570906\n",
      "Iteration is: 16245 and loss is: 0.00013956967450212687\n",
      "Iteration is: 16246 and loss is: 0.00013950852735433728\n",
      "Iteration is: 16247 and loss is: 0.00013945004320703447\n",
      "Iteration is: 16248 and loss is: 0.00013939272321294993\n",
      "Iteration is: 16249 and loss is: 0.0001393340207869187\n",
      "Iteration is: 16250 and loss is: 0.0001392767153447494\n",
      "Iteration is: 16251 and loss is: 0.0001392196281813085\n",
      "Iteration is: 16252 and loss is: 0.0001391642726957798\n",
      "Iteration is: 16253 and loss is: 0.00013910894631408155\n",
      "Iteration is: 16254 and loss is: 0.00013905308151151985\n",
      "Iteration is: 16255 and loss is: 0.00013899768237024546\n",
      "Iteration is: 16256 and loss is: 0.00013894145376980305\n",
      "Iteration is: 16257 and loss is: 0.00013888670946471393\n",
      "Iteration is: 16258 and loss is: 0.0001388346281601116\n",
      "Iteration is: 16259 and loss is: 0.0001387813244946301\n",
      "Iteration is: 16260 and loss is: 0.00013872735144104809\n",
      "Iteration is: 16261 and loss is: 0.0001386744115734473\n",
      "Iteration is: 16262 and loss is: 0.0001386227668263018\n",
      "Iteration is: 16263 and loss is: 0.00013857005978934467\n",
      "Iteration is: 16264 and loss is: 0.00013851808034814894\n",
      "Iteration is: 16265 and loss is: 0.00013846639194525778\n",
      "Iteration is: 16266 and loss is: 0.00013841460167896003\n",
      "Iteration is: 16267 and loss is: 0.00013836695870850235\n",
      "Iteration is: 16268 and loss is: 0.0001383142953272909\n",
      "Iteration is: 16269 and loss is: 0.00013826553185936064\n",
      "Iteration is: 16270 and loss is: 0.00013821531319990754\n",
      "Iteration is: 16271 and loss is: 0.00013816633145324886\n",
      "Iteration is: 16272 and loss is: 0.0001381167530780658\n",
      "Iteration is: 16273 and loss is: 0.00013806988135911524\n",
      "Iteration is: 16274 and loss is: 0.00013801848399452865\n",
      "Iteration is: 16275 and loss is: 0.00013797267456538975\n",
      "Iteration is: 16276 and loss is: 0.0001379239110974595\n",
      "Iteration is: 16277 and loss is: 0.0001378765155095607\n",
      "Iteration is: 16278 and loss is: 0.0001378282904624939\n",
      "Iteration is: 16279 and loss is: 0.00013778252468910068\n",
      "Iteration is: 16280 and loss is: 0.00013773437240161002\n",
      "Iteration is: 16281 and loss is: 0.0001376904547214508\n",
      "Iteration is: 16282 and loss is: 0.00013764240429736674\n",
      "Iteration is: 16283 and loss is: 0.0001375968859065324\n",
      "Iteration is: 16284 and loss is: 0.00013754986866842955\n",
      "Iteration is: 16285 and loss is: 0.00013750509242527187\n",
      "Iteration is: 16286 and loss is: 0.00013746117474511266\n",
      "Iteration is: 16287 and loss is: 0.00013741607835981995\n",
      "Iteration is: 16288 and loss is: 0.00013736987602896988\n",
      "Iteration is: 16289 and loss is: 0.00013732734078075737\n",
      "Iteration is: 16290 and loss is: 0.00013728238991461694\n",
      "Iteration is: 16291 and loss is: 0.00013723816664423794\n",
      "Iteration is: 16292 and loss is: 0.00013719379785470665\n",
      "Iteration is: 16293 and loss is: 0.0001371486287098378\n",
      "Iteration is: 16294 and loss is: 0.00013710735947825015\n",
      "Iteration is: 16295 and loss is: 0.00013706398021895438\n",
      "Iteration is: 16296 and loss is: 0.00013702023716177791\n",
      "Iteration is: 16297 and loss is: 0.00013697892427444458\n",
      "Iteration is: 16298 and loss is: 0.00013693564687855542\n",
      "Iteration is: 16299 and loss is: 0.0001368915691273287\n",
      "Iteration is: 16300 and loss is: 0.0001368489902233705\n",
      "Iteration is: 16301 and loss is: 0.00013680612028110772\n",
      "Iteration is: 16302 and loss is: 0.00013676434173248708\n",
      "Iteration is: 16303 and loss is: 0.0001367231016047299\n",
      "Iteration is: 16304 and loss is: 0.00013668174506165087\n",
      "Iteration is: 16305 and loss is: 0.0001366402138955891\n",
      "Iteration is: 16306 and loss is: 0.00013659955584444106\n",
      "Iteration is: 16307 and loss is: 0.00013655783550348133\n",
      "Iteration is: 16308 and loss is: 0.00013651835615746677\n",
      "Iteration is: 16309 and loss is: 0.00013647593732457608\n",
      "Iteration is: 16310 and loss is: 0.0001364346535410732\n",
      "Iteration is: 16311 and loss is: 0.0001363974588457495\n",
      "Iteration is: 16312 and loss is: 0.00013635458890348673\n",
      "Iteration is: 16313 and loss is: 0.00013631412002723664\n",
      "Iteration is: 16314 and loss is: 0.00013627519365400076\n",
      "Iteration is: 16315 and loss is: 0.0001362360781058669\n",
      "Iteration is: 16316 and loss is: 0.00013619571109302342\n",
      "Iteration is: 16317 and loss is: 0.00013615554780699313\n",
      "Iteration is: 16318 and loss is: 0.00013611698523163795\n",
      "Iteration is: 16319 and loss is: 0.00013607708388008177\n",
      "Iteration is: 16320 and loss is: 0.00013603948173113167\n",
      "Iteration is: 16321 and loss is: 0.00013600023521576077\n",
      "Iteration is: 16322 and loss is: 0.0001359609013888985\n",
      "Iteration is: 16323 and loss is: 0.00013592260074801743\n",
      "Iteration is: 16324 and loss is: 0.0001358840090688318\n",
      "Iteration is: 16325 and loss is: 0.00013584591215476394\n",
      "Iteration is: 16326 and loss is: 0.00013580553059000522\n",
      "Iteration is: 16327 and loss is: 0.00013576980563811958\n",
      "Iteration is: 16328 and loss is: 0.0001357324217678979\n",
      "Iteration is: 16329 and loss is: 0.00013569463044404984\n",
      "Iteration is: 16330 and loss is: 0.00013565720291808248\n",
      "Iteration is: 16331 and loss is: 0.00013562047388404608\n",
      "Iteration is: 16332 and loss is: 0.00013558147475123405\n",
      "Iteration is: 16333 and loss is: 0.00013554401812143624\n",
      "Iteration is: 16334 and loss is: 0.0001355057756882161\n",
      "Iteration is: 16335 and loss is: 0.00013546974514611065\n",
      "Iteration is: 16336 and loss is: 0.00013543086242862046\n",
      "Iteration is: 16337 and loss is: 0.00013539445353671908\n",
      "Iteration is: 16338 and loss is: 0.00013535760808736086\n",
      "Iteration is: 16339 and loss is: 0.00013532163575291634\n",
      "Iteration is: 16340 and loss is: 0.00013528583804145455\n",
      "Iteration is: 16341 and loss is: 0.00013524663518182933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 16342 and loss is: 0.00013521357323043048\n",
      "Iteration is: 16343 and loss is: 0.00013517610204871744\n",
      "Iteration is: 16344 and loss is: 0.00013513979502022266\n",
      "Iteration is: 16345 and loss is: 0.00013510439021047205\n",
      "Iteration is: 16346 and loss is: 0.00013506910181604326\n",
      "Iteration is: 16347 and loss is: 0.0001350322854705155\n",
      "Iteration is: 16348 and loss is: 0.00013499445049092174\n",
      "Iteration is: 16349 and loss is: 0.00013496025349013507\n",
      "Iteration is: 16350 and loss is: 0.0001349233789369464\n",
      "Iteration is: 16351 and loss is: 0.00013488851254805923\n",
      "Iteration is: 16352 and loss is: 0.0001348536607110873\n",
      "Iteration is: 16353 and loss is: 0.00013481872156262398\n",
      "Iteration is: 16354 and loss is: 0.00013478413166012615\n",
      "Iteration is: 16355 and loss is: 0.00013474776642397046\n",
      "Iteration is: 16356 and loss is: 0.00013471374404616654\n",
      "Iteration is: 16357 and loss is: 0.00013467831013258547\n",
      "Iteration is: 16358 and loss is: 0.00013464372023008764\n",
      "Iteration is: 16359 and loss is: 0.00013460803893394768\n",
      "Iteration is: 16360 and loss is: 0.00013457336171995848\n",
      "Iteration is: 16361 and loss is: 0.00013453954306896776\n",
      "Iteration is: 16362 and loss is: 0.00013450518599711359\n",
      "Iteration is: 16363 and loss is: 0.0001344689226243645\n",
      "Iteration is: 16364 and loss is: 0.00013443673378787935\n",
      "Iteration is: 16365 and loss is: 0.00013440236216410995\n",
      "Iteration is: 16366 and loss is: 0.00013436631707008928\n",
      "Iteration is: 16367 and loss is: 0.00013433274580165744\n",
      "Iteration is: 16368 and loss is: 0.00013429854880087078\n",
      "Iteration is: 16369 and loss is: 0.0001342640316579491\n",
      "Iteration is: 16370 and loss is: 0.00013423027121461928\n",
      "Iteration is: 16371 and loss is: 0.00013419783499557525\n",
      "Iteration is: 16372 and loss is: 0.00013416423462331295\n",
      "Iteration is: 16373 and loss is: 0.0001341312745353207\n",
      "Iteration is: 16374 and loss is: 0.0001340970047749579\n",
      "Iteration is: 16375 and loss is: 0.00013406344805844128\n",
      "Iteration is: 16376 and loss is: 0.00013402983313426375\n",
      "Iteration is: 16377 and loss is: 0.00013399669842328876\n",
      "Iteration is: 16378 and loss is: 0.00013396231224760413\n",
      "Iteration is: 16379 and loss is: 0.00013392933760769665\n",
      "Iteration is: 16380 and loss is: 0.00013389589730650187\n",
      "Iteration is: 16381 and loss is: 0.00013386341743171215\n",
      "Iteration is: 16382 and loss is: 0.00013383074838202447\n",
      "Iteration is: 16383 and loss is: 0.00013379835581872612\n",
      "Iteration is: 16384 and loss is: 0.0001337657158728689\n",
      "Iteration is: 16385 and loss is: 0.0001337313442490995\n",
      "Iteration is: 16386 and loss is: 0.000133699388243258\n",
      "Iteration is: 16387 and loss is: 0.00013366658822633326\n",
      "Iteration is: 16388 and loss is: 0.00013363410835154355\n",
      "Iteration is: 16389 and loss is: 0.00013360060984268785\n",
      "Iteration is: 16390 and loss is: 0.00013356932322494686\n",
      "Iteration is: 16391 and loss is: 0.00013353434042073786\n",
      "Iteration is: 16392 and loss is: 0.0001335033302893862\n",
      "Iteration is: 16393 and loss is: 0.00013347217463888228\n",
      "Iteration is: 16394 and loss is: 0.00013344005856197327\n",
      "Iteration is: 16395 and loss is: 0.00013340784062165767\n",
      "Iteration is: 16396 and loss is: 0.00013337531709112227\n",
      "Iteration is: 16397 and loss is: 0.0001333438412984833\n",
      "Iteration is: 16398 and loss is: 0.00013331157970242202\n",
      "Iteration is: 16399 and loss is: 0.00013327988563105464\n",
      "Iteration is: 16400 and loss is: 0.0001332475512754172\n",
      "Iteration is: 16401 and loss is: 0.00013321664300747216\n",
      "Iteration is: 16402 and loss is: 0.0001331855310127139\n",
      "Iteration is: 16403 and loss is: 0.00013315257092472166\n",
      "Iteration is: 16404 and loss is: 0.00013312285591382533\n",
      "Iteration is: 16405 and loss is: 0.00013309142377693206\n",
      "Iteration is: 16406 and loss is: 0.00013306058826856315\n",
      "Iteration is: 16407 and loss is: 0.00013302871957421303\n",
      "Iteration is: 16408 and loss is: 0.00013300092541612685\n",
      "Iteration is: 16409 and loss is: 0.00013297420809976757\n",
      "Iteration is: 16410 and loss is: 0.00013294829113874584\n",
      "Iteration is: 16411 and loss is: 0.00013292490621097386\n",
      "Iteration is: 16412 and loss is: 0.00013290715287439525\n",
      "Iteration is: 16413 and loss is: 0.00013289741764310747\n",
      "Iteration is: 16414 and loss is: 0.0001328971702605486\n",
      "Iteration is: 16415 and loss is: 0.00013291591312736273\n",
      "Iteration is: 16416 and loss is: 0.00013296553515829146\n",
      "Iteration is: 16417 and loss is: 0.00013306166511029005\n",
      "Iteration is: 16418 and loss is: 0.0001332403626292944\n",
      "Iteration is: 16419 and loss is: 0.0001335467677563429\n",
      "Iteration is: 16420 and loss is: 0.00013406573270913213\n",
      "Iteration is: 16421 and loss is: 0.0001349494996247813\n",
      "Iteration is: 16422 and loss is: 0.00013642237172462046\n",
      "Iteration is: 16423 and loss is: 0.00013892623246647418\n",
      "Iteration is: 16424 and loss is: 0.00014308362733572721\n",
      "Iteration is: 16425 and loss is: 0.00015025134780444205\n",
      "Iteration is: 16426 and loss is: 0.00016206278814934194\n",
      "Iteration is: 16427 and loss is: 0.00018259431817568839\n",
      "Iteration is: 16428 and loss is: 0.00021545379422605038\n",
      "Iteration is: 16429 and loss is: 0.00027212026179768145\n",
      "Iteration is: 16430 and loss is: 0.00035436690086498857\n",
      "Iteration is: 16431 and loss is: 0.0004847489472012967\n",
      "Iteration is: 16432 and loss is: 0.0006172393914312124\n",
      "Iteration is: 16433 and loss is: 0.0007370476378127933\n",
      "Iteration is: 16434 and loss is: 0.0006850774516351521\n",
      "Iteration is: 16435 and loss is: 0.00045285176020115614\n",
      "Iteration is: 16436 and loss is: 0.0002177242422476411\n",
      "Iteration is: 16437 and loss is: 0.0001793623378034681\n",
      "Iteration is: 16438 and loss is: 0.0003158471954520792\n",
      "Iteration is: 16439 and loss is: 0.0004111384623683989\n",
      "Iteration is: 16440 and loss is: 0.00033807463478296995\n",
      "Iteration is: 16441 and loss is: 0.00019983600941486657\n",
      "Iteration is: 16442 and loss is: 0.00017067434964701533\n",
      "Iteration is: 16443 and loss is: 0.0002562405134085566\n",
      "Iteration is: 16444 and loss is: 0.00031166919507086277\n",
      "Iteration is: 16445 and loss is: 0.00025933299912139773\n",
      "Iteration is: 16446 and loss is: 0.00017813041631598026\n",
      "Iteration is: 16447 and loss is: 0.00016530395078007132\n",
      "Iteration is: 16448 and loss is: 0.0002138207491952926\n",
      "Iteration is: 16449 and loss is: 0.0002488761383574456\n",
      "Iteration is: 16450 and loss is: 0.00022956861357670277\n",
      "Iteration is: 16451 and loss is: 0.00018023033044300973\n",
      "Iteration is: 16452 and loss is: 0.00014666657079942524\n",
      "Iteration is: 16453 and loss is: 0.00014929819735698402\n",
      "Iteration is: 16454 and loss is: 0.0001819138997234404\n",
      "Iteration is: 16455 and loss is: 0.00023299118038266897\n",
      "Iteration is: 16456 and loss is: 0.00030983221950009465\n",
      "Iteration is: 16457 and loss is: 0.00045464449794963\n",
      "Iteration is: 16458 and loss is: 0.0006905291229486465\n",
      "Iteration is: 16459 and loss is: 0.0011024607811123133\n",
      "Iteration is: 16460 and loss is: 0.0014441804960370064\n",
      "Iteration is: 16461 and loss is: 0.001412493409588933\n",
      "Iteration is: 16462 and loss is: 0.0007746856426820159\n",
      "Iteration is: 16463 and loss is: 0.0003008999628946185\n",
      "Iteration is: 16464 and loss is: 0.0004302965826354921\n",
      "Iteration is: 16465 and loss is: 0.0005810089060105383\n",
      "Iteration is: 16466 and loss is: 0.0005127229378558695\n",
      "Iteration is: 16467 and loss is: 0.0004156524082645774\n",
      "Iteration is: 16468 and loss is: 0.00033806246938183904\n",
      "Iteration is: 16469 and loss is: 0.0004446690436452627\n",
      "Iteration is: 16470 and loss is: 0.0004351236275397241\n",
      "Iteration is: 16471 and loss is: 0.00022903569333720952\n",
      "Iteration is: 16472 and loss is: 0.00027358808438293636\n",
      "Iteration is: 16473 and loss is: 0.00039066499448381364\n",
      "Iteration is: 16474 and loss is: 0.0002513038634788245\n",
      "Iteration is: 16475 and loss is: 0.0002088281762553379\n",
      "Iteration is: 16476 and loss is: 0.0003004179452545941\n",
      "Iteration is: 16477 and loss is: 0.0002265333605464548\n",
      "Iteration is: 16478 and loss is: 0.00016113347373902798\n",
      "Iteration is: 16479 and loss is: 0.000246663810685277\n",
      "Iteration is: 16480 and loss is: 0.00030407606391236186\n",
      "Iteration is: 16481 and loss is: 0.00036805029958486557\n",
      "Iteration is: 16482 and loss is: 0.0005802364321425557\n",
      "Iteration is: 16483 and loss is: 0.0009782807901501656\n",
      "Iteration is: 16484 and loss is: 0.0013744281604886055\n",
      "Iteration is: 16485 and loss is: 0.0015731238527223468\n",
      "Iteration is: 16486 and loss is: 0.0009727475699037313\n",
      "Iteration is: 16487 and loss is: 0.00027701613726094365\n",
      "Iteration is: 16488 and loss is: 0.0004972419701516628\n",
      "Iteration is: 16489 and loss is: 0.0006450850050896406\n",
      "Iteration is: 16490 and loss is: 0.0004493207379709929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 16491 and loss is: 0.0005131990765221417\n",
      "Iteration is: 16492 and loss is: 0.000414303969591856\n",
      "Iteration is: 16493 and loss is: 0.00042756303446367383\n",
      "Iteration is: 16494 and loss is: 0.0004389630048535764\n",
      "Iteration is: 16495 and loss is: 0.00029167914181016386\n",
      "Iteration is: 16496 and loss is: 0.0002970497589558363\n",
      "Iteration is: 16497 and loss is: 0.0003690466983243823\n",
      "Iteration is: 16498 and loss is: 0.0002533776278141886\n",
      "Iteration is: 16499 and loss is: 0.00020525035506580025\n",
      "Iteration is: 16500 and loss is: 0.00033276344765909016\n",
      "Iteration is: 16501 and loss is: 0.00022828078363090754\n",
      "Iteration is: 16502 and loss is: 0.00017567072063684464\n",
      "Iteration is: 16503 and loss is: 0.00026652426458895206\n",
      "Iteration is: 16504 and loss is: 0.0002451990148983896\n",
      "Iteration is: 16505 and loss is: 0.0002476273220963776\n",
      "Iteration is: 16506 and loss is: 0.00033763929968699813\n",
      "Iteration is: 16507 and loss is: 0.000476990855531767\n",
      "Iteration is: 16508 and loss is: 0.0006354430806823075\n",
      "Iteration is: 16509 and loss is: 0.0008624563924968243\n",
      "Iteration is: 16510 and loss is: 0.0009418216068297625\n",
      "Iteration is: 16511 and loss is: 0.0006676424527540803\n",
      "Iteration is: 16512 and loss is: 0.0003143871726933867\n",
      "Iteration is: 16513 and loss is: 0.0002851426543202251\n",
      "Iteration is: 16514 and loss is: 0.0003501080791465938\n",
      "Iteration is: 16515 and loss is: 0.0003607807739172131\n",
      "Iteration is: 16516 and loss is: 0.0003408795455470681\n",
      "Iteration is: 16517 and loss is: 0.0002736796741373837\n",
      "Iteration is: 16518 and loss is: 0.00025071005802601576\n",
      "Iteration is: 16519 and loss is: 0.00032490119338035583\n",
      "Iteration is: 16520 and loss is: 0.000285603862721473\n",
      "Iteration is: 16521 and loss is: 0.00017692134133540094\n",
      "Iteration is: 16522 and loss is: 0.00022481803898699582\n",
      "Iteration is: 16523 and loss is: 0.0002751488354988396\n",
      "Iteration is: 16524 and loss is: 0.0002008661103900522\n",
      "Iteration is: 16525 and loss is: 0.00019473975407890975\n",
      "Iteration is: 16526 and loss is: 0.00021951700909994543\n",
      "Iteration is: 16527 and loss is: 0.00017242340254597366\n",
      "Iteration is: 16528 and loss is: 0.00015507321222685277\n",
      "Iteration is: 16529 and loss is: 0.00020313210552558303\n",
      "Iteration is: 16530 and loss is: 0.0002479443501215428\n",
      "Iteration is: 16531 and loss is: 0.0003121006302535534\n",
      "Iteration is: 16532 and loss is: 0.0004790799575857818\n",
      "Iteration is: 16533 and loss is: 0.0007856345619075\n",
      "Iteration is: 16534 and loss is: 0.001214920193888247\n",
      "Iteration is: 16535 and loss is: 0.0017607643967494369\n",
      "Iteration is: 16536 and loss is: 0.0015660787466913462\n",
      "Iteration is: 16537 and loss is: 0.0006494546541944146\n",
      "Iteration is: 16538 and loss is: 0.00044509011786431074\n",
      "Iteration is: 16539 and loss is: 0.0007343034958466887\n",
      "Iteration is: 16540 and loss is: 0.0006447091582231224\n",
      "Iteration is: 16541 and loss is: 0.0007052516448311508\n",
      "Iteration is: 16542 and loss is: 0.0005433844635263085\n",
      "Iteration is: 16543 and loss is: 0.0005144049064256251\n",
      "Iteration is: 16544 and loss is: 0.0006085497443564236\n",
      "Iteration is: 16545 and loss is: 0.0004480613861232996\n",
      "Iteration is: 16546 and loss is: 0.00032255641417577863\n",
      "Iteration is: 16547 and loss is: 0.00044661981519311666\n",
      "Iteration is: 16548 and loss is: 0.00042479863623157144\n",
      "Iteration is: 16549 and loss is: 0.00020350552222225815\n",
      "Iteration is: 16550 and loss is: 0.00036829087184742093\n",
      "Iteration is: 16551 and loss is: 0.0003383041184861213\n",
      "Iteration is: 16552 and loss is: 0.0001807771041058004\n",
      "Iteration is: 16553 and loss is: 0.0003104252100456506\n",
      "Iteration is: 16554 and loss is: 0.00036574507248587906\n",
      "Iteration is: 16555 and loss is: 0.0004466187674552202\n",
      "Iteration is: 16556 and loss is: 0.0006453010719269514\n",
      "Iteration is: 16557 and loss is: 0.0010123065439984202\n",
      "Iteration is: 16558 and loss is: 0.0012792712077498436\n",
      "Iteration is: 16559 and loss is: 0.0012398684630170465\n",
      "Iteration is: 16560 and loss is: 0.0006809516926296055\n",
      "Iteration is: 16561 and loss is: 0.00020436811610125005\n",
      "Iteration is: 16562 and loss is: 0.00042935297824442387\n",
      "Iteration is: 16563 and loss is: 0.0006141158519312739\n",
      "Iteration is: 16564 and loss is: 0.00036622921470552683\n",
      "Iteration is: 16565 and loss is: 0.0003292174369562417\n",
      "Iteration is: 16566 and loss is: 0.00038174691144376993\n",
      "Iteration is: 16567 and loss is: 0.0004025895905215293\n",
      "Iteration is: 16568 and loss is: 0.0003661802620626986\n",
      "Iteration is: 16569 and loss is: 0.0002727651735767722\n",
      "Iteration is: 16570 and loss is: 0.00030882697319611907\n",
      "Iteration is: 16571 and loss is: 0.0003573205030988902\n",
      "Iteration is: 16572 and loss is: 0.00025778228882700205\n",
      "Iteration is: 16573 and loss is: 0.00019016582518815994\n",
      "Iteration is: 16574 and loss is: 0.00029504825943149626\n",
      "Iteration is: 16575 and loss is: 0.00029120975523255765\n",
      "Iteration is: 16576 and loss is: 0.000185970522579737\n",
      "Iteration is: 16577 and loss is: 0.0002295786835020408\n",
      "Iteration is: 16578 and loss is: 0.00022624776465818286\n",
      "Iteration is: 16579 and loss is: 0.0001600087562110275\n",
      "Iteration is: 16580 and loss is: 0.00018359796376898885\n",
      "Iteration is: 16581 and loss is: 0.00022919510956853628\n",
      "Iteration is: 16582 and loss is: 0.00025826579076237977\n",
      "Iteration is: 16583 and loss is: 0.0003234603791497648\n",
      "Iteration is: 16584 and loss is: 0.00048573504318483174\n",
      "Iteration is: 16585 and loss is: 0.0006966558285057545\n",
      "Iteration is: 16586 and loss is: 0.0009286176646128297\n",
      "Iteration is: 16587 and loss is: 0.0009445618488825858\n",
      "Iteration is: 16588 and loss is: 0.0005932941567152739\n",
      "Iteration is: 16589 and loss is: 0.00022888679814059287\n",
      "Iteration is: 16590 and loss is: 0.0002812159655150026\n",
      "Iteration is: 16591 and loss is: 0.00042204168858006597\n",
      "Iteration is: 16592 and loss is: 0.0003715561470016837\n",
      "Iteration is: 16593 and loss is: 0.00028753391234204173\n",
      "Iteration is: 16594 and loss is: 0.00026951893232762814\n",
      "Iteration is: 16595 and loss is: 0.0002928504254668951\n",
      "Iteration is: 16596 and loss is: 0.00031421976746059954\n",
      "Iteration is: 16597 and loss is: 0.0002429857267998159\n",
      "Iteration is: 16598 and loss is: 0.00018009608902502805\n",
      "Iteration is: 16599 and loss is: 0.00025350830401293933\n",
      "Iteration is: 16600 and loss is: 0.00027485552709549665\n",
      "Iteration is: 16601 and loss is: 0.00018254510359838605\n",
      "Iteration is: 16602 and loss is: 0.0001856862654676661\n",
      "Iteration is: 16603 and loss is: 0.00021845819719601423\n",
      "Iteration is: 16604 and loss is: 0.00017929915338754654\n",
      "Iteration is: 16605 and loss is: 0.00017914461204782128\n",
      "Iteration is: 16606 and loss is: 0.00023972932831384242\n",
      "Iteration is: 16607 and loss is: 0.0002815606421791017\n",
      "Iteration is: 16608 and loss is: 0.0003528447705321014\n",
      "Iteration is: 16609 and loss is: 0.0005270960973575711\n",
      "Iteration is: 16610 and loss is: 0.0008333242149092257\n",
      "Iteration is: 16611 and loss is: 0.0011999023845419288\n",
      "Iteration is: 16612 and loss is: 0.001544684055261314\n",
      "Iteration is: 16613 and loss is: 0.0011837880592793226\n",
      "Iteration is: 16614 and loss is: 0.0004349718801677227\n",
      "Iteration is: 16615 and loss is: 0.0003979442408308387\n",
      "Iteration is: 16616 and loss is: 0.0006365865701809525\n",
      "Iteration is: 16617 and loss is: 0.0005382270319387317\n",
      "Iteration is: 16618 and loss is: 0.0005491322372108698\n",
      "Iteration is: 16619 and loss is: 0.00043899254524149\n",
      "Iteration is: 16620 and loss is: 0.0004306024929974228\n",
      "Iteration is: 16621 and loss is: 0.0005080692935734987\n",
      "Iteration is: 16622 and loss is: 0.0003472416428849101\n",
      "Iteration is: 16623 and loss is: 0.0002737881732173264\n",
      "Iteration is: 16624 and loss is: 0.0003944327763747424\n",
      "Iteration is: 16625 and loss is: 0.0003543097118381411\n",
      "Iteration is: 16626 and loss is: 0.00018848937179427594\n",
      "Iteration is: 16627 and loss is: 0.00030778662767261267\n",
      "Iteration is: 16628 and loss is: 0.0003074979467783123\n",
      "Iteration is: 16629 and loss is: 0.00016747706104069948\n",
      "Iteration is: 16630 and loss is: 0.000249685428570956\n",
      "Iteration is: 16631 and loss is: 0.0003071283863391727\n",
      "Iteration is: 16632 and loss is: 0.00032905221451073885\n",
      "Iteration is: 16633 and loss is: 0.00045420811511576176\n",
      "Iteration is: 16634 and loss is: 0.0007320553995668888\n",
      "Iteration is: 16635 and loss is: 0.0010316313710063696\n",
      "Iteration is: 16636 and loss is: 0.0012859087437391281\n",
      "Iteration is: 16637 and loss is: 0.0010781544260680676\n",
      "Iteration is: 16638 and loss is: 0.0004432018904481083\n",
      "Iteration is: 16639 and loss is: 0.00022765263565815985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 16640 and loss is: 0.0005416812491603196\n",
      "Iteration is: 16641 and loss is: 0.0005157320993021131\n",
      "Iteration is: 16642 and loss is: 0.0003618622140493244\n",
      "Iteration is: 16643 and loss is: 0.0003745199355762452\n",
      "Iteration is: 16644 and loss is: 0.0003869769861921668\n",
      "Iteration is: 16645 and loss is: 0.00040559234912507236\n",
      "Iteration is: 16646 and loss is: 0.0003400240675546229\n",
      "Iteration is: 16647 and loss is: 0.00026751053519546986\n",
      "Iteration is: 16648 and loss is: 0.00031484710052609444\n",
      "Iteration is: 16649 and loss is: 0.0003325632424093783\n",
      "Iteration is: 16650 and loss is: 0.00020394910825416446\n",
      "Iteration is: 16651 and loss is: 0.00020936100918333977\n",
      "Iteration is: 16652 and loss is: 0.0003109288227278739\n",
      "Iteration is: 16653 and loss is: 0.00022090398124419153\n",
      "Iteration is: 16654 and loss is: 0.00018451575306244195\n",
      "Iteration is: 16655 and loss is: 0.0002462148549966514\n",
      "Iteration is: 16656 and loss is: 0.00021209688566159457\n",
      "Iteration is: 16657 and loss is: 0.00016842741752043366\n",
      "Iteration is: 16658 and loss is: 0.00019395293202251196\n",
      "Iteration is: 16659 and loss is: 0.00020701001631096005\n",
      "Iteration is: 16660 and loss is: 0.0001673697988735512\n",
      "Iteration is: 16661 and loss is: 0.0001628781610634178\n",
      "Iteration is: 16662 and loss is: 0.00018302591342944652\n",
      "Iteration is: 16663 and loss is: 0.0001689869095571339\n",
      "Iteration is: 16664 and loss is: 0.00015454032109118998\n",
      "Iteration is: 16665 and loss is: 0.00018438571714796126\n",
      "Iteration is: 16666 and loss is: 0.00018275281763635576\n",
      "Iteration is: 16667 and loss is: 0.0001660287962295115\n",
      "Iteration is: 16668 and loss is: 0.00017521483823657036\n",
      "Iteration is: 16669 and loss is: 0.00017464981647208333\n",
      "Iteration is: 16670 and loss is: 0.0001627112360438332\n",
      "Iteration is: 16671 and loss is: 0.00016337729175575078\n",
      "Iteration is: 16672 and loss is: 0.00017421069787815213\n",
      "Iteration is: 16673 and loss is: 0.00018293678294867277\n",
      "Iteration is: 16674 and loss is: 0.00020665465854108334\n",
      "Iteration is: 16675 and loss is: 0.0002643383922986686\n",
      "Iteration is: 16676 and loss is: 0.0003763917775359005\n",
      "Iteration is: 16677 and loss is: 0.0005355441244319081\n",
      "Iteration is: 16678 and loss is: 0.0007936602924019098\n",
      "Iteration is: 16679 and loss is: 0.0009695863700471818\n",
      "Iteration is: 16680 and loss is: 0.0008668930968269706\n",
      "Iteration is: 16681 and loss is: 0.0004923156229779124\n",
      "Iteration is: 16682 and loss is: 0.00019702364807017148\n",
      "Iteration is: 16683 and loss is: 0.00024933862732723355\n",
      "Iteration is: 16684 and loss is: 0.00045808529830537736\n",
      "Iteration is: 16685 and loss is: 0.000455968314781785\n",
      "Iteration is: 16686 and loss is: 0.0002641606261022389\n",
      "Iteration is: 16687 and loss is: 0.0001980709785129875\n",
      "Iteration is: 16688 and loss is: 0.0003075970453210175\n",
      "Iteration is: 16689 and loss is: 0.0003753210185095668\n",
      "Iteration is: 16690 and loss is: 0.0002816917549353093\n",
      "Iteration is: 16691 and loss is: 0.00016666491865180433\n",
      "Iteration is: 16692 and loss is: 0.00018780308892019093\n",
      "Iteration is: 16693 and loss is: 0.0002772877342067659\n",
      "Iteration is: 16694 and loss is: 0.00029809772968292236\n",
      "Iteration is: 16695 and loss is: 0.0002511526399757713\n",
      "Iteration is: 16696 and loss is: 0.0002039753890130669\n",
      "Iteration is: 16697 and loss is: 0.0001652410428505391\n",
      "Iteration is: 16698 and loss is: 0.00014443221152760088\n",
      "Iteration is: 16699 and loss is: 0.000171619511093013\n",
      "Iteration is: 16700 and loss is: 0.0002469919854775071\n",
      "Iteration is: 16701 and loss is: 0.00037736352533102036\n",
      "Iteration is: 16702 and loss is: 0.000669141358230263\n",
      "Iteration is: 16703 and loss is: 0.0011937854578718543\n",
      "Iteration is: 16704 and loss is: 0.00205429270863533\n",
      "Iteration is: 16705 and loss is: 0.0023203259333968163\n",
      "Iteration is: 16706 and loss is: 0.0013416243018582463\n",
      "Iteration is: 16707 and loss is: 0.0004844306968152523\n",
      "Iteration is: 16708 and loss is: 0.000922050909139216\n",
      "Iteration is: 16709 and loss is: 0.0009588495595380664\n",
      "Iteration is: 16710 and loss is: 0.0008970900671556592\n",
      "Iteration is: 16711 and loss is: 0.000708963256329298\n",
      "Iteration is: 16712 and loss is: 0.0008085843874141574\n",
      "Iteration is: 16713 and loss is: 0.0007483471999876201\n",
      "Iteration is: 16714 and loss is: 0.0005577825359068811\n",
      "Iteration is: 16715 and loss is: 0.0005487676826305687\n",
      "Iteration is: 16716 and loss is: 0.00046479294542223215\n",
      "Iteration is: 16717 and loss is: 0.0005977377877570689\n",
      "Iteration is: 16718 and loss is: 0.0003112850245088339\n",
      "Iteration is: 16719 and loss is: 0.0002967248437926173\n",
      "Iteration is: 16720 and loss is: 0.0005291078705340624\n",
      "Iteration is: 16721 and loss is: 0.0002628082875162363\n",
      "Iteration is: 16722 and loss is: 0.000419695395976305\n",
      "Iteration is: 16723 and loss is: 0.0006491157691925764\n",
      "Iteration is: 16724 and loss is: 0.0010018860921263695\n",
      "Iteration is: 16725 and loss is: 0.0016497084870934486\n",
      "Iteration is: 16726 and loss is: 0.002417039591819048\n",
      "Iteration is: 16727 and loss is: 0.0018962612375617027\n",
      "Iteration is: 16728 and loss is: 0.0007132291793823242\n",
      "Iteration is: 16729 and loss is: 0.0007500444771721959\n",
      "Iteration is: 16730 and loss is: 0.0010336545528843999\n",
      "Iteration is: 16731 and loss is: 0.0009250253206118941\n",
      "Iteration is: 16732 and loss is: 0.0006506687495857477\n",
      "Iteration is: 16733 and loss is: 0.001016533002257347\n",
      "Iteration is: 16734 and loss is: 0.0007470959099009633\n",
      "Iteration is: 16735 and loss is: 0.00046634033787995577\n",
      "Iteration is: 16736 and loss is: 0.0006488157669082284\n",
      "Iteration is: 16737 and loss is: 0.0005920259864069521\n",
      "Iteration is: 16738 and loss is: 0.00029073399491608143\n",
      "Iteration is: 16739 and loss is: 0.000484683841932565\n",
      "Iteration is: 16740 and loss is: 0.0004435404553078115\n",
      "Iteration is: 16741 and loss is: 0.0003280967939645052\n",
      "Iteration is: 16742 and loss is: 0.00029745823121629655\n",
      "Iteration is: 16743 and loss is: 0.0004035498423036188\n",
      "Iteration is: 16744 and loss is: 0.0005284179351292551\n",
      "Iteration is: 16745 and loss is: 0.000541471061296761\n",
      "Iteration is: 16746 and loss is: 0.00047827005619183183\n",
      "Iteration is: 16747 and loss is: 0.0006005726754665375\n",
      "Iteration is: 16748 and loss is: 0.0006830450729466975\n",
      "Iteration is: 16749 and loss is: 0.0006881061126478016\n",
      "Iteration is: 16750 and loss is: 0.0006836818065494299\n",
      "Iteration is: 16751 and loss is: 0.0005420212401077151\n",
      "Iteration is: 16752 and loss is: 0.00030679270275868475\n",
      "Iteration is: 16753 and loss is: 0.00022921038907952607\n",
      "Iteration is: 16754 and loss is: 0.0004198004608042538\n",
      "Iteration is: 16755 and loss is: 0.0004274649836588651\n",
      "Iteration is: 16756 and loss is: 0.00023329885152634233\n",
      "Iteration is: 16757 and loss is: 0.0002370275615248829\n",
      "Iteration is: 16758 and loss is: 0.00039046176243573427\n",
      "Iteration is: 16759 and loss is: 0.0002893298806156963\n",
      "Iteration is: 16760 and loss is: 0.0001936834742082283\n",
      "Iteration is: 16761 and loss is: 0.0002478433307260275\n",
      "Iteration is: 16762 and loss is: 0.0002945350715890527\n",
      "Iteration is: 16763 and loss is: 0.00024534546537324786\n",
      "Iteration is: 16764 and loss is: 0.0001849540276452899\n",
      "Iteration is: 16765 and loss is: 0.00019487990357447416\n",
      "Iteration is: 16766 and loss is: 0.00023344490909948945\n",
      "Iteration is: 16767 and loss is: 0.0002595177502371371\n",
      "Iteration is: 16768 and loss is: 0.00022596739290747792\n",
      "Iteration is: 16769 and loss is: 0.00017484229465480894\n",
      "Iteration is: 16770 and loss is: 0.00014704449858982116\n",
      "Iteration is: 16771 and loss is: 0.0001673834049142897\n",
      "Iteration is: 16772 and loss is: 0.0002038154489127919\n",
      "Iteration is: 16773 and loss is: 0.0002584231551736593\n",
      "Iteration is: 16774 and loss is: 0.00034929285175167024\n",
      "Iteration is: 16775 and loss is: 0.0004695754614658654\n",
      "Iteration is: 16776 and loss is: 0.0006678852951154113\n",
      "Iteration is: 16777 and loss is: 0.0008550975471735001\n",
      "Iteration is: 16778 and loss is: 0.0009751467150636017\n",
      "Iteration is: 16779 and loss is: 0.0007205003639683127\n",
      "Iteration is: 16780 and loss is: 0.00031362721347250044\n",
      "Iteration is: 16781 and loss is: 0.00024567393120378256\n",
      "Iteration is: 16782 and loss is: 0.00041935170884244144\n",
      "Iteration is: 16783 and loss is: 0.00045037895324639976\n",
      "Iteration is: 16784 and loss is: 0.0003301764081697911\n",
      "Iteration is: 16785 and loss is: 0.00025928299874067307\n",
      "Iteration is: 16786 and loss is: 0.0003460777807049453\n",
      "Iteration is: 16787 and loss is: 0.0003861297154799104\n",
      "Iteration is: 16788 and loss is: 0.00023866807168815285\n",
      "Iteration is: 16789 and loss is: 0.00020435964688658714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 16790 and loss is: 0.0003038758004549891\n",
      "Iteration is: 16791 and loss is: 0.00029937364161014557\n",
      "Iteration is: 16792 and loss is: 0.0001915003958856687\n",
      "Iteration is: 16793 and loss is: 0.0001902304938994348\n",
      "Iteration is: 16794 and loss is: 0.00023062783293426037\n",
      "Iteration is: 16795 and loss is: 0.00020947237499058247\n",
      "Iteration is: 16796 and loss is: 0.00022820806771051139\n",
      "Iteration is: 16797 and loss is: 0.0002942332939710468\n",
      "Iteration is: 16798 and loss is: 0.00033135322155430913\n",
      "Iteration is: 16799 and loss is: 0.00038631746429018676\n",
      "Iteration is: 16800 and loss is: 0.0005475169746205211\n",
      "Iteration is: 16801 and loss is: 0.0007294358219951391\n",
      "Iteration is: 16802 and loss is: 0.0009334783535450697\n",
      "Iteration is: 16803 and loss is: 0.0009206925169564784\n",
      "Iteration is: 16804 and loss is: 0.00059436698211357\n",
      "Iteration is: 16805 and loss is: 0.0002223076589871198\n",
      "Iteration is: 16806 and loss is: 0.00024812703486531973\n",
      "Iteration is: 16807 and loss is: 0.00043610387365333736\n",
      "Iteration is: 16808 and loss is: 0.0003797571989707649\n",
      "Iteration is: 16809 and loss is: 0.00025549798738211393\n",
      "Iteration is: 16810 and loss is: 0.0002534481172915548\n",
      "Iteration is: 16811 and loss is: 0.0002980058779940009\n",
      "Iteration is: 16812 and loss is: 0.00030550180235877633\n",
      "Iteration is: 16813 and loss is: 0.0002394338953308761\n",
      "Iteration is: 16814 and loss is: 0.00017921440303325653\n",
      "Iteration is: 16815 and loss is: 0.00024124060291796923\n",
      "Iteration is: 16816 and loss is: 0.000288718641968444\n",
      "Iteration is: 16817 and loss is: 0.0002079323894577101\n",
      "Iteration is: 16818 and loss is: 0.0001640438276808709\n",
      "Iteration is: 16819 and loss is: 0.0001988483127206564\n",
      "Iteration is: 16820 and loss is: 0.00019404894555918872\n",
      "Iteration is: 16821 and loss is: 0.0001889928535092622\n",
      "Iteration is: 16822 and loss is: 0.0002514023217372596\n",
      "Iteration is: 16823 and loss is: 0.00032920800731517375\n",
      "Iteration is: 16824 and loss is: 0.00044372427510097623\n",
      "Iteration is: 16825 and loss is: 0.0006577895837835968\n",
      "Iteration is: 16826 and loss is: 0.0010258605470880866\n",
      "Iteration is: 16827 and loss is: 0.0012657553888857365\n",
      "Iteration is: 16828 and loss is: 0.0011250922689214349\n",
      "Iteration is: 16829 and loss is: 0.0005201987223699689\n",
      "Iteration is: 16830 and loss is: 0.00019606912974268198\n",
      "Iteration is: 16831 and loss is: 0.00044749066000804305\n",
      "Iteration is: 16832 and loss is: 0.0005533109651878476\n",
      "Iteration is: 16833 and loss is: 0.00034296902595087886\n",
      "Iteration is: 16834 and loss is: 0.0002931766794063151\n",
      "Iteration is: 16835 and loss is: 0.0003755885409191251\n",
      "Iteration is: 16836 and loss is: 0.000372675945982337\n",
      "Iteration is: 16837 and loss is: 0.00030352792236953974\n",
      "Iteration is: 16838 and loss is: 0.00024018573458306491\n",
      "Iteration is: 16839 and loss is: 0.00028130257851444185\n",
      "Iteration is: 16840 and loss is: 0.0003246171399950981\n",
      "Iteration is: 16841 and loss is: 0.00022275610535871238\n",
      "Iteration is: 16842 and loss is: 0.00017341630882583559\n",
      "Iteration is: 16843 and loss is: 0.000262951769400388\n",
      "Iteration is: 16844 and loss is: 0.00026383891236037016\n",
      "Iteration is: 16845 and loss is: 0.00019867038645315915\n",
      "Iteration is: 16846 and loss is: 0.0002296456805197522\n",
      "Iteration is: 16847 and loss is: 0.0002720185730140656\n",
      "Iteration is: 16848 and loss is: 0.0002590807853266597\n",
      "Iteration is: 16849 and loss is: 0.0003130424302071333\n",
      "Iteration is: 16850 and loss is: 0.0004630276816897094\n",
      "Iteration is: 16851 and loss is: 0.0006597437895834446\n",
      "Iteration is: 16852 and loss is: 0.000989614985883236\n",
      "Iteration is: 16853 and loss is: 0.0012188517721369863\n",
      "Iteration is: 16854 and loss is: 0.0010416797595098615\n",
      "Iteration is: 16855 and loss is: 0.00045133160892874\n",
      "Iteration is: 16856 and loss is: 0.0002692274865694344\n",
      "Iteration is: 16857 and loss is: 0.00047744385665282607\n",
      "Iteration is: 16858 and loss is: 0.0004367480578366667\n",
      "Iteration is: 16859 and loss is: 0.00037890792009420693\n",
      "Iteration is: 16860 and loss is: 0.0003740799438674003\n",
      "Iteration is: 16861 and loss is: 0.0003033180255442858\n",
      "Iteration is: 16862 and loss is: 0.0003624698147177696\n",
      "Iteration is: 16863 and loss is: 0.00033938465639948845\n",
      "Iteration is: 16864 and loss is: 0.00021442814613692462\n",
      "Iteration is: 16865 and loss is: 0.00025866247597150505\n",
      "Iteration is: 16866 and loss is: 0.00031337072141468525\n",
      "Iteration is: 16867 and loss is: 0.00020559327094815671\n",
      "Iteration is: 16868 and loss is: 0.00018851010827347636\n",
      "Iteration is: 16869 and loss is: 0.0002678188029676676\n",
      "Iteration is: 16870 and loss is: 0.00020675192354246974\n",
      "Iteration is: 16871 and loss is: 0.00015485503536183387\n",
      "Iteration is: 16872 and loss is: 0.00022644601995125413\n",
      "Iteration is: 16873 and loss is: 0.00025744049344211817\n",
      "Iteration is: 16874 and loss is: 0.0002762351941782981\n",
      "Iteration is: 16875 and loss is: 0.0004327082133386284\n",
      "Iteration is: 16876 and loss is: 0.0007371190586127341\n",
      "Iteration is: 16877 and loss is: 0.0011792548466473818\n",
      "Iteration is: 16878 and loss is: 0.001806588494218886\n",
      "Iteration is: 16879 and loss is: 0.001664359006099403\n",
      "Iteration is: 16880 and loss is: 0.0006612063152715564\n",
      "Iteration is: 16881 and loss is: 0.00043281575199216604\n",
      "Iteration is: 16882 and loss is: 0.0008577527478337288\n",
      "Iteration is: 16883 and loss is: 0.0006098338635638356\n",
      "Iteration is: 16884 and loss is: 0.0006600210908800364\n",
      "Iteration is: 16885 and loss is: 0.0006123529747128487\n",
      "Iteration is: 16886 and loss is: 0.0005694779101759195\n",
      "Iteration is: 16887 and loss is: 0.0005189265939407051\n",
      "Iteration is: 16888 and loss is: 0.000444273988250643\n",
      "Iteration is: 16889 and loss is: 0.000401934637920931\n",
      "Iteration is: 16890 and loss is: 0.0003647069097496569\n",
      "Iteration is: 16891 and loss is: 0.00042011842015199363\n",
      "Iteration is: 16892 and loss is: 0.00024157292500603944\n",
      "Iteration is: 16893 and loss is: 0.000292252836516127\n",
      "Iteration is: 16894 and loss is: 0.0003907427308149636\n",
      "Iteration is: 16895 and loss is: 0.00018056141561828554\n",
      "Iteration is: 16896 and loss is: 0.0002869565214496106\n",
      "Iteration is: 16897 and loss is: 0.000328822061419487\n",
      "Iteration is: 16898 and loss is: 0.00031863804906606674\n",
      "Iteration is: 16899 and loss is: 0.0004265795578248799\n",
      "Iteration is: 16900 and loss is: 0.0006640177452936769\n",
      "Iteration is: 16901 and loss is: 0.0009143341449089348\n",
      "Iteration is: 16902 and loss is: 0.0011318176984786987\n",
      "Iteration is: 16903 and loss is: 0.0009489287622272968\n",
      "Iteration is: 16904 and loss is: 0.00041771435644477606\n",
      "Iteration is: 16905 and loss is: 0.0002023620472755283\n",
      "Iteration is: 16906 and loss is: 0.0004771383246406913\n",
      "Iteration is: 16907 and loss is: 0.0005006123101338744\n",
      "Iteration is: 16908 and loss is: 0.00028444244526326656\n",
      "Iteration is: 16909 and loss is: 0.0003369693586137146\n",
      "Iteration is: 16910 and loss is: 0.00037211834569461644\n",
      "Iteration is: 16911 and loss is: 0.00034324059379287064\n",
      "Iteration is: 16912 and loss is: 0.0002946014574263245\n",
      "Iteration is: 16913 and loss is: 0.00026785704540088773\n",
      "Iteration is: 16914 and loss is: 0.0002760227653197944\n",
      "Iteration is: 16915 and loss is: 0.00028488622047007084\n",
      "Iteration is: 16916 and loss is: 0.00021271241712383926\n",
      "Iteration is: 16917 and loss is: 0.00019161681120749563\n",
      "Iteration is: 16918 and loss is: 0.0002686759107746184\n",
      "Iteration is: 16919 and loss is: 0.0002409971202723682\n",
      "Iteration is: 16920 and loss is: 0.00015889847418293357\n",
      "Iteration is: 16921 and loss is: 0.00019988146959803998\n",
      "Iteration is: 16922 and loss is: 0.0002050400071311742\n",
      "Iteration is: 16923 and loss is: 0.00015371224435511976\n",
      "Iteration is: 16924 and loss is: 0.00016742231673561037\n",
      "Iteration is: 16925 and loss is: 0.00020469989976845682\n",
      "Iteration is: 16926 and loss is: 0.00020121378474868834\n",
      "Iteration is: 16927 and loss is: 0.00022357104171533138\n",
      "Iteration is: 16928 and loss is: 0.00033219868782907724\n",
      "Iteration is: 16929 and loss is: 0.000503836665302515\n",
      "Iteration is: 16930 and loss is: 0.0006941934116184711\n",
      "Iteration is: 16931 and loss is: 0.0008443465922027826\n",
      "Iteration is: 16932 and loss is: 0.000704041332937777\n",
      "Iteration is: 16933 and loss is: 0.00034223010879941285\n",
      "Iteration is: 16934 and loss is: 0.00019997387425974011\n",
      "Iteration is: 16935 and loss is: 0.0003238944336771965\n",
      "Iteration is: 16936 and loss is: 0.00040028878720477223\n",
      "Iteration is: 16937 and loss is: 0.00033052079379558563\n",
      "Iteration is: 16938 and loss is: 0.00024732513702474535\n",
      "Iteration is: 16939 and loss is: 0.0002500559785403311\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 16940 and loss is: 0.00030256551690399647\n",
      "Iteration is: 16941 and loss is: 0.0002873586490750313\n",
      "Iteration is: 16942 and loss is: 0.00020114355720579624\n",
      "Iteration is: 16943 and loss is: 0.00017108733300119638\n",
      "Iteration is: 16944 and loss is: 0.00024933204986155033\n",
      "Iteration is: 16945 and loss is: 0.000269063632003963\n",
      "Iteration is: 16946 and loss is: 0.00019571298616938293\n",
      "Iteration is: 16947 and loss is: 0.00018311079475097358\n",
      "Iteration is: 16948 and loss is: 0.0001891670690383762\n",
      "Iteration is: 16949 and loss is: 0.00015597455785609782\n",
      "Iteration is: 16950 and loss is: 0.00014716423174832016\n",
      "Iteration is: 16951 and loss is: 0.0002020488027483225\n",
      "Iteration is: 16952 and loss is: 0.0002744730736594647\n",
      "Iteration is: 16953 and loss is: 0.0004302759189158678\n",
      "Iteration is: 16954 and loss is: 0.0008057305822148919\n",
      "Iteration is: 16955 and loss is: 0.0015910824295133352\n",
      "Iteration is: 16956 and loss is: 0.002341778948903084\n",
      "Iteration is: 16957 and loss is: 0.002117778407409787\n",
      "Iteration is: 16958 and loss is: 0.0007297159754671156\n",
      "Iteration is: 16959 and loss is: 0.00045534269884228706\n",
      "Iteration is: 16960 and loss is: 0.000918446690775454\n",
      "Iteration is: 16961 and loss is: 0.0007623433484695852\n",
      "Iteration is: 16962 and loss is: 0.0005996581166982651\n",
      "Iteration is: 16963 and loss is: 0.0007393842097371817\n",
      "Iteration is: 16964 and loss is: 0.0007664999575354159\n",
      "Iteration is: 16965 and loss is: 0.0003913575201295316\n",
      "Iteration is: 16966 and loss is: 0.0005226130597293377\n",
      "Iteration is: 16967 and loss is: 0.0005105516174808145\n",
      "Iteration is: 16968 and loss is: 0.0003191078139934689\n",
      "Iteration is: 16969 and loss is: 0.0003895247937180102\n",
      "Iteration is: 16970 and loss is: 0.0002841167151927948\n",
      "Iteration is: 16971 and loss is: 0.00035573882632888854\n",
      "Iteration is: 16972 and loss is: 0.00044199448893778026\n",
      "Iteration is: 16973 and loss is: 0.00023269563098438084\n",
      "Iteration is: 16974 and loss is: 0.0003125466173514724\n",
      "Iteration is: 16975 and loss is: 0.0003909515799023211\n",
      "Iteration is: 16976 and loss is: 0.00040421081939712167\n",
      "Iteration is: 16977 and loss is: 0.0006036348058842123\n",
      "Iteration is: 16978 and loss is: 0.000982882222160697\n",
      "Iteration is: 16979 and loss is: 0.0015459676505997777\n",
      "Iteration is: 16980 and loss is: 0.0016193089541047812\n",
      "Iteration is: 16981 and loss is: 0.000875031400937587\n",
      "Iteration is: 16982 and loss is: 0.00022346468176692724\n",
      "Iteration is: 16983 and loss is: 0.0006655596662312746\n",
      "Iteration is: 16984 and loss is: 0.0007912650471553206\n",
      "Iteration is: 16985 and loss is: 0.0004237624816596508\n",
      "Iteration is: 16986 and loss is: 0.0005256441654637456\n",
      "Iteration is: 16987 and loss is: 0.000545620743650943\n",
      "Iteration is: 16988 and loss is: 0.0004900599014945328\n",
      "Iteration is: 16989 and loss is: 0.00041350594256073236\n",
      "Iteration is: 16990 and loss is: 0.0003793348732870072\n",
      "Iteration is: 16991 and loss is: 0.0004361980827525258\n",
      "Iteration is: 16992 and loss is: 0.0003546312218531966\n",
      "Iteration is: 16993 and loss is: 0.00025399605510756373\n",
      "Iteration is: 16994 and loss is: 0.00033086404437199235\n",
      "Iteration is: 16995 and loss is: 0.00038863191730342805\n",
      "Iteration is: 16996 and loss is: 0.00021180155454203486\n",
      "Iteration is: 16997 and loss is: 0.0002024871646426618\n",
      "Iteration is: 16998 and loss is: 0.0003307870647404343\n",
      "Iteration is: 16999 and loss is: 0.00021668040426447988\n",
      "Iteration is: 17000 and loss is: 0.0001762808533385396\n",
      "Iteration is: 17001 and loss is: 0.00026871010777540505\n",
      "Iteration is: 17002 and loss is: 0.0002242514310637489\n",
      "Iteration is: 17003 and loss is: 0.00016304566815961152\n",
      "Iteration is: 17004 and loss is: 0.0001998050865950063\n",
      "Iteration is: 17005 and loss is: 0.0002107897016685456\n",
      "Iteration is: 17006 and loss is: 0.00017459197260905057\n",
      "Iteration is: 17007 and loss is: 0.00017674984701443464\n",
      "Iteration is: 17008 and loss is: 0.00020226459309924394\n",
      "Iteration is: 17009 and loss is: 0.0001771473907865584\n",
      "Iteration is: 17010 and loss is: 0.0001497940393164754\n",
      "Iteration is: 17011 and loss is: 0.0001742760941851884\n",
      "Iteration is: 17012 and loss is: 0.00016189985035452992\n",
      "Iteration is: 17013 and loss is: 0.00013464989024214447\n",
      "Iteration is: 17014 and loss is: 0.0001551507448311895\n",
      "Iteration is: 17015 and loss is: 0.00015832064673304558\n",
      "Iteration is: 17016 and loss is: 0.00013802730245515704\n",
      "Iteration is: 17017 and loss is: 0.00014232288231141865\n",
      "Iteration is: 17018 and loss is: 0.00015025874017737806\n",
      "Iteration is: 17019 and loss is: 0.00014243085752241313\n",
      "Iteration is: 17020 and loss is: 0.00014007808931637555\n",
      "Iteration is: 17021 and loss is: 0.00015274049655999988\n",
      "Iteration is: 17022 and loss is: 0.00016650749603286386\n",
      "Iteration is: 17023 and loss is: 0.00017908508016262203\n",
      "Iteration is: 17024 and loss is: 0.000211990307434462\n",
      "Iteration is: 17025 and loss is: 0.00028126113465987146\n",
      "Iteration is: 17026 and loss is: 0.0003775082004722208\n",
      "Iteration is: 17027 and loss is: 0.0005590702057816088\n",
      "Iteration is: 17028 and loss is: 0.000799217086751014\n",
      "Iteration is: 17029 and loss is: 0.0010737617267295718\n",
      "Iteration is: 17030 and loss is: 0.0010922017972916365\n",
      "Iteration is: 17031 and loss is: 0.0007253742660395801\n",
      "Iteration is: 17032 and loss is: 0.0002637646975927055\n",
      "Iteration is: 17033 and loss is: 0.00023346715897787362\n",
      "Iteration is: 17034 and loss is: 0.0004943897947669029\n",
      "Iteration is: 17035 and loss is: 0.0005029022577218711\n",
      "Iteration is: 17036 and loss is: 0.0002967979235108942\n",
      "Iteration is: 17037 and loss is: 0.00023155275266617537\n",
      "Iteration is: 17038 and loss is: 0.00033260328928008676\n",
      "Iteration is: 17039 and loss is: 0.0003776973462663591\n",
      "Iteration is: 17040 and loss is: 0.0002918285026680678\n",
      "Iteration is: 17041 and loss is: 0.00017202171147800982\n",
      "Iteration is: 17042 and loss is: 0.00021566590294241905\n",
      "Iteration is: 17043 and loss is: 0.0003284722042735666\n",
      "Iteration is: 17044 and loss is: 0.00031004787888377905\n",
      "Iteration is: 17045 and loss is: 0.00022745261958334595\n",
      "Iteration is: 17046 and loss is: 0.00019537303887773305\n",
      "Iteration is: 17047 and loss is: 0.00018279641517437994\n",
      "Iteration is: 17048 and loss is: 0.00014640914741903543\n",
      "Iteration is: 17049 and loss is: 0.0001626137236598879\n",
      "Iteration is: 17050 and loss is: 0.0002462332195136696\n",
      "Iteration is: 17051 and loss is: 0.0004010643460787833\n",
      "Iteration is: 17052 and loss is: 0.0007310366490855813\n",
      "Iteration is: 17053 and loss is: 0.001508578541688621\n",
      "Iteration is: 17054 and loss is: 0.0023946035653352737\n",
      "Iteration is: 17055 and loss is: 0.0024011998903006315\n",
      "Iteration is: 17056 and loss is: 0.0009629128035157919\n",
      "Iteration is: 17057 and loss is: 0.0004299313295632601\n",
      "Iteration is: 17058 and loss is: 0.000938455923460424\n",
      "Iteration is: 17059 and loss is: 0.0008409946458414197\n",
      "Iteration is: 17060 and loss is: 0.0006083310581743717\n",
      "Iteration is: 17061 and loss is: 0.0007627674494870007\n",
      "Iteration is: 17062 and loss is: 0.0008704842766746879\n",
      "Iteration is: 17063 and loss is: 0.00041284802136942744\n",
      "Iteration is: 17064 and loss is: 0.0004842339549213648\n",
      "Iteration is: 17065 and loss is: 0.000590129813645035\n",
      "Iteration is: 17066 and loss is: 0.00030212319688871503\n",
      "Iteration is: 17067 and loss is: 0.00035784399369731545\n",
      "Iteration is: 17068 and loss is: 0.00034110399428755045\n",
      "Iteration is: 17069 and loss is: 0.00034043932100757957\n",
      "Iteration is: 17070 and loss is: 0.0004763354081660509\n",
      "Iteration is: 17071 and loss is: 0.00032268225913867354\n",
      "Iteration is: 17072 and loss is: 0.00035930477315559983\n",
      "Iteration is: 17073 and loss is: 0.0005952482460997999\n",
      "Iteration is: 17074 and loss is: 0.0008357215556316078\n",
      "Iteration is: 17075 and loss is: 0.0015279317740350962\n",
      "Iteration is: 17076 and loss is: 0.0024755066260695457\n",
      "Iteration is: 17077 and loss is: 0.002469322644174099\n",
      "Iteration is: 17078 and loss is: 0.0010494269663468003\n",
      "Iteration is: 17079 and loss is: 0.0007452851277776062\n",
      "Iteration is: 17080 and loss is: 0.0014127963222563267\n",
      "Iteration is: 17081 and loss is: 0.001311787054874003\n",
      "Iteration is: 17082 and loss is: 0.0005704900249838829\n",
      "Iteration is: 17083 and loss is: 0.001251224079169333\n",
      "Iteration is: 17084 and loss is: 0.0012649835553020239\n",
      "Iteration is: 17085 and loss is: 0.000402838020818308\n",
      "Iteration is: 17086 and loss is: 0.0008491494227200747\n",
      "Iteration is: 17087 and loss is: 0.0010170689783990383\n",
      "Iteration is: 17088 and loss is: 0.0003524843486957252\n",
      "Iteration is: 17089 and loss is: 0.0005878617521375418\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 17090 and loss is: 0.0007423884235322475\n",
      "Iteration is: 17091 and loss is: 0.0007167884614318609\n",
      "Iteration is: 17092 and loss is: 0.0004018467734567821\n",
      "Iteration is: 17093 and loss is: 0.00023382662038784474\n",
      "Iteration is: 17094 and loss is: 0.0005697907181456685\n",
      "Iteration is: 17095 and loss is: 0.0008486831793561578\n",
      "Iteration is: 17096 and loss is: 0.0011479699751362205\n",
      "Iteration is: 17097 and loss is: 0.0015317591605708003\n",
      "Iteration is: 17098 and loss is: 0.0017948822351172566\n",
      "Iteration is: 17099 and loss is: 0.0010546788107603788\n",
      "Iteration is: 17100 and loss is: 0.00034920338657684624\n",
      "Iteration is: 17101 and loss is: 0.0008114536176435649\n",
      "Iteration is: 17102 and loss is: 0.0005727612297050655\n",
      "Iteration is: 17103 and loss is: 0.0005492921336553991\n",
      "Iteration is: 17104 and loss is: 0.0007836745353415608\n",
      "Iteration is: 17105 and loss is: 0.00040741951670497656\n",
      "Iteration is: 17106 and loss is: 0.0005821683444082737\n",
      "Iteration is: 17107 and loss is: 0.000503916060552001\n",
      "Iteration is: 17108 and loss is: 0.00044218366383574903\n",
      "Iteration is: 17109 and loss is: 0.0003662448434624821\n",
      "Iteration is: 17110 and loss is: 0.0003741271502804011\n",
      "Iteration is: 17111 and loss is: 0.00033294869353994727\n",
      "Iteration is: 17112 and loss is: 0.0002866470313165337\n",
      "Iteration is: 17113 and loss is: 0.00035507348366081715\n",
      "Iteration is: 17114 and loss is: 0.00021776859648525715\n",
      "Iteration is: 17115 and loss is: 0.0003130317199975252\n",
      "Iteration is: 17116 and loss is: 0.00037228994187898934\n",
      "Iteration is: 17117 and loss is: 0.000309012015350163\n",
      "Iteration is: 17118 and loss is: 0.0004152426845394075\n",
      "Iteration is: 17119 and loss is: 0.0003819954872597009\n",
      "Iteration is: 17120 and loss is: 0.00044401627383194864\n",
      "Iteration is: 17121 and loss is: 0.00040689160232432187\n",
      "Iteration is: 17122 and loss is: 0.0003628433623816818\n",
      "Iteration is: 17123 and loss is: 0.0002880673564504832\n",
      "Iteration is: 17124 and loss is: 0.00020323391072452068\n",
      "Iteration is: 17125 and loss is: 0.00019249771139584482\n",
      "Iteration is: 17126 and loss is: 0.00023022726236376911\n",
      "Iteration is: 17127 and loss is: 0.00023175356909632683\n",
      "Iteration is: 17128 and loss is: 0.00023396345204673707\n",
      "Iteration is: 17129 and loss is: 0.00019673585484270006\n",
      "Iteration is: 17130 and loss is: 0.00015046141925267875\n",
      "Iteration is: 17131 and loss is: 0.00018704459944274276\n",
      "Iteration is: 17132 and loss is: 0.00018215618911199272\n",
      "Iteration is: 17133 and loss is: 0.0002041954721789807\n",
      "Iteration is: 17134 and loss is: 0.0002128403284586966\n",
      "Iteration is: 17135 and loss is: 0.00021026683680247515\n",
      "Iteration is: 17136 and loss is: 0.0002148695639334619\n",
      "Iteration is: 17137 and loss is: 0.0002026881993515417\n",
      "Iteration is: 17138 and loss is: 0.00019856938160955906\n",
      "Iteration is: 17139 and loss is: 0.00019234110368415713\n",
      "Iteration is: 17140 and loss is: 0.00017553380166646093\n",
      "Iteration is: 17141 and loss is: 0.0001632661442272365\n",
      "Iteration is: 17142 and loss is: 0.0001562234538141638\n",
      "Iteration is: 17143 and loss is: 0.0001374854618916288\n",
      "Iteration is: 17144 and loss is: 0.0001377732987748459\n",
      "Iteration is: 17145 and loss is: 0.00013609103916678578\n",
      "Iteration is: 17146 and loss is: 0.00013595703057944775\n",
      "Iteration is: 17147 and loss is: 0.00014586499310098588\n",
      "Iteration is: 17148 and loss is: 0.0001549503649584949\n",
      "Iteration is: 17149 and loss is: 0.0001788533991202712\n",
      "Iteration is: 17150 and loss is: 0.00022093983716331422\n",
      "Iteration is: 17151 and loss is: 0.00030593640985898674\n",
      "Iteration is: 17152 and loss is: 0.0004396834410727024\n",
      "Iteration is: 17153 and loss is: 0.0006376963574439287\n",
      "Iteration is: 17154 and loss is: 0.0007619275711476803\n",
      "Iteration is: 17155 and loss is: 0.0007078272756189108\n",
      "Iteration is: 17156 and loss is: 0.00043178675696253777\n",
      "Iteration is: 17157 and loss is: 0.0001844452926889062\n",
      "Iteration is: 17158 and loss is: 0.00021150097018107772\n",
      "Iteration is: 17159 and loss is: 0.000379259348846972\n",
      "Iteration is: 17160 and loss is: 0.0003929950180463493\n",
      "Iteration is: 17161 and loss is: 0.00023506872821599245\n",
      "Iteration is: 17162 and loss is: 0.00017428350111003965\n",
      "Iteration is: 17163 and loss is: 0.0002697136951610446\n",
      "Iteration is: 17164 and loss is: 0.00032200326677411795\n",
      "Iteration is: 17165 and loss is: 0.0002384454128332436\n",
      "Iteration is: 17166 and loss is: 0.0001548751606605947\n",
      "Iteration is: 17167 and loss is: 0.00018350427853874862\n",
      "Iteration is: 17168 and loss is: 0.00025403941981494427\n",
      "Iteration is: 17169 and loss is: 0.00025584251852706075\n",
      "Iteration is: 17170 and loss is: 0.0001982157409656793\n",
      "Iteration is: 17171 and loss is: 0.00015657641051802784\n",
      "Iteration is: 17172 and loss is: 0.00015298754442483187\n",
      "Iteration is: 17173 and loss is: 0.00017317822494078428\n",
      "Iteration is: 17174 and loss is: 0.000215903390198946\n",
      "Iteration is: 17175 and loss is: 0.00027416431112214923\n",
      "Iteration is: 17176 and loss is: 0.0003499298181850463\n",
      "Iteration is: 17177 and loss is: 0.0005035800859332085\n",
      "Iteration is: 17178 and loss is: 0.0007335608825087547\n",
      "Iteration is: 17179 and loss is: 0.0010773316025733948\n",
      "Iteration is: 17180 and loss is: 0.0011995550012215972\n",
      "Iteration is: 17181 and loss is: 0.0008506294689141214\n",
      "Iteration is: 17182 and loss is: 0.00030811282340437174\n",
      "Iteration is: 17183 and loss is: 0.00025153072783723474\n",
      "Iteration is: 17184 and loss is: 0.0005198862636461854\n",
      "Iteration is: 17185 and loss is: 0.000490727077703923\n",
      "Iteration is: 17186 and loss is: 0.0002986413601320237\n",
      "Iteration is: 17187 and loss is: 0.0002877319639082998\n",
      "Iteration is: 17188 and loss is: 0.00036334164906293154\n",
      "Iteration is: 17189 and loss is: 0.0003496472490951419\n",
      "Iteration is: 17190 and loss is: 0.00026146695017814636\n",
      "Iteration is: 17191 and loss is: 0.00020102201960980892\n",
      "Iteration is: 17192 and loss is: 0.00027851935010403395\n",
      "Iteration is: 17193 and loss is: 0.0003252508176956326\n",
      "Iteration is: 17194 and loss is: 0.000209738063858822\n",
      "Iteration is: 17195 and loss is: 0.00015996937872841954\n",
      "Iteration is: 17196 and loss is: 0.00022160490334499627\n",
      "Iteration is: 17197 and loss is: 0.00022844791237730533\n",
      "Iteration is: 17198 and loss is: 0.00021046798792667687\n",
      "Iteration is: 17199 and loss is: 0.00031171776936389506\n",
      "Iteration is: 17200 and loss is: 0.0005019264644943178\n",
      "Iteration is: 17201 and loss is: 0.000825330731458962\n",
      "Iteration is: 17202 and loss is: 0.0013190378667786717\n",
      "Iteration is: 17203 and loss is: 0.001876095891930163\n",
      "Iteration is: 17204 and loss is: 0.001488622510805726\n",
      "Iteration is: 17205 and loss is: 0.0005294778966344893\n",
      "Iteration is: 17206 and loss is: 0.0005179272266104817\n",
      "Iteration is: 17207 and loss is: 0.0008205972844734788\n",
      "Iteration is: 17208 and loss is: 0.0007118406938388944\n",
      "Iteration is: 17209 and loss is: 0.0005603826139122248\n",
      "Iteration is: 17210 and loss is: 0.0005816146731376648\n",
      "Iteration is: 17211 and loss is: 0.0006522455951198936\n",
      "Iteration is: 17212 and loss is: 0.00039783472311683\n",
      "Iteration is: 17213 and loss is: 0.0004055992467328906\n",
      "Iteration is: 17214 and loss is: 0.00040494909626431763\n",
      "Iteration is: 17215 and loss is: 0.00031282135751098394\n",
      "Iteration is: 17216 and loss is: 0.0003289699961896986\n",
      "Iteration is: 17217 and loss is: 0.0002074662916129455\n",
      "Iteration is: 17218 and loss is: 0.00028957342146895826\n",
      "Iteration is: 17219 and loss is: 0.0003405001771170646\n",
      "Iteration is: 17220 and loss is: 0.00024265676620416343\n",
      "Iteration is: 17221 and loss is: 0.00044154346687719226\n",
      "Iteration is: 17222 and loss is: 0.0006507762591354549\n",
      "Iteration is: 17223 and loss is: 0.00117602595128119\n",
      "Iteration is: 17224 and loss is: 0.002070218324661255\n",
      "Iteration is: 17225 and loss is: 0.0028105168603360653\n",
      "Iteration is: 17226 and loss is: 0.0014696947764605284\n",
      "Iteration is: 17227 and loss is: 0.0007838219753466547\n",
      "Iteration is: 17228 and loss is: 0.0018087042262777686\n",
      "Iteration is: 17229 and loss is: 0.0021176605951040983\n",
      "Iteration is: 17230 and loss is: 0.0009295043419115245\n",
      "Iteration is: 17231 and loss is: 0.0008576196851208806\n",
      "Iteration is: 17232 and loss is: 0.0016614454798400402\n",
      "Iteration is: 17233 and loss is: 0.0012284116819500923\n",
      "Iteration is: 17234 and loss is: 0.0006082422332838178\n",
      "Iteration is: 17235 and loss is: 0.00079586252104491\n",
      "Iteration is: 17236 and loss is: 0.000700939679518342\n",
      "Iteration is: 17237 and loss is: 0.0002749859413597733\n",
      "Iteration is: 17238 and loss is: 0.0006860704161226749\n",
      "Iteration is: 17239 and loss is: 0.0009791747434064746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 17240 and loss is: 0.0006657691556029022\n",
      "Iteration is: 17241 and loss is: 0.00036065521999262273\n",
      "Iteration is: 17242 and loss is: 0.00019759942370001227\n",
      "Iteration is: 17243 and loss is: 0.00026180967688560486\n",
      "Iteration is: 17244 and loss is: 0.0004732699890155345\n",
      "Iteration is: 17245 and loss is: 0.0006893554236739874\n",
      "Iteration is: 17246 and loss is: 0.0008549498161301017\n",
      "Iteration is: 17247 and loss is: 0.0006733910413458943\n",
      "Iteration is: 17248 and loss is: 0.0003310318570584059\n",
      "Iteration is: 17249 and loss is: 0.0002185517078032717\n",
      "Iteration is: 17250 and loss is: 0.0003625025856308639\n",
      "Iteration is: 17251 and loss is: 0.0003892499953508377\n",
      "Iteration is: 17252 and loss is: 0.0002702860510908067\n",
      "Iteration is: 17253 and loss is: 0.0002457575756125152\n",
      "Iteration is: 17254 and loss is: 0.0002941525890491903\n",
      "Iteration is: 17255 and loss is: 0.0002958769036922604\n",
      "Iteration is: 17256 and loss is: 0.0002389030414633453\n",
      "Iteration is: 17257 and loss is: 0.00021121783356647938\n",
      "Iteration is: 17258 and loss is: 0.000261822366155684\n",
      "Iteration is: 17259 and loss is: 0.00024309934815391898\n",
      "Iteration is: 17260 and loss is: 0.00016776303527876735\n",
      "Iteration is: 17261 and loss is: 0.00019613679614849389\n",
      "Iteration is: 17262 and loss is: 0.00023771205451339483\n",
      "Iteration is: 17263 and loss is: 0.00019325167522765696\n",
      "Iteration is: 17264 and loss is: 0.00015714990149717778\n",
      "Iteration is: 17265 and loss is: 0.00018447090405970812\n",
      "Iteration is: 17266 and loss is: 0.00017675949493423104\n",
      "Iteration is: 17267 and loss is: 0.0001546268176753074\n",
      "Iteration is: 17268 and loss is: 0.0001839402102632448\n",
      "Iteration is: 17269 and loss is: 0.00021389014727901667\n",
      "Iteration is: 17270 and loss is: 0.00021646785899065435\n",
      "Iteration is: 17271 and loss is: 0.00024993938859552145\n",
      "Iteration is: 17272 and loss is: 0.00031086296075955033\n",
      "Iteration is: 17273 and loss is: 0.0003677214845083654\n",
      "Iteration is: 17274 and loss is: 0.0004223170690238476\n",
      "Iteration is: 17275 and loss is: 0.00046919117448851466\n",
      "Iteration is: 17276 and loss is: 0.0004001586639788002\n",
      "Iteration is: 17277 and loss is: 0.00025843270123004913\n",
      "Iteration is: 17278 and loss is: 0.0001734471006784588\n",
      "Iteration is: 17279 and loss is: 0.0001860210468294099\n",
      "Iteration is: 17280 and loss is: 0.0002395321207586676\n",
      "Iteration is: 17281 and loss is: 0.00026385291130281985\n",
      "Iteration is: 17282 and loss is: 0.00022117869229987264\n",
      "Iteration is: 17283 and loss is: 0.0001588916638866067\n",
      "Iteration is: 17284 and loss is: 0.0001601186377229169\n",
      "Iteration is: 17285 and loss is: 0.00021182297496125102\n",
      "Iteration is: 17286 and loss is: 0.00022213647025637329\n",
      "Iteration is: 17287 and loss is: 0.00017868346185423434\n",
      "Iteration is: 17288 and loss is: 0.0001503252424299717\n",
      "Iteration is: 17289 and loss is: 0.00015959626762196422\n",
      "Iteration is: 17290 and loss is: 0.0001766158384270966\n",
      "Iteration is: 17291 and loss is: 0.0001846046798164025\n",
      "Iteration is: 17292 and loss is: 0.00017855397891253233\n",
      "Iteration is: 17293 and loss is: 0.00015687820268794894\n",
      "Iteration is: 17294 and loss is: 0.0001347784127574414\n",
      "Iteration is: 17295 and loss is: 0.00012956582941114902\n",
      "Iteration is: 17296 and loss is: 0.0001379266905132681\n",
      "Iteration is: 17297 and loss is: 0.00015372814959846437\n",
      "Iteration is: 17298 and loss is: 0.0001849054533522576\n",
      "Iteration is: 17299 and loss is: 0.0002480387920513749\n",
      "Iteration is: 17300 and loss is: 0.0003587943210732192\n",
      "Iteration is: 17301 and loss is: 0.0005673681735061109\n",
      "Iteration is: 17302 and loss is: 0.000820716202724725\n",
      "Iteration is: 17303 and loss is: 0.0010340457083657384\n",
      "Iteration is: 17304 and loss is: 0.0008706529624760151\n",
      "Iteration is: 17305 and loss is: 0.00042251127888448536\n",
      "Iteration is: 17306 and loss is: 0.00022668883320875466\n",
      "Iteration is: 17307 and loss is: 0.00039881482371129096\n",
      "Iteration is: 17308 and loss is: 0.0005050316685810685\n",
      "Iteration is: 17309 and loss is: 0.0003927440266124904\n",
      "Iteration is: 17310 and loss is: 0.00027567287907004356\n",
      "Iteration is: 17311 and loss is: 0.00031088461400941014\n",
      "Iteration is: 17312 and loss is: 0.0003856469411402941\n",
      "Iteration is: 17313 and loss is: 0.00031152652809396386\n",
      "Iteration is: 17314 and loss is: 0.00020049884915351868\n",
      "Iteration is: 17315 and loss is: 0.00021046571782790124\n",
      "Iteration is: 17316 and loss is: 0.00031573366140946746\n",
      "Iteration is: 17317 and loss is: 0.0002840635715983808\n",
      "Iteration is: 17318 and loss is: 0.0001718673447612673\n",
      "Iteration is: 17319 and loss is: 0.00018911020015366375\n",
      "Iteration is: 17320 and loss is: 0.00021030700008850545\n",
      "Iteration is: 17321 and loss is: 0.00016601495735812932\n",
      "Iteration is: 17322 and loss is: 0.00017197401029989123\n",
      "Iteration is: 17323 and loss is: 0.0002812601160258055\n",
      "Iteration is: 17324 and loss is: 0.00040792577783577144\n",
      "Iteration is: 17325 and loss is: 0.000684386701323092\n",
      "Iteration is: 17326 and loss is: 0.0011898452648892999\n",
      "Iteration is: 17327 and loss is: 0.001771074952557683\n",
      "Iteration is: 17328 and loss is: 0.0015182073693722486\n",
      "Iteration is: 17329 and loss is: 0.0005796372424811125\n",
      "Iteration is: 17330 and loss is: 0.00042796035995706916\n",
      "Iteration is: 17331 and loss is: 0.0008397413766942918\n",
      "Iteration is: 17332 and loss is: 0.0006686347769573331\n",
      "Iteration is: 17333 and loss is: 0.0005632305983453989\n",
      "Iteration is: 17334 and loss is: 0.000535367988049984\n",
      "Iteration is: 17335 and loss is: 0.0006158003816381097\n",
      "Iteration is: 17336 and loss is: 0.00043488823575899005\n",
      "Iteration is: 17337 and loss is: 0.0003797431418206543\n",
      "Iteration is: 17338 and loss is: 0.0004122325626667589\n",
      "Iteration is: 17339 and loss is: 0.0003377360990270972\n",
      "Iteration is: 17340 and loss is: 0.00036467524478212\n",
      "Iteration is: 17341 and loss is: 0.00023425565450452268\n",
      "Iteration is: 17342 and loss is: 0.00025928893592208624\n",
      "Iteration is: 17343 and loss is: 0.00036734878085553646\n",
      "Iteration is: 17344 and loss is: 0.00019211658218409866\n",
      "Iteration is: 17345 and loss is: 0.00027508774655871093\n",
      "Iteration is: 17346 and loss is: 0.0003629346319939941\n",
      "Iteration is: 17347 and loss is: 0.000407943909522146\n",
      "Iteration is: 17348 and loss is: 0.0005919246468693018\n",
      "Iteration is: 17349 and loss is: 0.0009756464278325438\n",
      "Iteration is: 17350 and loss is: 0.001243152073584497\n",
      "Iteration is: 17351 and loss is: 0.0010551245650276542\n",
      "Iteration is: 17352 and loss is: 0.00044227964826859534\n",
      "Iteration is: 17353 and loss is: 0.00021279737120494246\n",
      "Iteration is: 17354 and loss is: 0.0005970039637759328\n",
      "Iteration is: 17355 and loss is: 0.000568408751860261\n",
      "Iteration is: 17356 and loss is: 0.00028943130746483803\n",
      "Iteration is: 17357 and loss is: 0.00034243485424667597\n",
      "Iteration is: 17358 and loss is: 0.00046524853678420186\n",
      "Iteration is: 17359 and loss is: 0.0003500028979033232\n",
      "Iteration is: 17360 and loss is: 0.0002696430019568652\n",
      "Iteration is: 17361 and loss is: 0.0002940592239610851\n",
      "Iteration is: 17362 and loss is: 0.0003311655600555241\n",
      "Iteration is: 17363 and loss is: 0.00027225996018387377\n",
      "Iteration is: 17364 and loss is: 0.00018376632942818105\n",
      "Iteration is: 17365 and loss is: 0.00025102891959249973\n",
      "Iteration is: 17366 and loss is: 0.00030594089184887707\n",
      "Iteration is: 17367 and loss is: 0.00022979566711001098\n",
      "Iteration is: 17368 and loss is: 0.00016651052283123136\n",
      "Iteration is: 17369 and loss is: 0.00020659095025621355\n",
      "Iteration is: 17370 and loss is: 0.00020267762010917068\n",
      "Iteration is: 17371 and loss is: 0.00014714297140017152\n",
      "Iteration is: 17372 and loss is: 0.00017477251822128892\n",
      "Iteration is: 17373 and loss is: 0.00021233144798316061\n",
      "Iteration is: 17374 and loss is: 0.00020087440498173237\n",
      "Iteration is: 17375 and loss is: 0.0002380705118412152\n",
      "Iteration is: 17376 and loss is: 0.00036841953988187015\n",
      "Iteration is: 17377 and loss is: 0.0005015921778976917\n",
      "Iteration is: 17378 and loss is: 0.0005980373243801296\n",
      "Iteration is: 17379 and loss is: 0.0005472003249451518\n",
      "Iteration is: 17380 and loss is: 0.00034115483867935836\n",
      "Iteration is: 17381 and loss is: 0.0001624362193979323\n",
      "Iteration is: 17382 and loss is: 0.00020209158537909389\n",
      "Iteration is: 17383 and loss is: 0.00032324454514309764\n",
      "Iteration is: 17384 and loss is: 0.0002951485221274197\n",
      "Iteration is: 17385 and loss is: 0.00020245065388735384\n",
      "Iteration is: 17386 and loss is: 0.00018521747551858425\n",
      "Iteration is: 17387 and loss is: 0.00022490756236948073\n",
      "Iteration is: 17388 and loss is: 0.0002462443080730736\n",
      "Iteration is: 17389 and loss is: 0.00021816184744238853\n",
      "Iteration is: 17390 and loss is: 0.0001670829951763153\n",
      "Iteration is: 17391 and loss is: 0.00016547316045034677\n",
      "Iteration is: 17392 and loss is: 0.00020883663091808558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 17393 and loss is: 0.00021689769346266985\n",
      "Iteration is: 17394 and loss is: 0.00017540217959322035\n",
      "Iteration is: 17395 and loss is: 0.0001450087147532031\n",
      "Iteration is: 17396 and loss is: 0.00015311270544771105\n",
      "Iteration is: 17397 and loss is: 0.0001606685691513121\n",
      "Iteration is: 17398 and loss is: 0.00016347444034181535\n",
      "Iteration is: 17399 and loss is: 0.00019997236086055636\n",
      "Iteration is: 17400 and loss is: 0.0002714183065108955\n",
      "Iteration is: 17401 and loss is: 0.00038934964686632156\n",
      "Iteration is: 17402 and loss is: 0.0006699537043459713\n",
      "Iteration is: 17403 and loss is: 0.0011349847773090005\n",
      "Iteration is: 17404 and loss is: 0.0017454505432397127\n",
      "Iteration is: 17405 and loss is: 0.001671002828516066\n",
      "Iteration is: 17406 and loss is: 0.0007789715309627354\n",
      "Iteration is: 17407 and loss is: 0.0003893073298968375\n",
      "Iteration is: 17408 and loss is: 0.000827567302621901\n",
      "Iteration is: 17409 and loss is: 0.00073294120375067\n",
      "Iteration is: 17410 and loss is: 0.0006168548716232181\n",
      "Iteration is: 17411 and loss is: 0.0005747484974563122\n",
      "Iteration is: 17412 and loss is: 0.0006322151748463511\n",
      "Iteration is: 17413 and loss is: 0.0005034514470025897\n",
      "Iteration is: 17414 and loss is: 0.0004223763826303184\n",
      "Iteration is: 17415 and loss is: 0.0004409759712871164\n",
      "Iteration is: 17416 and loss is: 0.00035518466029316187\n",
      "Iteration is: 17417 and loss is: 0.0004389110254123807\n",
      "Iteration is: 17418 and loss is: 0.0003132648707833141\n",
      "Iteration is: 17419 and loss is: 0.00020031088206451386\n",
      "Iteration is: 17420 and loss is: 0.00043122837087139487\n",
      "Iteration is: 17421 and loss is: 0.0002731714048422873\n",
      "Iteration is: 17422 and loss is: 0.00022500546765513718\n",
      "Iteration is: 17423 and loss is: 0.00036675858427770436\n",
      "Iteration is: 17424 and loss is: 0.0004115965566597879\n",
      "Iteration is: 17425 and loss is: 0.0005025541759096086\n",
      "Iteration is: 17426 and loss is: 0.0008059606188908219\n",
      "Iteration is: 17427 and loss is: 0.0011592695955187082\n",
      "Iteration is: 17428 and loss is: 0.0011956033995375037\n",
      "Iteration is: 17429 and loss is: 0.0007040526252239943\n",
      "Iteration is: 17430 and loss is: 0.00020006965496577322\n",
      "Iteration is: 17431 and loss is: 0.0004411576082929969\n",
      "Iteration is: 17432 and loss is: 0.0006688099820166826\n",
      "Iteration is: 17433 and loss is: 0.0003785618464462459\n",
      "Iteration is: 17434 and loss is: 0.00027727149426937103\n",
      "Iteration is: 17435 and loss is: 0.0004815010470338166\n",
      "Iteration is: 17436 and loss is: 0.0004209617036394775\n",
      "Iteration is: 17437 and loss is: 0.0002762925287242979\n",
      "Iteration is: 17438 and loss is: 0.0003018065181095153\n",
      "Iteration is: 17439 and loss is: 0.000358745688572526\n",
      "Iteration is: 17440 and loss is: 0.00031913185375742614\n",
      "Iteration is: 17441 and loss is: 0.0002033771015703678\n",
      "Iteration is: 17442 and loss is: 0.0002392502356087789\n",
      "Iteration is: 17443 and loss is: 0.0003242596867494285\n",
      "Iteration is: 17444 and loss is: 0.0002928706235252321\n",
      "Iteration is: 17445 and loss is: 0.0001966996060218662\n",
      "Iteration is: 17446 and loss is: 0.00019080034689977765\n",
      "Iteration is: 17447 and loss is: 0.00022628571605309844\n",
      "Iteration is: 17448 and loss is: 0.00017056925571523607\n",
      "Iteration is: 17449 and loss is: 0.00015132225234992802\n",
      "Iteration is: 17450 and loss is: 0.0001941484515555203\n",
      "Iteration is: 17451 and loss is: 0.00017329359252471477\n",
      "Iteration is: 17452 and loss is: 0.00013648555614054203\n",
      "Iteration is: 17453 and loss is: 0.00016210658941417933\n",
      "Iteration is: 17454 and loss is: 0.00017618612037040293\n",
      "Iteration is: 17455 and loss is: 0.00016365080955438316\n",
      "Iteration is: 17456 and loss is: 0.0001715482649160549\n",
      "Iteration is: 17457 and loss is: 0.00019216760119888932\n",
      "Iteration is: 17458 and loss is: 0.0001926020922837779\n",
      "Iteration is: 17459 and loss is: 0.00017983413999900222\n",
      "Iteration is: 17460 and loss is: 0.00018972363614011556\n",
      "Iteration is: 17461 and loss is: 0.00020165197202004492\n",
      "Iteration is: 17462 and loss is: 0.00020526401931419969\n",
      "Iteration is: 17463 and loss is: 0.00024261097132693976\n",
      "Iteration is: 17464 and loss is: 0.0003213125455658883\n",
      "Iteration is: 17465 and loss is: 0.000449823186499998\n",
      "Iteration is: 17466 and loss is: 0.0006118374294601381\n",
      "Iteration is: 17467 and loss is: 0.0008161783334799111\n",
      "Iteration is: 17468 and loss is: 0.0008466597064398229\n",
      "Iteration is: 17469 and loss is: 0.0005961072165518999\n",
      "Iteration is: 17470 and loss is: 0.0002493125793989748\n",
      "Iteration is: 17471 and loss is: 0.00017339730402454734\n",
      "Iteration is: 17472 and loss is: 0.000356486503733322\n",
      "Iteration is: 17473 and loss is: 0.0004390105605125427\n",
      "Iteration is: 17474 and loss is: 0.0002970470522996038\n",
      "Iteration is: 17475 and loss is: 0.00017098966054618359\n",
      "Iteration is: 17476 and loss is: 0.0002367980487179011\n",
      "Iteration is: 17477 and loss is: 0.0003360713308211416\n",
      "Iteration is: 17478 and loss is: 0.0002956256503239274\n",
      "Iteration is: 17479 and loss is: 0.00018486782209947705\n",
      "Iteration is: 17480 and loss is: 0.00015087425708770752\n",
      "Iteration is: 17481 and loss is: 0.0002178146824007854\n",
      "Iteration is: 17482 and loss is: 0.00028020344325341284\n",
      "Iteration is: 17483 and loss is: 0.0002664581115823239\n",
      "Iteration is: 17484 and loss is: 0.0002118410775437951\n",
      "Iteration is: 17485 and loss is: 0.0001697260158834979\n",
      "Iteration is: 17486 and loss is: 0.00014186873158905655\n",
      "Iteration is: 17487 and loss is: 0.000130821717903018\n",
      "Iteration is: 17488 and loss is: 0.00015181647904682904\n",
      "Iteration is: 17489 and loss is: 0.000211424267035909\n",
      "Iteration is: 17490 and loss is: 0.0003321767726447433\n",
      "Iteration is: 17491 and loss is: 0.0006268311990424991\n",
      "Iteration is: 17492 and loss is: 0.0011937745148316026\n",
      "Iteration is: 17493 and loss is: 0.0021184138022363186\n",
      "Iteration is: 17494 and loss is: 0.0022553298622369766\n",
      "Iteration is: 17495 and loss is: 0.0010963866952806711\n",
      "Iteration is: 17496 and loss is: 0.0005777559708803892\n",
      "Iteration is: 17497 and loss is: 0.001098903943784535\n",
      "Iteration is: 17498 and loss is: 0.001146617578342557\n",
      "Iteration is: 17499 and loss is: 0.0008132741786539555\n",
      "Iteration is: 17500 and loss is: 0.0005975775420665741\n",
      "Iteration is: 17501 and loss is: 0.0011269435053691268\n",
      "Iteration is: 17502 and loss is: 0.0007563995895907283\n",
      "Iteration is: 17503 and loss is: 0.00035742545151151717\n",
      "Iteration is: 17504 and loss is: 0.0006555159343406558\n",
      "Iteration is: 17505 and loss is: 0.0006213572341948748\n",
      "Iteration is: 17506 and loss is: 0.0004357408033683896\n",
      "Iteration is: 17507 and loss is: 0.00039233651477843523\n",
      "Iteration is: 17508 and loss is: 0.00028591466252692044\n",
      "Iteration is: 17509 and loss is: 0.0007133088074624538\n",
      "Iteration is: 17510 and loss is: 0.001150130177848041\n",
      "Iteration is: 17511 and loss is: 0.0019951099529862404\n",
      "Iteration is: 17512 and loss is: 0.0036276786122471094\n",
      "Iteration is: 17513 and loss is: 0.00527560617774725\n",
      "Iteration is: 17514 and loss is: 0.0019664629362523556\n",
      "Iteration is: 17515 and loss is: 0.0015655681490898132\n",
      "Iteration is: 17516 and loss is: 0.006385729182511568\n",
      "Iteration is: 17517 and loss is: 0.00614476203918457\n",
      "Iteration is: 17518 and loss is: 0.002790280384942889\n",
      "Iteration is: 17519 and loss is: 0.0016335111577063799\n",
      "Iteration is: 17520 and loss is: 0.0034767729230225086\n",
      "Iteration is: 17521 and loss is: 0.0059914011508226395\n",
      "Iteration is: 17522 and loss is: 0.006232012063264847\n",
      "Iteration is: 17523 and loss is: 0.0031025016214698553\n",
      "Iteration is: 17524 and loss is: 0.0007465538801625371\n",
      "Iteration is: 17525 and loss is: 0.002098849043250084\n",
      "Iteration is: 17526 and loss is: 0.001582642551511526\n",
      "Iteration is: 17527 and loss is: 0.0010998124489560723\n",
      "Iteration is: 17528 and loss is: 0.0029121609404683113\n",
      "Iteration is: 17529 and loss is: 0.0044380538165569305\n",
      "Iteration is: 17530 and loss is: 0.0029279724694788456\n",
      "Iteration is: 17531 and loss is: 0.0013426836812868714\n",
      "Iteration is: 17532 and loss is: 0.0006898708525113761\n",
      "Iteration is: 17533 and loss is: 0.0011192432139068842\n",
      "Iteration is: 17534 and loss is: 0.0010336125269532204\n",
      "Iteration is: 17535 and loss is: 0.0007424522191286087\n",
      "Iteration is: 17536 and loss is: 0.0007794917328283191\n",
      "Iteration is: 17537 and loss is: 0.0006422238075174391\n",
      "Iteration is: 17538 and loss is: 0.0009495553094893694\n",
      "Iteration is: 17539 and loss is: 0.00039223942440003157\n",
      "Iteration is: 17540 and loss is: 0.0007983166724443436\n",
      "Iteration is: 17541 and loss is: 0.0006967203225940466\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 17542 and loss is: 0.0004589055897668004\n",
      "Iteration is: 17543 and loss is: 0.000581228407099843\n",
      "Iteration is: 17544 and loss is: 0.0005899213720113039\n",
      "Iteration is: 17545 and loss is: 0.00047050163266249\n",
      "Iteration is: 17546 and loss is: 0.00033369328593835235\n",
      "Iteration is: 17547 and loss is: 0.0006069189985282719\n",
      "Iteration is: 17548 and loss is: 0.00036028260365128517\n",
      "Iteration is: 17549 and loss is: 0.0003000044380314648\n",
      "Iteration is: 17550 and loss is: 0.0004885070957243443\n",
      "Iteration is: 17551 and loss is: 0.00039669708348810673\n",
      "Iteration is: 17552 and loss is: 0.00024412291531916708\n",
      "Iteration is: 17553 and loss is: 0.00032852977165021\n",
      "Iteration is: 17554 and loss is: 0.00042710802517831326\n",
      "Iteration is: 17555 and loss is: 0.00024476993712596595\n",
      "Iteration is: 17556 and loss is: 0.00022375822300091386\n",
      "Iteration is: 17557 and loss is: 0.0003356300003360957\n",
      "Iteration is: 17558 and loss is: 0.00030481902649626136\n",
      "Iteration is: 17559 and loss is: 0.00021796944201923907\n",
      "Iteration is: 17560 and loss is: 0.00020420228247530758\n",
      "Iteration is: 17561 and loss is: 0.0002913282660301775\n",
      "Iteration is: 17562 and loss is: 0.00025291185011155903\n",
      "Iteration is: 17563 and loss is: 0.00017005373956635594\n",
      "Iteration is: 17564 and loss is: 0.00020009117724839598\n",
      "Iteration is: 17565 and loss is: 0.00021990598179399967\n",
      "Iteration is: 17566 and loss is: 0.00022249712492339313\n",
      "Iteration is: 17567 and loss is: 0.00019376784621272236\n",
      "Iteration is: 17568 and loss is: 0.00015496398555114865\n",
      "Iteration is: 17569 and loss is: 0.00016975287871900946\n",
      "Iteration is: 17570 and loss is: 0.00019703480938915163\n",
      "Iteration is: 17571 and loss is: 0.0002043290005531162\n",
      "Iteration is: 17572 and loss is: 0.00017491314793005586\n",
      "Iteration is: 17573 and loss is: 0.0001491006842115894\n",
      "Iteration is: 17574 and loss is: 0.00015031537623144686\n",
      "Iteration is: 17575 and loss is: 0.00015672831796109676\n",
      "Iteration is: 17576 and loss is: 0.00016959760978352278\n",
      "Iteration is: 17577 and loss is: 0.00016248969768639654\n",
      "Iteration is: 17578 and loss is: 0.00014203839236870408\n",
      "Iteration is: 17579 and loss is: 0.00013777939602732658\n",
      "Iteration is: 17580 and loss is: 0.00014054347411729395\n",
      "Iteration is: 17581 and loss is: 0.00014659053704235703\n",
      "Iteration is: 17582 and loss is: 0.00015241873916238546\n",
      "Iteration is: 17583 and loss is: 0.0001474752207286656\n",
      "Iteration is: 17584 and loss is: 0.00013890941045247018\n",
      "Iteration is: 17585 and loss is: 0.00013174374180380255\n",
      "Iteration is: 17586 and loss is: 0.00012966117355972528\n",
      "Iteration is: 17587 and loss is: 0.00013244838919490576\n",
      "Iteration is: 17588 and loss is: 0.0001352940744254738\n",
      "Iteration is: 17589 and loss is: 0.00013766609481535852\n",
      "Iteration is: 17590 and loss is: 0.0001350846141576767\n",
      "Iteration is: 17591 and loss is: 0.0001297588605666533\n",
      "Iteration is: 17592 and loss is: 0.00012721116945613176\n",
      "Iteration is: 17593 and loss is: 0.000125003483844921\n",
      "Iteration is: 17594 and loss is: 0.00012585995136760175\n",
      "Iteration is: 17595 and loss is: 0.00012869853526353836\n",
      "Iteration is: 17596 and loss is: 0.00012904111645184457\n",
      "Iteration is: 17597 and loss is: 0.00012879124551545829\n",
      "Iteration is: 17598 and loss is: 0.00012721731036435813\n",
      "Iteration is: 17599 and loss is: 0.00012462589074857533\n",
      "Iteration is: 17600 and loss is: 0.0001228870387421921\n",
      "Iteration is: 17601 and loss is: 0.00012210871500428766\n",
      "Iteration is: 17602 and loss is: 0.0001224765437655151\n",
      "Iteration is: 17603 and loss is: 0.00012298546789679676\n",
      "Iteration is: 17604 and loss is: 0.00012358088861219585\n",
      "Iteration is: 17605 and loss is: 0.00012401386629790068\n",
      "Iteration is: 17606 and loss is: 0.00012335926294326782\n",
      "Iteration is: 17607 and loss is: 0.00012249333667568862\n",
      "Iteration is: 17608 and loss is: 0.00012174373841844499\n",
      "Iteration is: 17609 and loss is: 0.00012074998812749982\n",
      "Iteration is: 17610 and loss is: 0.00012033217353746295\n",
      "Iteration is: 17611 and loss is: 0.0001204155123559758\n",
      "Iteration is: 17612 and loss is: 0.00012041952868457884\n",
      "Iteration is: 17613 and loss is: 0.00012066737690474838\n",
      "Iteration is: 17614 and loss is: 0.00012090836389688775\n",
      "Iteration is: 17615 and loss is: 0.00012087086361134425\n",
      "Iteration is: 17616 and loss is: 0.00012072637036908418\n",
      "Iteration is: 17617 and loss is: 0.00012042984599247575\n",
      "Iteration is: 17618 and loss is: 0.00012009356578346342\n",
      "Iteration is: 17619 and loss is: 0.00011969468323513865\n",
      "Iteration is: 17620 and loss is: 0.00011934193025808781\n",
      "Iteration is: 17621 and loss is: 0.00011917752271983773\n",
      "Iteration is: 17622 and loss is: 0.00011901216930709779\n",
      "Iteration is: 17623 and loss is: 0.00011894834460690618\n",
      "Iteration is: 17624 and loss is: 0.00011900704703293741\n",
      "Iteration is: 17625 and loss is: 0.00011899502715095878\n",
      "Iteration is: 17626 and loss is: 0.00011899910168722272\n",
      "Iteration is: 17627 and loss is: 0.00011901244579348713\n",
      "Iteration is: 17628 and loss is: 0.00011894862109329551\n",
      "Iteration is: 17629 and loss is: 0.00011885970161529258\n",
      "Iteration is: 17630 and loss is: 0.00011876520875375718\n",
      "Iteration is: 17631 and loss is: 0.0001186305598821491\n",
      "Iteration is: 17632 and loss is: 0.00011849033762700856\n",
      "Iteration is: 17633 and loss is: 0.00011835650366265327\n",
      "Iteration is: 17634 and loss is: 0.00011822787928394973\n",
      "Iteration is: 17635 and loss is: 0.00011811732110800222\n",
      "Iteration is: 17636 and loss is: 0.00011800252832472324\n",
      "Iteration is: 17637 and loss is: 0.00011792461737059057\n",
      "Iteration is: 17638 and loss is: 0.00011785264359787107\n",
      "Iteration is: 17639 and loss is: 0.00011777796316891909\n",
      "Iteration is: 17640 and loss is: 0.00011772937432397157\n",
      "Iteration is: 17641 and loss is: 0.00011768049444071949\n",
      "Iteration is: 17642 and loss is: 0.00011762955546146259\n",
      "Iteration is: 17643 and loss is: 0.0001175914439954795\n",
      "Iteration is: 17644 and loss is: 0.00011755141895264387\n",
      "Iteration is: 17645 and loss is: 0.00011750759585993364\n",
      "Iteration is: 17646 and loss is: 0.00011746988457161933\n",
      "Iteration is: 17647 and loss is: 0.00011743244976969436\n",
      "Iteration is: 17648 and loss is: 0.00011739874025806785\n",
      "Iteration is: 17649 and loss is: 0.00011736436863429844\n",
      "Iteration is: 17650 and loss is: 0.00011733408609870821\n",
      "Iteration is: 17651 and loss is: 0.00011730857659131289\n",
      "Iteration is: 17652 and loss is: 0.00011728602112270892\n",
      "Iteration is: 17653 and loss is: 0.00011726829688996077\n",
      "Iteration is: 17654 and loss is: 0.00011726266529876739\n",
      "Iteration is: 17655 and loss is: 0.0001172668271465227\n",
      "Iteration is: 17656 and loss is: 0.00011728766548912972\n",
      "Iteration is: 17657 and loss is: 0.00011733004066627473\n",
      "Iteration is: 17658 and loss is: 0.00011740337504306808\n",
      "Iteration is: 17659 and loss is: 0.00011751658894354478\n",
      "Iteration is: 17660 and loss is: 0.0001176896330434829\n",
      "Iteration is: 17661 and loss is: 0.0001179414102807641\n",
      "Iteration is: 17662 and loss is: 0.00011830856965389103\n",
      "Iteration is: 17663 and loss is: 0.00011883328261319548\n",
      "Iteration is: 17664 and loss is: 0.00011959769472014159\n",
      "Iteration is: 17665 and loss is: 0.00012068117212038487\n",
      "Iteration is: 17666 and loss is: 0.0001222820283146575\n",
      "Iteration is: 17667 and loss is: 0.00012455496471375227\n",
      "Iteration is: 17668 and loss is: 0.0001279680582229048\n",
      "Iteration is: 17669 and loss is: 0.0001327934442088008\n",
      "Iteration is: 17670 and loss is: 0.00014012318570166826\n",
      "Iteration is: 17671 and loss is: 0.00015029855421744287\n",
      "Iteration is: 17672 and loss is: 0.00016578735085204244\n",
      "Iteration is: 17673 and loss is: 0.00018632493447512388\n",
      "Iteration is: 17674 and loss is: 0.00021698579075746238\n",
      "Iteration is: 17675 and loss is: 0.0002529358316678554\n",
      "Iteration is: 17676 and loss is: 0.0003015348338522017\n",
      "Iteration is: 17677 and loss is: 0.00034073946881107986\n",
      "Iteration is: 17678 and loss is: 0.00037318599061109126\n",
      "Iteration is: 17679 and loss is: 0.0003563830687198788\n",
      "Iteration is: 17680 and loss is: 0.0002989433123730123\n",
      "Iteration is: 17681 and loss is: 0.00021146266954019666\n",
      "Iteration is: 17682 and loss is: 0.0001426457311026752\n",
      "Iteration is: 17683 and loss is: 0.00012412990326993167\n",
      "Iteration is: 17684 and loss is: 0.00015419397095683962\n",
      "Iteration is: 17685 and loss is: 0.00020734431745950133\n",
      "Iteration is: 17686 and loss is: 0.00026588316541165113\n",
      "Iteration is: 17687 and loss is: 0.0003386370954103768\n",
      "Iteration is: 17688 and loss is: 0.00041672715451568365\n",
      "Iteration is: 17689 and loss is: 0.0005232193507254124\n",
      "Iteration is: 17690 and loss is: 0.0005734715377911925\n",
      "Iteration is: 17691 and loss is: 0.0005440606037154794\n",
      "Iteration is: 17692 and loss is: 0.0003912119718734175\n",
      "Iteration is: 17693 and loss is: 0.0002119919372489676\n",
      "Iteration is: 17694 and loss is: 0.00014950725017115474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 17695 and loss is: 0.0002226779906777665\n",
      "Iteration is: 17696 and loss is: 0.00032226915936917067\n",
      "Iteration is: 17697 and loss is: 0.00032478454522788525\n",
      "Iteration is: 17698 and loss is: 0.00022322378936223686\n",
      "Iteration is: 17699 and loss is: 0.00014074398495722562\n",
      "Iteration is: 17700 and loss is: 0.00014749642286915332\n",
      "Iteration is: 17701 and loss is: 0.00020263472106307745\n",
      "Iteration is: 17702 and loss is: 0.0002599171129986644\n",
      "Iteration is: 17703 and loss is: 0.0003135128354188055\n",
      "Iteration is: 17704 and loss is: 0.00037185358814895153\n",
      "Iteration is: 17705 and loss is: 0.00045636467984877527\n",
      "Iteration is: 17706 and loss is: 0.0005055010551586747\n",
      "Iteration is: 17707 and loss is: 0.0005275504663586617\n",
      "Iteration is: 17708 and loss is: 0.0004491718136705458\n",
      "Iteration is: 17709 and loss is: 0.0002885015564970672\n",
      "Iteration is: 17710 and loss is: 0.00016222088015638292\n",
      "Iteration is: 17711 and loss is: 0.00016041746130213141\n",
      "Iteration is: 17712 and loss is: 0.0002561895817052573\n",
      "Iteration is: 17713 and loss is: 0.00032480148365721107\n",
      "Iteration is: 17714 and loss is: 0.00028234621277078986\n",
      "Iteration is: 17715 and loss is: 0.00019062322098761797\n",
      "Iteration is: 17716 and loss is: 0.00013413175474852324\n",
      "Iteration is: 17717 and loss is: 0.00013617057993542403\n",
      "Iteration is: 17718 and loss is: 0.00017714484420139343\n",
      "Iteration is: 17719 and loss is: 0.00025496346643194556\n",
      "Iteration is: 17720 and loss is: 0.00037851708475500345\n",
      "Iteration is: 17721 and loss is: 0.0005807291017845273\n",
      "Iteration is: 17722 and loss is: 0.0007869012770242989\n",
      "Iteration is: 17723 and loss is: 0.0009574427967891097\n",
      "Iteration is: 17724 and loss is: 0.0008357655606232584\n",
      "Iteration is: 17725 and loss is: 0.00046002891031093895\n",
      "Iteration is: 17726 and loss is: 0.0002290720585733652\n",
      "Iteration is: 17727 and loss is: 0.00028270596521906555\n",
      "Iteration is: 17728 and loss is: 0.0004541676607914269\n",
      "Iteration is: 17729 and loss is: 0.0005047876620665193\n",
      "Iteration is: 17730 and loss is: 0.00031508749816566706\n",
      "Iteration is: 17731 and loss is: 0.00017613358795642853\n",
      "Iteration is: 17732 and loss is: 0.0002637481957208365\n",
      "Iteration is: 17733 and loss is: 0.0003741037508007139\n",
      "Iteration is: 17734 and loss is: 0.00032958859810605645\n",
      "Iteration is: 17735 and loss is: 0.0001908800913952291\n",
      "Iteration is: 17736 and loss is: 0.00016982735542114824\n",
      "Iteration is: 17737 and loss is: 0.0002226463402621448\n",
      "Iteration is: 17738 and loss is: 0.0002393573522567749\n",
      "Iteration is: 17739 and loss is: 0.00027031611534766853\n",
      "Iteration is: 17740 and loss is: 0.00030998094007372856\n",
      "Iteration is: 17741 and loss is: 0.00031293914071284235\n",
      "Iteration is: 17742 and loss is: 0.00030259013874456286\n",
      "Iteration is: 17743 and loss is: 0.0003096600412391126\n",
      "Iteration is: 17744 and loss is: 0.0003313855268061161\n",
      "Iteration is: 17745 and loss is: 0.000319556740578264\n",
      "Iteration is: 17746 and loss is: 0.00028648131410591304\n",
      "Iteration is: 17747 and loss is: 0.00023782803327776492\n",
      "Iteration is: 17748 and loss is: 0.00018781429389491677\n",
      "Iteration is: 17749 and loss is: 0.00014048977755010128\n",
      "Iteration is: 17750 and loss is: 0.0001262359437532723\n",
      "Iteration is: 17751 and loss is: 0.00013949863205198199\n",
      "Iteration is: 17752 and loss is: 0.00016148209397215396\n",
      "Iteration is: 17753 and loss is: 0.00019170495215803385\n",
      "Iteration is: 17754 and loss is: 0.00022884081408847123\n",
      "Iteration is: 17755 and loss is: 0.00027385770226828754\n",
      "Iteration is: 17756 and loss is: 0.0003232041490264237\n",
      "Iteration is: 17757 and loss is: 0.00040639282087795436\n",
      "Iteration is: 17758 and loss is: 0.0004844923969358206\n",
      "Iteration is: 17759 and loss is: 0.0005473073688335717\n",
      "Iteration is: 17760 and loss is: 0.000497598375659436\n",
      "Iteration is: 17761 and loss is: 0.00035035377368330956\n",
      "Iteration is: 17762 and loss is: 0.0002051280898740515\n",
      "Iteration is: 17763 and loss is: 0.00015242592780850828\n",
      "Iteration is: 17764 and loss is: 0.00021515542175620794\n",
      "Iteration is: 17765 and loss is: 0.0003049316583201289\n",
      "Iteration is: 17766 and loss is: 0.000308649759972468\n",
      "Iteration is: 17767 and loss is: 0.00022466163500212133\n",
      "Iteration is: 17768 and loss is: 0.00014545752492267638\n",
      "Iteration is: 17769 and loss is: 0.00014255012501962483\n",
      "Iteration is: 17770 and loss is: 0.00018886194447986782\n",
      "Iteration is: 17771 and loss is: 0.00022940681083127856\n",
      "Iteration is: 17772 and loss is: 0.0002692393900360912\n",
      "Iteration is: 17773 and loss is: 0.0003299606905784458\n",
      "Iteration is: 17774 and loss is: 0.00038927100831642747\n",
      "Iteration is: 17775 and loss is: 0.0004730957152787596\n",
      "Iteration is: 17776 and loss is: 0.0005227672518230975\n",
      "Iteration is: 17777 and loss is: 0.000514889310579747\n",
      "Iteration is: 17778 and loss is: 0.00040340048144571483\n",
      "Iteration is: 17779 and loss is: 0.00023799034534022212\n",
      "Iteration is: 17780 and loss is: 0.0001415307924617082\n",
      "Iteration is: 17781 and loss is: 0.00017266107897739857\n",
      "Iteration is: 17782 and loss is: 0.00027210067491978407\n",
      "Iteration is: 17783 and loss is: 0.00032704061595723033\n",
      "Iteration is: 17784 and loss is: 0.00028897312586195767\n",
      "Iteration is: 17785 and loss is: 0.00021348103473428637\n",
      "Iteration is: 17786 and loss is: 0.0001539145305287093\n",
      "Iteration is: 17787 and loss is: 0.00012738928489852697\n",
      "Iteration is: 17788 and loss is: 0.0001279455900657922\n",
      "Iteration is: 17789 and loss is: 0.00015741426614113152\n",
      "Iteration is: 17790 and loss is: 0.00022010751126799732\n",
      "Iteration is: 17791 and loss is: 0.0003322658594697714\n",
      "Iteration is: 17792 and loss is: 0.0004954815376549959\n",
      "Iteration is: 17793 and loss is: 0.0007510105497203767\n",
      "Iteration is: 17794 and loss is: 0.0009246930130757391\n",
      "Iteration is: 17795 and loss is: 0.000833413505461067\n",
      "Iteration is: 17796 and loss is: 0.0004986013518646359\n",
      "Iteration is: 17797 and loss is: 0.00022212891781236976\n",
      "Iteration is: 17798 and loss is: 0.0002202268224209547\n",
      "Iteration is: 17799 and loss is: 0.00037834554677829146\n",
      "Iteration is: 17800 and loss is: 0.0004630988696590066\n",
      "Iteration is: 17801 and loss is: 0.0003700366651173681\n",
      "Iteration is: 17802 and loss is: 0.00019903152133338153\n",
      "Iteration is: 17803 and loss is: 0.00018516826094128191\n",
      "Iteration is: 17804 and loss is: 0.00030344462720677257\n",
      "Iteration is: 17805 and loss is: 0.0003486741043161601\n",
      "Iteration is: 17806 and loss is: 0.00026558671379461884\n",
      "Iteration is: 17807 and loss is: 0.00016815410344861448\n",
      "Iteration is: 17808 and loss is: 0.00016441827756352723\n",
      "Iteration is: 17809 and loss is: 0.00019025366054847836\n",
      "Iteration is: 17810 and loss is: 0.0002032485353993252\n",
      "Iteration is: 17811 and loss is: 0.0002724356600083411\n",
      "Iteration is: 17812 and loss is: 0.00043914743582718074\n",
      "Iteration is: 17813 and loss is: 0.0006724470295011997\n",
      "Iteration is: 17814 and loss is: 0.0010571710299700499\n",
      "Iteration is: 17815 and loss is: 0.0013403415214270353\n",
      "Iteration is: 17816 and loss is: 0.0012092232936993241\n",
      "Iteration is: 17817 and loss is: 0.0007078954949975014\n",
      "Iteration is: 17818 and loss is: 0.0003159613406751305\n",
      "Iteration is: 17819 and loss is: 0.0003385180316399783\n",
      "Iteration is: 17820 and loss is: 0.0005322449142113328\n",
      "Iteration is: 17821 and loss is: 0.000573321245610714\n",
      "Iteration is: 17822 and loss is: 0.0003928302030544728\n",
      "Iteration is: 17823 and loss is: 0.0002247494412586093\n",
      "Iteration is: 17824 and loss is: 0.0003603774239309132\n",
      "Iteration is: 17825 and loss is: 0.00048723682994022965\n",
      "Iteration is: 17826 and loss is: 0.00031105882953852415\n",
      "Iteration is: 17827 and loss is: 0.00017782852228265256\n",
      "Iteration is: 17828 and loss is: 0.0002419646771159023\n",
      "Iteration is: 17829 and loss is: 0.00029004295356571674\n",
      "Iteration is: 17830 and loss is: 0.000261290610069409\n",
      "Iteration is: 17831 and loss is: 0.00028998410562053323\n",
      "Iteration is: 17832 and loss is: 0.00038952514296397567\n",
      "Iteration is: 17833 and loss is: 0.00046275745262391865\n",
      "Iteration is: 17834 and loss is: 0.0005077191744931042\n",
      "Iteration is: 17835 and loss is: 0.0006377511890605092\n",
      "Iteration is: 17836 and loss is: 0.0007036549504846334\n",
      "Iteration is: 17837 and loss is: 0.0005927197635173798\n",
      "Iteration is: 17838 and loss is: 0.0003487740468699485\n",
      "Iteration is: 17839 and loss is: 0.00016330202925018966\n",
      "Iteration is: 17840 and loss is: 0.00018759950762614608\n",
      "Iteration is: 17841 and loss is: 0.00033839233219623566\n",
      "Iteration is: 17842 and loss is: 0.0003994324360974133\n",
      "Iteration is: 17843 and loss is: 0.000301908963592723\n",
      "Iteration is: 17844 and loss is: 0.00017283909255638719\n",
      "Iteration is: 17845 and loss is: 0.0001483459200244397\n",
      "Iteration is: 17846 and loss is: 0.00022858372540213168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 17847 and loss is: 0.00029544634162448347\n",
      "Iteration is: 17848 and loss is: 0.00026614771923050284\n",
      "Iteration is: 17849 and loss is: 0.00018389888282399625\n",
      "Iteration is: 17850 and loss is: 0.00013483708607964218\n",
      "Iteration is: 17851 and loss is: 0.00015115682617761195\n",
      "Iteration is: 17852 and loss is: 0.00020017425413243473\n",
      "Iteration is: 17853 and loss is: 0.00023780064657330513\n",
      "Iteration is: 17854 and loss is: 0.0002513569488655776\n",
      "Iteration is: 17855 and loss is: 0.00025462114717811346\n",
      "Iteration is: 17856 and loss is: 0.0002643441839609295\n",
      "Iteration is: 17857 and loss is: 0.0002677009324543178\n",
      "Iteration is: 17858 and loss is: 0.00027703793602995574\n",
      "Iteration is: 17859 and loss is: 0.0002760792849585414\n",
      "Iteration is: 17860 and loss is: 0.0002706685627344996\n",
      "Iteration is: 17861 and loss is: 0.00024740968365222216\n",
      "Iteration is: 17862 and loss is: 0.0002147249469999224\n",
      "Iteration is: 17863 and loss is: 0.00017696915892884135\n",
      "Iteration is: 17864 and loss is: 0.00014626247866544873\n",
      "Iteration is: 17865 and loss is: 0.00012520256859716028\n",
      "Iteration is: 17866 and loss is: 0.0001178179809357971\n",
      "Iteration is: 17867 and loss is: 0.000123875419376418\n",
      "Iteration is: 17868 and loss is: 0.00014076163643039763\n",
      "Iteration is: 17869 and loss is: 0.00017007994756568223\n",
      "Iteration is: 17870 and loss is: 0.00022052557324059308\n",
      "Iteration is: 17871 and loss is: 0.00031977909384295344\n",
      "Iteration is: 17872 and loss is: 0.00048426882131025195\n",
      "Iteration is: 17873 and loss is: 0.0007587237050756812\n",
      "Iteration is: 17874 and loss is: 0.0009704379481263459\n",
      "Iteration is: 17875 and loss is: 0.000918124511372298\n",
      "Iteration is: 17876 and loss is: 0.0005640117451548576\n",
      "Iteration is: 17877 and loss is: 0.0002535698586143553\n",
      "Iteration is: 17878 and loss is: 0.0002352588635403663\n",
      "Iteration is: 17879 and loss is: 0.0003705259587150067\n",
      "Iteration is: 17880 and loss is: 0.000451497791800648\n",
      "Iteration is: 17881 and loss is: 0.0003928676596842706\n",
      "Iteration is: 17882 and loss is: 0.00022829274530522525\n",
      "Iteration is: 17883 and loss is: 0.00019086088286712766\n",
      "Iteration is: 17884 and loss is: 0.0003015664406120777\n",
      "Iteration is: 17885 and loss is: 0.00035123591078445315\n",
      "Iteration is: 17886 and loss is: 0.0002655116841197014\n",
      "Iteration is: 17887 and loss is: 0.0001638493122300133\n",
      "Iteration is: 17888 and loss is: 0.00017527336603961885\n",
      "Iteration is: 17889 and loss is: 0.0002040121762547642\n",
      "Iteration is: 17890 and loss is: 0.00018589317915029824\n",
      "Iteration is: 17891 and loss is: 0.000230003337492235\n",
      "Iteration is: 17892 and loss is: 0.0004240648413542658\n",
      "Iteration is: 17893 and loss is: 0.0007603100384585559\n",
      "Iteration is: 17894 and loss is: 0.0014213849790394306\n",
      "Iteration is: 17895 and loss is: 0.0021051650401204824\n",
      "Iteration is: 17896 and loss is: 0.0020739687606692314\n",
      "Iteration is: 17897 and loss is: 0.001137223094701767\n",
      "Iteration is: 17898 and loss is: 0.0005655630375258625\n",
      "Iteration is: 17899 and loss is: 0.0005988809280097485\n",
      "Iteration is: 17900 and loss is: 0.0007611130713485181\n",
      "Iteration is: 17901 and loss is: 0.0008221690659411252\n",
      "Iteration is: 17902 and loss is: 0.0005767723778262734\n",
      "Iteration is: 17903 and loss is: 0.0005189264775253832\n",
      "Iteration is: 17904 and loss is: 0.0005532422801479697\n",
      "Iteration is: 17905 and loss is: 0.0005036885850131512\n",
      "Iteration is: 17906 and loss is: 0.00038876692997291684\n",
      "Iteration is: 17907 and loss is: 0.0002533216029405594\n",
      "Iteration is: 17908 and loss is: 0.00039576765266247094\n",
      "Iteration is: 17909 and loss is: 0.0003942448238376528\n",
      "Iteration is: 17910 and loss is: 0.00022297278337646276\n",
      "Iteration is: 17911 and loss is: 0.0003048913204111159\n",
      "Iteration is: 17912 and loss is: 0.00036536704283207655\n",
      "Iteration is: 17913 and loss is: 0.00031502253841608763\n",
      "Iteration is: 17914 and loss is: 0.00041832009446807206\n",
      "Iteration is: 17915 and loss is: 0.0006486066267825663\n",
      "Iteration is: 17916 and loss is: 0.0010220271069556475\n",
      "Iteration is: 17917 and loss is: 0.0015962528996169567\n",
      "Iteration is: 17918 and loss is: 0.0017113970825448632\n",
      "Iteration is: 17919 and loss is: 0.0010037901811301708\n",
      "Iteration is: 17920 and loss is: 0.0002888722228817642\n",
      "Iteration is: 17921 and loss is: 0.0006128530949354172\n",
      "Iteration is: 17922 and loss is: 0.0006665592081844807\n",
      "Iteration is: 17923 and loss is: 0.000515592924784869\n",
      "Iteration is: 17924 and loss is: 0.000609814771451056\n",
      "Iteration is: 17925 and loss is: 0.0004173368215560913\n",
      "Iteration is: 17926 and loss is: 0.0004868484102189541\n",
      "Iteration is: 17927 and loss is: 0.0004516809422057122\n",
      "Iteration is: 17928 and loss is: 0.0003193323500454426\n",
      "Iteration is: 17929 and loss is: 0.00032684989855624735\n",
      "Iteration is: 17930 and loss is: 0.00033330562291666865\n",
      "Iteration is: 17931 and loss is: 0.00030618906021118164\n",
      "Iteration is: 17932 and loss is: 0.00021974036644678563\n",
      "Iteration is: 17933 and loss is: 0.0002993927919305861\n",
      "Iteration is: 17934 and loss is: 0.00031736260280013084\n",
      "Iteration is: 17935 and loss is: 0.00018106724019162357\n",
      "Iteration is: 17936 and loss is: 0.00022147211711853743\n",
      "Iteration is: 17937 and loss is: 0.0002596834092400968\n",
      "Iteration is: 17938 and loss is: 0.00017094046052079648\n",
      "Iteration is: 17939 and loss is: 0.00017733508138917387\n",
      "Iteration is: 17940 and loss is: 0.00021192035637795925\n",
      "Iteration is: 17941 and loss is: 0.00017139996634796262\n",
      "Iteration is: 17942 and loss is: 0.00016644358402118087\n",
      "Iteration is: 17943 and loss is: 0.00018971471581608057\n",
      "Iteration is: 17944 and loss is: 0.00018219032790511847\n",
      "Iteration is: 17945 and loss is: 0.00017536681843921542\n",
      "Iteration is: 17946 and loss is: 0.0001588179438840598\n",
      "Iteration is: 17947 and loss is: 0.00015746147255413234\n",
      "Iteration is: 17948 and loss is: 0.0001619564718566835\n",
      "Iteration is: 17949 and loss is: 0.00013222621055319905\n",
      "Iteration is: 17950 and loss is: 0.0001335978740826249\n",
      "Iteration is: 17951 and loss is: 0.00015355058712884784\n",
      "Iteration is: 17952 and loss is: 0.0001349918165942654\n",
      "Iteration is: 17953 and loss is: 0.0001256331306649372\n",
      "Iteration is: 17954 and loss is: 0.0001424659276381135\n",
      "Iteration is: 17955 and loss is: 0.0001382282644044608\n",
      "Iteration is: 17956 and loss is: 0.00012776406947523355\n",
      "Iteration is: 17957 and loss is: 0.00014100869884714484\n",
      "Iteration is: 17958 and loss is: 0.00015261411317624152\n",
      "Iteration is: 17959 and loss is: 0.00016238284297287464\n",
      "Iteration is: 17960 and loss is: 0.00018370168982073665\n",
      "Iteration is: 17961 and loss is: 0.00021242524962872267\n",
      "Iteration is: 17962 and loss is: 0.00026048990548588336\n",
      "Iteration is: 17963 and loss is: 0.0003167387912981212\n",
      "Iteration is: 17964 and loss is: 0.00040059146704152226\n",
      "Iteration is: 17965 and loss is: 0.0004907025722786784\n",
      "Iteration is: 17966 and loss is: 0.0005943608703091741\n",
      "Iteration is: 17967 and loss is: 0.0006047649658285081\n",
      "Iteration is: 17968 and loss is: 0.0004976748023182154\n",
      "Iteration is: 17969 and loss is: 0.0003106995136477053\n",
      "Iteration is: 17970 and loss is: 0.0001619483227841556\n",
      "Iteration is: 17971 and loss is: 0.00015448604244738817\n",
      "Iteration is: 17972 and loss is: 0.0002533952356316149\n",
      "Iteration is: 17973 and loss is: 0.0003294268390163779\n",
      "Iteration is: 17974 and loss is: 0.0003019269206561148\n",
      "Iteration is: 17975 and loss is: 0.0002108716289512813\n",
      "Iteration is: 17976 and loss is: 0.00014344387454912066\n",
      "Iteration is: 17977 and loss is: 0.00014849909348413348\n",
      "Iteration is: 17978 and loss is: 0.000206799668376334\n",
      "Iteration is: 17979 and loss is: 0.0002511678612791002\n",
      "Iteration is: 17980 and loss is: 0.000247143761953339\n",
      "Iteration is: 17981 and loss is: 0.00021994969574734569\n",
      "Iteration is: 17982 and loss is: 0.00019299614359624684\n",
      "Iteration is: 17983 and loss is: 0.0001735956611810252\n",
      "Iteration is: 17984 and loss is: 0.00016195066564250737\n",
      "Iteration is: 17985 and loss is: 0.00016002364282030612\n",
      "Iteration is: 17986 and loss is: 0.00017267055227421224\n",
      "Iteration is: 17987 and loss is: 0.00019855053687933832\n",
      "Iteration is: 17988 and loss is: 0.0002524328010622412\n",
      "Iteration is: 17989 and loss is: 0.0003477106220088899\n",
      "Iteration is: 17990 and loss is: 0.0005222120671533048\n",
      "Iteration is: 17991 and loss is: 0.0007061233045533299\n",
      "Iteration is: 17992 and loss is: 0.000827471842058003\n",
      "Iteration is: 17993 and loss is: 0.0007305387407541275\n",
      "Iteration is: 17994 and loss is: 0.00044579501263797283\n",
      "Iteration is: 17995 and loss is: 0.00021236081374809146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 17996 and loss is: 0.00020206997578497976\n",
      "Iteration is: 17997 and loss is: 0.0003466206544544548\n",
      "Iteration is: 17998 and loss is: 0.00043527514208108187\n",
      "Iteration is: 17999 and loss is: 0.0003387646866030991\n",
      "Iteration is: 18000 and loss is: 0.00019351494847796857\n",
      "Iteration is: 18001 and loss is: 0.0001628937607165426\n",
      "Iteration is: 18002 and loss is: 0.0002550236531533301\n",
      "Iteration is: 18003 and loss is: 0.0003302490513306111\n",
      "Iteration is: 18004 and loss is: 0.0002782308147288859\n",
      "Iteration is: 18005 and loss is: 0.00018953454855363816\n",
      "Iteration is: 18006 and loss is: 0.00015386650920845568\n",
      "Iteration is: 18007 and loss is: 0.0001554526388645172\n",
      "Iteration is: 18008 and loss is: 0.00016578026406932622\n",
      "Iteration is: 18009 and loss is: 0.00021254875173326582\n",
      "Iteration is: 18010 and loss is: 0.00034511066041886806\n",
      "Iteration is: 18011 and loss is: 0.0006404264131560922\n",
      "Iteration is: 18012 and loss is: 0.00114183290861547\n",
      "Iteration is: 18013 and loss is: 0.001969059929251671\n",
      "Iteration is: 18014 and loss is: 0.00219932128675282\n",
      "Iteration is: 18015 and loss is: 0.0013452090788632631\n",
      "Iteration is: 18016 and loss is: 0.0007936395704746246\n",
      "Iteration is: 18017 and loss is: 0.0009330340544693172\n",
      "Iteration is: 18018 and loss is: 0.0010332649108022451\n",
      "Iteration is: 18019 and loss is: 0.0011981501011177897\n",
      "Iteration is: 18020 and loss is: 0.0005704056238755584\n",
      "Iteration is: 18021 and loss is: 0.0008696714648976922\n",
      "Iteration is: 18022 and loss is: 0.000928581808693707\n",
      "Iteration is: 18023 and loss is: 0.0005209543742239475\n",
      "Iteration is: 18024 and loss is: 0.0005802372470498085\n",
      "Iteration is: 18025 and loss is: 0.000489504134748131\n",
      "Iteration is: 18026 and loss is: 0.0005618235445581377\n",
      "Iteration is: 18027 and loss is: 0.0008418641518801451\n",
      "Iteration is: 18028 and loss is: 0.0004138106887694448\n",
      "Iteration is: 18029 and loss is: 0.0005680043832398951\n",
      "Iteration is: 18030 and loss is: 0.0007870481931604445\n",
      "Iteration is: 18031 and loss is: 0.0008534614462405443\n",
      "Iteration is: 18032 and loss is: 0.0013186177238821983\n",
      "Iteration is: 18033 and loss is: 0.0017060585087165236\n",
      "Iteration is: 18034 and loss is: 0.0014636230189353228\n",
      "Iteration is: 18035 and loss is: 0.0007971649174578488\n",
      "Iteration is: 18036 and loss is: 0.0004460086638573557\n",
      "Iteration is: 18037 and loss is: 0.0007093227468430996\n",
      "Iteration is: 18038 and loss is: 0.001079445006325841\n",
      "Iteration is: 18039 and loss is: 0.0004658762481994927\n",
      "Iteration is: 18040 and loss is: 0.0006663491949439049\n",
      "Iteration is: 18041 and loss is: 0.0008522431016899645\n",
      "Iteration is: 18042 and loss is: 0.0004908073460683227\n",
      "Iteration is: 18043 and loss is: 0.000462852418422699\n",
      "Iteration is: 18044 and loss is: 0.0006311887409538031\n",
      "Iteration is: 18045 and loss is: 0.0004982682876288891\n",
      "Iteration is: 18046 and loss is: 0.0003014904214069247\n",
      "Iteration is: 18047 and loss is: 0.00036251911660656333\n",
      "Iteration is: 18048 and loss is: 0.0005463652196340263\n",
      "Iteration is: 18049 and loss is: 0.00044416426680982113\n",
      "Iteration is: 18050 and loss is: 0.00018345567514188588\n",
      "Iteration is: 18051 and loss is: 0.0002323367225471884\n",
      "Iteration is: 18052 and loss is: 0.00035316188586875796\n",
      "Iteration is: 18053 and loss is: 0.00042174209374934435\n",
      "Iteration is: 18054 and loss is: 0.0005928749451413751\n",
      "Iteration is: 18055 and loss is: 0.0011169201461598277\n",
      "Iteration is: 18056 and loss is: 0.0016040707705542445\n",
      "Iteration is: 18057 and loss is: 0.001766260014846921\n",
      "Iteration is: 18058 and loss is: 0.000983246136456728\n",
      "Iteration is: 18059 and loss is: 0.0006516074645332992\n",
      "Iteration is: 18060 and loss is: 0.000787430617492646\n",
      "Iteration is: 18061 and loss is: 0.0006294177146628499\n",
      "Iteration is: 18062 and loss is: 0.0007626320002600551\n",
      "Iteration is: 18063 and loss is: 0.0007187793962657452\n",
      "Iteration is: 18064 and loss is: 0.0007682491559535265\n",
      "Iteration is: 18065 and loss is: 0.0004177744558546692\n",
      "Iteration is: 18066 and loss is: 0.00048079714179039\n",
      "Iteration is: 18067 and loss is: 0.0004782353062182665\n",
      "Iteration is: 18068 and loss is: 0.00031151287839747965\n",
      "Iteration is: 18069 and loss is: 0.0003069996018894017\n",
      "Iteration is: 18070 and loss is: 0.00032033282332122326\n",
      "Iteration is: 18071 and loss is: 0.0002460037067066878\n",
      "Iteration is: 18072 and loss is: 0.00039105722680687904\n",
      "Iteration is: 18073 and loss is: 0.00035329366801306605\n",
      "Iteration is: 18074 and loss is: 0.0003184442757628858\n",
      "Iteration is: 18075 and loss is: 0.00045032132766209543\n",
      "Iteration is: 18076 and loss is: 0.0006824242300353944\n",
      "Iteration is: 18077 and loss is: 0.0010916827013716102\n",
      "Iteration is: 18078 and loss is: 0.0017389404820278287\n",
      "Iteration is: 18079 and loss is: 0.0021693166345357895\n",
      "Iteration is: 18080 and loss is: 0.001148357754573226\n",
      "Iteration is: 18081 and loss is: 0.000641771825030446\n",
      "Iteration is: 18082 and loss is: 0.0012104215566068888\n",
      "Iteration is: 18083 and loss is: 0.0012401785934343934\n",
      "Iteration is: 18084 and loss is: 0.0005802511004731059\n",
      "Iteration is: 18085 and loss is: 0.0009080393938347697\n",
      "Iteration is: 18086 and loss is: 0.0012268911814317107\n",
      "Iteration is: 18087 and loss is: 0.0005566623294726014\n",
      "Iteration is: 18088 and loss is: 0.0004992549074813724\n",
      "Iteration is: 18089 and loss is: 0.0008210086380131543\n",
      "Iteration is: 18090 and loss is: 0.00037579837953671813\n",
      "Iteration is: 18091 and loss is: 0.0002984973543789238\n",
      "Iteration is: 18092 and loss is: 0.0006558132590726018\n",
      "Iteration is: 18093 and loss is: 0.0006080575403757393\n",
      "Iteration is: 18094 and loss is: 0.0004909014096483588\n",
      "Iteration is: 18095 and loss is: 0.00033040999551303685\n",
      "Iteration is: 18096 and loss is: 0.00020710188255179673\n",
      "Iteration is: 18097 and loss is: 0.0002320731000509113\n",
      "Iteration is: 18098 and loss is: 0.0002122288424288854\n",
      "Iteration is: 18099 and loss is: 0.00024053509696386755\n",
      "Iteration is: 18100 and loss is: 0.00033550377702340484\n",
      "Iteration is: 18101 and loss is: 0.00032780427136458457\n",
      "Iteration is: 18102 and loss is: 0.0003651542356237769\n",
      "Iteration is: 18103 and loss is: 0.00034244987182319164\n",
      "Iteration is: 18104 and loss is: 0.0002287376846652478\n",
      "Iteration is: 18105 and loss is: 0.0001897809561342001\n",
      "Iteration is: 18106 and loss is: 0.0001791513932403177\n",
      "Iteration is: 18107 and loss is: 0.00021588434174191207\n",
      "Iteration is: 18108 and loss is: 0.0002428623556625098\n",
      "Iteration is: 18109 and loss is: 0.00019753158267121762\n",
      "Iteration is: 18110 and loss is: 0.00016831480024848133\n",
      "Iteration is: 18111 and loss is: 0.0001528888096800074\n",
      "Iteration is: 18112 and loss is: 0.0001507497509010136\n",
      "Iteration is: 18113 and loss is: 0.00019142866949550807\n",
      "Iteration is: 18114 and loss is: 0.00021880156418774277\n",
      "Iteration is: 18115 and loss is: 0.00022571739100385457\n",
      "Iteration is: 18116 and loss is: 0.00023542006965726614\n",
      "Iteration is: 18117 and loss is: 0.00021105562336742878\n",
      "Iteration is: 18118 and loss is: 0.00018284283578395844\n",
      "Iteration is: 18119 and loss is: 0.00015789896133355796\n",
      "Iteration is: 18120 and loss is: 0.00013129621220286936\n",
      "Iteration is: 18121 and loss is: 0.00012791651533916593\n",
      "Iteration is: 18122 and loss is: 0.00013460156333167106\n",
      "Iteration is: 18123 and loss is: 0.0001455759775126353\n",
      "Iteration is: 18124 and loss is: 0.00016525181126780808\n",
      "Iteration is: 18125 and loss is: 0.0001854744041338563\n",
      "Iteration is: 18126 and loss is: 0.00021455110982060432\n",
      "Iteration is: 18127 and loss is: 0.0002742705401033163\n",
      "Iteration is: 18128 and loss is: 0.00036170968087390065\n",
      "Iteration is: 18129 and loss is: 0.0005141557194292545\n",
      "Iteration is: 18130 and loss is: 0.0006571438862010837\n",
      "Iteration is: 18131 and loss is: 0.000720823707524687\n",
      "Iteration is: 18132 and loss is: 0.0005548124318011105\n",
      "Iteration is: 18133 and loss is: 0.00026730404351837933\n",
      "Iteration is: 18134 and loss is: 0.0001560581149533391\n",
      "Iteration is: 18135 and loss is: 0.0002752606524154544\n",
      "Iteration is: 18136 and loss is: 0.00038525229319930077\n",
      "Iteration is: 18137 and loss is: 0.0003075192216783762\n",
      "Iteration is: 18138 and loss is: 0.00017399186617694795\n",
      "Iteration is: 18139 and loss is: 0.0001883162622107193\n",
      "Iteration is: 18140 and loss is: 0.00028456709696911275\n",
      "Iteration is: 18141 and loss is: 0.0002782853553071618\n",
      "Iteration is: 18142 and loss is: 0.00018169486429542303\n",
      "Iteration is: 18143 and loss is: 0.00014093436766415834\n",
      "Iteration is: 18144 and loss is: 0.00019400834571570158\n",
      "Iteration is: 18145 and loss is: 0.00024327344726771116\n",
      "Iteration is: 18146 and loss is: 0.00021895591635257006\n",
      "Iteration is: 18147 and loss is: 0.00017098819080274552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 18148 and loss is: 0.00013982231030240655\n",
      "Iteration is: 18149 and loss is: 0.0001293093664571643\n",
      "Iteration is: 18150 and loss is: 0.00015024287858977914\n",
      "Iteration is: 18151 and loss is: 0.00020831577421631664\n",
      "Iteration is: 18152 and loss is: 0.000307524052914232\n",
      "Iteration is: 18153 and loss is: 0.0005015217466279864\n",
      "Iteration is: 18154 and loss is: 0.0008160399738699198\n",
      "Iteration is: 18155 and loss is: 0.0013101693475618958\n",
      "Iteration is: 18156 and loss is: 0.0014792407164350152\n",
      "Iteration is: 18157 and loss is: 0.0009395134402438998\n",
      "Iteration is: 18158 and loss is: 0.00030682486249133945\n",
      "Iteration is: 18159 and loss is: 0.00042315933387726545\n",
      "Iteration is: 18160 and loss is: 0.0007084257085807621\n",
      "Iteration is: 18161 and loss is: 0.0005583933088928461\n",
      "Iteration is: 18162 and loss is: 0.0003633963642641902\n",
      "Iteration is: 18163 and loss is: 0.0004286363546270877\n",
      "Iteration is: 18164 and loss is: 0.0005016418872401118\n",
      "Iteration is: 18165 and loss is: 0.0003604427329264581\n",
      "Iteration is: 18166 and loss is: 0.0002827274729497731\n",
      "Iteration is: 18167 and loss is: 0.00030884609441272914\n",
      "Iteration is: 18168 and loss is: 0.00036109471693634987\n",
      "Iteration is: 18169 and loss is: 0.00029036609339527786\n",
      "Iteration is: 18170 and loss is: 0.00015343815903179348\n",
      "Iteration is: 18171 and loss is: 0.0002539350534789264\n",
      "Iteration is: 18172 and loss is: 0.000304416345898062\n",
      "Iteration is: 18173 and loss is: 0.00023799960035830736\n",
      "Iteration is: 18174 and loss is: 0.0003048646613024175\n",
      "Iteration is: 18175 and loss is: 0.0005302627105265856\n",
      "Iteration is: 18176 and loss is: 0.0007667712634429336\n",
      "Iteration is: 18177 and loss is: 0.0012119616148993373\n",
      "Iteration is: 18178 and loss is: 0.0016031451523303986\n",
      "Iteration is: 18179 and loss is: 0.0012782582780346274\n",
      "Iteration is: 18180 and loss is: 0.0005243357736617327\n",
      "Iteration is: 18181 and loss is: 0.0003450970980338752\n",
      "Iteration is: 18182 and loss is: 0.0007558929501101375\n",
      "Iteration is: 18183 and loss is: 0.0007591235917061567\n",
      "Iteration is: 18184 and loss is: 0.0003882379096467048\n",
      "Iteration is: 18185 and loss is: 0.000513718812726438\n",
      "Iteration is: 18186 and loss is: 0.000653186347335577\n",
      "Iteration is: 18187 and loss is: 0.00036878592800348997\n",
      "Iteration is: 18188 and loss is: 0.00031347127514891326\n",
      "Iteration is: 18189 and loss is: 0.00047120958333835006\n",
      "Iteration is: 18190 and loss is: 0.00038908811984583735\n",
      "Iteration is: 18191 and loss is: 0.0002480897237546742\n",
      "Iteration is: 18192 and loss is: 0.0002156099653802812\n",
      "Iteration is: 18193 and loss is: 0.00036502760485745966\n",
      "Iteration is: 18194 and loss is: 0.00035424716770648956\n",
      "Iteration is: 18195 and loss is: 0.0003522619081195444\n",
      "Iteration is: 18196 and loss is: 0.0006148252869024873\n",
      "Iteration is: 18197 and loss is: 0.0009539318270981312\n",
      "Iteration is: 18198 and loss is: 0.0012688892893493176\n",
      "Iteration is: 18199 and loss is: 0.001545147504657507\n",
      "Iteration is: 18200 and loss is: 0.0011307008098810911\n",
      "Iteration is: 18201 and loss is: 0.00041672633960843086\n",
      "Iteration is: 18202 and loss is: 0.0005709828110411763\n",
      "Iteration is: 18203 and loss is: 0.0006561859045177698\n",
      "Iteration is: 18204 and loss is: 0.0006804520962759852\n",
      "Iteration is: 18205 and loss is: 0.0004255103995092213\n",
      "Iteration is: 18206 and loss is: 0.0005231425748206675\n",
      "Iteration is: 18207 and loss is: 0.0005146907060407102\n",
      "Iteration is: 18208 and loss is: 0.0003446262562647462\n",
      "Iteration is: 18209 and loss is: 0.0003560828627087176\n",
      "Iteration is: 18210 and loss is: 0.0003433736856095493\n",
      "Iteration is: 18211 and loss is: 0.0002819095680024475\n",
      "Iteration is: 18212 and loss is: 0.0002387062122579664\n",
      "Iteration is: 18213 and loss is: 0.0002254353166790679\n",
      "Iteration is: 18214 and loss is: 0.00029344853828661144\n",
      "Iteration is: 18215 and loss is: 0.0002522434515412897\n",
      "Iteration is: 18216 and loss is: 0.000255676859524101\n",
      "Iteration is: 18217 and loss is: 0.00040016116690821946\n",
      "Iteration is: 18218 and loss is: 0.00040216819616034627\n",
      "Iteration is: 18219 and loss is: 0.0006019288557581604\n",
      "Iteration is: 18220 and loss is: 0.0008128746412694454\n",
      "Iteration is: 18221 and loss is: 0.0010101101361215115\n",
      "Iteration is: 18222 and loss is: 0.0008169406210072339\n",
      "Iteration is: 18223 and loss is: 0.00033976114355027676\n",
      "Iteration is: 18224 and loss is: 0.00021685310639441013\n",
      "Iteration is: 18225 and loss is: 0.00047609920147806406\n",
      "Iteration is: 18226 and loss is: 0.0005052319029346108\n",
      "Iteration is: 18227 and loss is: 0.00022372236708179116\n",
      "Iteration is: 18228 and loss is: 0.00029330438701435924\n",
      "Iteration is: 18229 and loss is: 0.00040349317714571953\n",
      "Iteration is: 18230 and loss is: 0.0003094379208050668\n",
      "Iteration is: 18231 and loss is: 0.00020947505254298449\n",
      "Iteration is: 18232 and loss is: 0.0002788066631183028\n",
      "Iteration is: 18233 and loss is: 0.00031062925700098276\n",
      "Iteration is: 18234 and loss is: 0.00022337387781590223\n",
      "Iteration is: 18235 and loss is: 0.00016185615095309913\n",
      "Iteration is: 18236 and loss is: 0.00022436281142290682\n",
      "Iteration is: 18237 and loss is: 0.00028239120729267597\n",
      "Iteration is: 18238 and loss is: 0.00021711796580348164\n",
      "Iteration is: 18239 and loss is: 0.00016175238124560565\n",
      "Iteration is: 18240 and loss is: 0.0001587600272614509\n",
      "Iteration is: 18241 and loss is: 0.00015781036927364767\n",
      "Iteration is: 18242 and loss is: 0.0001560467208037153\n",
      "Iteration is: 18243 and loss is: 0.00020725281501654536\n",
      "Iteration is: 18244 and loss is: 0.00030683603836223483\n",
      "Iteration is: 18245 and loss is: 0.0004287178162485361\n",
      "Iteration is: 18246 and loss is: 0.0006779817631468177\n",
      "Iteration is: 18247 and loss is: 0.0009289523586630821\n",
      "Iteration is: 18248 and loss is: 0.0009745208080857992\n",
      "Iteration is: 18249 and loss is: 0.0005735056474804878\n",
      "Iteration is: 18250 and loss is: 0.00019927285029552877\n",
      "Iteration is: 18251 and loss is: 0.00032027706038206816\n",
      "Iteration is: 18252 and loss is: 0.0004921764484606683\n",
      "Iteration is: 18253 and loss is: 0.00035432871663942933\n",
      "Iteration is: 18254 and loss is: 0.00023620660067535937\n",
      "Iteration is: 18255 and loss is: 0.00033229991095140576\n",
      "Iteration is: 18256 and loss is: 0.00036948948400095105\n",
      "Iteration is: 18257 and loss is: 0.00023994332877919078\n",
      "Iteration is: 18258 and loss is: 0.0002082345017697662\n",
      "Iteration is: 18259 and loss is: 0.00028594734612852335\n",
      "Iteration is: 18260 and loss is: 0.00027017632964998484\n",
      "Iteration is: 18261 and loss is: 0.0002003937552217394\n",
      "Iteration is: 18262 and loss is: 0.00016559939831495285\n",
      "Iteration is: 18263 and loss is: 0.00020522461272776127\n",
      "Iteration is: 18264 and loss is: 0.00025199883384630084\n",
      "Iteration is: 18265 and loss is: 0.00024937852867878973\n",
      "Iteration is: 18266 and loss is: 0.00028726906748488545\n",
      "Iteration is: 18267 and loss is: 0.00037956482265144587\n",
      "Iteration is: 18268 and loss is: 0.000507933262269944\n",
      "Iteration is: 18269 and loss is: 0.0006744567072018981\n",
      "Iteration is: 18270 and loss is: 0.0009517662692815065\n",
      "Iteration is: 18271 and loss is: 0.0009506687056273222\n",
      "Iteration is: 18272 and loss is: 0.0005637638969346881\n",
      "Iteration is: 18273 and loss is: 0.00022307102335616946\n",
      "Iteration is: 18274 and loss is: 0.0002878560917451978\n",
      "Iteration is: 18275 and loss is: 0.0005243757623247802\n",
      "Iteration is: 18276 and loss is: 0.0004097418859601021\n",
      "Iteration is: 18277 and loss is: 0.0002078231773339212\n",
      "Iteration is: 18278 and loss is: 0.00030841195257380605\n",
      "Iteration is: 18279 and loss is: 0.0004154457710683346\n",
      "Iteration is: 18280 and loss is: 0.0002656980650499463\n",
      "Iteration is: 18281 and loss is: 0.00017778831534087658\n",
      "Iteration is: 18282 and loss is: 0.00025517650647088885\n",
      "Iteration is: 18283 and loss is: 0.00032254119287244976\n",
      "Iteration is: 18284 and loss is: 0.0002692416019272059\n",
      "Iteration is: 18285 and loss is: 0.0001557600626256317\n",
      "Iteration is: 18286 and loss is: 0.00016757039702497423\n",
      "Iteration is: 18287 and loss is: 0.00021912765805609524\n",
      "Iteration is: 18288 and loss is: 0.0002669252862688154\n",
      "Iteration is: 18289 and loss is: 0.00034042171319015324\n",
      "Iteration is: 18290 and loss is: 0.00048076669918373227\n",
      "Iteration is: 18291 and loss is: 0.0006368662579916418\n",
      "Iteration is: 18292 and loss is: 0.0008274755091406405\n",
      "Iteration is: 18293 and loss is: 0.0008646517526358366\n",
      "Iteration is: 18294 and loss is: 0.000605835928581655\n",
      "Iteration is: 18295 and loss is: 0.0002585975453257561\n",
      "Iteration is: 18296 and loss is: 0.00019605847774073482\n",
      "Iteration is: 18297 and loss is: 0.00039981200825423\n",
      "Iteration is: 18298 and loss is: 0.0004323098692111671\n",
      "Iteration is: 18299 and loss is: 0.00023546887678094208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 18300 and loss is: 0.00019285790040157735\n",
      "Iteration is: 18301 and loss is: 0.0003215311444364488\n",
      "Iteration is: 18302 and loss is: 0.0003282350953668356\n",
      "Iteration is: 18303 and loss is: 0.00018925238691736013\n",
      "Iteration is: 18304 and loss is: 0.00015767385775689036\n",
      "Iteration is: 18305 and loss is: 0.0002449117600917816\n",
      "Iteration is: 18306 and loss is: 0.00028656094218604267\n",
      "Iteration is: 18307 and loss is: 0.0002209920930908993\n",
      "Iteration is: 18308 and loss is: 0.0001609760947758332\n",
      "Iteration is: 18309 and loss is: 0.00015096882998477668\n",
      "Iteration is: 18310 and loss is: 0.00016112685261759907\n",
      "Iteration is: 18311 and loss is: 0.00020967049931641668\n",
      "Iteration is: 18312 and loss is: 0.0003087044751737267\n",
      "Iteration is: 18313 and loss is: 0.0004517274210229516\n",
      "Iteration is: 18314 and loss is: 0.0006941432366147637\n",
      "Iteration is: 18315 and loss is: 0.0009458600543439388\n",
      "Iteration is: 18316 and loss is: 0.001085273688659072\n",
      "Iteration is: 18317 and loss is: 0.0007599998498335481\n",
      "Iteration is: 18318 and loss is: 0.0002847755386028439\n",
      "Iteration is: 18319 and loss is: 0.0002682702033780515\n",
      "Iteration is: 18320 and loss is: 0.000502520299050957\n",
      "Iteration is: 18321 and loss is: 0.000460100534837693\n",
      "Iteration is: 18322 and loss is: 0.0002866848953999579\n",
      "Iteration is: 18323 and loss is: 0.00029643886955454946\n",
      "Iteration is: 18324 and loss is: 0.0004038567130919546\n",
      "Iteration is: 18325 and loss is: 0.0003476569545455277\n",
      "Iteration is: 18326 and loss is: 0.000208871832001023\n",
      "Iteration is: 18327 and loss is: 0.00026127559249289334\n",
      "Iteration is: 18328 and loss is: 0.0003184145607519895\n",
      "Iteration is: 18329 and loss is: 0.00026512445765547454\n",
      "Iteration is: 18330 and loss is: 0.0001877512113424018\n",
      "Iteration is: 18331 and loss is: 0.0001698462583590299\n",
      "Iteration is: 18332 and loss is: 0.00023749937827233225\n",
      "Iteration is: 18333 and loss is: 0.0002555924584157765\n",
      "Iteration is: 18334 and loss is: 0.0002894889621529728\n",
      "Iteration is: 18335 and loss is: 0.000442826742073521\n",
      "Iteration is: 18336 and loss is: 0.0007400832837447524\n",
      "Iteration is: 18337 and loss is: 0.0010900633642449975\n",
      "Iteration is: 18338 and loss is: 0.0015360201941803098\n",
      "Iteration is: 18339 and loss is: 0.0013213467318564653\n",
      "Iteration is: 18340 and loss is: 0.0005342545337043703\n",
      "Iteration is: 18341 and loss is: 0.00048626342322677374\n",
      "Iteration is: 18342 and loss is: 0.0006769462488591671\n",
      "Iteration is: 18343 and loss is: 0.0006783889839425683\n",
      "Iteration is: 18344 and loss is: 0.000568590359762311\n",
      "Iteration is: 18345 and loss is: 0.00044950618757866323\n",
      "Iteration is: 18346 and loss is: 0.0005942444549873471\n",
      "Iteration is: 18347 and loss is: 0.0004411099653225392\n",
      "Iteration is: 18348 and loss is: 0.00033404864370822906\n",
      "Iteration is: 18349 and loss is: 0.0003921911120414734\n",
      "Iteration is: 18350 and loss is: 0.00035561705590225756\n",
      "Iteration is: 18351 and loss is: 0.00034860041341744363\n",
      "Iteration is: 18352 and loss is: 0.0002563303569331765\n",
      "Iteration is: 18353 and loss is: 0.0002220609167125076\n",
      "Iteration is: 18354 and loss is: 0.00035971973557025194\n",
      "Iteration is: 18355 and loss is: 0.00021473478409461677\n",
      "Iteration is: 18356 and loss is: 0.0003021497104782611\n",
      "Iteration is: 18357 and loss is: 0.00048690594849176705\n",
      "Iteration is: 18358 and loss is: 0.0006552399718202651\n",
      "Iteration is: 18359 and loss is: 0.0009957600850611925\n",
      "Iteration is: 18360 and loss is: 0.001357708708383143\n",
      "Iteration is: 18361 and loss is: 0.0011331066489219666\n",
      "Iteration is: 18362 and loss is: 0.00047809851821511984\n",
      "Iteration is: 18363 and loss is: 0.0004053737211506814\n",
      "Iteration is: 18364 and loss is: 0.0006227241829037666\n",
      "Iteration is: 18365 and loss is: 0.0006484550540335476\n",
      "Iteration is: 18366 and loss is: 0.0003633687156252563\n",
      "Iteration is: 18367 and loss is: 0.0004434561124071479\n",
      "Iteration is: 18368 and loss is: 0.0005022273398935795\n",
      "Iteration is: 18369 and loss is: 0.0003400240675546229\n",
      "Iteration is: 18370 and loss is: 0.00028272351482883096\n",
      "Iteration is: 18371 and loss is: 0.0003467281931079924\n",
      "Iteration is: 18372 and loss is: 0.00032460904913023114\n",
      "Iteration is: 18373 and loss is: 0.000265972368652001\n",
      "Iteration is: 18374 and loss is: 0.00018996649305336177\n",
      "Iteration is: 18375 and loss is: 0.0002472707419656217\n",
      "Iteration is: 18376 and loss is: 0.00032869676942937076\n",
      "Iteration is: 18377 and loss is: 0.0002846794086508453\n",
      "Iteration is: 18378 and loss is: 0.0004719923890661448\n",
      "Iteration is: 18379 and loss is: 0.0007166051072999835\n",
      "Iteration is: 18380 and loss is: 0.0010810329113155603\n",
      "Iteration is: 18381 and loss is: 0.0013642575358971953\n",
      "Iteration is: 18382 and loss is: 0.0011761588975787163\n",
      "Iteration is: 18383 and loss is: 0.0005416357307694852\n",
      "Iteration is: 18384 and loss is: 0.00033068700577132404\n",
      "Iteration is: 18385 and loss is: 0.0005951914936304092\n",
      "Iteration is: 18386 and loss is: 0.0006274438928812742\n",
      "Iteration is: 18387 and loss is: 0.00039790174923837185\n",
      "Iteration is: 18388 and loss is: 0.00042249338002875447\n",
      "Iteration is: 18389 and loss is: 0.0005525611923076212\n",
      "Iteration is: 18390 and loss is: 0.0002891026670113206\n",
      "Iteration is: 18391 and loss is: 0.0003323306445963681\n",
      "Iteration is: 18392 and loss is: 0.00037437761784531176\n",
      "Iteration is: 18393 and loss is: 0.0002886560105253011\n",
      "Iteration is: 18394 and loss is: 0.0002484644646756351\n",
      "Iteration is: 18395 and loss is: 0.00021652245777659118\n",
      "Iteration is: 18396 and loss is: 0.0003038808354176581\n",
      "Iteration is: 18397 and loss is: 0.00035352271515876055\n",
      "Iteration is: 18398 and loss is: 0.00033185817301273346\n",
      "Iteration is: 18399 and loss is: 0.0005494700744748116\n",
      "Iteration is: 18400 and loss is: 0.0008241623290814459\n",
      "Iteration is: 18401 and loss is: 0.001046531368046999\n",
      "Iteration is: 18402 and loss is: 0.0012917285785079002\n",
      "Iteration is: 18403 and loss is: 0.0009186692186631262\n",
      "Iteration is: 18404 and loss is: 0.00035286429920233786\n",
      "Iteration is: 18405 and loss is: 0.00046589155681431293\n",
      "Iteration is: 18406 and loss is: 0.000552561366930604\n",
      "Iteration is: 18407 and loss is: 0.0005500054685398936\n",
      "Iteration is: 18408 and loss is: 0.00037509610410779715\n",
      "Iteration is: 18409 and loss is: 0.00039759246283210814\n",
      "Iteration is: 18410 and loss is: 0.00047090533189475536\n",
      "Iteration is: 18411 and loss is: 0.0002797734341584146\n",
      "Iteration is: 18412 and loss is: 0.00029045838164165616\n",
      "Iteration is: 18413 and loss is: 0.0003195295576006174\n",
      "Iteration is: 18414 and loss is: 0.0002559749409556389\n",
      "Iteration is: 18415 and loss is: 0.0002412738249404356\n",
      "Iteration is: 18416 and loss is: 0.00018611572158988565\n",
      "Iteration is: 18417 and loss is: 0.000252917263424024\n",
      "Iteration is: 18418 and loss is: 0.0002530926140025258\n",
      "Iteration is: 18419 and loss is: 0.0002080759877571836\n",
      "Iteration is: 18420 and loss is: 0.0003341138071846217\n",
      "Iteration is: 18421 and loss is: 0.0003849025524687022\n",
      "Iteration is: 18422 and loss is: 0.0005141408182680607\n",
      "Iteration is: 18423 and loss is: 0.0007362272590398788\n",
      "Iteration is: 18424 and loss is: 0.0009740925161167979\n",
      "Iteration is: 18425 and loss is: 0.0008556582033634186\n",
      "Iteration is: 18426 and loss is: 0.00043128919787704945\n",
      "Iteration is: 18427 and loss is: 0.00018835024093277752\n",
      "Iteration is: 18428 and loss is: 0.0004011291894130409\n",
      "Iteration is: 18429 and loss is: 0.0005455246428027749\n",
      "Iteration is: 18430 and loss is: 0.0002685106883291155\n",
      "Iteration is: 18431 and loss is: 0.00023656558187212795\n",
      "Iteration is: 18432 and loss is: 0.0003962826740462333\n",
      "Iteration is: 18433 and loss is: 0.0003490831586532295\n",
      "Iteration is: 18434 and loss is: 0.00019020252511836588\n",
      "Iteration is: 18435 and loss is: 0.0002445108548272401\n",
      "Iteration is: 18436 and loss is: 0.0003187335387337953\n",
      "Iteration is: 18437 and loss is: 0.00025711022317409515\n",
      "Iteration is: 18438 and loss is: 0.0001633308856980875\n",
      "Iteration is: 18439 and loss is: 0.00017813887097872794\n",
      "Iteration is: 18440 and loss is: 0.0002686727966647595\n",
      "Iteration is: 18441 and loss is: 0.00027018922264687717\n",
      "Iteration is: 18442 and loss is: 0.00022058446484152228\n",
      "Iteration is: 18443 and loss is: 0.0001725456240819767\n",
      "Iteration is: 18444 and loss is: 0.00015032901137601584\n",
      "Iteration is: 18445 and loss is: 0.00013334274990484118\n",
      "Iteration is: 18446 and loss is: 0.00013600212696474046\n",
      "Iteration is: 18447 and loss is: 0.00017617078265175223\n",
      "Iteration is: 18448 and loss is: 0.0002227249788120389\n",
      "Iteration is: 18449 and loss is: 0.00032222026493400335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 18450 and loss is: 0.00048594409599900246\n",
      "Iteration is: 18451 and loss is: 0.0007310553919523954\n",
      "Iteration is: 18452 and loss is: 0.0008486704318784177\n",
      "Iteration is: 18453 and loss is: 0.0006684524705633521\n",
      "Iteration is: 18454 and loss is: 0.0002879435196518898\n",
      "Iteration is: 18455 and loss is: 0.00016318853886332363\n",
      "Iteration is: 18456 and loss is: 0.0003608377301134169\n",
      "Iteration is: 18457 and loss is: 0.0004144866543356329\n",
      "Iteration is: 18458 and loss is: 0.000236125459196046\n",
      "Iteration is: 18459 and loss is: 0.0001950010482687503\n",
      "Iteration is: 18460 and loss is: 0.0003103325725533068\n",
      "Iteration is: 18461 and loss is: 0.0002978513075504452\n",
      "Iteration is: 18462 and loss is: 0.0001877275644801557\n",
      "Iteration is: 18463 and loss is: 0.0001671816862653941\n",
      "Iteration is: 18464 and loss is: 0.00023833531304262578\n",
      "Iteration is: 18465 and loss is: 0.00028031558031216264\n",
      "Iteration is: 18466 and loss is: 0.00022160049411468208\n",
      "Iteration is: 18467 and loss is: 0.00014238803123589605\n",
      "Iteration is: 18468 and loss is: 0.0001360064052278176\n",
      "Iteration is: 18469 and loss is: 0.0001782439649105072\n",
      "Iteration is: 18470 and loss is: 0.00022941635688766837\n",
      "Iteration is: 18471 and loss is: 0.0003295690403319895\n",
      "Iteration is: 18472 and loss is: 0.0005279083270579576\n",
      "Iteration is: 18473 and loss is: 0.0009243771783076227\n",
      "Iteration is: 18474 and loss is: 0.0013586875284090638\n",
      "Iteration is: 18475 and loss is: 0.0015663059893995523\n",
      "Iteration is: 18476 and loss is: 0.0009357140515930951\n",
      "Iteration is: 18477 and loss is: 0.00031422916799783707\n",
      "Iteration is: 18478 and loss is: 0.0005276333540678024\n",
      "Iteration is: 18479 and loss is: 0.0006575183942914009\n",
      "Iteration is: 18480 and loss is: 0.0004884424270130694\n",
      "Iteration is: 18481 and loss is: 0.00044677790720015764\n",
      "Iteration is: 18482 and loss is: 0.0005162418237887323\n",
      "Iteration is: 18483 and loss is: 0.0004065845860168338\n",
      "Iteration is: 18484 and loss is: 0.00028009587549604475\n",
      "Iteration is: 18485 and loss is: 0.00036283308872953057\n",
      "Iteration is: 18486 and loss is: 0.0002656285068951547\n",
      "Iteration is: 18487 and loss is: 0.00023544099531136453\n",
      "Iteration is: 18488 and loss is: 0.0002522433060221374\n",
      "Iteration is: 18489 and loss is: 0.0001758987200446427\n",
      "Iteration is: 18490 and loss is: 0.0002727738465182483\n",
      "Iteration is: 18491 and loss is: 0.0003118189633823931\n",
      "Iteration is: 18492 and loss is: 0.0002952134527731687\n",
      "Iteration is: 18493 and loss is: 0.0005447076400741935\n",
      "Iteration is: 18494 and loss is: 0.0009004258899949491\n",
      "Iteration is: 18495 and loss is: 0.0015999166062101722\n",
      "Iteration is: 18496 and loss is: 0.0020982003770768642\n",
      "Iteration is: 18497 and loss is: 0.00139937165658921\n",
      "Iteration is: 18498 and loss is: 0.0006468159845098853\n",
      "Iteration is: 18499 and loss is: 0.0007735394174233079\n",
      "Iteration is: 18500 and loss is: 0.0009017069824039936\n",
      "Iteration is: 18501 and loss is: 0.000979483826085925\n",
      "Iteration is: 18502 and loss is: 0.0005591815570369363\n",
      "Iteration is: 18503 and loss is: 0.0010395317804068327\n",
      "Iteration is: 18504 and loss is: 0.0007370924577116966\n",
      "Iteration is: 18505 and loss is: 0.0004241842543706298\n",
      "Iteration is: 18506 and loss is: 0.0007803848711773753\n",
      "Iteration is: 18507 and loss is: 0.000557822932023555\n",
      "Iteration is: 18508 and loss is: 0.0004765764460898936\n",
      "Iteration is: 18509 and loss is: 0.0003982565540354699\n",
      "Iteration is: 18510 and loss is: 0.0004507700214162469\n",
      "Iteration is: 18511 and loss is: 0.0007560462108813226\n",
      "Iteration is: 18512 and loss is: 0.0004699192359112203\n",
      "Iteration is: 18513 and loss is: 0.00029359475593082607\n",
      "Iteration is: 18514 and loss is: 0.000298636092338711\n",
      "Iteration is: 18515 and loss is: 0.00021709731663577259\n",
      "Iteration is: 18516 and loss is: 0.0003446686896495521\n",
      "Iteration is: 18517 and loss is: 0.00042351314914412796\n",
      "Iteration is: 18518 and loss is: 0.000650973292067647\n",
      "Iteration is: 18519 and loss is: 0.0007833364070393145\n",
      "Iteration is: 18520 and loss is: 0.0008238910231739283\n",
      "Iteration is: 18521 and loss is: 0.0005937924142926931\n",
      "Iteration is: 18522 and loss is: 0.00031192300957627594\n",
      "Iteration is: 18523 and loss is: 0.0002651253016665578\n",
      "Iteration is: 18524 and loss is: 0.00043041532626375556\n",
      "Iteration is: 18525 and loss is: 0.00039596669375896454\n",
      "Iteration is: 18526 and loss is: 0.00021961853781249374\n",
      "Iteration is: 18527 and loss is: 0.0003026582708116621\n",
      "Iteration is: 18528 and loss is: 0.0003403863520361483\n",
      "Iteration is: 18529 and loss is: 0.00026868697023019195\n",
      "Iteration is: 18530 and loss is: 0.0001779812155291438\n",
      "Iteration is: 18531 and loss is: 0.0002725514932535589\n",
      "Iteration is: 18532 and loss is: 0.00027797132497653365\n",
      "Iteration is: 18533 and loss is: 0.00018493528477847576\n",
      "Iteration is: 18534 and loss is: 0.00015415437519550323\n",
      "Iteration is: 18535 and loss is: 0.0002348041016375646\n",
      "Iteration is: 18536 and loss is: 0.00024332555767614394\n",
      "Iteration is: 18537 and loss is: 0.0001944665564224124\n",
      "Iteration is: 18538 and loss is: 0.00019292315118946135\n",
      "Iteration is: 18539 and loss is: 0.00018037424888461828\n",
      "Iteration is: 18540 and loss is: 0.0001519255165476352\n",
      "Iteration is: 18541 and loss is: 0.0001395581930410117\n",
      "Iteration is: 18542 and loss is: 0.00015168596291914582\n",
      "Iteration is: 18543 and loss is: 0.00013721451978199184\n",
      "Iteration is: 18544 and loss is: 0.0001256814575754106\n",
      "Iteration is: 18545 and loss is: 0.00013518836931325495\n",
      "Iteration is: 18546 and loss is: 0.00013388012303039432\n",
      "Iteration is: 18547 and loss is: 0.00012744587729685009\n",
      "Iteration is: 18548 and loss is: 0.00012757041258737445\n",
      "Iteration is: 18549 and loss is: 0.00013578707876149565\n",
      "Iteration is: 18550 and loss is: 0.00012832533684559166\n",
      "Iteration is: 18551 and loss is: 0.00012510112719610333\n",
      "Iteration is: 18552 and loss is: 0.00013262155698612332\n",
      "Iteration is: 18553 and loss is: 0.00013361973105929792\n",
      "Iteration is: 18554 and loss is: 0.00013786955969408154\n",
      "Iteration is: 18555 and loss is: 0.00015395738591905683\n",
      "Iteration is: 18556 and loss is: 0.0001857073075370863\n",
      "Iteration is: 18557 and loss is: 0.0002346021356061101\n",
      "Iteration is: 18558 and loss is: 0.00033378187799826264\n",
      "Iteration is: 18559 and loss is: 0.00047428617835976183\n",
      "Iteration is: 18560 and loss is: 0.0006537052686326206\n",
      "Iteration is: 18561 and loss is: 0.0007182296831160784\n",
      "Iteration is: 18562 and loss is: 0.0005744830123148859\n",
      "Iteration is: 18563 and loss is: 0.0002961044665426016\n",
      "Iteration is: 18564 and loss is: 0.00014522370474878699\n",
      "Iteration is: 18565 and loss is: 0.0002446544240228832\n",
      "Iteration is: 18566 and loss is: 0.00037712129415012896\n",
      "Iteration is: 18567 and loss is: 0.00032299512531608343\n",
      "Iteration is: 18568 and loss is: 0.0001734044635668397\n",
      "Iteration is: 18569 and loss is: 0.00016626562864985317\n",
      "Iteration is: 18570 and loss is: 0.0002713474677875638\n",
      "Iteration is: 18571 and loss is: 0.00028815827681683004\n",
      "Iteration is: 18572 and loss is: 0.00019890772819053382\n",
      "Iteration is: 18573 and loss is: 0.0001336802524747327\n",
      "Iteration is: 18574 and loss is: 0.00016594798944424838\n",
      "Iteration is: 18575 and loss is: 0.00022857180738355964\n",
      "Iteration is: 18576 and loss is: 0.00024314600159414113\n",
      "Iteration is: 18577 and loss is: 0.0002058755635516718\n",
      "Iteration is: 18578 and loss is: 0.00015517517749685794\n",
      "Iteration is: 18579 and loss is: 0.00012202888319734484\n",
      "Iteration is: 18580 and loss is: 0.00011763138172682375\n",
      "Iteration is: 18581 and loss is: 0.00014419606304727495\n",
      "Iteration is: 18582 and loss is: 0.00020512743503786623\n",
      "Iteration is: 18583 and loss is: 0.00032694937544874847\n",
      "Iteration is: 18584 and loss is: 0.0006086342036724091\n",
      "Iteration is: 18585 and loss is: 0.0010997786885127425\n",
      "Iteration is: 18586 and loss is: 0.0018312378088012338\n",
      "Iteration is: 18587 and loss is: 0.0018405929440632463\n",
      "Iteration is: 18588 and loss is: 0.000862330081872642\n",
      "Iteration is: 18589 and loss is: 0.0004418168682605028\n",
      "Iteration is: 18590 and loss is: 0.0009649550775066018\n",
      "Iteration is: 18591 and loss is: 0.0008988972986117005\n",
      "Iteration is: 18592 and loss is: 0.0005755851743742824\n",
      "Iteration is: 18593 and loss is: 0.000551957287825644\n",
      "Iteration is: 18594 and loss is: 0.0008411041926592588\n",
      "Iteration is: 18595 and loss is: 0.0003921699244529009\n",
      "Iteration is: 18596 and loss is: 0.0003890342777594924\n",
      "Iteration is: 18597 and loss is: 0.0005497283418662846\n",
      "Iteration is: 18598 and loss is: 0.0003350874176248908\n",
      "Iteration is: 18599 and loss is: 0.000373100396245718\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 18600 and loss is: 0.0003102205810137093\n",
      "Iteration is: 18601 and loss is: 0.00019967136904597282\n",
      "Iteration is: 18602 and loss is: 0.0004999116063117981\n",
      "Iteration is: 18603 and loss is: 0.0005628942744806409\n",
      "Iteration is: 18604 and loss is: 0.0010208828607574105\n",
      "Iteration is: 18605 and loss is: 0.0019943728111684322\n",
      "Iteration is: 18606 and loss is: 0.0034771475475281477\n",
      "Iteration is: 18607 and loss is: 0.002957633463665843\n",
      "Iteration is: 18608 and loss is: 0.0006384272710420191\n",
      "Iteration is: 18609 and loss is: 0.002424676436930895\n",
      "Iteration is: 18610 and loss is: 0.0029962172266095877\n",
      "Iteration is: 18611 and loss is: 0.0018507830100134015\n",
      "Iteration is: 18612 and loss is: 0.0007509596180170774\n",
      "Iteration is: 18613 and loss is: 0.001384470029734075\n",
      "Iteration is: 18614 and loss is: 0.0016269651241600513\n",
      "Iteration is: 18615 and loss is: 0.0005062370910309255\n",
      "Iteration is: 18616 and loss is: 0.000886210473254323\n",
      "Iteration is: 18617 and loss is: 0.0011450336314737797\n",
      "Iteration is: 18618 and loss is: 0.00048583350144326687\n",
      "Iteration is: 18619 and loss is: 0.0006842957809567451\n",
      "Iteration is: 18620 and loss is: 0.0005088048055768013\n",
      "Iteration is: 18621 and loss is: 0.0009259221842512488\n",
      "Iteration is: 18622 and loss is: 0.0013527842238545418\n",
      "Iteration is: 18623 and loss is: 0.0018883752636611462\n",
      "Iteration is: 18624 and loss is: 0.002717400435358286\n",
      "Iteration is: 18625 and loss is: 0.0015664385864511132\n",
      "Iteration is: 18626 and loss is: 0.0003727659350261092\n",
      "Iteration is: 18627 and loss is: 0.0010147372959181666\n",
      "Iteration is: 18628 and loss is: 0.0008672014810144901\n",
      "Iteration is: 18629 and loss is: 0.00048182334285229445\n",
      "Iteration is: 18630 and loss is: 0.001040097908116877\n",
      "Iteration is: 18631 and loss is: 0.0006399836856871843\n",
      "Iteration is: 18632 and loss is: 0.0004535555199254304\n",
      "Iteration is: 18633 and loss is: 0.0006894973339512944\n",
      "Iteration is: 18634 and loss is: 0.00034604279790073633\n",
      "Iteration is: 18635 and loss is: 0.0004708627238869667\n",
      "Iteration is: 18636 and loss is: 0.00048260955372825265\n",
      "Iteration is: 18637 and loss is: 0.0003339274553582072\n",
      "Iteration is: 18638 and loss is: 0.00033754014293663204\n",
      "Iteration is: 18639 and loss is: 0.00041752023389562964\n",
      "Iteration is: 18640 and loss is: 0.00034863234031945467\n",
      "Iteration is: 18641 and loss is: 0.0005373161984607577\n",
      "Iteration is: 18642 and loss is: 0.0004065491957589984\n",
      "Iteration is: 18643 and loss is: 0.00041047894046641886\n",
      "Iteration is: 18644 and loss is: 0.0004112187016289681\n",
      "Iteration is: 18645 and loss is: 0.0002619507722556591\n",
      "Iteration is: 18646 and loss is: 0.00027439664700068533\n",
      "Iteration is: 18647 and loss is: 0.0001949024444911629\n",
      "Iteration is: 18648 and loss is: 0.0002666761865839362\n",
      "Iteration is: 18649 and loss is: 0.0002096952375723049\n",
      "Iteration is: 18650 and loss is: 0.00023318648163694888\n",
      "Iteration is: 18651 and loss is: 0.00023763024364598095\n",
      "Iteration is: 18652 and loss is: 0.00015471786900889128\n",
      "Iteration is: 18653 and loss is: 0.0001941139344125986\n",
      "Iteration is: 18654 and loss is: 0.0002041598199866712\n",
      "Iteration is: 18655 and loss is: 0.0001712917728582397\n",
      "Iteration is: 18656 and loss is: 0.00018944220209959894\n",
      "Iteration is: 18657 and loss is: 0.00018932120292447507\n",
      "Iteration is: 18658 and loss is: 0.00015838486433494836\n",
      "Iteration is: 18659 and loss is: 0.00015948264626786113\n",
      "Iteration is: 18660 and loss is: 0.00014436774654313922\n",
      "Iteration is: 18661 and loss is: 0.00015221102512441576\n",
      "Iteration is: 18662 and loss is: 0.00013163207040634006\n",
      "Iteration is: 18663 and loss is: 0.00013854065036866814\n",
      "Iteration is: 18664 and loss is: 0.00015103875193744898\n",
      "Iteration is: 18665 and loss is: 0.00012533491826616228\n",
      "Iteration is: 18666 and loss is: 0.000141027761856094\n",
      "Iteration is: 18667 and loss is: 0.00013500441855285317\n",
      "Iteration is: 18668 and loss is: 0.00012241430522408336\n",
      "Iteration is: 18669 and loss is: 0.00012527243234217167\n",
      "Iteration is: 18670 and loss is: 0.0001228792534675449\n",
      "Iteration is: 18671 and loss is: 0.00011977121175732464\n",
      "Iteration is: 18672 and loss is: 0.00012002713629044592\n",
      "Iteration is: 18673 and loss is: 0.00012115509889554232\n",
      "Iteration is: 18674 and loss is: 0.0001231019850820303\n",
      "Iteration is: 18675 and loss is: 0.00012259527284186333\n",
      "Iteration is: 18676 and loss is: 0.00012670077558141202\n",
      "Iteration is: 18677 and loss is: 0.00013606874563265592\n",
      "Iteration is: 18678 and loss is: 0.00013612092880066484\n",
      "Iteration is: 18679 and loss is: 0.00014627948985435069\n",
      "Iteration is: 18680 and loss is: 0.0001532277965452522\n",
      "Iteration is: 18681 and loss is: 0.00015323437401093543\n",
      "Iteration is: 18682 and loss is: 0.00015783210983499885\n",
      "Iteration is: 18683 and loss is: 0.00015890673967078328\n",
      "Iteration is: 18684 and loss is: 0.00015821299166418612\n",
      "Iteration is: 18685 and loss is: 0.00015817605890333652\n",
      "Iteration is: 18686 and loss is: 0.00015724197146482766\n",
      "Iteration is: 18687 and loss is: 0.0001591804320923984\n",
      "Iteration is: 18688 and loss is: 0.00015986358630470932\n",
      "Iteration is: 18689 and loss is: 0.00016473265714012086\n",
      "Iteration is: 18690 and loss is: 0.00017157910042442381\n",
      "Iteration is: 18691 and loss is: 0.00018115725833922625\n",
      "Iteration is: 18692 and loss is: 0.00019072191207669675\n",
      "Iteration is: 18693 and loss is: 0.00020681523892562836\n",
      "Iteration is: 18694 and loss is: 0.0002162917808163911\n",
      "Iteration is: 18695 and loss is: 0.00022732318029738963\n",
      "Iteration is: 18696 and loss is: 0.00022659843671135604\n",
      "Iteration is: 18697 and loss is: 0.00021703881793655455\n",
      "Iteration is: 18698 and loss is: 0.00019279572006780654\n",
      "Iteration is: 18699 and loss is: 0.00016460756887681782\n",
      "Iteration is: 18700 and loss is: 0.00013723128358833492\n",
      "Iteration is: 18701 and loss is: 0.0001178326565423049\n",
      "Iteration is: 18702 and loss is: 0.00010845751967281103\n",
      "Iteration is: 18703 and loss is: 0.0001087315395125188\n",
      "Iteration is: 18704 and loss is: 0.00011543388973223045\n",
      "Iteration is: 18705 and loss is: 0.00012834016524720937\n",
      "Iteration is: 18706 and loss is: 0.00015204912051558495\n",
      "Iteration is: 18707 and loss is: 0.0001914637250592932\n",
      "Iteration is: 18708 and loss is: 0.000261396897258237\n",
      "Iteration is: 18709 and loss is: 0.0003618013288360089\n",
      "Iteration is: 18710 and loss is: 0.0005071093910373747\n",
      "Iteration is: 18711 and loss is: 0.0005998250562697649\n",
      "Iteration is: 18712 and loss is: 0.0005630636005662382\n",
      "Iteration is: 18713 and loss is: 0.00036889599869027734\n",
      "Iteration is: 18714 and loss is: 0.00017255805141758174\n",
      "Iteration is: 18715 and loss is: 0.00015221416833810508\n",
      "Iteration is: 18716 and loss is: 0.0002755738969426602\n",
      "Iteration is: 18717 and loss is: 0.0003489226510282606\n",
      "Iteration is: 18718 and loss is: 0.0002679055032785982\n",
      "Iteration is: 18719 and loss is: 0.0001543390389997512\n",
      "Iteration is: 18720 and loss is: 0.0001467741240048781\n",
      "Iteration is: 18721 and loss is: 0.00022333803644869477\n",
      "Iteration is: 18722 and loss is: 0.00026304388302378356\n",
      "Iteration is: 18723 and loss is: 0.00021222348732408136\n",
      "Iteration is: 18724 and loss is: 0.0001414547732565552\n",
      "Iteration is: 18725 and loss is: 0.00012909460929222405\n",
      "Iteration is: 18726 and loss is: 0.0001690464123385027\n",
      "Iteration is: 18727 and loss is: 0.0002061922277789563\n",
      "Iteration is: 18728 and loss is: 0.00021654184092767537\n",
      "Iteration is: 18729 and loss is: 0.0002085894375341013\n",
      "Iteration is: 18730 and loss is: 0.00019526734831742942\n",
      "Iteration is: 18731 and loss is: 0.00018843795987777412\n",
      "Iteration is: 18732 and loss is: 0.000189324448001571\n",
      "Iteration is: 18733 and loss is: 0.00020492430485319346\n",
      "Iteration is: 18734 and loss is: 0.00022950531274545938\n",
      "Iteration is: 18735 and loss is: 0.00027508422499522567\n",
      "Iteration is: 18736 and loss is: 0.0003261190722696483\n",
      "Iteration is: 18737 and loss is: 0.00038393298746086657\n",
      "Iteration is: 18738 and loss is: 0.00039234597352333367\n",
      "Iteration is: 18739 and loss is: 0.00033694016747176647\n",
      "Iteration is: 18740 and loss is: 0.000229181518079713\n",
      "Iteration is: 18741 and loss is: 0.0001393789134453982\n",
      "Iteration is: 18742 and loss is: 0.0001240700512425974\n",
      "Iteration is: 18743 and loss is: 0.0001751852105371654\n",
      "Iteration is: 18744 and loss is: 0.00023632074589841068\n",
      "Iteration is: 18745 and loss is: 0.0002654895361047238\n",
      "Iteration is: 18746 and loss is: 0.00026340727345086634\n",
      "Iteration is: 18747 and loss is: 0.00024100320297293365\n",
      "Iteration is: 18748 and loss is: 0.00021871413628105074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 18749 and loss is: 0.00019077032629866153\n",
      "Iteration is: 18750 and loss is: 0.00016742644947953522\n",
      "Iteration is: 18751 and loss is: 0.00014680212188977748\n",
      "Iteration is: 18752 and loss is: 0.00013064367522019893\n",
      "Iteration is: 18753 and loss is: 0.00011783235095208511\n",
      "Iteration is: 18754 and loss is: 0.00011016080679837614\n",
      "Iteration is: 18755 and loss is: 0.00010759499855339527\n",
      "Iteration is: 18756 and loss is: 0.0001079909925465472\n",
      "Iteration is: 18757 and loss is: 0.00011054343485739082\n",
      "Iteration is: 18758 and loss is: 0.00011678898590616882\n",
      "Iteration is: 18759 and loss is: 0.00012875131506007165\n",
      "Iteration is: 18760 and loss is: 0.0001504395913798362\n",
      "Iteration is: 18761 and loss is: 0.0001948333519976586\n",
      "Iteration is: 18762 and loss is: 0.00027705999673344195\n",
      "Iteration is: 18763 and loss is: 0.00043589790584519506\n",
      "Iteration is: 18764 and loss is: 0.0006509547820314765\n",
      "Iteration is: 18765 and loss is: 0.0008958938415162265\n",
      "Iteration is: 18766 and loss is: 0.0009124013013206422\n",
      "Iteration is: 18767 and loss is: 0.00059462187346071\n",
      "Iteration is: 18768 and loss is: 0.0002311045245733112\n",
      "Iteration is: 18769 and loss is: 0.00020115409279242158\n",
      "Iteration is: 18770 and loss is: 0.0004089016292709857\n",
      "Iteration is: 18771 and loss is: 0.0004601396503858268\n",
      "Iteration is: 18772 and loss is: 0.00030266211251728237\n",
      "Iteration is: 18773 and loss is: 0.0001838016032706946\n",
      "Iteration is: 18774 and loss is: 0.00023547938326373696\n",
      "Iteration is: 18775 and loss is: 0.0003358958929311484\n",
      "Iteration is: 18776 and loss is: 0.0003270604065619409\n",
      "Iteration is: 18777 and loss is: 0.00019584517576731741\n",
      "Iteration is: 18778 and loss is: 0.00012885226169601083\n",
      "Iteration is: 18779 and loss is: 0.00019688537577167153\n",
      "Iteration is: 18780 and loss is: 0.0002639515441842377\n",
      "Iteration is: 18781 and loss is: 0.0002798856876324862\n",
      "Iteration is: 18782 and loss is: 0.0002965752501040697\n",
      "Iteration is: 18783 and loss is: 0.00036031362833455205\n",
      "Iteration is: 18784 and loss is: 0.0004665939777623862\n",
      "Iteration is: 18785 and loss is: 0.0005752565921284258\n",
      "Iteration is: 18786 and loss is: 0.0007474651793017983\n",
      "Iteration is: 18787 and loss is: 0.0007970246952027082\n",
      "Iteration is: 18788 and loss is: 0.0005933933425694704\n",
      "Iteration is: 18789 and loss is: 0.00027085698093287647\n",
      "Iteration is: 18790 and loss is: 0.00014876064960844815\n",
      "Iteration is: 18791 and loss is: 0.0002978304401040077\n",
      "Iteration is: 18792 and loss is: 0.00043013732647523284\n",
      "Iteration is: 18793 and loss is: 0.00033771770540624857\n",
      "Iteration is: 18794 and loss is: 0.0001684297458268702\n",
      "Iteration is: 18795 and loss is: 0.00017069472232833505\n",
      "Iteration is: 18796 and loss is: 0.0002901633270084858\n",
      "Iteration is: 18797 and loss is: 0.0003201374493073672\n",
      "Iteration is: 18798 and loss is: 0.00022376872948370874\n",
      "Iteration is: 18799 and loss is: 0.00013434558059088886\n",
      "Iteration is: 18800 and loss is: 0.00015312305185943842\n",
      "Iteration is: 18801 and loss is: 0.00022504618391394615\n",
      "Iteration is: 18802 and loss is: 0.00026129119214601815\n",
      "Iteration is: 18803 and loss is: 0.00024015785311348736\n",
      "Iteration is: 18804 and loss is: 0.00019895326113328338\n",
      "Iteration is: 18805 and loss is: 0.00016140966909006238\n",
      "Iteration is: 18806 and loss is: 0.00013043932267464697\n",
      "Iteration is: 18807 and loss is: 0.00011557326070033014\n",
      "Iteration is: 18808 and loss is: 0.00011430252197897062\n",
      "Iteration is: 18809 and loss is: 0.0001174614008050412\n",
      "Iteration is: 18810 and loss is: 0.00012377614621073008\n",
      "Iteration is: 18811 and loss is: 0.0001429816911695525\n",
      "Iteration is: 18812 and loss is: 0.0001889788400148973\n",
      "Iteration is: 18813 and loss is: 0.00027435136144049466\n",
      "Iteration is: 18814 and loss is: 0.0004370317328721285\n",
      "Iteration is: 18815 and loss is: 0.0006519846501760185\n",
      "Iteration is: 18816 and loss is: 0.0008682828629389405\n",
      "Iteration is: 18817 and loss is: 0.000840113265439868\n",
      "Iteration is: 18818 and loss is: 0.0005218223668634892\n",
      "Iteration is: 18819 and loss is: 0.00021858204854652286\n",
      "Iteration is: 18820 and loss is: 0.00022857569274492562\n",
      "Iteration is: 18821 and loss is: 0.00040575332241132855\n",
      "Iteration is: 18822 and loss is: 0.0004231292987242341\n",
      "Iteration is: 18823 and loss is: 0.00027811637846753\n",
      "Iteration is: 18824 and loss is: 0.00019472022540867329\n",
      "Iteration is: 18825 and loss is: 0.0002403130056336522\n",
      "Iteration is: 18826 and loss is: 0.0003193675074726343\n",
      "Iteration is: 18827 and loss is: 0.00030799463274888694\n",
      "Iteration is: 18828 and loss is: 0.00018266765982843935\n",
      "Iteration is: 18829 and loss is: 0.00012749421875923872\n",
      "Iteration is: 18830 and loss is: 0.0001979221124202013\n",
      "Iteration is: 18831 and loss is: 0.0002532310609240085\n",
      "Iteration is: 18832 and loss is: 0.0002716404851526022\n",
      "Iteration is: 18833 and loss is: 0.0003467596252448857\n",
      "Iteration is: 18834 and loss is: 0.0005480668041855097\n",
      "Iteration is: 18835 and loss is: 0.0009409577469341457\n",
      "Iteration is: 18836 and loss is: 0.001360972528345883\n",
      "Iteration is: 18837 and loss is: 0.00158025068230927\n",
      "Iteration is: 18838 and loss is: 0.0010837358422577381\n",
      "Iteration is: 18839 and loss is: 0.00045712850987911224\n",
      "Iteration is: 18840 and loss is: 0.00046066747745499015\n",
      "Iteration is: 18841 and loss is: 0.0006112988921813667\n",
      "Iteration is: 18842 and loss is: 0.0005864571430720389\n",
      "Iteration is: 18843 and loss is: 0.0005370189901441336\n",
      "Iteration is: 18844 and loss is: 0.0004269850323908031\n",
      "Iteration is: 18845 and loss is: 0.00041761164902709424\n",
      "Iteration is: 18846 and loss is: 0.0003948592348024249\n",
      "Iteration is: 18847 and loss is: 0.0003587383253034204\n",
      "Iteration is: 18848 and loss is: 0.00020993624639231712\n",
      "Iteration is: 18849 and loss is: 0.00029103635461069643\n",
      "Iteration is: 18850 and loss is: 0.00037113839061930776\n",
      "Iteration is: 18851 and loss is: 0.00020605076861102134\n",
      "Iteration is: 18852 and loss is: 0.00020673930703196675\n",
      "Iteration is: 18853 and loss is: 0.0003572651476133615\n",
      "Iteration is: 18854 and loss is: 0.00040620408253744245\n",
      "Iteration is: 18855 and loss is: 0.0006143292994238436\n",
      "Iteration is: 18856 and loss is: 0.0011439879890531301\n",
      "Iteration is: 18857 and loss is: 0.0018088389188051224\n",
      "Iteration is: 18858 and loss is: 0.0019916160963475704\n",
      "Iteration is: 18859 and loss is: 0.0009713640320114791\n",
      "Iteration is: 18860 and loss is: 0.00030185101786628366\n",
      "Iteration is: 18861 and loss is: 0.000806857249699533\n",
      "Iteration is: 18862 and loss is: 0.0008692378760315478\n",
      "Iteration is: 18863 and loss is: 0.0004884882946498692\n",
      "Iteration is: 18864 and loss is: 0.0006938186124898493\n",
      "Iteration is: 18865 and loss is: 0.0008242807816714048\n",
      "Iteration is: 18866 and loss is: 0.0003552970301825553\n",
      "Iteration is: 18867 and loss is: 0.0004373473930172622\n",
      "Iteration is: 18868 and loss is: 0.0005633259424939752\n",
      "Iteration is: 18869 and loss is: 0.00029904008260928094\n",
      "Iteration is: 18870 and loss is: 0.0002949714835267514\n",
      "Iteration is: 18871 and loss is: 0.00035987820592708886\n",
      "Iteration is: 18872 and loss is: 0.00040030007949098945\n",
      "Iteration is: 18873 and loss is: 0.0004988694563508034\n",
      "Iteration is: 18874 and loss is: 0.0003731636388693005\n",
      "Iteration is: 18875 and loss is: 0.0003488997754175216\n",
      "Iteration is: 18876 and loss is: 0.00052685133414343\n",
      "Iteration is: 18877 and loss is: 0.0007013050490058959\n",
      "Iteration is: 18878 and loss is: 0.0010111689334735274\n",
      "Iteration is: 18879 and loss is: 0.0012588273966684937\n",
      "Iteration is: 18880 and loss is: 0.0010394463315606117\n",
      "Iteration is: 18881 and loss is: 0.0005920993280597031\n",
      "Iteration is: 18882 and loss is: 0.0003539042954798788\n",
      "Iteration is: 18883 and loss is: 0.0005847283755429089\n",
      "Iteration is: 18884 and loss is: 0.0007269986672326922\n",
      "Iteration is: 18885 and loss is: 0.00041282945312559605\n",
      "Iteration is: 18886 and loss is: 0.0004498538328334689\n",
      "Iteration is: 18887 and loss is: 0.0005327413091436028\n",
      "Iteration is: 18888 and loss is: 0.0003966465010307729\n",
      "Iteration is: 18889 and loss is: 0.0003122589841950685\n",
      "Iteration is: 18890 and loss is: 0.0003139897598885\n",
      "Iteration is: 18891 and loss is: 0.0003991025732830167\n",
      "Iteration is: 18892 and loss is: 0.00028006720822304487\n",
      "Iteration is: 18893 and loss is: 0.00015271941083483398\n",
      "Iteration is: 18894 and loss is: 0.00032001087674871087\n",
      "Iteration is: 18895 and loss is: 0.0003383082221262157\n",
      "Iteration is: 18896 and loss is: 0.000265343434875831\n",
      "Iteration is: 18897 and loss is: 0.0004298658750485629\n",
      "Iteration is: 18898 and loss is: 0.0007500287028960884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 18899 and loss is: 0.0011188037460669875\n",
      "Iteration is: 18900 and loss is: 0.0015639421762898564\n",
      "Iteration is: 18901 and loss is: 0.0014273810666054487\n",
      "Iteration is: 18902 and loss is: 0.0006982077029533684\n",
      "Iteration is: 18903 and loss is: 0.0006180947530083358\n",
      "Iteration is: 18904 and loss is: 0.0006885991897433996\n",
      "Iteration is: 18905 and loss is: 0.0008550052880309522\n",
      "Iteration is: 18906 and loss is: 0.0005954927182756364\n",
      "Iteration is: 18907 and loss is: 0.0006033056997694075\n",
      "Iteration is: 18908 and loss is: 0.0006245474796742201\n",
      "Iteration is: 18909 and loss is: 0.0004469925770536065\n",
      "Iteration is: 18910 and loss is: 0.0004580134991556406\n",
      "Iteration is: 18911 and loss is: 0.0003387838078197092\n",
      "Iteration is: 18912 and loss is: 0.0004068752168677747\n",
      "Iteration is: 18913 and loss is: 0.00034250254975631833\n",
      "Iteration is: 18914 and loss is: 0.00019146029080729932\n",
      "Iteration is: 18915 and loss is: 0.00034865306224673986\n",
      "Iteration is: 18916 and loss is: 0.00039942492730915546\n",
      "Iteration is: 18917 and loss is: 0.00033214862924069166\n",
      "Iteration is: 18918 and loss is: 0.0005197604186832905\n",
      "Iteration is: 18919 and loss is: 0.0007418830646201968\n",
      "Iteration is: 18920 and loss is: 0.0011465842835605145\n",
      "Iteration is: 18921 and loss is: 0.001368689234368503\n",
      "Iteration is: 18922 and loss is: 0.0012008395278826356\n",
      "Iteration is: 18923 and loss is: 0.0007035114103928208\n",
      "Iteration is: 18924 and loss is: 0.00035824987571686506\n",
      "Iteration is: 18925 and loss is: 0.0006988267996348441\n",
      "Iteration is: 18926 and loss is: 0.0007862919010221958\n",
      "Iteration is: 18927 and loss is: 0.0004318301798775792\n",
      "Iteration is: 18928 and loss is: 0.0006187805556692183\n",
      "Iteration is: 18929 and loss is: 0.0006159694166854024\n",
      "Iteration is: 18930 and loss is: 0.00037106318632140756\n",
      "Iteration is: 18931 and loss is: 0.0004787859506905079\n",
      "Iteration is: 18932 and loss is: 0.0004104588588234037\n",
      "Iteration is: 18933 and loss is: 0.0003965191717725247\n",
      "Iteration is: 18934 and loss is: 0.00023661955492570996\n",
      "Iteration is: 18935 and loss is: 0.0003258158394601196\n",
      "Iteration is: 18936 and loss is: 0.0005019403761252761\n",
      "Iteration is: 18937 and loss is: 0.00034129113191738725\n",
      "Iteration is: 18938 and loss is: 0.0002803179668262601\n",
      "Iteration is: 18939 and loss is: 0.00034573039738461375\n",
      "Iteration is: 18940 and loss is: 0.0002860192907974124\n",
      "Iteration is: 18941 and loss is: 0.0002636840217746794\n",
      "Iteration is: 18942 and loss is: 0.00028355466201901436\n",
      "Iteration is: 18943 and loss is: 0.00030486154719255865\n",
      "Iteration is: 18944 and loss is: 0.0002559226122684777\n",
      "Iteration is: 18945 and loss is: 0.0002129114291165024\n",
      "Iteration is: 18946 and loss is: 0.00022615148918703198\n",
      "Iteration is: 18947 and loss is: 0.00017007619317155331\n",
      "Iteration is: 18948 and loss is: 0.00013158419460523874\n",
      "Iteration is: 18949 and loss is: 0.00016628648154437542\n",
      "Iteration is: 18950 and loss is: 0.00015561163309030235\n",
      "Iteration is: 18951 and loss is: 0.0001517488999525085\n",
      "Iteration is: 18952 and loss is: 0.00017971107445191592\n",
      "Iteration is: 18953 and loss is: 0.0001757963327690959\n",
      "Iteration is: 18954 and loss is: 0.00017249060329049826\n",
      "Iteration is: 18955 and loss is: 0.00018109027587343007\n",
      "Iteration is: 18956 and loss is: 0.00019782462914008647\n",
      "Iteration is: 18957 and loss is: 0.00021783990086987615\n",
      "Iteration is: 18958 and loss is: 0.00023012561723589897\n",
      "Iteration is: 18959 and loss is: 0.0002633234835229814\n",
      "Iteration is: 18960 and loss is: 0.00026460489607416093\n",
      "Iteration is: 18961 and loss is: 0.00022788318165112287\n",
      "Iteration is: 18962 and loss is: 0.00018818039097823203\n",
      "Iteration is: 18963 and loss is: 0.00014492572518065572\n",
      "Iteration is: 18964 and loss is: 0.00011715837172232568\n",
      "Iteration is: 18965 and loss is: 0.00012695029727183282\n",
      "Iteration is: 18966 and loss is: 0.00015430567145813257\n",
      "Iteration is: 18967 and loss is: 0.0001921948860399425\n",
      "Iteration is: 18968 and loss is: 0.00025145377730950713\n",
      "Iteration is: 18969 and loss is: 0.0003417176485527307\n",
      "Iteration is: 18970 and loss is: 0.0005174707621335983\n",
      "Iteration is: 18971 and loss is: 0.0007427202654071152\n",
      "Iteration is: 18972 and loss is: 0.0009844120359048247\n",
      "Iteration is: 18973 and loss is: 0.000906347471754998\n",
      "Iteration is: 18974 and loss is: 0.00046642113011330366\n",
      "Iteration is: 18975 and loss is: 0.00018595546134747565\n",
      "Iteration is: 18976 and loss is: 0.0003459120634943247\n",
      "Iteration is: 18977 and loss is: 0.0005139400018379092\n",
      "Iteration is: 18978 and loss is: 0.0003577327588573098\n",
      "Iteration is: 18979 and loss is: 0.0002177828282583505\n",
      "Iteration is: 18980 and loss is: 0.0003123289789073169\n",
      "Iteration is: 18981 and loss is: 0.0003630444116424769\n",
      "Iteration is: 18982 and loss is: 0.0002638280566316098\n",
      "Iteration is: 18983 and loss is: 0.00018086671479977667\n",
      "Iteration is: 18984 and loss is: 0.0002050574985332787\n",
      "Iteration is: 18985 and loss is: 0.0003085782518610358\n",
      "Iteration is: 18986 and loss is: 0.00029933120822533965\n",
      "Iteration is: 18987 and loss is: 0.00018045355682261288\n",
      "Iteration is: 18988 and loss is: 0.0001461269275750965\n",
      "Iteration is: 18989 and loss is: 0.00015996480942703784\n",
      "Iteration is: 18990 and loss is: 0.000170680636074394\n",
      "Iteration is: 18991 and loss is: 0.00021852939971722662\n",
      "Iteration is: 18992 and loss is: 0.00039737773477099836\n",
      "Iteration is: 18993 and loss is: 0.0007181346300058067\n",
      "Iteration is: 18994 and loss is: 0.0013205119175836444\n",
      "Iteration is: 18995 and loss is: 0.001828475040383637\n",
      "Iteration is: 18996 and loss is: 0.001524561084806919\n",
      "Iteration is: 18997 and loss is: 0.0005590703804045916\n",
      "Iteration is: 18998 and loss is: 0.00039233791176229715\n",
      "Iteration is: 18999 and loss is: 0.000826965260785073\n",
      "Iteration is: 19000 and loss is: 0.0006935345008969307\n",
      "Iteration is: 19001 and loss is: 0.00046055205166339874\n",
      "Iteration is: 19002 and loss is: 0.0006583015201613307\n",
      "Iteration is: 19003 and loss is: 0.0005704767536371946\n",
      "Iteration is: 19004 and loss is: 0.000352485163602978\n",
      "Iteration is: 19005 and loss is: 0.0004615475772880018\n",
      "Iteration is: 19006 and loss is: 0.0004055557947140187\n",
      "Iteration is: 19007 and loss is: 0.0003146116796415299\n",
      "Iteration is: 19008 and loss is: 0.000306077825371176\n",
      "Iteration is: 19009 and loss is: 0.00026609725318849087\n",
      "Iteration is: 19010 and loss is: 0.0004490602877922356\n",
      "Iteration is: 19011 and loss is: 0.00048562034498900175\n",
      "Iteration is: 19012 and loss is: 0.0004830540274269879\n",
      "Iteration is: 19013 and loss is: 0.000915474200155586\n",
      "Iteration is: 19014 and loss is: 0.0016670952318236232\n",
      "Iteration is: 19015 and loss is: 0.0023021858651190996\n",
      "Iteration is: 19016 and loss is: 0.002015382517129183\n",
      "Iteration is: 19017 and loss is: 0.000801607093308121\n",
      "Iteration is: 19018 and loss is: 0.001047534286044538\n",
      "Iteration is: 19019 and loss is: 0.0012048465432599187\n",
      "Iteration is: 19020 and loss is: 0.0009461647132411599\n",
      "Iteration is: 19021 and loss is: 0.0006135758012533188\n",
      "Iteration is: 19022 and loss is: 0.0011661192402243614\n",
      "Iteration is: 19023 and loss is: 0.0009383776923641562\n",
      "Iteration is: 19024 and loss is: 0.0003486205532681197\n",
      "Iteration is: 19025 and loss is: 0.0007390769897028804\n",
      "Iteration is: 19026 and loss is: 0.000662090489640832\n",
      "Iteration is: 19027 and loss is: 0.00034285400761291385\n",
      "Iteration is: 19028 and loss is: 0.00043417414417490363\n",
      "Iteration is: 19029 and loss is: 0.0006218529306352139\n",
      "Iteration is: 19030 and loss is: 0.0006572105921804905\n",
      "Iteration is: 19031 and loss is: 0.0004189558676443994\n",
      "Iteration is: 19032 and loss is: 0.00019290210912004113\n",
      "Iteration is: 19033 and loss is: 0.0002930278715211898\n",
      "Iteration is: 19034 and loss is: 0.0003845799947157502\n",
      "Iteration is: 19035 and loss is: 0.000664282706566155\n",
      "Iteration is: 19036 and loss is: 0.000729768886230886\n",
      "Iteration is: 19037 and loss is: 0.0008027409785427153\n",
      "Iteration is: 19038 and loss is: 0.0005970725324004889\n",
      "Iteration is: 19039 and loss is: 0.00023214406974148005\n",
      "Iteration is: 19040 and loss is: 0.00027218705508857965\n",
      "Iteration is: 19041 and loss is: 0.00036368309520184994\n",
      "Iteration is: 19042 and loss is: 0.0003397298278287053\n",
      "Iteration is: 19043 and loss is: 0.0002612974785733968\n",
      "Iteration is: 19044 and loss is: 0.00023425818653777242\n",
      "Iteration is: 19045 and loss is: 0.0003039685543626547\n",
      "Iteration is: 19046 and loss is: 0.0002581571170594543\n",
      "Iteration is: 19047 and loss is: 0.00022057321621105075\n",
      "Iteration is: 19048 and loss is: 0.00022482528584077954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 19049 and loss is: 0.0002177347196266055\n",
      "Iteration is: 19050 and loss is: 0.00025104719679802656\n",
      "Iteration is: 19051 and loss is: 0.0001633909996598959\n",
      "Iteration is: 19052 and loss is: 0.0001641901326365769\n",
      "Iteration is: 19053 and loss is: 0.00023570058692712337\n",
      "Iteration is: 19054 and loss is: 0.00019418603915255517\n",
      "Iteration is: 19055 and loss is: 0.00015430049097631127\n",
      "Iteration is: 19056 and loss is: 0.00015111718676052988\n",
      "Iteration is: 19057 and loss is: 0.0001518754434073344\n",
      "Iteration is: 19058 and loss is: 0.0001440499909222126\n",
      "Iteration is: 19059 and loss is: 0.00015385085134766996\n",
      "Iteration is: 19060 and loss is: 0.00020598959235940129\n",
      "Iteration is: 19061 and loss is: 0.00023829683777876198\n",
      "Iteration is: 19062 and loss is: 0.0002972848014906049\n",
      "Iteration is: 19063 and loss is: 0.0004045580280944705\n",
      "Iteration is: 19064 and loss is: 0.000496890745125711\n",
      "Iteration is: 19065 and loss is: 0.0004963989485986531\n",
      "Iteration is: 19066 and loss is: 0.0003786466259043664\n",
      "Iteration is: 19067 and loss is: 0.00019756580877583474\n",
      "Iteration is: 19068 and loss is: 0.00013870665861759335\n",
      "Iteration is: 19069 and loss is: 0.00021592066332232207\n",
      "Iteration is: 19070 and loss is: 0.0002775665489025414\n",
      "Iteration is: 19071 and loss is: 0.00022610556334257126\n",
      "Iteration is: 19072 and loss is: 0.00014398200437426567\n",
      "Iteration is: 19073 and loss is: 0.00015995290596038103\n",
      "Iteration is: 19074 and loss is: 0.00021208917314652354\n",
      "Iteration is: 19075 and loss is: 0.00020747912640217692\n",
      "Iteration is: 19076 and loss is: 0.00015865752357058227\n",
      "Iteration is: 19077 and loss is: 0.00013364943151827902\n",
      "Iteration is: 19078 and loss is: 0.0001627336605452001\n",
      "Iteration is: 19079 and loss is: 0.00019029872782994062\n",
      "Iteration is: 19080 and loss is: 0.00016806114581413567\n",
      "Iteration is: 19081 and loss is: 0.00012985127978026867\n",
      "Iteration is: 19082 and loss is: 0.00012189955305075273\n",
      "Iteration is: 19083 and loss is: 0.000138798204716295\n",
      "Iteration is: 19084 and loss is: 0.0001526222622487694\n",
      "Iteration is: 19085 and loss is: 0.00016844438505358994\n",
      "Iteration is: 19086 and loss is: 0.00019346954650245607\n",
      "Iteration is: 19087 and loss is: 0.00022268894826993346\n",
      "Iteration is: 19088 and loss is: 0.0002805822005029768\n",
      "Iteration is: 19089 and loss is: 0.0003705009585246444\n",
      "Iteration is: 19090 and loss is: 0.0005212442483752966\n",
      "Iteration is: 19091 and loss is: 0.0006493616965599358\n",
      "Iteration is: 19092 and loss is: 0.0006804472068324685\n",
      "Iteration is: 19093 and loss is: 0.00048606388736516237\n",
      "Iteration is: 19094 and loss is: 0.0002171462110709399\n",
      "Iteration is: 19095 and loss is: 0.00015486688062082976\n",
      "Iteration is: 19096 and loss is: 0.00030102598248049617\n",
      "Iteration is: 19097 and loss is: 0.0003718019579537213\n",
      "Iteration is: 19098 and loss is: 0.00024274352472275496\n",
      "Iteration is: 19099 and loss is: 0.00015543148037977517\n",
      "Iteration is: 19100 and loss is: 0.0002198185247834772\n",
      "Iteration is: 19101 and loss is: 0.00028228340670466423\n",
      "Iteration is: 19102 and loss is: 0.00023530746693722904\n",
      "Iteration is: 19103 and loss is: 0.00014796324830967933\n",
      "Iteration is: 19104 and loss is: 0.00014191392983775586\n",
      "Iteration is: 19105 and loss is: 0.00021231162827461958\n",
      "Iteration is: 19106 and loss is: 0.0002430391905363649\n",
      "Iteration is: 19107 and loss is: 0.000197281944565475\n",
      "Iteration is: 19108 and loss is: 0.00014859234215691686\n",
      "Iteration is: 19109 and loss is: 0.0001269172498723492\n",
      "Iteration is: 19110 and loss is: 0.00011977909889537841\n",
      "Iteration is: 19111 and loss is: 0.00012909772340208292\n",
      "Iteration is: 19112 and loss is: 0.0001766506175044924\n",
      "Iteration is: 19113 and loss is: 0.00026624894235283136\n",
      "Iteration is: 19114 and loss is: 0.00044129896559752524\n",
      "Iteration is: 19115 and loss is: 0.0007356827263720334\n",
      "Iteration is: 19116 and loss is: 0.0012001119321212173\n",
      "Iteration is: 19117 and loss is: 0.0013558458304032683\n",
      "Iteration is: 19118 and loss is: 0.0008321822970174253\n",
      "Iteration is: 19119 and loss is: 0.0002606667694635689\n",
      "Iteration is: 19120 and loss is: 0.00041257552220486104\n",
      "Iteration is: 19121 and loss is: 0.0006984780775383115\n",
      "Iteration is: 19122 and loss is: 0.00048273010179400444\n",
      "Iteration is: 19123 and loss is: 0.00031039625173434615\n",
      "Iteration is: 19124 and loss is: 0.0004638259415514767\n",
      "Iteration is: 19125 and loss is: 0.0004586906870827079\n",
      "Iteration is: 19126 and loss is: 0.0003011566586792469\n",
      "Iteration is: 19127 and loss is: 0.00029095227364450693\n",
      "Iteration is: 19128 and loss is: 0.00031817544368095696\n",
      "Iteration is: 19129 and loss is: 0.0003497013822197914\n",
      "Iteration is: 19130 and loss is: 0.00026056409114971757\n",
      "Iteration is: 19131 and loss is: 0.00014202782767824829\n",
      "Iteration is: 19132 and loss is: 0.00026496488135308027\n",
      "Iteration is: 19133 and loss is: 0.00034247132134623826\n",
      "Iteration is: 19134 and loss is: 0.0003171734279021621\n",
      "Iteration is: 19135 and loss is: 0.00039034534711390734\n",
      "Iteration is: 19136 and loss is: 0.0006518938462249935\n",
      "Iteration is: 19137 and loss is: 0.000958966847974807\n",
      "Iteration is: 19138 and loss is: 0.0013033607974648476\n",
      "Iteration is: 19139 and loss is: 0.001294566667638719\n",
      "Iteration is: 19140 and loss is: 0.0007165064453147352\n",
      "Iteration is: 19141 and loss is: 0.00040988365071825683\n",
      "Iteration is: 19142 and loss is: 0.0005517363315448165\n",
      "Iteration is: 19143 and loss is: 0.0007296751718968153\n",
      "Iteration is: 19144 and loss is: 0.0005884846905246377\n",
      "Iteration is: 19145 and loss is: 0.00041780530591495335\n",
      "Iteration is: 19146 and loss is: 0.0005445375572890043\n",
      "Iteration is: 19147 and loss is: 0.0005239804740995169\n",
      "Iteration is: 19148 and loss is: 0.0003712406905833632\n",
      "Iteration is: 19149 and loss is: 0.00030976528069004416\n",
      "Iteration is: 19150 and loss is: 0.0004513812600634992\n",
      "Iteration is: 19151 and loss is: 0.0004224594449624419\n",
      "Iteration is: 19152 and loss is: 0.00018301818636246026\n",
      "Iteration is: 19153 and loss is: 0.00025280335103161633\n",
      "Iteration is: 19154 and loss is: 0.0004175687790848315\n",
      "Iteration is: 19155 and loss is: 0.0002861516550183296\n",
      "Iteration is: 19156 and loss is: 0.00033857201924547553\n",
      "Iteration is: 19157 and loss is: 0.0005922271520830691\n",
      "Iteration is: 19158 and loss is: 0.0008525872835889459\n",
      "Iteration is: 19159 and loss is: 0.001111118239350617\n",
      "Iteration is: 19160 and loss is: 0.0013137806672602892\n",
      "Iteration is: 19161 and loss is: 0.0009649638668633997\n",
      "Iteration is: 19162 and loss is: 0.00035792801645584404\n",
      "Iteration is: 19163 and loss is: 0.0004519820213317871\n",
      "Iteration is: 19164 and loss is: 0.0007074549794197083\n",
      "Iteration is: 19165 and loss is: 0.0004949775175191462\n",
      "Iteration is: 19166 and loss is: 0.000352416536770761\n",
      "Iteration is: 19167 and loss is: 0.0005175115074962378\n",
      "Iteration is: 19168 and loss is: 0.00036631315015256405\n",
      "Iteration is: 19169 and loss is: 0.00028220750391483307\n",
      "Iteration is: 19170 and loss is: 0.0003090337850153446\n",
      "Iteration is: 19171 and loss is: 0.00029939610976725817\n",
      "Iteration is: 19172 and loss is: 0.0002528139448259026\n",
      "Iteration is: 19173 and loss is: 0.00016461256018374115\n",
      "Iteration is: 19174 and loss is: 0.000235003957641311\n",
      "Iteration is: 19175 and loss is: 0.0003405657480470836\n",
      "Iteration is: 19176 and loss is: 0.0003089876554440707\n",
      "Iteration is: 19177 and loss is: 0.00047316073323599994\n",
      "Iteration is: 19178 and loss is: 0.0008435661438852549\n",
      "Iteration is: 19179 and loss is: 0.001207153545692563\n",
      "Iteration is: 19180 and loss is: 0.0016105860704556108\n",
      "Iteration is: 19181 and loss is: 0.0011888365261256695\n",
      "Iteration is: 19182 and loss is: 0.0004772743850480765\n",
      "Iteration is: 19183 and loss is: 0.0006522402982227504\n",
      "Iteration is: 19184 and loss is: 0.0006362749845720828\n",
      "Iteration is: 19185 and loss is: 0.000734158675186336\n",
      "Iteration is: 19186 and loss is: 0.0005272190319374204\n",
      "Iteration is: 19187 and loss is: 0.0006972069386392832\n",
      "Iteration is: 19188 and loss is: 0.000494260573759675\n",
      "Iteration is: 19189 and loss is: 0.00035940419184044003\n",
      "Iteration is: 19190 and loss is: 0.0004411815316416323\n",
      "Iteration is: 19191 and loss is: 0.0003080480673816055\n",
      "Iteration is: 19192 and loss is: 0.0002564381284173578\n",
      "Iteration is: 19193 and loss is: 0.0002473191125318408\n",
      "Iteration is: 19194 and loss is: 0.0002522735740058124\n",
      "Iteration is: 19195 and loss is: 0.0004231823841109872\n",
      "Iteration is: 19196 and loss is: 0.00045257233432494104\n",
      "Iteration is: 19197 and loss is: 0.0006292478647083044\n",
      "Iteration is: 19198 and loss is: 0.001112873200327158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 19199 and loss is: 0.0018197355093434453\n",
      "Iteration is: 19200 and loss is: 0.0025913920253515244\n",
      "Iteration is: 19201 and loss is: 0.0013971514999866486\n",
      "Iteration is: 19202 and loss is: 0.0008308958495035768\n",
      "Iteration is: 19203 and loss is: 0.0022240488324314356\n",
      "Iteration is: 19204 and loss is: 0.0024014930240809917\n",
      "Iteration is: 19205 and loss is: 0.0012220166390761733\n",
      "Iteration is: 19206 and loss is: 0.0006701984093524516\n",
      "Iteration is: 19207 and loss is: 0.0016402800101786852\n",
      "Iteration is: 19208 and loss is: 0.0015208139084279537\n",
      "Iteration is: 19209 and loss is: 0.0005766265094280243\n",
      "Iteration is: 19210 and loss is: 0.0006527372170239687\n",
      "Iteration is: 19211 and loss is: 0.0007715618121437728\n",
      "Iteration is: 19212 and loss is: 0.0002666656509973109\n",
      "Iteration is: 19213 and loss is: 0.0005440261447802186\n",
      "Iteration is: 19214 and loss is: 0.0011732386192306876\n",
      "Iteration is: 19215 and loss is: 0.0017844348913058639\n",
      "Iteration is: 19216 and loss is: 0.002689154352992773\n",
      "Iteration is: 19217 and loss is: 0.0033841009717434645\n",
      "Iteration is: 19218 and loss is: 0.0027303423266857862\n",
      "Iteration is: 19219 and loss is: 0.0005726399831473827\n",
      "Iteration is: 19220 and loss is: 0.0019979821518063545\n",
      "Iteration is: 19221 and loss is: 0.002608770737424493\n",
      "Iteration is: 19222 and loss is: 0.0013845069333910942\n",
      "Iteration is: 19223 and loss is: 0.0010815690038725734\n",
      "Iteration is: 19224 and loss is: 0.002340157050639391\n",
      "Iteration is: 19225 and loss is: 0.002380800899118185\n",
      "Iteration is: 19226 and loss is: 0.0013690360356122255\n",
      "Iteration is: 19227 and loss is: 0.0007665829034522176\n",
      "Iteration is: 19228 and loss is: 0.001137044746428728\n",
      "Iteration is: 19229 and loss is: 0.000962967867963016\n",
      "Iteration is: 19230 and loss is: 0.0006700007943436503\n",
      "Iteration is: 19231 and loss is: 0.000822603062260896\n",
      "Iteration is: 19232 and loss is: 0.0005635926499962807\n",
      "Iteration is: 19233 and loss is: 0.0004599660460371524\n",
      "Iteration is: 19234 and loss is: 0.0009226500988006592\n",
      "Iteration is: 19235 and loss is: 0.0020449936855584383\n",
      "Iteration is: 19236 and loss is: 0.0038044697139412165\n",
      "Iteration is: 19237 and loss is: 0.005811372306197882\n",
      "Iteration is: 19238 and loss is: 0.007890581153333187\n",
      "Iteration is: 19239 and loss is: 0.00837580393999815\n",
      "Iteration is: 19240 and loss is: 0.004905153531581163\n",
      "Iteration is: 19241 and loss is: 0.0010516834445297718\n",
      "Iteration is: 19242 and loss is: 0.004013163037598133\n",
      "Iteration is: 19243 and loss is: 0.004114285111427307\n",
      "Iteration is: 19244 and loss is: 0.0013603630941361189\n",
      "Iteration is: 19245 and loss is: 0.003474348923191428\n",
      "Iteration is: 19246 and loss is: 0.0038958515506237745\n",
      "Iteration is: 19247 and loss is: 0.0016093781450763345\n",
      "Iteration is: 19248 and loss is: 0.0016434331191703677\n",
      "Iteration is: 19249 and loss is: 0.0023689069785177708\n",
      "Iteration is: 19250 and loss is: 0.000507857883349061\n",
      "Iteration is: 19251 and loss is: 0.0014736056327819824\n",
      "Iteration is: 19252 and loss is: 0.0008469450986012816\n",
      "Iteration is: 19253 and loss is: 0.0008671647519804537\n",
      "Iteration is: 19254 and loss is: 0.0012685686815530062\n",
      "Iteration is: 19255 and loss is: 0.0009070766973309219\n",
      "Iteration is: 19256 and loss is: 0.0008770236745476723\n",
      "Iteration is: 19257 and loss is: 0.000667686341330409\n",
      "Iteration is: 19258 and loss is: 0.001005243742838502\n",
      "Iteration is: 19259 and loss is: 0.000656215357594192\n",
      "Iteration is: 19260 and loss is: 0.0006473082466982305\n",
      "Iteration is: 19261 and loss is: 0.0005180244334042072\n",
      "Iteration is: 19262 and loss is: 0.0005065999575890601\n",
      "Iteration is: 19263 and loss is: 0.0005806843983009458\n",
      "Iteration is: 19264 and loss is: 0.00033207284286618233\n",
      "Iteration is: 19265 and loss is: 0.0004713528323918581\n",
      "Iteration is: 19266 and loss is: 0.00028784156893379986\n",
      "Iteration is: 19267 and loss is: 0.00043454740080051124\n",
      "Iteration is: 19268 and loss is: 0.00024972151732072234\n",
      "Iteration is: 19269 and loss is: 0.000387331674573943\n",
      "Iteration is: 19270 and loss is: 0.0002189200749853626\n",
      "Iteration is: 19271 and loss is: 0.0003230447182431817\n",
      "Iteration is: 19272 and loss is: 0.00022690912010148168\n",
      "Iteration is: 19273 and loss is: 0.00026001030346378684\n",
      "Iteration is: 19274 and loss is: 0.0002531935751903802\n",
      "Iteration is: 19275 and loss is: 0.00020442255481611937\n",
      "Iteration is: 19276 and loss is: 0.0002646792563609779\n",
      "Iteration is: 19277 and loss is: 0.0001819728349801153\n",
      "Iteration is: 19278 and loss is: 0.00024269628920592368\n",
      "Iteration is: 19279 and loss is: 0.00017798080807551742\n",
      "Iteration is: 19280 and loss is: 0.0002025405556196347\n",
      "Iteration is: 19281 and loss is: 0.0001782210310921073\n",
      "Iteration is: 19282 and loss is: 0.00017467467114329338\n",
      "Iteration is: 19283 and loss is: 0.00017606256005819887\n",
      "Iteration is: 19284 and loss is: 0.0001583319390192628\n",
      "Iteration is: 19285 and loss is: 0.00016900859191082418\n",
      "Iteration is: 19286 and loss is: 0.0001442769425921142\n",
      "Iteration is: 19287 and loss is: 0.00016016163863241673\n",
      "Iteration is: 19288 and loss is: 0.0001406842638971284\n",
      "Iteration is: 19289 and loss is: 0.00014138763071969151\n",
      "Iteration is: 19290 and loss is: 0.00013976002810522914\n",
      "Iteration is: 19291 and loss is: 0.0001326285710092634\n",
      "Iteration is: 19292 and loss is: 0.0001432208955520764\n",
      "Iteration is: 19293 and loss is: 0.00012634838640224189\n",
      "Iteration is: 19294 and loss is: 0.00013461531489156187\n",
      "Iteration is: 19295 and loss is: 0.00012590766709763557\n",
      "Iteration is: 19296 and loss is: 0.00012504466576501727\n",
      "Iteration is: 19297 and loss is: 0.0001277000701520592\n",
      "Iteration is: 19298 and loss is: 0.0001192002382595092\n",
      "Iteration is: 19299 and loss is: 0.0001246713218279183\n",
      "Iteration is: 19300 and loss is: 0.0001178721577161923\n",
      "Iteration is: 19301 and loss is: 0.00012134059215895832\n",
      "Iteration is: 19302 and loss is: 0.00011583375453483313\n",
      "Iteration is: 19303 and loss is: 0.0001162225817097351\n",
      "Iteration is: 19304 and loss is: 0.00011595594696700573\n",
      "Iteration is: 19305 and loss is: 0.00011286187509540468\n",
      "Iteration is: 19306 and loss is: 0.00011559505219338462\n",
      "Iteration is: 19307 and loss is: 0.00011126264871563762\n",
      "Iteration is: 19308 and loss is: 0.00011320535850245506\n",
      "Iteration is: 19309 and loss is: 0.00011110184277640656\n",
      "Iteration is: 19310 and loss is: 0.00011037242074962705\n",
      "Iteration is: 19311 and loss is: 0.0001109441727749072\n",
      "Iteration is: 19312 and loss is: 0.00010857870802283287\n",
      "Iteration is: 19313 and loss is: 0.00010958639904856682\n",
      "Iteration is: 19314 and loss is: 0.0001084382165572606\n",
      "Iteration is: 19315 and loss is: 0.00010843494237633422\n",
      "Iteration is: 19316 and loss is: 0.00010810262756422162\n",
      "Iteration is: 19317 and loss is: 0.00010714651580201462\n",
      "Iteration is: 19318 and loss is: 0.00010752788512036204\n",
      "Iteration is: 19319 and loss is: 0.00010635342914611101\n",
      "Iteration is: 19320 and loss is: 0.00010687383473850787\n",
      "Iteration is: 19321 and loss is: 0.00010623886191751808\n",
      "Iteration is: 19322 and loss is: 0.00010589216253720224\n",
      "Iteration is: 19323 and loss is: 0.0001061063157976605\n",
      "Iteration is: 19324 and loss is: 0.00010520977957639843\n",
      "Iteration is: 19325 and loss is: 0.00010556215420365334\n",
      "Iteration is: 19326 and loss is: 0.00010501553333597258\n",
      "Iteration is: 19327 and loss is: 0.00010494566231500357\n",
      "Iteration is: 19328 and loss is: 0.00010487715189810842\n",
      "Iteration is: 19329 and loss is: 0.00010450518311699852\n",
      "Iteration is: 19330 and loss is: 0.00010456056770635769\n",
      "Iteration is: 19331 and loss is: 0.00010422531340736896\n",
      "Iteration is: 19332 and loss is: 0.00010423306957818568\n",
      "Iteration is: 19333 and loss is: 0.00010399307939223945\n",
      "Iteration is: 19334 and loss is: 0.00010383934568380937\n",
      "Iteration is: 19335 and loss is: 0.00010385190398665145\n",
      "Iteration is: 19336 and loss is: 0.00010357167047914118\n",
      "Iteration is: 19337 and loss is: 0.00010358900908613577\n",
      "Iteration is: 19338 and loss is: 0.00010344360634917393\n",
      "Iteration is: 19339 and loss is: 0.0001032746586133726\n",
      "Iteration is: 19340 and loss is: 0.00010326825577067211\n",
      "Iteration is: 19341 and loss is: 0.00010308472701581195\n",
      "Iteration is: 19342 and loss is: 0.00010305580508429557\n",
      "Iteration is: 19343 and loss is: 0.00010294147068634629\n",
      "Iteration is: 19344 and loss is: 0.00010285365715390071\n",
      "Iteration is: 19345 and loss is: 0.0001027831676765345\n",
      "Iteration is: 19346 and loss is: 0.00010264533193549141\n",
      "Iteration is: 19347 and loss is: 0.00010261850547976792\n",
      "Iteration is: 19348 and loss is: 0.00010249429033137858\n",
      "Iteration is: 19349 and loss is: 0.00010244415898341686\n",
      "Iteration is: 19350 and loss is: 0.00010238459799438715\n",
      "Iteration is: 19351 and loss is: 0.00010228480095975101\n",
      "Iteration is: 19352 and loss is: 0.00010224410652881488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 19353 and loss is: 0.00010214878420811146\n",
      "Iteration is: 19354 and loss is: 0.00010208948515355587\n",
      "Iteration is: 19355 and loss is: 0.00010202986595686525\n",
      "Iteration is: 19356 and loss is: 0.00010195464710704982\n",
      "Iteration is: 19357 and loss is: 0.00010190297325607389\n",
      "Iteration is: 19358 and loss is: 0.0001018312614178285\n",
      "Iteration is: 19359 and loss is: 0.00010177505464525893\n",
      "Iteration is: 19360 and loss is: 0.00010171395842917264\n",
      "Iteration is: 19361 and loss is: 0.00010165059211431071\n",
      "Iteration is: 19362 and loss is: 0.00010160329838981852\n",
      "Iteration is: 19363 and loss is: 0.00010153934272238985\n",
      "Iteration is: 19364 and loss is: 0.0001014907902572304\n",
      "Iteration is: 19365 and loss is: 0.0001014383087749593\n",
      "Iteration is: 19366 and loss is: 0.00010137847129954025\n",
      "Iteration is: 19367 and loss is: 0.0001013337605400011\n",
      "Iteration is: 19368 and loss is: 0.000101275640190579\n",
      "Iteration is: 19369 and loss is: 0.00010122844105353579\n",
      "Iteration is: 19370 and loss is: 0.0001011786371236667\n",
      "Iteration is: 19371 and loss is: 0.00010112655581906438\n",
      "Iteration is: 19372 and loss is: 0.0001010824489640072\n",
      "Iteration is: 19373 and loss is: 0.00010103180829901248\n",
      "Iteration is: 19374 and loss is: 0.00010098567145178095\n",
      "Iteration is: 19375 and loss is: 0.00010093892342410982\n",
      "Iteration is: 19376 and loss is: 0.00010089341958519071\n",
      "Iteration is: 19377 and loss is: 0.00010085041867569089\n",
      "Iteration is: 19378 and loss is: 0.00010080411448143423\n",
      "Iteration is: 19379 and loss is: 0.00010076060425490141\n",
      "Iteration is: 19380 and loss is: 0.00010071732685901225\n",
      "Iteration is: 19381 and loss is: 0.00010067141556646675\n",
      "Iteration is: 19382 and loss is: 0.00010063010995509103\n",
      "Iteration is: 19383 and loss is: 0.00010058867337647825\n",
      "Iteration is: 19384 and loss is: 0.00010054761514766142\n",
      "Iteration is: 19385 and loss is: 0.00010050644050352275\n",
      "Iteration is: 19386 and loss is: 0.00010046549869002774\n",
      "Iteration is: 19387 and loss is: 0.00010042634676210582\n",
      "Iteration is: 19388 and loss is: 0.00010038535401690751\n",
      "Iteration is: 19389 and loss is: 0.00010034708247985691\n",
      "Iteration is: 19390 and loss is: 0.00010030663543147966\n",
      "Iteration is: 19391 and loss is: 0.00010026802192442119\n",
      "Iteration is: 19392 and loss is: 0.00010023004870163277\n",
      "Iteration is: 19393 and loss is: 0.0001001932832878083\n",
      "Iteration is: 19394 and loss is: 0.00010015446605393663\n",
      "Iteration is: 19395 and loss is: 0.00010011711128754541\n",
      "Iteration is: 19396 and loss is: 0.00010007838136516511\n",
      "Iteration is: 19397 and loss is: 0.00010004298383137211\n",
      "Iteration is: 19398 and loss is: 0.00010000699694501236\n",
      "Iteration is: 19399 and loss is: 9.997127199312672e-05\n",
      "Iteration is: 19400 and loss is: 9.993538697017357e-05\n",
      "Iteration is: 19401 and loss is: 9.989862883230671e-05\n",
      "Iteration is: 19402 and loss is: 9.986328223021701e-05\n",
      "Iteration is: 19403 and loss is: 9.982976916944608e-05\n",
      "Iteration is: 19404 and loss is: 9.979450260289013e-05\n",
      "Iteration is: 19405 and loss is: 9.975858120014891e-05\n",
      "Iteration is: 19406 and loss is: 9.972559928428382e-05\n",
      "Iteration is: 19407 and loss is: 9.969203529180959e-05\n",
      "Iteration is: 19408 and loss is: 9.965876233763993e-05\n",
      "Iteration is: 19409 and loss is: 9.962482727132738e-05\n",
      "Iteration is: 19410 and loss is: 9.959236194845289e-05\n",
      "Iteration is: 19411 and loss is: 9.955962013918906e-05\n",
      "Iteration is: 19412 and loss is: 9.952719119610265e-05\n",
      "Iteration is: 19413 and loss is: 9.949349623639137e-05\n",
      "Iteration is: 19414 and loss is: 9.946225327439606e-05\n",
      "Iteration is: 19415 and loss is: 9.942917677108198e-05\n",
      "Iteration is: 19416 and loss is: 9.939685696735978e-05\n",
      "Iteration is: 19417 and loss is: 9.936640708474442e-05\n",
      "Iteration is: 19418 and loss is: 9.933519322657958e-05\n",
      "Iteration is: 19419 and loss is: 9.930403030011803e-05\n",
      "Iteration is: 19420 and loss is: 9.927128121489659e-05\n",
      "Iteration is: 19421 and loss is: 9.924211190082133e-05\n",
      "Iteration is: 19422 and loss is: 9.921106538968161e-05\n",
      "Iteration is: 19423 and loss is: 9.918054274749011e-05\n",
      "Iteration is: 19424 and loss is: 9.914962720358744e-05\n",
      "Iteration is: 19425 and loss is: 9.911906818160787e-05\n",
      "Iteration is: 19426 and loss is: 9.908956417348236e-05\n",
      "Iteration is: 19427 and loss is: 9.906027844408527e-05\n",
      "Iteration is: 19428 and loss is: 9.903086902340874e-05\n",
      "Iteration is: 19429 and loss is: 9.900019358610734e-05\n",
      "Iteration is: 19430 and loss is: 9.897242853185162e-05\n",
      "Iteration is: 19431 and loss is: 9.89437394309789e-05\n",
      "Iteration is: 19432 and loss is: 9.891427180264145e-05\n",
      "Iteration is: 19433 and loss is: 9.88842875813134e-05\n",
      "Iteration is: 19434 and loss is: 9.885660256259143e-05\n",
      "Iteration is: 19435 and loss is: 9.882786253001541e-05\n",
      "Iteration is: 19436 and loss is: 9.879944263957441e-05\n",
      "Iteration is: 19437 and loss is: 9.877220873022452e-05\n",
      "Iteration is: 19438 and loss is: 9.87421881291084e-05\n",
      "Iteration is: 19439 and loss is: 9.871539077721536e-05\n",
      "Iteration is: 19440 and loss is: 9.868726192507893e-05\n",
      "Iteration is: 19441 and loss is: 9.865880565484986e-05\n",
      "Iteration is: 19442 and loss is: 9.863288141787052e-05\n",
      "Iteration is: 19443 and loss is: 9.860291902441531e-05\n",
      "Iteration is: 19444 and loss is: 9.85757724265568e-05\n",
      "Iteration is: 19445 and loss is: 9.855088137555867e-05\n",
      "Iteration is: 19446 and loss is: 9.85236038104631e-05\n",
      "Iteration is: 19447 and loss is: 9.849619527813047e-05\n",
      "Iteration is: 19448 and loss is: 9.846885950537398e-05\n",
      "Iteration is: 19449 and loss is: 9.844154556049034e-05\n",
      "Iteration is: 19450 and loss is: 9.841706196311861e-05\n",
      "Iteration is: 19451 and loss is: 9.838988626142964e-05\n",
      "Iteration is: 19452 and loss is: 9.836185199674219e-05\n",
      "Iteration is: 19453 and loss is: 9.833756485022604e-05\n",
      "Iteration is: 19454 and loss is: 9.831012721406296e-05\n",
      "Iteration is: 19455 and loss is: 9.8284705018159e-05\n",
      "Iteration is: 19456 and loss is: 9.825876622926444e-05\n",
      "Iteration is: 19457 and loss is: 9.823382424656302e-05\n",
      "Iteration is: 19458 and loss is: 9.820669947657734e-05\n",
      "Iteration is: 19459 and loss is: 9.818164835451171e-05\n",
      "Iteration is: 19460 and loss is: 9.815662633627653e-05\n",
      "Iteration is: 19461 and loss is: 9.813089855015278e-05\n",
      "Iteration is: 19462 and loss is: 9.810576011659577e-05\n",
      "Iteration is: 19463 and loss is: 9.808117465581745e-05\n",
      "Iteration is: 19464 and loss is: 9.805653098737821e-05\n",
      "Iteration is: 19465 and loss is: 9.80304685072042e-05\n",
      "Iteration is: 19466 and loss is: 9.800542466109619e-05\n",
      "Iteration is: 19467 and loss is: 9.798003156902269e-05\n",
      "Iteration is: 19468 and loss is: 9.795518417377025e-05\n",
      "Iteration is: 19469 and loss is: 9.793242497835308e-05\n",
      "Iteration is: 19470 and loss is: 9.790735202841461e-05\n",
      "Iteration is: 19471 and loss is: 9.788218449102715e-05\n",
      "Iteration is: 19472 and loss is: 9.785720612853765e-05\n",
      "Iteration is: 19473 and loss is: 9.783375571714714e-05\n",
      "Iteration is: 19474 and loss is: 9.781024709809572e-05\n",
      "Iteration is: 19475 and loss is: 9.778641106095165e-05\n",
      "Iteration is: 19476 and loss is: 9.776101796887815e-05\n",
      "Iteration is: 19477 and loss is: 9.773849160410464e-05\n",
      "Iteration is: 19478 and loss is: 9.771481563802809e-05\n",
      "Iteration is: 19479 and loss is: 9.768955351319164e-05\n",
      "Iteration is: 19480 and loss is: 9.766758739715442e-05\n",
      "Iteration is: 19481 and loss is: 9.764306014403701e-05\n",
      "Iteration is: 19482 and loss is: 9.762059198692441e-05\n",
      "Iteration is: 19483 and loss is: 9.759739623405039e-05\n",
      "Iteration is: 19484 and loss is: 9.757448424352333e-05\n",
      "Iteration is: 19485 and loss is: 9.755033534020185e-05\n",
      "Iteration is: 19486 and loss is: 9.752998448675498e-05\n",
      "Iteration is: 19487 and loss is: 9.750922617968172e-05\n",
      "Iteration is: 19488 and loss is: 9.748975571710616e-05\n",
      "Iteration is: 19489 and loss is: 9.747287549544126e-05\n",
      "Iteration is: 19490 and loss is: 9.746035357238725e-05\n",
      "Iteration is: 19491 and loss is: 9.745043644215912e-05\n",
      "Iteration is: 19492 and loss is: 9.745233546709642e-05\n",
      "Iteration is: 19493 and loss is: 9.746768773766235e-05\n",
      "Iteration is: 19494 and loss is: 9.750845492817461e-05\n",
      "Iteration is: 19495 and loss is: 9.759068780113012e-05\n",
      "Iteration is: 19496 and loss is: 9.77407325990498e-05\n",
      "Iteration is: 19497 and loss is: 9.800569387152791e-05\n",
      "Iteration is: 19498 and loss is: 9.846422472037375e-05\n",
      "Iteration is: 19499 and loss is: 9.925992344506085e-05\n",
      "Iteration is: 19500 and loss is: 0.00010061243665404618\n",
      "Iteration is: 19501 and loss is: 0.00010296335676684976\n",
      "Iteration is: 19502 and loss is: 0.00010695972014218569\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 19503 and loss is: 0.00011398597416700795\n",
      "Iteration is: 19504 and loss is: 0.00012587467790581286\n",
      "Iteration is: 19505 and loss is: 0.0001469730632379651\n",
      "Iteration is: 19506 and loss is: 0.0001816921285353601\n",
      "Iteration is: 19507 and loss is: 0.000242721289396286\n",
      "Iteration is: 19508 and loss is: 0.0003336268709972501\n",
      "Iteration is: 19509 and loss is: 0.00047893397277221084\n",
      "Iteration is: 19510 and loss is: 0.0006262502865865827\n",
      "Iteration is: 19511 and loss is: 0.0007421503541991115\n",
      "Iteration is: 19512 and loss is: 0.0006702175014652312\n",
      "Iteration is: 19513 and loss is: 0.00040765153244137764\n",
      "Iteration is: 19514 and loss is: 0.00018098113650921732\n",
      "Iteration is: 19515 and loss is: 0.00015991437248885632\n",
      "Iteration is: 19516 and loss is: 0.0003038504801224917\n",
      "Iteration is: 19517 and loss is: 0.0004065374087076634\n",
      "Iteration is: 19518 and loss is: 0.00033274799352511764\n",
      "Iteration is: 19519 and loss is: 0.00018423277651891112\n",
      "Iteration is: 19520 and loss is: 0.00013007411325816065\n",
      "Iteration is: 19521 and loss is: 0.00020652200328186154\n",
      "Iteration is: 19522 and loss is: 0.00028818269493058324\n",
      "Iteration is: 19523 and loss is: 0.00026766181690618396\n",
      "Iteration is: 19524 and loss is: 0.00018727379210758954\n",
      "Iteration is: 19525 and loss is: 0.00012749803136102855\n",
      "Iteration is: 19526 and loss is: 0.00011955969239352271\n",
      "Iteration is: 19527 and loss is: 0.0001563397527206689\n",
      "Iteration is: 19528 and loss is: 0.00022355713008437306\n",
      "Iteration is: 19529 and loss is: 0.00032561973785050213\n",
      "Iteration is: 19530 and loss is: 0.000499861896969378\n",
      "Iteration is: 19531 and loss is: 0.0007302393787540495\n",
      "Iteration is: 19532 and loss is: 0.0010572089813649654\n",
      "Iteration is: 19533 and loss is: 0.0011794460006058216\n",
      "Iteration is: 19534 and loss is: 0.0008805508841760457\n",
      "Iteration is: 19535 and loss is: 0.0003996805171482265\n",
      "Iteration is: 19536 and loss is: 0.00019869243260473013\n",
      "Iteration is: 19537 and loss is: 0.00035821698838844895\n",
      "Iteration is: 19538 and loss is: 0.000535562343429774\n",
      "Iteration is: 19539 and loss is: 0.0004557137144729495\n",
      "Iteration is: 19540 and loss is: 0.00023764900106471032\n",
      "Iteration is: 19541 and loss is: 0.00018240860663354397\n",
      "Iteration is: 19542 and loss is: 0.00034203979885205626\n",
      "Iteration is: 19543 and loss is: 0.00041392038110643625\n",
      "Iteration is: 19544 and loss is: 0.0002535904641263187\n",
      "Iteration is: 19545 and loss is: 0.0001457067410228774\n",
      "Iteration is: 19546 and loss is: 0.00019017737940885127\n",
      "Iteration is: 19547 and loss is: 0.00023435722687281668\n",
      "Iteration is: 19548 and loss is: 0.00026713332044892013\n",
      "Iteration is: 19549 and loss is: 0.0003386197495274246\n",
      "Iteration is: 19550 and loss is: 0.00040525110671296716\n",
      "Iteration is: 19551 and loss is: 0.00045542020234279335\n",
      "Iteration is: 19552 and loss is: 0.0004617081140168011\n",
      "Iteration is: 19553 and loss is: 0.00046370812924578786\n",
      "Iteration is: 19554 and loss is: 0.00038290160591714084\n",
      "Iteration is: 19555 and loss is: 0.00022941449424251914\n",
      "Iteration is: 19556 and loss is: 0.00012440112186595798\n",
      "Iteration is: 19557 and loss is: 0.00013901092461310327\n",
      "Iteration is: 19558 and loss is: 0.00021730997832491994\n",
      "Iteration is: 19559 and loss is: 0.0002781321236398071\n",
      "Iteration is: 19560 and loss is: 0.00030278172926045954\n",
      "Iteration is: 19561 and loss is: 0.00028937976458109915\n",
      "Iteration is: 19562 and loss is: 0.0002510763588361442\n",
      "Iteration is: 19563 and loss is: 0.00019252602942287922\n",
      "Iteration is: 19564 and loss is: 0.00014888073201291263\n",
      "Iteration is: 19565 and loss is: 0.00011933067435165867\n",
      "Iteration is: 19566 and loss is: 0.00010325327457394451\n",
      "Iteration is: 19567 and loss is: 0.0001096754422178492\n",
      "Iteration is: 19568 and loss is: 0.00013422832125797868\n",
      "Iteration is: 19569 and loss is: 0.00016340198635589331\n",
      "Iteration is: 19570 and loss is: 0.0001956227933987975\n",
      "Iteration is: 19571 and loss is: 0.00024047463375609368\n",
      "Iteration is: 19572 and loss is: 0.0002798032946884632\n",
      "Iteration is: 19573 and loss is: 0.0003090367536060512\n",
      "Iteration is: 19574 and loss is: 0.00030034390510991216\n",
      "Iteration is: 19575 and loss is: 0.0002632152463775128\n",
      "Iteration is: 19576 and loss is: 0.00019738977425731719\n",
      "Iteration is: 19577 and loss is: 0.000133814086439088\n",
      "Iteration is: 19578 and loss is: 0.00010428616951685399\n",
      "Iteration is: 19579 and loss is: 0.00011487644223961979\n",
      "Iteration is: 19580 and loss is: 0.00014767696848139167\n",
      "Iteration is: 19581 and loss is: 0.00019419770978856832\n",
      "Iteration is: 19582 and loss is: 0.0002689285029191524\n",
      "Iteration is: 19583 and loss is: 0.0003740329702850431\n",
      "Iteration is: 19584 and loss is: 0.000532596604898572\n",
      "Iteration is: 19585 and loss is: 0.0006582576897926629\n",
      "Iteration is: 19586 and loss is: 0.0007031660061329603\n",
      "Iteration is: 19587 and loss is: 0.0005613499670289457\n",
      "Iteration is: 19588 and loss is: 0.00030118212453089654\n",
      "Iteration is: 19589 and loss is: 0.00014775828458368778\n",
      "Iteration is: 19590 and loss is: 0.00020063816918991506\n",
      "Iteration is: 19591 and loss is: 0.00035060103982686996\n",
      "Iteration is: 19592 and loss is: 0.000398281030356884\n",
      "Iteration is: 19593 and loss is: 0.00027560623129829764\n",
      "Iteration is: 19594 and loss is: 0.00014581470168195665\n",
      "Iteration is: 19595 and loss is: 0.00014308000390883535\n",
      "Iteration is: 19596 and loss is: 0.00023214287648443133\n",
      "Iteration is: 19597 and loss is: 0.00028754514642059803\n",
      "Iteration is: 19598 and loss is: 0.0002504721633158624\n",
      "Iteration is: 19599 and loss is: 0.0001791816030163318\n",
      "Iteration is: 19600 and loss is: 0.00012768266606144607\n",
      "Iteration is: 19601 and loss is: 0.00011236613499931991\n",
      "Iteration is: 19602 and loss is: 0.0001334050903096795\n",
      "Iteration is: 19603 and loss is: 0.0001974189653992653\n",
      "Iteration is: 19604 and loss is: 0.0003094053827226162\n",
      "Iteration is: 19605 and loss is: 0.0005085748271085322\n",
      "Iteration is: 19606 and loss is: 0.0007772142416797578\n",
      "Iteration is: 19607 and loss is: 0.0011333017610013485\n",
      "Iteration is: 19608 and loss is: 0.001211346942000091\n",
      "Iteration is: 19609 and loss is: 0.0007997159846127033\n",
      "Iteration is: 19610 and loss is: 0.00033005978912115097\n",
      "Iteration is: 19611 and loss is: 0.0002463621785864234\n",
      "Iteration is: 19612 and loss is: 0.0004707368207164109\n",
      "Iteration is: 19613 and loss is: 0.0005982777802273631\n",
      "Iteration is: 19614 and loss is: 0.0004300234722904861\n",
      "Iteration is: 19615 and loss is: 0.00021018029656261206\n",
      "Iteration is: 19616 and loss is: 0.00023243622854351997\n",
      "Iteration is: 19617 and loss is: 0.0004061786748934537\n",
      "Iteration is: 19618 and loss is: 0.00042034394573420286\n",
      "Iteration is: 19619 and loss is: 0.00022059834736865014\n",
      "Iteration is: 19620 and loss is: 0.00014571467181667686\n",
      "Iteration is: 19621 and loss is: 0.00022647219884674996\n",
      "Iteration is: 19622 and loss is: 0.00024697056505829096\n",
      "Iteration is: 19623 and loss is: 0.0002775067405309528\n",
      "Iteration is: 19624 and loss is: 0.00041776220314204693\n",
      "Iteration is: 19625 and loss is: 0.0005868359003216028\n",
      "Iteration is: 19626 and loss is: 0.00077197200153023\n",
      "Iteration is: 19627 and loss is: 0.0008400316582992673\n",
      "Iteration is: 19628 and loss is: 0.0007646079175174236\n",
      "Iteration is: 19629 and loss is: 0.0005049996543675661\n",
      "Iteration is: 19630 and loss is: 0.00021335852215997875\n",
      "Iteration is: 19631 and loss is: 0.00019481271738186479\n",
      "Iteration is: 19632 and loss is: 0.00040106161031872034\n",
      "Iteration is: 19633 and loss is: 0.00046774110523983836\n",
      "Iteration is: 19634 and loss is: 0.0002905355941038579\n",
      "Iteration is: 19635 and loss is: 0.00013962466618977487\n",
      "Iteration is: 19636 and loss is: 0.00021747822756879032\n",
      "Iteration is: 19637 and loss is: 0.0003487918002065271\n",
      "Iteration is: 19638 and loss is: 0.00031287127058021724\n",
      "Iteration is: 19639 and loss is: 0.00019346266344655305\n",
      "Iteration is: 19640 and loss is: 0.00013978852075524628\n",
      "Iteration is: 19641 and loss is: 0.00016765855252742767\n",
      "Iteration is: 19642 and loss is: 0.00022680264373775572\n",
      "Iteration is: 19643 and loss is: 0.00026900414377450943\n",
      "Iteration is: 19644 and loss is: 0.0002783310483209789\n",
      "Iteration is: 19645 and loss is: 0.0002570617652963847\n",
      "Iteration is: 19646 and loss is: 0.0002089061017613858\n",
      "Iteration is: 19647 and loss is: 0.00017464203119743615\n",
      "Iteration is: 19648 and loss is: 0.0001479802594985813\n",
      "Iteration is: 19649 and loss is: 0.00012169005640316755\n",
      "Iteration is: 19650 and loss is: 0.00010540134098846465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 19651 and loss is: 0.00010520424984861165\n",
      "Iteration is: 19652 and loss is: 0.0001100737790693529\n",
      "Iteration is: 19653 and loss is: 0.00011368713603587821\n",
      "Iteration is: 19654 and loss is: 0.00012516639253590256\n",
      "Iteration is: 19655 and loss is: 0.0001454593293601647\n",
      "Iteration is: 19656 and loss is: 0.00017704942729324102\n",
      "Iteration is: 19657 and loss is: 0.00022275444644037634\n",
      "Iteration is: 19658 and loss is: 0.0002977682161144912\n",
      "Iteration is: 19659 and loss is: 0.0003812649520114064\n",
      "Iteration is: 19660 and loss is: 0.0004616905644070357\n",
      "Iteration is: 19661 and loss is: 0.00047526886919513345\n",
      "Iteration is: 19662 and loss is: 0.0004102990496903658\n",
      "Iteration is: 19663 and loss is: 0.00027013951330445707\n",
      "Iteration is: 19664 and loss is: 0.00014627428026869893\n",
      "Iteration is: 19665 and loss is: 0.00012030052312184125\n",
      "Iteration is: 19666 and loss is: 0.00018289605213794857\n",
      "Iteration is: 19667 and loss is: 0.00025730193010531366\n",
      "Iteration is: 19668 and loss is: 0.00028013275004923344\n",
      "Iteration is: 19669 and loss is: 0.00024224509252235293\n",
      "Iteration is: 19670 and loss is: 0.00017857558850664645\n",
      "Iteration is: 19671 and loss is: 0.00012845444143749774\n",
      "Iteration is: 19672 and loss is: 0.00010719736746978015\n",
      "Iteration is: 19673 and loss is: 0.00010525387187954038\n",
      "Iteration is: 19674 and loss is: 0.00011580422869883478\n",
      "Iteration is: 19675 and loss is: 0.00014518701937049627\n",
      "Iteration is: 19676 and loss is: 0.00020545186998788267\n",
      "Iteration is: 19677 and loss is: 0.000302482076222077\n",
      "Iteration is: 19678 and loss is: 0.0004696930991485715\n",
      "Iteration is: 19679 and loss is: 0.0006525072385556996\n",
      "Iteration is: 19680 and loss is: 0.0007791311945766211\n",
      "Iteration is: 19681 and loss is: 0.0006917890277691185\n",
      "Iteration is: 19682 and loss is: 0.0004023838846478611\n",
      "Iteration is: 19683 and loss is: 0.00017025308625306934\n",
      "Iteration is: 19684 and loss is: 0.00018089640070684254\n",
      "Iteration is: 19685 and loss is: 0.00035014463355764747\n",
      "Iteration is: 19686 and loss is: 0.0004294970422051847\n",
      "Iteration is: 19687 and loss is: 0.0003165572416037321\n",
      "Iteration is: 19688 and loss is: 0.0001663590664975345\n",
      "Iteration is: 19689 and loss is: 0.00014350532728713006\n",
      "Iteration is: 19690 and loss is: 0.0002407694119028747\n",
      "Iteration is: 19691 and loss is: 0.000314599194098264\n",
      "Iteration is: 19692 and loss is: 0.000268438394414261\n",
      "Iteration is: 19693 and loss is: 0.00017282880435232073\n",
      "Iteration is: 19694 and loss is: 0.00012396983220241964\n",
      "Iteration is: 19695 and loss is: 0.0001324432232649997\n",
      "Iteration is: 19696 and loss is: 0.00017272293916903436\n",
      "Iteration is: 19697 and loss is: 0.00023855388280935585\n",
      "Iteration is: 19698 and loss is: 0.00034993240842595696\n",
      "Iteration is: 19699 and loss is: 0.000561682041734457\n",
      "Iteration is: 19700 and loss is: 0.0008292099228128791\n",
      "Iteration is: 19701 and loss is: 0.0011547652538865805\n",
      "Iteration is: 19702 and loss is: 0.0011737567838281393\n",
      "Iteration is: 19703 and loss is: 0.000749436323530972\n",
      "Iteration is: 19704 and loss is: 0.0003376969543751329\n",
      "Iteration is: 19705 and loss is: 0.00029801821801811457\n",
      "Iteration is: 19706 and loss is: 0.0005050579202361405\n",
      "Iteration is: 19707 and loss is: 0.0006039184518158436\n",
      "Iteration is: 19708 and loss is: 0.0004121373640373349\n",
      "Iteration is: 19709 and loss is: 0.000217154884012416\n",
      "Iteration is: 19710 and loss is: 0.0002804397663567215\n",
      "Iteration is: 19711 and loss is: 0.00043163701775483787\n",
      "Iteration is: 19712 and loss is: 0.0003876459668390453\n",
      "Iteration is: 19713 and loss is: 0.00018370733596384525\n",
      "Iteration is: 19714 and loss is: 0.00015940182493068278\n",
      "Iteration is: 19715 and loss is: 0.00026991224149242043\n",
      "Iteration is: 19716 and loss is: 0.00025201556854881346\n",
      "Iteration is: 19717 and loss is: 0.00026181997964158654\n",
      "Iteration is: 19718 and loss is: 0.0004601140390150249\n",
      "Iteration is: 19719 and loss is: 0.000742736563552171\n",
      "Iteration is: 19720 and loss is: 0.0011402209056541324\n",
      "Iteration is: 19721 and loss is: 0.0013836892321705818\n",
      "Iteration is: 19722 and loss is: 0.0012232443550601602\n",
      "Iteration is: 19723 and loss is: 0.000719146803021431\n",
      "Iteration is: 19724 and loss is: 0.0003249497967772186\n",
      "Iteration is: 19725 and loss is: 0.00041516555938869715\n",
      "Iteration is: 19726 and loss is: 0.0006777357193641365\n",
      "Iteration is: 19727 and loss is: 0.0005506513407453895\n",
      "Iteration is: 19728 and loss is: 0.0002795378677546978\n",
      "Iteration is: 19729 and loss is: 0.00031021074391901493\n",
      "Iteration is: 19730 and loss is: 0.00046962109627202153\n",
      "Iteration is: 19731 and loss is: 0.0003867144405376166\n",
      "Iteration is: 19732 and loss is: 0.00017560328706167638\n",
      "Iteration is: 19733 and loss is: 0.00022607832215726376\n",
      "Iteration is: 19734 and loss is: 0.0003609245759434998\n",
      "Iteration is: 19735 and loss is: 0.000281777058262378\n",
      "Iteration is: 19736 and loss is: 0.00023043635883368552\n",
      "Iteration is: 19737 and loss is: 0.00028916733572259545\n",
      "Iteration is: 19738 and loss is: 0.00026270002126693726\n",
      "Iteration is: 19739 and loss is: 0.00018503546016290784\n",
      "Iteration is: 19740 and loss is: 0.00018780444224830717\n",
      "Iteration is: 19741 and loss is: 0.00021947798086330295\n",
      "Iteration is: 19742 and loss is: 0.00019798114954028279\n",
      "Iteration is: 19743 and loss is: 0.00017756639863364398\n",
      "Iteration is: 19744 and loss is: 0.00021841777197550982\n",
      "Iteration is: 19745 and loss is: 0.00025933695724233985\n",
      "Iteration is: 19746 and loss is: 0.0002587783383205533\n",
      "Iteration is: 19747 and loss is: 0.0002723898505792022\n",
      "Iteration is: 19748 and loss is: 0.00028845653287135065\n",
      "Iteration is: 19749 and loss is: 0.00023771903943270445\n",
      "Iteration is: 19750 and loss is: 0.00015482649905607104\n",
      "Iteration is: 19751 and loss is: 0.0001221098063979298\n",
      "Iteration is: 19752 and loss is: 0.00012986281944904476\n",
      "Iteration is: 19753 and loss is: 0.00014128147449810058\n",
      "Iteration is: 19754 and loss is: 0.00016953505109995604\n",
      "Iteration is: 19755 and loss is: 0.00023865305411163718\n",
      "Iteration is: 19756 and loss is: 0.0003448752104304731\n",
      "Iteration is: 19757 and loss is: 0.00046782277058809996\n",
      "Iteration is: 19758 and loss is: 0.0006487264181487262\n",
      "Iteration is: 19759 and loss is: 0.0007404637290164828\n",
      "Iteration is: 19760 and loss is: 0.000606214627623558\n",
      "Iteration is: 19761 and loss is: 0.0003198441118001938\n",
      "Iteration is: 19762 and loss is: 0.00014436934725381434\n",
      "Iteration is: 19763 and loss is: 0.00021849150652997196\n",
      "Iteration is: 19764 and loss is: 0.0003719436936080456\n",
      "Iteration is: 19765 and loss is: 0.0003678400535136461\n",
      "Iteration is: 19766 and loss is: 0.00021639713668264449\n",
      "Iteration is: 19767 and loss is: 0.00013233264326117933\n",
      "Iteration is: 19768 and loss is: 0.00020517206576187164\n",
      "Iteration is: 19769 and loss is: 0.00029136892408132553\n",
      "Iteration is: 19770 and loss is: 0.0002681380428839475\n",
      "Iteration is: 19771 and loss is: 0.00017519092943985015\n",
      "Iteration is: 19772 and loss is: 0.00011940162221435457\n",
      "Iteration is: 19773 and loss is: 0.00014462528633885086\n",
      "Iteration is: 19774 and loss is: 0.00020073546329513192\n",
      "Iteration is: 19775 and loss is: 0.0002406680869171396\n",
      "Iteration is: 19776 and loss is: 0.000278018182143569\n",
      "Iteration is: 19777 and loss is: 0.0003330901963636279\n",
      "Iteration is: 19778 and loss is: 0.0003889720537699759\n",
      "Iteration is: 19779 and loss is: 0.0004762590688187629\n",
      "Iteration is: 19780 and loss is: 0.0005284714861772954\n",
      "Iteration is: 19781 and loss is: 0.0005155943799763918\n",
      "Iteration is: 19782 and loss is: 0.0003863092861138284\n",
      "Iteration is: 19783 and loss is: 0.00021039570856373757\n",
      "Iteration is: 19784 and loss is: 0.0001223251165356487\n",
      "Iteration is: 19785 and loss is: 0.00017505945288576186\n",
      "Iteration is: 19786 and loss is: 0.00027725190739147365\n",
      "Iteration is: 19787 and loss is: 0.0003059497394133359\n",
      "Iteration is: 19788 and loss is: 0.00023801036877557635\n",
      "Iteration is: 19789 and loss is: 0.0001475326280342415\n",
      "Iteration is: 19790 and loss is: 0.00011236994760110974\n",
      "Iteration is: 19791 and loss is: 0.00014251629181671888\n",
      "Iteration is: 19792 and loss is: 0.00020464599947445095\n",
      "Iteration is: 19793 and loss is: 0.0002685411018319428\n",
      "Iteration is: 19794 and loss is: 0.00033174254349432886\n",
      "Iteration is: 19795 and loss is: 0.00041946847341023386\n",
      "Iteration is: 19796 and loss is: 0.0004789665399584919\n",
      "Iteration is: 19797 and loss is: 0.0004998691729269922\n",
      "Iteration is: 19798 and loss is: 0.00041920095100067556\n",
      "Iteration is: 19799 and loss is: 0.0002667179796844721\n",
      "Iteration is: 19800 and loss is: 0.00014648149954155087\n",
      "Iteration is: 19801 and loss is: 0.00013590460002887994\n",
      "Iteration is: 19802 and loss is: 0.0002175746631110087\n",
      "Iteration is: 19803 and loss is: 0.00028935004957020283\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 19804 and loss is: 0.00026913295732811093\n",
      "Iteration is: 19805 and loss is: 0.00018753542099148035\n",
      "Iteration is: 19806 and loss is: 0.00012127595255151391\n",
      "Iteration is: 19807 and loss is: 0.00011238009756198153\n",
      "Iteration is: 19808 and loss is: 0.00014782504877075553\n",
      "Iteration is: 19809 and loss is: 0.00020451025920920074\n",
      "Iteration is: 19810 and loss is: 0.00029084415291436017\n",
      "Iteration is: 19811 and loss is: 0.0004504747921600938\n",
      "Iteration is: 19812 and loss is: 0.0006569334072992206\n",
      "Iteration is: 19813 and loss is: 0.0009252277668565512\n",
      "Iteration is: 19814 and loss is: 0.000987617066130042\n",
      "Iteration is: 19815 and loss is: 0.0006989687099121511\n",
      "Iteration is: 19816 and loss is: 0.0003208157140761614\n",
      "Iteration is: 19817 and loss is: 0.0002098183031193912\n",
      "Iteration is: 19818 and loss is: 0.000358592311386019\n",
      "Iteration is: 19819 and loss is: 0.00048224988859146833\n",
      "Iteration is: 19820 and loss is: 0.0003902129246853292\n",
      "Iteration is: 19821 and loss is: 0.0002173667016904801\n",
      "Iteration is: 19822 and loss is: 0.00017812071018852293\n",
      "Iteration is: 19823 and loss is: 0.0002947346947621554\n",
      "Iteration is: 19824 and loss is: 0.00037123035872355103\n",
      "Iteration is: 19825 and loss is: 0.00026964728021994233\n",
      "Iteration is: 19826 and loss is: 0.00014370580902323127\n",
      "Iteration is: 19827 and loss is: 0.00014532201748806983\n",
      "Iteration is: 19828 and loss is: 0.00019627847359515727\n",
      "Iteration is: 19829 and loss is: 0.00021759024821221828\n",
      "Iteration is: 19830 and loss is: 0.0002960881683975458\n",
      "Iteration is: 19831 and loss is: 0.0005204220651648939\n",
      "Iteration is: 19832 and loss is: 0.000993911875411868\n",
      "Iteration is: 19833 and loss is: 0.0015205132076516747\n",
      "Iteration is: 19834 and loss is: 0.0018004466546699405\n",
      "Iteration is: 19835 and loss is: 0.0012059691362082958\n",
      "Iteration is: 19836 and loss is: 0.0005728136748075485\n",
      "Iteration is: 19837 and loss is: 0.0005776573671028018\n",
      "Iteration is: 19838 and loss is: 0.0005918106180615723\n",
      "Iteration is: 19839 and loss is: 0.0007266628090292215\n",
      "Iteration is: 19840 and loss is: 0.000645107589662075\n",
      "Iteration is: 19841 and loss is: 0.0005521518760360777\n",
      "Iteration is: 19842 and loss is: 0.0005448599113151431\n",
      "Iteration is: 19843 and loss is: 0.00040450377855449915\n",
      "Iteration is: 19844 and loss is: 0.00043319250107742846\n",
      "Iteration is: 19845 and loss is: 0.0002658316516317427\n",
      "Iteration is: 19846 and loss is: 0.0002741164353210479\n",
      "Iteration is: 19847 and loss is: 0.00039165234193205833\n",
      "Iteration is: 19848 and loss is: 0.00028060789918527007\n",
      "Iteration is: 19849 and loss is: 0.00020928496087435633\n",
      "Iteration is: 19850 and loss is: 0.0003685881965793669\n",
      "Iteration is: 19851 and loss is: 0.00047132972395047545\n",
      "Iteration is: 19852 and loss is: 0.0007623719284310937\n",
      "Iteration is: 19853 and loss is: 0.0015672064619138837\n",
      "Iteration is: 19854 and loss is: 0.002946163760498166\n",
      "Iteration is: 19855 and loss is: 0.003827144159004092\n",
      "Iteration is: 19856 and loss is: 0.001549440436065197\n",
      "Iteration is: 19857 and loss is: 0.0012829223414883018\n",
      "Iteration is: 19858 and loss is: 0.003165431087836623\n",
      "Iteration is: 19859 and loss is: 0.003316640853881836\n",
      "Iteration is: 19860 and loss is: 0.0010177927324548364\n",
      "Iteration is: 19861 and loss is: 0.0014116273960098624\n",
      "Iteration is: 19862 and loss is: 0.0024535234551876783\n",
      "Iteration is: 19863 and loss is: 0.0014678247971460223\n",
      "Iteration is: 19864 and loss is: 0.0010570522863417864\n",
      "Iteration is: 19865 and loss is: 0.0014985688030719757\n",
      "Iteration is: 19866 and loss is: 0.000752597174141556\n",
      "Iteration is: 19867 and loss is: 0.0004625473520718515\n",
      "Iteration is: 19868 and loss is: 0.0015619511250406504\n",
      "Iteration is: 19869 and loss is: 0.0021875728853046894\n",
      "Iteration is: 19870 and loss is: 0.002086387015879154\n",
      "Iteration is: 19871 and loss is: 0.0016226264415308833\n",
      "Iteration is: 19872 and loss is: 0.000470426632091403\n",
      "Iteration is: 19873 and loss is: 0.0004494363092817366\n",
      "Iteration is: 19874 and loss is: 0.0008261740440502763\n",
      "Iteration is: 19875 and loss is: 0.00035574642242863774\n",
      "Iteration is: 19876 and loss is: 0.0006573427235707641\n",
      "Iteration is: 19877 and loss is: 0.0004986993735656142\n",
      "Iteration is: 19878 and loss is: 0.0005395063199102879\n",
      "Iteration is: 19879 and loss is: 0.00037610402796417475\n",
      "Iteration is: 19880 and loss is: 0.000500258756801486\n",
      "Iteration is: 19881 and loss is: 0.0003274062473792583\n",
      "Iteration is: 19882 and loss is: 0.0003419391578063369\n",
      "Iteration is: 19883 and loss is: 0.0003390474303159863\n",
      "Iteration is: 19884 and loss is: 0.0002487350720912218\n",
      "Iteration is: 19885 and loss is: 0.00028782623121514916\n",
      "Iteration is: 19886 and loss is: 0.0002776485634967685\n",
      "Iteration is: 19887 and loss is: 0.00020471218158490956\n",
      "Iteration is: 19888 and loss is: 0.0002836854546330869\n",
      "Iteration is: 19889 and loss is: 0.00019239795801695436\n",
      "Iteration is: 19890 and loss is: 0.00022114482999313623\n",
      "Iteration is: 19891 and loss is: 0.00022444326896220446\n",
      "Iteration is: 19892 and loss is: 0.00015473869279958308\n",
      "Iteration is: 19893 and loss is: 0.0002330924617126584\n",
      "Iteration is: 19894 and loss is: 0.00014250777894631028\n",
      "Iteration is: 19895 and loss is: 0.00018311974417883903\n",
      "Iteration is: 19896 and loss is: 0.0001706382754491642\n",
      "Iteration is: 19897 and loss is: 0.00013857970770914108\n",
      "Iteration is: 19898 and loss is: 0.0001735627738526091\n",
      "Iteration is: 19899 and loss is: 0.00013682204007636756\n",
      "Iteration is: 19900 and loss is: 0.00014986141468398273\n",
      "Iteration is: 19901 and loss is: 0.00013729104830417782\n",
      "Iteration is: 19902 and loss is: 0.00013455312000587583\n",
      "Iteration is: 19903 and loss is: 0.00013105091056786478\n",
      "Iteration is: 19904 and loss is: 0.00012811881606467068\n",
      "Iteration is: 19905 and loss is: 0.00012521569442469627\n",
      "Iteration is: 19906 and loss is: 0.0001221545971930027\n",
      "Iteration is: 19907 and loss is: 0.00012683533714152873\n",
      "Iteration is: 19908 and loss is: 0.00011286681547062472\n",
      "Iteration is: 19909 and loss is: 0.0001236156385857612\n",
      "Iteration is: 19910 and loss is: 0.00011267133231740445\n",
      "Iteration is: 19911 and loss is: 0.00011015100608346984\n",
      "Iteration is: 19912 and loss is: 0.0001170361356344074\n",
      "Iteration is: 19913 and loss is: 0.00010362430475652218\n",
      "Iteration is: 19914 and loss is: 0.00011217988503631204\n",
      "Iteration is: 19915 and loss is: 0.00010822846525115892\n",
      "Iteration is: 19916 and loss is: 0.00010410696995677426\n",
      "Iteration is: 19917 and loss is: 0.00010809739615069702\n",
      "Iteration is: 19918 and loss is: 0.00010267000470776111\n",
      "Iteration is: 19919 and loss is: 0.00010379459126852453\n",
      "Iteration is: 19920 and loss is: 0.00010311660298611969\n",
      "Iteration is: 19921 and loss is: 0.00010171976464334875\n",
      "Iteration is: 19922 and loss is: 0.00010167917935177684\n",
      "Iteration is: 19923 and loss is: 0.00010099049541167915\n",
      "Iteration is: 19924 and loss is: 0.0001002354547381401\n",
      "Iteration is: 19925 and loss is: 9.966445213649422e-05\n",
      "Iteration is: 19926 and loss is: 9.999796748161316e-05\n",
      "Iteration is: 19927 and loss is: 9.836636309046298e-05\n",
      "Iteration is: 19928 and loss is: 9.941132157109678e-05\n",
      "Iteration is: 19929 and loss is: 9.880241850623861e-05\n",
      "Iteration is: 19930 and loss is: 9.759639942785725e-05\n",
      "Iteration is: 19931 and loss is: 9.914113616105169e-05\n",
      "Iteration is: 19932 and loss is: 9.750991011969745e-05\n",
      "Iteration is: 19933 and loss is: 9.801297710509971e-05\n",
      "Iteration is: 19934 and loss is: 9.874251554720104e-05\n",
      "Iteration is: 19935 and loss is: 9.815073281060904e-05\n",
      "Iteration is: 19936 and loss is: 9.984828648157418e-05\n",
      "Iteration is: 19937 and loss is: 0.0001009338375297375\n",
      "Iteration is: 19938 and loss is: 0.00010331366502214223\n",
      "Iteration is: 19939 and loss is: 0.00010781431046780199\n",
      "Iteration is: 19940 and loss is: 0.00011455422645667568\n",
      "Iteration is: 19941 and loss is: 0.00012636600877158344\n",
      "Iteration is: 19942 and loss is: 0.00014405359979718924\n",
      "Iteration is: 19943 and loss is: 0.00017248177027795464\n",
      "Iteration is: 19944 and loss is: 0.00020980786939617246\n",
      "Iteration is: 19945 and loss is: 0.000264986592810601\n",
      "Iteration is: 19946 and loss is: 0.00031928232056088746\n",
      "Iteration is: 19947 and loss is: 0.0003761402331292629\n",
      "Iteration is: 19948 and loss is: 0.0003795550437644124\n",
      "Iteration is: 19949 and loss is: 0.00031975616002455354\n",
      "Iteration is: 19950 and loss is: 0.00021094930707477033\n",
      "Iteration is: 19951 and loss is: 0.0001240496785612777\n",
      "Iteration is: 19952 and loss is: 0.00011136899411212653\n",
      "Iteration is: 19953 and loss is: 0.00016245993901975453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 19954 and loss is: 0.0002256099396618083\n",
      "Iteration is: 19955 and loss is: 0.0002653587725944817\n",
      "Iteration is: 19956 and loss is: 0.00028517169994302094\n",
      "Iteration is: 19957 and loss is: 0.00028174216276966035\n",
      "Iteration is: 19958 and loss is: 0.0002762435469776392\n",
      "Iteration is: 19959 and loss is: 0.00024804045096971095\n",
      "Iteration is: 19960 and loss is: 0.00020923827833030373\n",
      "Iteration is: 19961 and loss is: 0.00016030349070206285\n",
      "Iteration is: 19962 and loss is: 0.00012037565466016531\n",
      "Iteration is: 19963 and loss is: 0.0001007857354125008\n",
      "Iteration is: 19964 and loss is: 0.00010443988867336884\n",
      "Iteration is: 19965 and loss is: 0.00012528870138339698\n",
      "Iteration is: 19966 and loss is: 0.0001552672329125926\n",
      "Iteration is: 19967 and loss is: 0.0001943942770594731\n",
      "Iteration is: 19968 and loss is: 0.0002464664285071194\n",
      "Iteration is: 19969 and loss is: 0.00033517275005578995\n",
      "Iteration is: 19970 and loss is: 0.0004400230827741325\n",
      "Iteration is: 19971 and loss is: 0.0005668495432473719\n",
      "Iteration is: 19972 and loss is: 0.0005880177486687899\n",
      "Iteration is: 19973 and loss is: 0.0004461963544599712\n",
      "Iteration is: 19974 and loss is: 0.00023455852351617068\n",
      "Iteration is: 19975 and loss is: 0.00012914769467897713\n",
      "Iteration is: 19976 and loss is: 0.00019776679982896894\n",
      "Iteration is: 19977 and loss is: 0.0003137834428343922\n",
      "Iteration is: 19978 and loss is: 0.0003186482354067266\n",
      "Iteration is: 19979 and loss is: 0.00021079476573504508\n",
      "Iteration is: 19980 and loss is: 0.00012402291758917272\n",
      "Iteration is: 19981 and loss is: 0.00014353280130308121\n",
      "Iteration is: 19982 and loss is: 0.00021841225679963827\n",
      "Iteration is: 19983 and loss is: 0.00025263242423534393\n",
      "Iteration is: 19984 and loss is: 0.00022488202375825495\n",
      "Iteration is: 19985 and loss is: 0.00017271106480620801\n",
      "Iteration is: 19986 and loss is: 0.0001342717296211049\n",
      "Iteration is: 19987 and loss is: 0.0001098595603252761\n",
      "Iteration is: 19988 and loss is: 9.871210204437375e-05\n",
      "Iteration is: 19989 and loss is: 0.0001043918528012\n",
      "Iteration is: 19990 and loss is: 0.0001247991167474538\n",
      "Iteration is: 19991 and loss is: 0.00015768237062729895\n",
      "Iteration is: 19992 and loss is: 0.00021681308862753212\n",
      "Iteration is: 19993 and loss is: 0.00031469392706640065\n",
      "Iteration is: 19994 and loss is: 0.0004762238240800798\n",
      "Iteration is: 19995 and loss is: 0.0006283863331191242\n",
      "Iteration is: 19996 and loss is: 0.0006925718626007438\n",
      "Iteration is: 19997 and loss is: 0.000545199029147625\n",
      "Iteration is: 19998 and loss is: 0.0002739750489126891\n",
      "Iteration is: 19999 and loss is: 0.00013564134133048356\n",
      "Iteration is: 20000 and loss is: 0.00022193652694113553\n",
      "Training time: 757.8967\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(seed=1234)\n",
    "tf.random.set_seed(1234)\n",
    "tf.config.experimental.enable_tensor_float_32_execution(False)\n",
    "#os.environ[‘TF_ENABLE_AUTO_MIXED_PRECISION’] = ‘1’\n",
    "\n",
    "# Initalization of Network\n",
    "def hyper_initial(size):\n",
    "    in_dim = size[0]\n",
    "    out_dim = size[1]\n",
    "    std = np.sqrt(2.0/(in_dim + out_dim))\n",
    "    return tf.Variable(tf.random.truncated_normal(shape=size, stddev = std))\n",
    "\n",
    "# Neural Network \n",
    "def DNN(X, W, b):\n",
    "    A = X\n",
    "    L = len(W)\n",
    "    for i in range(L-1):\n",
    "        A = tf.tanh(tf.add(tf.matmul(A, W[i]), b[i]))\n",
    "    Y = tf.add(tf.matmul(A, W[-1]), b[-1])\n",
    "    return Y\n",
    "\n",
    "def train_vars(W, b):\n",
    "    return W + b\n",
    "\n",
    "def net_u(x, t, w, b):\n",
    "    u = DNN(tf.concat([x,t],1), w, b)\n",
    "    return u\n",
    "\n",
    "\n",
    "#@tf.function(jit_compile=True)\n",
    "@tf.function\n",
    "def net_f(x,t,W, b, nu):\n",
    "    with tf.GradientTape(persistent=True) as tape1:\n",
    "        tape1.watch([x, t])\n",
    "        with tf.GradientTape(persistent=True) as tape2:\n",
    "            tape2.watch([x, t])\n",
    "            u=net_u(x,t, W, b)\n",
    "        u_t = tape2.gradient(u, t)\n",
    "        u_x = tape2.gradient(u, x)\n",
    "    u_xx = tape1.gradient(u_x, x)  \n",
    "    f = u_t + u*u_x - nu*u_xx\n",
    "    return f\n",
    "\n",
    "\n",
    "\n",
    "#@tf.function(jit_compile=True)\n",
    "@tf.function\n",
    "def train_step(W, b, X_u_train_tf, u_train_tf, X_f_train_tf, opt, nu):\n",
    "    x_u = X_u_train_tf[:,0:1]\n",
    "    t_u = X_u_train_tf[:,1:2]\n",
    "    x_f = X_f_train_tf[:,0:1]\n",
    "    t_f = X_f_train_tf[:,1:2]\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch([W,b])\n",
    "        u_nn = net_u(x_u, t_u, W, b) \n",
    "        f_nn = net_f(x_f,t_f, W, b, nu)\n",
    "        loss =  tf.reduce_mean(tf.square(u_nn - u_train_tf)) + tf.reduce_mean(tf.square(f_nn)) \n",
    "    grads = tape.gradient(loss, train_vars(W,b))\n",
    "    opt.apply_gradients(zip(grads, train_vars(W,b)))\n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "nu = 0.01/np.pi # Viscosity\n",
    "N_u = 100 # Number of Initial and Boundary data points\n",
    "N_f = 10000 # Number of residual point\n",
    "Nmax= 20000\n",
    "\n",
    "layers = [2, 20, 20, 20, 20, 20, 20, 20, 20, 1]\n",
    "\n",
    "L = len(layers)\n",
    "W = [hyper_initial([layers[l-1], layers[l]]) for l in range(1, L)] \n",
    "b = [tf.Variable(tf.zeros([1, layers[l]])) for l in range(1, L)] \n",
    "\n",
    "data = scipy.io.loadmat('./Data/burgers_shock.mat')\n",
    "\n",
    "t = data['t'].flatten()[:,None]\n",
    "x = data['x'].flatten()[:,None]\n",
    "Exact = np.real(data['usol']).T\n",
    "X, T = np.meshgrid(x,t)\n",
    "X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))\n",
    "u_star = Exact.flatten()[:,None]              \n",
    "\n",
    "# Doman bounds\n",
    "lb = X_star.min(0)\n",
    "ub = X_star.max(0)  \n",
    "\n",
    "# Initial Condition\n",
    "xx1 = np.hstack((X[0:1,:].T, T[0:1,:].T))\n",
    "uu1 = Exact[0:1,:].T\n",
    "\n",
    "# Boundary condition -1\n",
    "xx2 = np.hstack((X[:,0:1], T[:,0:1]))\n",
    "uu2 = Exact[:,0:1]\n",
    "\n",
    "# Boundary condition 1\n",
    "xx3 = np.hstack((X[:,-1:], T[:,-1:]))\n",
    "uu3 = Exact[:,-1:]\n",
    "\n",
    "X_u_train = np.vstack([xx1, xx2, xx3])\n",
    "X_f_train = lb + (ub-lb)*lhs(2, N_f)\n",
    "X_f_train = np.vstack((X_f_train, X_u_train))\n",
    "u_train = np.vstack([uu1, uu2, uu3])\n",
    "\n",
    "idx = np.random.choice(X_u_train.shape[0], N_u, replace=False)\n",
    "\n",
    "X_u_train = X_u_train[idx, :]\n",
    "u_train = u_train[idx,:]\n",
    "\n",
    "X_u_train_tf = tf.convert_to_tensor(X_u_train, dtype=tf.float32)\n",
    "u_train_tf =   tf.convert_to_tensor(u_train, dtype=tf.float32)\n",
    "X_f_train_tf = tf.convert_to_tensor(X_f_train, dtype=tf.float32)\n",
    "\n",
    "lr = 1e-3\n",
    "optimizer = tf.optimizers.Adam(learning_rate=lr)\n",
    "\n",
    "start_time = time.time()\n",
    "n=0\n",
    "loss = []\n",
    "while n <= Nmax:\n",
    "    loss_= train_step(W, b, X_u_train_tf, u_train_tf, X_f_train_tf, optimizer, nu)\n",
    "    loss.append(loss_)    \n",
    "    print(f\"Iteration is: {n} and loss is: {loss_}\")\n",
    "    n+=1\n",
    "\n",
    "elapsed = time.time() - start_time                \n",
    "print('Training time: %.4f' % (elapsed))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6fe12351",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "net_u() takes 3 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-f28d2b56131a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mX_star_tf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_star\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mu_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_star_tf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0merror_u\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu_star\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mu_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu_star\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Error u: %e'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_u\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-f28d2b56131a>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(X_star_tf, w, b)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mx_star\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_star_tf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mt_star\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_star_tf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mu_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet_u\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_star\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_star\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mu_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: net_u() takes 3 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "def predict(X_star_tf, w, b):\n",
    "    x_star = X_star_tf[:,0:1]\n",
    "    t_star = X_star_tf[:,1:2]\n",
    "    u_pred = net_u(x_star, t_star, w, b)\n",
    "    return u_pred\n",
    "    \n",
    "X_star_tf = tf.convert_to_tensor(X_star, dtype=tf.float32)    \n",
    "u_pred = predict(X_star_tf, W, b)\n",
    "error_u = np.linalg.norm(u_star-u_pred,2)/np.linalg.norm(u_star,2)\n",
    "print('Error u: %e' %(error_u))                     \n",
    "U_pred = griddata(X_star, u_pred.numpy().flatten(), (X, T), method='cubic')\n",
    "Error = 100* np.linalg.norm(Exact - U_pred) / np.linalg.norm(U_pred)\n",
    "\n",
    "\n",
    "fig, ax = newfig(1.0, 1.1)\n",
    "ax.axis('off')\n",
    "\n",
    "####### Row 0: u(t,x) ##################    \n",
    "gs0 = gridspec.GridSpec(1, 2)\n",
    "gs0.update(top=1-0.06, bottom=1-1/3, left=0.15, right=0.85, wspace=0)\n",
    "ax = plt.subplot(gs0[:, :])\n",
    "\n",
    "h = ax.imshow(U_pred.T, interpolation='nearest', cmap='rainbow', \n",
    "              extent=[t.min(), t.max(), x.min(), x.max()], \n",
    "              origin='lower', aspect='auto')\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "fig.colorbar(h, cax=cax)\n",
    "\n",
    "ax.plot(X_u_train[:,1], X_u_train[:,0], 'kx', label = 'Data (%d points)' % (u_train.shape[0]), markersize = 4, clip_on = False)\n",
    "#ax.plot(X_f_train[0:100,1], X_f_train[0:100,0], 'bx', label = 'Data (%d points)' % (u_train.shape[0]), markersize = 4, clip_on = False)\n",
    "\n",
    "\n",
    "line = np.linspace(x.min(), x.max(), 2)[:,None]\n",
    "ax.plot(t[25]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
    "ax.plot(t[50]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
    "ax.plot(t[75]*np.ones((2,1)), line, 'w-', linewidth = 1)    \n",
    "\n",
    "ax.set_xlabel('$t$')\n",
    "ax.set_ylabel('$x$')\n",
    "ax.legend(frameon=False, loc = 'best')\n",
    "ax.set_title('$u(t,x)$', fontsize = 10)\n",
    "\n",
    "####### Row 1: u(t,x) slices ##################    \n",
    "gs1 = gridspec.GridSpec(1, 3)\n",
    "gs1.update(top=1-1/3, bottom=0, left=0.1, right=0.9, wspace=0.5)\n",
    "\n",
    "ax = plt.subplot(gs1[0, 0])\n",
    "ax.plot(x,Exact[25,:], 'b-', linewidth = 2, label = 'Exact')       \n",
    "ax.plot(x,U_pred[25,:], 'r--', linewidth = 2, label = 'Prediction')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$u(t,x)$')    \n",
    "ax.set_title('$t = 0.25$', fontsize = 10)\n",
    "ax.axis('square')\n",
    "ax.set_xlim([-1.1,1.1])\n",
    "ax.set_ylim([-1.1,1.1])\n",
    "\n",
    "ax = plt.subplot(gs1[0, 1])\n",
    "ax.plot(x,Exact[50,:], 'b-', linewidth = 2, label = 'Exact')       \n",
    "ax.plot(x,U_pred[50,:], 'r--', linewidth = 2, label = 'Prediction')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$u(t,x)$')\n",
    "ax.axis('square')\n",
    "ax.set_xlim([-1.1,1.1])\n",
    "ax.set_ylim([-1.1,1.1])\n",
    "ax.set_title('$t = 0.50$', fontsize = 10)\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.35), ncol=5, frameon=False)\n",
    "\n",
    "ax = plt.subplot(gs1[0, 2])\n",
    "ax.plot(x,Exact[75,:], 'b-', linewidth = 2, label = 'Exact')       \n",
    "ax.plot(x,U_pred[75,:], 'r--', linewidth = 2, label = 'Prediction')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$u(t,x)$')\n",
    "ax.axis('square')\n",
    "ax.set_xlim([-1.1,1.1])\n",
    "ax.set_ylim([-1.1,1.1])    \n",
    "ax.set_title('$t = 0.75$', fontsize = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffb8cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "it = np.arange(0,20001, 1)\n",
    "loss_list = [loss[i].numpy() for i in range(0,20001)]\n",
    "plt.semilogy(it, np.asarray(loss_list), 'b-', linewidth = 2, label = 'Exact')       \n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('$\\mathcal{L}$')    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ab4ee8",
   "metadata": {},
   "source": [
    "## PINN-for-Burger's-Equation-in-JAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b89ab85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap\n",
    "from jax import random\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "from pyDOE import lhs\n",
    "from jax.nn import tanh, relu\n",
    "import sys\n",
    "from jax.example_libraries import optimizers\n",
    "from tqdm import trange\n",
    "sys.path.insert(0, 'Utilities/')\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import griddata\n",
    "from plotting import newfig, savefig\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb8c083",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_layer_params(m, n, key, scale):\n",
    "    w_key, b_key = random.split(key)\n",
    "    return scale*random.normal(w_key, (m, n)), jnp.zeros(n)\n",
    "\n",
    "def init_network_params(sizes, key):\n",
    "    keys = random.split(key, len(sizes))\n",
    "    return [random_layer_params(m, n, k, 2.0/(jnp.sqrt(m+n))) \\\n",
    "            for m, n, k in zip(sizes[:-1], sizes[1:], keys)]\n",
    "\n",
    "\n",
    "@jit\n",
    "def predict(params, X, lb, ub):\n",
    "    H =  2.0*(X - lb)/(ub - lb) - 1.0\n",
    "    for w, b in params[:-1]:\n",
    "        H = tanh(jnp.dot(H, w) + b)\n",
    "        final_w, final_b = params[-1]\n",
    "    H = jnp.dot(H, final_w) + final_b\n",
    "    return H\n",
    "\n",
    "\n",
    "@jit\n",
    "def net_u(params, x, t, lb, ub):\n",
    "    x_con =jnp.array([x, t])\n",
    "    y_pred = predict(params, x_con, lb, ub)\n",
    "    return y_pred\n",
    "\n",
    "@jit\n",
    "def net_u_grad(params, x, t, lb, ub):\n",
    "    x_con =jnp.array([x, t])\n",
    "    y_pred = predict(params, x_con, lb, ub)\n",
    "    return y_pred[0]\n",
    "\n",
    "def net_f(params, lb, ub):\n",
    "    def u_t(x, t):\n",
    "        ut = grad(net_u_grad, argnums=2)(params, x, t, lb, ub) \n",
    "        return ut\n",
    "\n",
    "    def u_x(x, t):\n",
    "        ux = grad(net_u_grad, argnums=1)(params, x, t, lb, ub) \n",
    "        return ux   \n",
    "    return jit(u_t), jit(u_x)\n",
    "\n",
    "\n",
    "def net_fxx(params, lb, ub):\n",
    "    def u_xx(x, t):\n",
    "        _, u_x = net_f(params, lb, ub) \n",
    "        ux = grad(u_x, argnums=0)(x, t) \n",
    "        return ux   \n",
    "    return jit(u_xx)\n",
    "\n",
    "\n",
    "@jit\n",
    "def loss_data(params,x,t, lb, ub, u_train):\n",
    "    u_pred = vmap(net_u, (None, 0, 0, None, None))(params, x, t, lb, ub)\n",
    "    loss = jnp.mean((u_pred - u_train)**2 )\n",
    "    return loss\n",
    "\n",
    "@jit\n",
    "def loss_f(params, x, t, lb, ub, nu):\n",
    "    u = vmap(net_u, (None, 0, 0, None, None))(params, x, t, lb, ub)\n",
    "    u_tf, u_xf = net_f(params, lb, ub)\n",
    "    u_xxf = net_fxx(params, lb, ub)\n",
    "    u_t = vmap(u_tf, (0, 0))(x, t)\n",
    "    u_x = vmap(u_xf, (0, 0))(x, t)\n",
    "    u_xx = vmap(u_xxf, (0, 0))(x, t)\n",
    "    res = u_t + u.flatten() * u_x - nu * u_xx \n",
    "    loss_f = jnp.mean((res.flatten())**2)\n",
    "    return loss_f\n",
    "\n",
    "@jit\n",
    "def predict_u(params, x_star, t_star, lb, ub):\n",
    "    u_pred = vmap(net_u, (None, 0, 0, None, None))\\\n",
    "    (params, x_star, t_star, lb, ub)\n",
    "    return u_pred\n",
    "\n",
    "def loss_fn(params, x_f, t_f,x_d, t_d, lb, ub, nu, y_d):\n",
    "    loss_res = loss_f(params, x_f, t_f, lb, ub, nu)\n",
    "    data_loss = loss_data(params, x_d, t_d, lb, ub, y_d) \n",
    "    return loss_res + data_loss\n",
    "\n",
    "@jit\n",
    "def step(istep, opt_state, t_d, x_d, y_d, t_f, x_f, lb, ub):\n",
    "    param = get_params(opt_state) \n",
    "    g = grad(loss_fn, argnums=0)(param, x_f, t_f,x_d, t_d, lb, ub, nu, y_d)\n",
    "    return opt_update(istep, g, opt_state)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    nu = 0.01/np.pi\n",
    "    N_u = 100\n",
    "    N_f = 10000\n",
    "    layers = [2, 20, 20, 20, 20, 20, 20, 20,20, 20, 1]\n",
    "    data = scipy.io.loadmat('Data/burgers_shock.mat')\n",
    "    t = data['t'].flatten()[:,None]\n",
    "    x = data['x'].flatten()[:,None]\n",
    "    Exact = np.real(data['usol']).T    \n",
    "    X, T = np.meshgrid(x,t)\n",
    "    X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))\n",
    "    u_star = Exact.flatten()[:,None]              \n",
    "    lb = X_star.min(0)\n",
    "    ub = X_star.max(0)\n",
    "    xx1 = np.hstack((X[0:1,:].T, T[0:1,:].T))\n",
    "    uu1 = Exact[0:1,:].T\n",
    "    xx2 = np.hstack((X[:,0:1], T[:,0:1]))\n",
    "    uu2 = Exact[:,0:1]\n",
    "    xx3 = np.hstack((X[:,-1:], T[:,-1:]))\n",
    "    uu3 = Exact[:,-1:]\n",
    "    X_u_train = np.vstack([xx1, xx2, xx3])\n",
    "    X_f_train = lb + (ub-lb)*lhs(2, N_f)\n",
    "    X_f_train = np.vstack((X_f_train, X_u_train))\n",
    "    u_train = np.vstack([uu1, uu2, uu3])\n",
    "    idx = np.random.choice(X_u_train.shape[0], N_u, replace=False)\n",
    "    X_u_train = X_u_train[idx, :]\n",
    "    u_train = u_train[idx,:]\n",
    "    x_d = X_u_train[:, 0]\n",
    "    t_d = X_u_train[:, 1]\n",
    "    x_f = X_f_train[:, 0]\n",
    "    t_f = X_f_train[:, 1]\n",
    "    x_star = X_star[:, 0]\n",
    "    t_star = X_star[:, 1]\n",
    "    params = init_network_params(layers, random.PRNGKey(1234))\n",
    "    opt_init, opt_update, get_params = optimizers.adam(5e-4)\n",
    "    opt_state = opt_init(params)\n",
    "    nIter = 20000 + 1\n",
    "    ld_list = []\n",
    "    lf_list = []\n",
    "    pbar = trange(nIter)\n",
    "    \n",
    "    for it in pbar:\n",
    "        opt_state = step(it, opt_state, t_d, x_d, u_train, t_f, x_f, lb, ub)\n",
    "        if it % 1 == 0:\n",
    "            params = get_params(opt_state)\n",
    "            l_d = loss_data(params, x_d, t_d, lb, ub, u_train)\n",
    "            l_f = loss_f(params, x_f, t_f, lb, ub, nu)\n",
    "            pbar.set_postfix({'Loss': l_d, 'loss_physics': l_f})\n",
    "            ld_list.append(l_d)\n",
    "            lf_list.append(l_f)\n",
    "\n",
    "\n",
    "    u_pred = predict_u(params, x_star, t_star, lb, ub)\n",
    "            \n",
    "    error_u = jnp.linalg.norm(u_star-u_pred,2)/np.linalg.norm(u_star,2)\n",
    "    print('Error u: %e' % (error_u))\n",
    "    np.save(\"ld_list.npy\", np.array(ld_list), allow_pickle=True) \n",
    "    np.save(\"lf_list.npy\", np.array(lf_list), allow_pickle=True)  \n",
    " \n",
    "    \n",
    "    U_pred = griddata(X_star, u_pred.flatten(), (X, T), method='cubic')\n",
    "    Error = np.abs(Exact - U_pred)\n",
    "    \n",
    "    fig, ax = newfig(1.0, 1.1)\n",
    "    ax.axis('off')\n",
    "    \n",
    "    ####### Row 0: u(t,x) ##################    \n",
    "    gs0 = gridspec.GridSpec(1, 2)\n",
    "    gs0.update(top=1-0.06, bottom=1-1/3, left=0.15, right=0.85, wspace=0)\n",
    "    ax = plt.subplot(gs0[:, :])\n",
    "    \n",
    "    h = ax.imshow(U_pred.T, interpolation='nearest', cmap='rainbow', \n",
    "                  extent=[t.min(), t.max(), x.min(), x.max()], \n",
    "                  origin='lower', aspect='auto')\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    fig.colorbar(h, cax=cax)\n",
    "    \n",
    "    ax.plot(X_u_train[:,1], X_u_train[:,0], 'kx', label = 'Data (%d points)' % (u_train.shape[0]), markersize = 4, clip_on = False)\n",
    "    \n",
    "    line = np.linspace(x.min(), x.max(), 2)[:,None]\n",
    "    ax.plot(t[25]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
    "    ax.plot(t[50]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
    "    ax.plot(t[75]*np.ones((2,1)), line, 'w-', linewidth = 1)    \n",
    "    \n",
    "    ax.set_xlabel('$t$')\n",
    "    ax.set_ylabel('$x$')\n",
    "    ax.legend(frameon=False, loc = 'best')\n",
    "    ax.set_title('$u(t,x)$', fontsize = 10)\n",
    "    \n",
    "    ####### Row 1: u(t,x) slices ##################    \n",
    "    gs1 = gridspec.GridSpec(1, 3)\n",
    "    gs1.update(top=1-1/3, bottom=0, left=0.1, right=0.9, wspace=0.5)\n",
    "    \n",
    "    ax = plt.subplot(gs1[0, 0])\n",
    "    ax.plot(x,Exact[25,:], 'b-', linewidth = 2, label = 'Exact')       \n",
    "    ax.plot(x,U_pred[25,:], 'r--', linewidth = 2, label = 'Prediction')\n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.set_ylabel('$u(t,x)$')    \n",
    "    ax.set_title('$t = 0.25$', fontsize = 10)\n",
    "    ax.axis('square')\n",
    "    ax.set_xlim([-1.1,1.1])\n",
    "    ax.set_ylim([-1.1,1.1])\n",
    "    \n",
    "    ax = plt.subplot(gs1[0, 1])\n",
    "    ax.plot(x,Exact[50,:], 'b-', linewidth = 2, label = 'Exact')       \n",
    "    ax.plot(x,U_pred[50,:], 'r--', linewidth = 2, label = 'Prediction')\n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.set_ylabel('$u(t,x)$')\n",
    "    ax.axis('square')\n",
    "    ax.set_xlim([-1.1,1.1])\n",
    "    ax.set_ylim([-1.1,1.1])\n",
    "    ax.set_title('$t = 0.50$', fontsize = 10)\n",
    "    ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.35), ncol=5, frameon=False)\n",
    "    \n",
    "    ax = plt.subplot(gs1[0, 2])\n",
    "    ax.plot(x,Exact[75,:], 'b-', linewidth = 2, label = 'Exact')       \n",
    "    ax.plot(x,U_pred[75,:], 'r--', linewidth = 2, label = 'Prediction')\n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.set_ylabel('$u(t,x)$')\n",
    "    ax.axis('square')\n",
    "    ax.set_xlim([-1.1,1.1])\n",
    "    ax.set_ylim([-1.1,1.1])    \n",
    "    ax.set_title('$t = 0.75$', fontsize = 10)\n",
    "\n",
    "    savefig(\"Burgers\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6b99fc",
   "metadata": {},
   "source": [
    "## PINN-for-a-Boundary-Layer-Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c66ea9eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Solution of Equation using Central Finite Difference Equation\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-poster')\n",
    "%matplotlib inline\n",
    "matplotlib.rc('font', family='serif', serif='cm10')\n",
    "\n",
    "matplotlib.rc('text', usetex=True)\n",
    "### Number of Gridpoints\n",
    "nu =1.0/10**-3\n",
    "n = 100\n",
    "h = (1+1) / n\n",
    "x = np.linspace(-1,1,n+1)\n",
    "# Difference Operator\n",
    "A = np.zeros((n+1, n+1))\n",
    "\n",
    "## Coefficient For Boundary Condition\n",
    "A[0, 0] = 1\n",
    "A[n, n] = 1\n",
    "\n",
    "### Maric for Interior Point\n",
    "for i in range(1, n):\n",
    "    A[i, i-1] = 1\n",
    "    A[i, i] = -(2 + (nu)*h**2)\n",
    "    A[i, i+1] = 1\n",
    "\n",
    "# Get b\n",
    "b = np.zeros(n+1)\n",
    "b = (np.exp(x))*h*h*(nu)\n",
    "\n",
    "#### Imposition Boundary Condition \n",
    "b[0] = 1\n",
    "b[-1] = 0\n",
    "# solve the linear equations\n",
    "y_act = np.linalg.solve(A, b)\n",
    "### Plot of Equation\n",
    "gs = gridspec.GridSpec(1, 1)\n",
    "ax = plt.subplot(gs[0, 0])\n",
    "ax.plot(x, y_act, \"-\", lw=2.0, color=\"b\")\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel(\"$u$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f7065149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 0 and loss is: 36.536865234375\n",
      "Iteration is: 1 and loss is: 32.31436538696289\n",
      "Iteration is: 2 and loss is: 28.28046226501465\n",
      "Iteration is: 3 and loss is: 24.432859420776367\n",
      "Iteration is: 4 and loss is: 20.79062843322754\n",
      "Iteration is: 5 and loss is: 17.39005470275879\n",
      "Iteration is: 6 and loss is: 14.26872730255127\n",
      "Iteration is: 7 and loss is: 11.459373474121094\n",
      "Iteration is: 8 and loss is: 8.98551082611084\n",
      "Iteration is: 9 and loss is: 6.85828161239624\n",
      "Iteration is: 10 and loss is: 5.0749030113220215\n",
      "Iteration is: 11 and loss is: 3.6195666790008545\n",
      "Iteration is: 12 and loss is: 2.4678053855895996\n",
      "Iteration is: 13 and loss is: 1.5934545993804932\n",
      "Iteration is: 14 and loss is: 0.9745383262634277\n",
      "Iteration is: 15 and loss is: 0.5949716567993164\n",
      "Iteration is: 16 and loss is: 0.4408942461013794\n",
      "Iteration is: 17 and loss is: 0.49178215861320496\n",
      "Iteration is: 18 and loss is: 0.7089072465896606\n",
      "Iteration is: 19 and loss is: 1.0280756950378418\n",
      "Iteration is: 20 and loss is: 1.3672088384628296\n",
      "Iteration is: 21 and loss is: 1.6508374214172363\n",
      "Iteration is: 22 and loss is: 1.834457278251648\n",
      "Iteration is: 23 and loss is: 1.9121466875076294\n",
      "Iteration is: 24 and loss is: 1.9070112705230713\n",
      "Iteration is: 25 and loss is: 1.852960467338562\n",
      "Iteration is: 26 and loss is: 1.7769479751586914\n",
      "Iteration is: 27 and loss is: 1.689255714416504\n",
      "Iteration is: 28 and loss is: 1.5855962038040161\n",
      "Iteration is: 29 and loss is: 1.4578702449798584\n",
      "Iteration is: 30 and loss is: 1.304720401763916\n",
      "Iteration is: 31 and loss is: 1.1350044012069702\n",
      "Iteration is: 32 and loss is: 0.9645580649375916\n",
      "Iteration is: 33 and loss is: 0.8103356957435608\n",
      "Iteration is: 34 and loss is: 0.6853448152542114\n",
      "Iteration is: 35 and loss is: 0.5960160493850708\n",
      "Iteration is: 36 and loss is: 0.5420729517936707\n",
      "Iteration is: 37 and loss is: 0.5181825160980225\n",
      "Iteration is: 38 and loss is: 0.5163622498512268\n",
      "Iteration is: 39 and loss is: 0.5282400846481323\n",
      "Iteration is: 40 and loss is: 0.5466253161430359\n",
      "Iteration is: 41 and loss is: 0.5662533044815063\n",
      "Iteration is: 42 and loss is: 0.583847165107727\n",
      "Iteration is: 43 and loss is: 0.5977616906166077\n",
      "Iteration is: 44 and loss is: 0.607437014579773\n",
      "Iteration is: 45 and loss is: 0.6128716468811035\n",
      "Iteration is: 46 and loss is: 0.6142054796218872\n",
      "Iteration is: 47 and loss is: 0.6114785671234131\n",
      "Iteration is: 48 and loss is: 0.6045836210250854\n",
      "Iteration is: 49 and loss is: 0.5933697819709778\n",
      "Iteration is: 50 and loss is: 0.5778489112854004\n",
      "Iteration is: 51 and loss is: 0.5584127902984619\n",
      "Iteration is: 52 and loss is: 0.5359739065170288\n",
      "Iteration is: 53 and loss is: 0.5119668841362\n",
      "Iteration is: 54 and loss is: 0.48817819356918335\n",
      "Iteration is: 55 and loss is: 0.46644091606140137\n",
      "Iteration is: 56 and loss is: 0.44826528429985046\n",
      "Iteration is: 57 and loss is: 0.4345240592956543\n",
      "Iteration is: 58 and loss is: 0.425285279750824\n",
      "Iteration is: 59 and loss is: 0.4198612570762634\n",
      "Iteration is: 60 and loss is: 0.41705840826034546\n",
      "Iteration is: 61 and loss is: 0.41553619503974915\n",
      "Iteration is: 62 and loss is: 0.41416290402412415\n",
      "Iteration is: 63 and loss is: 0.41223427653312683\n",
      "Iteration is: 64 and loss is: 0.40950632095336914\n",
      "Iteration is: 65 and loss is: 0.40605631470680237\n",
      "Iteration is: 66 and loss is: 0.4020676910877228\n",
      "Iteration is: 67 and loss is: 0.3976450264453888\n",
      "Iteration is: 68 and loss is: 0.392739474773407\n",
      "Iteration is: 69 and loss is: 0.3872043788433075\n",
      "Iteration is: 70 and loss is: 0.3809267282485962\n",
      "Iteration is: 71 and loss is: 0.37395307421684265\n",
      "Iteration is: 72 and loss is: 0.36653971672058105\n",
      "Iteration is: 73 and loss is: 0.359104722738266\n",
      "Iteration is: 74 and loss is: 0.35210445523262024\n",
      "Iteration is: 75 and loss is: 0.345897376537323\n",
      "Iteration is: 76 and loss is: 0.3406476378440857\n",
      "Iteration is: 77 and loss is: 0.33630549907684326\n",
      "Iteration is: 78 and loss is: 0.3326566517353058\n",
      "Iteration is: 79 and loss is: 0.3294159770011902\n",
      "Iteration is: 80 and loss is: 0.3263165056705475\n",
      "Iteration is: 81 and loss is: 0.3231722414493561\n",
      "Iteration is: 82 and loss is: 0.3198922276496887\n",
      "Iteration is: 83 and loss is: 0.31645873188972473\n",
      "Iteration is: 84 and loss is: 0.3128850758075714\n",
      "Iteration is: 85 and loss is: 0.3091859817504883\n",
      "Iteration is: 86 and loss is: 0.30536165833473206\n",
      "Iteration is: 87 and loss is: 0.3014037013053894\n",
      "Iteration is: 88 and loss is: 0.29732054471969604\n",
      "Iteration is: 89 and loss is: 0.29315298795700073\n",
      "Iteration is: 90 and loss is: 0.28897809982299805\n",
      "Iteration is: 91 and loss is: 0.2848922908306122\n",
      "Iteration is: 92 and loss is: 0.28098264336586\n",
      "Iteration is: 93 and loss is: 0.27729928493499756\n",
      "Iteration is: 94 and loss is: 0.2738405466079712\n",
      "Iteration is: 95 and loss is: 0.27055811882019043\n",
      "Iteration is: 96 and loss is: 0.2673778235912323\n",
      "Iteration is: 97 and loss is: 0.2642284333705902\n",
      "Iteration is: 98 and loss is: 0.2610616981983185\n",
      "Iteration is: 99 and loss is: 0.2578601837158203\n",
      "Iteration is: 100 and loss is: 0.2546323835849762\n",
      "Iteration is: 101 and loss is: 0.25139790773391724\n",
      "Iteration is: 102 and loss is: 0.24817532300949097\n",
      "Iteration is: 103 and loss is: 0.2449774146080017\n",
      "Iteration is: 104 and loss is: 0.24181197583675385\n",
      "Iteration is: 105 and loss is: 0.23868778347969055\n",
      "Iteration is: 106 and loss is: 0.23561780154705048\n",
      "Iteration is: 107 and loss is: 0.23261673748493195\n",
      "Iteration is: 108 and loss is: 0.22969591617584229\n",
      "Iteration is: 109 and loss is: 0.22685657441616058\n",
      "Iteration is: 110 and loss is: 0.22408607602119446\n",
      "Iteration is: 111 and loss is: 0.22136229276657104\n",
      "Iteration is: 112 and loss is: 0.21866005659103394\n",
      "Iteration is: 113 and loss is: 0.21595966815948486\n",
      "Iteration is: 114 and loss is: 0.21325291693210602\n",
      "Iteration is: 115 and loss is: 0.2105434536933899\n",
      "Iteration is: 116 and loss is: 0.20784308016300201\n",
      "Iteration is: 117 and loss is: 0.20516537129878998\n",
      "Iteration is: 118 and loss is: 0.20252113044261932\n",
      "Iteration is: 119 and loss is: 0.19991593062877655\n",
      "Iteration is: 120 and loss is: 0.1973510980606079\n",
      "Iteration is: 121 and loss is: 0.19482506811618805\n",
      "Iteration is: 122 and loss is: 0.19233527779579163\n",
      "Iteration is: 123 and loss is: 0.18987832963466644\n",
      "Iteration is: 124 and loss is: 0.18745069205760956\n",
      "Iteration is: 125 and loss is: 0.18504837155342102\n",
      "Iteration is: 126 and loss is: 0.18266701698303223\n",
      "Iteration is: 127 and loss is: 0.18030519783496857\n",
      "Iteration is: 128 and loss is: 0.1779639571905136\n",
      "Iteration is: 129 and loss is: 0.1756470650434494\n",
      "Iteration is: 130 and loss is: 0.173359215259552\n",
      "Iteration is: 131 and loss is: 0.1711045801639557\n",
      "Iteration is: 132 and loss is: 0.16888394951820374\n",
      "Iteration is: 133 and loss is: 0.166694775223732\n",
      "Iteration is: 134 and loss is: 0.16453221440315247\n",
      "Iteration is: 135 and loss is: 0.16239114105701447\n",
      "Iteration is: 136 and loss is: 0.16026830673217773\n",
      "Iteration is: 137 and loss is: 0.15816225111484528\n",
      "Iteration is: 138 and loss is: 0.15607412159442902\n",
      "Iteration is: 139 and loss is: 0.15400633215904236\n",
      "Iteration is: 140 and loss is: 0.15196099877357483\n",
      "Iteration is: 141 and loss is: 0.14993931353092194\n",
      "Iteration is: 142 and loss is: 0.1479422003030777\n",
      "Iteration is: 143 and loss is: 0.1459687203168869\n",
      "Iteration is: 144 and loss is: 0.1440182775259018\n",
      "Iteration is: 145 and loss is: 0.14208951592445374\n",
      "Iteration is: 146 and loss is: 0.14018158614635468\n",
      "Iteration is: 147 and loss is: 0.13829389214515686\n",
      "Iteration is: 148 and loss is: 0.13642650842666626\n",
      "Iteration is: 149 and loss is: 0.1345801055431366\n",
      "Iteration is: 150 and loss is: 0.13275566697120667\n",
      "Iteration is: 151 and loss is: 0.13095352053642273\n",
      "Iteration is: 152 and loss is: 0.12917321920394897\n",
      "Iteration is: 153 and loss is: 0.12741349637508392\n",
      "Iteration is: 154 and loss is: 0.1256730705499649\n",
      "Iteration is: 155 and loss is: 0.12395061552524567\n",
      "Iteration is: 156 and loss is: 0.12224575132131577\n",
      "Iteration is: 157 and loss is: 0.12055887281894684\n",
      "Iteration is: 158 and loss is: 0.11889088898897171\n",
      "Iteration is: 159 and loss is: 0.11724255234003067\n",
      "Iteration is: 160 and loss is: 0.11561386287212372\n",
      "Iteration is: 161 and loss is: 0.11400461196899414\n",
      "Iteration is: 162 and loss is: 0.11241398751735687\n",
      "Iteration is: 163 and loss is: 0.11084157973527908\n",
      "Iteration is: 164 and loss is: 0.10928736627101898\n",
      "Iteration is: 165 and loss is: 0.10775168985128403\n",
      "Iteration is: 166 and loss is: 0.10623445361852646\n",
      "Iteration is: 167 and loss is: 0.10473581403493881\n",
      "Iteration is: 168 and loss is: 0.10325534641742706\n",
      "Iteration is: 169 and loss is: 0.10179230570793152\n",
      "Iteration is: 170 and loss is: 0.10034632682800293\n",
      "Iteration is: 171 and loss is: 0.09891694039106369\n",
      "Iteration is: 172 and loss is: 0.09750448167324066\n",
      "Iteration is: 173 and loss is: 0.09610879421234131\n",
      "Iteration is: 174 and loss is: 0.09473031759262085\n",
      "Iteration is: 175 and loss is: 0.09336893260478973\n",
      "Iteration is: 176 and loss is: 0.09202439337968826\n",
      "Iteration is: 177 and loss is: 0.09069641679525375\n",
      "Iteration is: 178 and loss is: 0.08938486874103546\n",
      "Iteration is: 179 and loss is: 0.08808963745832443\n",
      "Iteration is: 180 and loss is: 0.0868108794093132\n",
      "Iteration is: 181 and loss is: 0.08554831147193909\n",
      "Iteration is: 182 and loss is: 0.08430159837007523\n",
      "Iteration is: 183 and loss is: 0.08307032287120819\n",
      "Iteration is: 184 and loss is: 0.08185447007417679\n",
      "Iteration is: 185 and loss is: 0.08065377175807953\n",
      "Iteration is: 186 and loss is: 0.07946836203336716\n",
      "Iteration is: 187 and loss is: 0.0782981812953949\n",
      "Iteration is: 188 and loss is: 0.07714326679706573\n",
      "Iteration is: 189 and loss is: 0.07600313425064087\n",
      "Iteration is: 190 and loss is: 0.07487782835960388\n",
      "Iteration is: 191 and loss is: 0.07376721501350403\n",
      "Iteration is: 192 and loss is: 0.07267120480537415\n",
      "Iteration is: 193 and loss is: 0.0715896338224411\n",
      "Iteration is: 194 and loss is: 0.07052222639322281\n",
      "Iteration is: 195 and loss is: 0.06946875900030136\n",
      "Iteration is: 196 and loss is: 0.06842923909425735\n",
      "Iteration is: 197 and loss is: 0.0674034059047699\n",
      "Iteration is: 198 and loss is: 0.06639134138822556\n",
      "Iteration is: 199 and loss is: 0.06539285182952881\n",
      "Iteration is: 200 and loss is: 0.06440774351358414\n",
      "Iteration is: 201 and loss is: 0.06343603879213333\n",
      "Iteration is: 202 and loss is: 0.06247763708233833\n",
      "Iteration is: 203 and loss is: 0.06153232976794243\n",
      "Iteration is: 204 and loss is: 0.06059994548559189\n",
      "Iteration is: 205 and loss is: 0.05968046933412552\n",
      "Iteration is: 206 and loss is: 0.05877351015806198\n",
      "Iteration is: 207 and loss is: 0.05787919834256172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 208 and loss is: 0.05699741095304489\n",
      "Iteration is: 209 and loss is: 0.05612797290086746\n",
      "Iteration is: 210 and loss is: 0.055270805954933167\n",
      "Iteration is: 211 and loss is: 0.054425839334726334\n",
      "Iteration is: 212 and loss is: 0.05359288677573204\n",
      "Iteration is: 213 and loss is: 0.05277203023433685\n",
      "Iteration is: 214 and loss is: 0.05196288228034973\n",
      "Iteration is: 215 and loss is: 0.051165640354156494\n",
      "Iteration is: 216 and loss is: 0.050379909574985504\n",
      "Iteration is: 217 and loss is: 0.049605775624513626\n",
      "Iteration is: 218 and loss is: 0.04884302243590355\n",
      "Iteration is: 219 and loss is: 0.04809174686670303\n",
      "Iteration is: 220 and loss is: 0.047351621091365814\n",
      "Iteration is: 221 and loss is: 0.04662265628576279\n",
      "Iteration is: 222 and loss is: 0.04590480774641037\n",
      "Iteration is: 223 and loss is: 0.04519791901111603\n",
      "Iteration is: 224 and loss is: 0.044501982629299164\n",
      "Iteration is: 225 and loss is: 0.04381677135825157\n",
      "Iteration is: 226 and loss is: 0.04314219951629639\n",
      "Iteration is: 227 and loss is: 0.04247827082872391\n",
      "Iteration is: 228 and loss is: 0.041824765503406525\n",
      "Iteration is: 229 and loss is: 0.04118182510137558\n",
      "Iteration is: 230 and loss is: 0.04054912179708481\n",
      "Iteration is: 231 and loss is: 0.03992670029401779\n",
      "Iteration is: 232 and loss is: 0.0393143966794014\n",
      "Iteration is: 233 and loss is: 0.03871220350265503\n",
      "Iteration is: 234 and loss is: 0.03811991959810257\n",
      "Iteration is: 235 and loss is: 0.03753752261400223\n",
      "Iteration is: 236 and loss is: 0.03696487843990326\n",
      "Iteration is: 237 and loss is: 0.03640201687812805\n",
      "Iteration is: 238 and loss is: 0.03584865480661392\n",
      "Iteration is: 239 and loss is: 0.03530484810471535\n",
      "Iteration is: 240 and loss is: 0.034770410507917404\n",
      "Iteration is: 241 and loss is: 0.03424527496099472\n",
      "Iteration is: 242 and loss is: 0.033729344606399536\n",
      "Iteration is: 243 and loss is: 0.03322260454297066\n",
      "Iteration is: 244 and loss is: 0.03272486850619316\n",
      "Iteration is: 245 and loss is: 0.032236043363809586\n",
      "Iteration is: 246 and loss is: 0.031756069511175156\n",
      "Iteration is: 247 and loss is: 0.03128483518958092\n",
      "Iteration is: 248 and loss is: 0.030822254717350006\n",
      "Iteration is: 249 and loss is: 0.030368223786354065\n",
      "Iteration is: 250 and loss is: 0.029922572895884514\n",
      "Iteration is: 251 and loss is: 0.029485348612070084\n",
      "Iteration is: 252 and loss is: 0.029056308791041374\n",
      "Iteration is: 253 and loss is: 0.028635457158088684\n",
      "Iteration is: 254 and loss is: 0.02822263352572918\n",
      "Iteration is: 255 and loss is: 0.027817726135253906\n",
      "Iteration is: 256 and loss is: 0.027420729398727417\n",
      "Iteration is: 257 and loss is: 0.02703138254582882\n",
      "Iteration is: 258 and loss is: 0.02664976194500923\n",
      "Iteration is: 259 and loss is: 0.02627561055123806\n",
      "Iteration is: 260 and loss is: 0.025908926501870155\n",
      "Iteration is: 261 and loss is: 0.0255495123565197\n",
      "Iteration is: 262 and loss is: 0.025197403505444527\n",
      "Iteration is: 263 and loss is: 0.024852365255355835\n",
      "Iteration is: 264 and loss is: 0.024514328688383102\n",
      "Iteration is: 265 and loss is: 0.024183230474591255\n",
      "Iteration is: 266 and loss is: 0.023858925327658653\n",
      "Iteration is: 267 and loss is: 0.023541325703263283\n",
      "Iteration is: 268 and loss is: 0.023230325430631638\n",
      "Iteration is: 269 and loss is: 0.022925756871700287\n",
      "Iteration is: 270 and loss is: 0.02262757159769535\n",
      "Iteration is: 271 and loss is: 0.022335762158036232\n",
      "Iteration is: 272 and loss is: 0.02205003798007965\n",
      "Iteration is: 273 and loss is: 0.021770386025309563\n",
      "Iteration is: 274 and loss is: 0.021496733650565147\n",
      "Iteration is: 275 and loss is: 0.02122889831662178\n",
      "Iteration is: 276 and loss is: 0.020966848358511925\n",
      "Iteration is: 277 and loss is: 0.020710429176688194\n",
      "Iteration is: 278 and loss is: 0.02045956254005432\n",
      "Iteration is: 279 and loss is: 0.020214136689901352\n",
      "Iteration is: 280 and loss is: 0.01997406594455242\n",
      "Iteration is: 281 and loss is: 0.019739212468266487\n",
      "Iteration is: 282 and loss is: 0.019509509205818176\n",
      "Iteration is: 283 and loss is: 0.019284872338175774\n",
      "Iteration is: 284 and loss is: 0.019065100699663162\n",
      "Iteration is: 285 and loss is: 0.018850218504667282\n",
      "Iteration is: 286 and loss is: 0.018640078604221344\n",
      "Iteration is: 287 and loss is: 0.018434565514326096\n",
      "Iteration is: 288 and loss is: 0.018233580514788628\n",
      "Iteration is: 289 and loss is: 0.018037037923932076\n",
      "Iteration is: 290 and loss is: 0.017844881862401962\n",
      "Iteration is: 291 and loss is: 0.01765695959329605\n",
      "Iteration is: 292 and loss is: 0.017473243176937103\n",
      "Iteration is: 293 and loss is: 0.017293574288487434\n",
      "Iteration is: 294 and loss is: 0.017117885872721672\n",
      "Iteration is: 295 and loss is: 0.016946084797382355\n",
      "Iteration is: 296 and loss is: 0.016778098419308662\n",
      "Iteration is: 297 and loss is: 0.016613798215985298\n",
      "Iteration is: 298 and loss is: 0.016453156247735023\n",
      "Iteration is: 299 and loss is: 0.016296042129397392\n",
      "Iteration is: 300 and loss is: 0.01614237017929554\n",
      "Iteration is: 301 and loss is: 0.015992101281881332\n",
      "Iteration is: 302 and loss is: 0.015845127403736115\n",
      "Iteration is: 303 and loss is: 0.015701361000537872\n",
      "Iteration is: 304 and loss is: 0.015560729429125786\n",
      "Iteration is: 305 and loss is: 0.015423157252371311\n",
      "Iteration is: 306 and loss is: 0.015288546681404114\n",
      "Iteration is: 307 and loss is: 0.015156886540353298\n",
      "Iteration is: 308 and loss is: 0.015028024092316628\n",
      "Iteration is: 309 and loss is: 0.014901872724294662\n",
      "Iteration is: 310 and loss is: 0.014778460375964642\n",
      "Iteration is: 311 and loss is: 0.01465767901390791\n",
      "Iteration is: 312 and loss is: 0.014539419673383236\n",
      "Iteration is: 313 and loss is: 0.014423643238842487\n",
      "Iteration is: 314 and loss is: 0.014310319907963276\n",
      "Iteration is: 315 and loss is: 0.014199294149875641\n",
      "Iteration is: 316 and loss is: 0.014090576209127903\n",
      "Iteration is: 317 and loss is: 0.013984079472720623\n",
      "Iteration is: 318 and loss is: 0.01387974712997675\n",
      "Iteration is: 319 and loss is: 0.013777550309896469\n",
      "Iteration is: 320 and loss is: 0.013677362352609634\n",
      "Iteration is: 321 and loss is: 0.013579180464148521\n",
      "Iteration is: 322 and loss is: 0.013482940383255482\n",
      "Iteration is: 323 and loss is: 0.01338860485702753\n",
      "Iteration is: 324 and loss is: 0.013296069577336311\n",
      "Iteration is: 325 and loss is: 0.013205314986407757\n",
      "Iteration is: 326 and loss is: 0.013116310350596905\n",
      "Iteration is: 327 and loss is: 0.013028975576162338\n",
      "Iteration is: 328 and loss is: 0.012943264096975327\n",
      "Iteration is: 329 and loss is: 0.012859145179390907\n",
      "Iteration is: 330 and loss is: 0.012776591815054417\n",
      "Iteration is: 331 and loss is: 0.012695517390966415\n",
      "Iteration is: 332 and loss is: 0.012615900486707687\n",
      "Iteration is: 333 and loss is: 0.012537702918052673\n",
      "Iteration is: 334 and loss is: 0.012460879981517792\n",
      "Iteration is: 335 and loss is: 0.012385404668748379\n",
      "Iteration is: 336 and loss is: 0.012311190366744995\n",
      "Iteration is: 337 and loss is: 0.01223826501518488\n",
      "Iteration is: 338 and loss is: 0.012166542932391167\n",
      "Iteration is: 339 and loss is: 0.012096027843654156\n",
      "Iteration is: 340 and loss is: 0.012026692740619183\n",
      "Iteration is: 341 and loss is: 0.011958417482674122\n",
      "Iteration is: 342 and loss is: 0.01189127191901207\n",
      "Iteration is: 343 and loss is: 0.011825182475149632\n",
      "Iteration is: 344 and loss is: 0.011760125868022442\n",
      "Iteration is: 345 and loss is: 0.011696044355630875\n",
      "Iteration is: 346 and loss is: 0.01163296028971672\n",
      "Iteration is: 347 and loss is: 0.011570810340344906\n",
      "Iteration is: 348 and loss is: 0.011509578675031662\n",
      "Iteration is: 349 and loss is: 0.011449238285422325\n",
      "Iteration is: 350 and loss is: 0.01138976775109768\n",
      "Iteration is: 351 and loss is: 0.011331134475767612\n",
      "Iteration is: 352 and loss is: 0.011273334734141827\n",
      "Iteration is: 353 and loss is: 0.011216310784220695\n",
      "Iteration is: 354 and loss is: 0.011160065419971943\n",
      "Iteration is: 355 and loss is: 0.011104578152298927\n",
      "Iteration is: 356 and loss is: 0.011049837805330753\n",
      "Iteration is: 357 and loss is: 0.010995800606906414\n",
      "Iteration is: 358 and loss is: 0.010942446999251842\n",
      "Iteration is: 359 and loss is: 0.01088977325707674\n",
      "Iteration is: 360 and loss is: 0.010837742127478123\n",
      "Iteration is: 361 and loss is: 0.010786370374262333\n",
      "Iteration is: 362 and loss is: 0.010735630989074707\n",
      "Iteration is: 363 and loss is: 0.010685487650334835\n",
      "Iteration is: 364 and loss is: 0.010635949671268463\n",
      "Iteration is: 365 and loss is: 0.010586956515908241\n",
      "Iteration is: 366 and loss is: 0.010538539849221706\n",
      "Iteration is: 367 and loss is: 0.010490681044757366\n",
      "Iteration is: 368 and loss is: 0.010443348437547684\n",
      "Iteration is: 369 and loss is: 0.010396534577012062\n",
      "Iteration is: 370 and loss is: 0.010350230149924755\n",
      "Iteration is: 371 and loss is: 0.010304420255124569\n",
      "Iteration is: 372 and loss is: 0.010259099304676056\n",
      "Iteration is: 373 and loss is: 0.010214253328740597\n",
      "Iteration is: 374 and loss is: 0.010169832035899162\n",
      "Iteration is: 375 and loss is: 0.010125894099473953\n",
      "Iteration is: 376 and loss is: 0.01008241344243288\n",
      "Iteration is: 377 and loss is: 0.010039331391453743\n",
      "Iteration is: 378 and loss is: 0.009996678680181503\n",
      "Iteration is: 379 and loss is: 0.009954443201422691\n",
      "Iteration is: 380 and loss is: 0.009912608191370964\n",
      "Iteration is: 381 and loss is: 0.009871142916381359\n",
      "Iteration is: 382 and loss is: 0.009830085560679436\n",
      "Iteration is: 383 and loss is: 0.009789410047233105\n",
      "Iteration is: 384 and loss is: 0.009749076329171658\n",
      "Iteration is: 385 and loss is: 0.009709130972623825\n",
      "Iteration is: 386 and loss is: 0.00966953206807375\n",
      "Iteration is: 387 and loss is: 0.009630278684198856\n",
      "Iteration is: 388 and loss is: 0.00959133729338646\n",
      "Iteration is: 389 and loss is: 0.009552774019539356\n",
      "Iteration is: 390 and loss is: 0.00951451063156128\n",
      "Iteration is: 391 and loss is: 0.009476574137806892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 392 and loss is: 0.00943894311785698\n",
      "Iteration is: 393 and loss is: 0.009401625953614712\n",
      "Iteration is: 394 and loss is: 0.009364627301692963\n",
      "Iteration is: 395 and loss is: 0.00932791456580162\n",
      "Iteration is: 396 and loss is: 0.009291480295360088\n",
      "Iteration is: 397 and loss is: 0.009255343116819859\n",
      "Iteration is: 398 and loss is: 0.009219483472406864\n",
      "Iteration is: 399 and loss is: 0.009183919988572598\n",
      "Iteration is: 400 and loss is: 0.009148613549768925\n",
      "Iteration is: 401 and loss is: 0.009113563224673271\n",
      "Iteration is: 402 and loss is: 0.009078794158995152\n",
      "Iteration is: 403 and loss is: 0.009044284000992775\n",
      "Iteration is: 404 and loss is: 0.00901002250611782\n",
      "Iteration is: 405 and loss is: 0.008976016193628311\n",
      "Iteration is: 406 and loss is: 0.008942252025008202\n",
      "Iteration is: 407 and loss is: 0.008908727206289768\n",
      "Iteration is: 408 and loss is: 0.008875477127730846\n",
      "Iteration is: 409 and loss is: 0.008842415176331997\n",
      "Iteration is: 410 and loss is: 0.008809629827737808\n",
      "Iteration is: 411 and loss is: 0.008777042850852013\n",
      "Iteration is: 412 and loss is: 0.00874469056725502\n",
      "Iteration is: 413 and loss is: 0.008712578564882278\n",
      "Iteration is: 414 and loss is: 0.008680667728185654\n",
      "Iteration is: 415 and loss is: 0.00864897109568119\n",
      "Iteration is: 416 and loss is: 0.008617505431175232\n",
      "Iteration is: 417 and loss is: 0.008586246520280838\n",
      "Iteration is: 418 and loss is: 0.008555191569030285\n",
      "Iteration is: 419 and loss is: 0.008524347096681595\n",
      "Iteration is: 420 and loss is: 0.008493692614138126\n",
      "Iteration is: 421 and loss is: 0.008463253267109394\n",
      "Iteration is: 422 and loss is: 0.008433022536337376\n",
      "Iteration is: 423 and loss is: 0.00840296596288681\n",
      "Iteration is: 424 and loss is: 0.008373121730983257\n",
      "Iteration is: 425 and loss is: 0.008343438617885113\n",
      "Iteration is: 426 and loss is: 0.0083139818161726\n",
      "Iteration is: 427 and loss is: 0.00828469917178154\n",
      "Iteration is: 428 and loss is: 0.008255591616034508\n",
      "Iteration is: 429 and loss is: 0.008226661942899227\n",
      "Iteration is: 430 and loss is: 0.008197933435440063\n",
      "Iteration is: 431 and loss is: 0.008169377222657204\n",
      "Iteration is: 432 and loss is: 0.008140990510582924\n",
      "Iteration is: 433 and loss is: 0.008112791925668716\n",
      "Iteration is: 434 and loss is: 0.008084755390882492\n",
      "Iteration is: 435 and loss is: 0.00805688463151455\n",
      "Iteration is: 436 and loss is: 0.008029197342693806\n",
      "Iteration is: 437 and loss is: 0.008001673966646194\n",
      "Iteration is: 438 and loss is: 0.00797431543469429\n",
      "Iteration is: 439 and loss is: 0.007947110570967197\n",
      "Iteration is: 440 and loss is: 0.007920080795884132\n",
      "Iteration is: 441 and loss is: 0.007893216796219349\n",
      "Iteration is: 442 and loss is: 0.007866499945521355\n",
      "Iteration is: 443 and loss is: 0.007839946076273918\n",
      "Iteration is: 444 and loss is: 0.007813540287315845\n",
      "Iteration is: 445 and loss is: 0.007787300739437342\n",
      "Iteration is: 446 and loss is: 0.0077612074092030525\n",
      "Iteration is: 447 and loss is: 0.0077352565713226795\n",
      "Iteration is: 448 and loss is: 0.007709474302828312\n",
      "Iteration is: 449 and loss is: 0.007683836854994297\n",
      "Iteration is: 450 and loss is: 0.007658329792320728\n",
      "Iteration is: 451 and loss is: 0.007632993161678314\n",
      "Iteration is: 452 and loss is: 0.007607793901115656\n",
      "Iteration is: 453 and loss is: 0.007582720369100571\n",
      "Iteration is: 454 and loss is: 0.007557807955890894\n",
      "Iteration is: 455 and loss is: 0.0075330184772610664\n",
      "Iteration is: 456 and loss is: 0.0075084008276462555\n",
      "Iteration is: 457 and loss is: 0.00748389633372426\n",
      "Iteration is: 458 and loss is: 0.007459528278559446\n",
      "Iteration is: 459 and loss is: 0.007435300387442112\n",
      "Iteration is: 460 and loss is: 0.007411218713968992\n",
      "Iteration is: 461 and loss is: 0.007387263234704733\n",
      "Iteration is: 462 and loss is: 0.007363429293036461\n",
      "Iteration is: 463 and loss is: 0.007339745294302702\n",
      "Iteration is: 464 and loss is: 0.007316171657294035\n",
      "Iteration is: 465 and loss is: 0.007292736321687698\n",
      "Iteration is: 466 and loss is: 0.007269436027854681\n",
      "Iteration is: 467 and loss is: 0.007246246095746756\n",
      "Iteration is: 468 and loss is: 0.007223182823508978\n",
      "Iteration is: 469 and loss is: 0.007200264371931553\n",
      "Iteration is: 470 and loss is: 0.007177455350756645\n",
      "Iteration is: 471 and loss is: 0.007154756225645542\n",
      "Iteration is: 472 and loss is: 0.00713220564648509\n",
      "Iteration is: 473 and loss is: 0.0071097686886787415\n",
      "Iteration is: 474 and loss is: 0.0070874448865652084\n",
      "Iteration is: 475 and loss is: 0.007065249606966972\n",
      "Iteration is: 476 and loss is: 0.007043154910206795\n",
      "Iteration is: 477 and loss is: 0.007021194323897362\n",
      "Iteration is: 478 and loss is: 0.006999339442700148\n",
      "Iteration is: 479 and loss is: 0.006977606564760208\n",
      "Iteration is: 480 and loss is: 0.006955974269658327\n",
      "Iteration is: 481 and loss is: 0.006934464909136295\n",
      "Iteration is: 482 and loss is: 0.006913067772984505\n",
      "Iteration is: 483 and loss is: 0.0068917968310415745\n",
      "Iteration is: 484 and loss is: 0.006870608776807785\n",
      "Iteration is: 485 and loss is: 0.006849533412605524\n",
      "Iteration is: 486 and loss is: 0.006828583311289549\n",
      "Iteration is: 487 and loss is: 0.006807724945247173\n",
      "Iteration is: 488 and loss is: 0.00678698904812336\n",
      "Iteration is: 489 and loss is: 0.006766356062144041\n",
      "Iteration is: 490 and loss is: 0.006745812948793173\n",
      "Iteration is: 491 and loss is: 0.006725388579070568\n",
      "Iteration is: 492 and loss is: 0.006705074105411768\n",
      "Iteration is: 493 and loss is: 0.0066848560236394405\n",
      "Iteration is: 494 and loss is: 0.006664744112640619\n",
      "Iteration is: 495 and loss is: 0.006644716951996088\n",
      "Iteration is: 496 and loss is: 0.006624788977205753\n",
      "Iteration is: 497 and loss is: 0.0066049834713339806\n",
      "Iteration is: 498 and loss is: 0.006585264578461647\n",
      "Iteration is: 499 and loss is: 0.006565658375620842\n",
      "Iteration is: 500 and loss is: 0.006546131335198879\n",
      "Iteration is: 501 and loss is: 0.006526709068566561\n",
      "Iteration is: 502 and loss is: 0.006507381331175566\n",
      "Iteration is: 503 and loss is: 0.006488157901912928\n",
      "Iteration is: 504 and loss is: 0.006469016429036856\n",
      "Iteration is: 505 and loss is: 0.0064499713480472565\n",
      "Iteration is: 506 and loss is: 0.006431026384234428\n",
      "Iteration is: 507 and loss is: 0.006412179209291935\n",
      "Iteration is: 508 and loss is: 0.0063934032805264\n",
      "Iteration is: 509 and loss is: 0.006374738644808531\n",
      "Iteration is: 510 and loss is: 0.006356155965477228\n",
      "Iteration is: 511 and loss is: 0.006337673868983984\n",
      "Iteration is: 512 and loss is: 0.006319267209619284\n",
      "Iteration is: 513 and loss is: 0.006300952285528183\n",
      "Iteration is: 514 and loss is: 0.00628272769972682\n",
      "Iteration is: 515 and loss is: 0.0062645962461829185\n",
      "Iteration is: 516 and loss is: 0.006246543489396572\n",
      "Iteration is: 517 and loss is: 0.0062285833992064\n",
      "Iteration is: 518 and loss is: 0.006210702937096357\n",
      "Iteration is: 519 and loss is: 0.006192921660840511\n",
      "Iteration is: 520 and loss is: 0.006175213027745485\n",
      "Iteration is: 521 and loss is: 0.006157583091408014\n",
      "Iteration is: 522 and loss is: 0.006140046287328005\n",
      "Iteration is: 523 and loss is: 0.006122591905295849\n",
      "Iteration is: 524 and loss is: 0.006105218548327684\n",
      "Iteration is: 525 and loss is: 0.00608792481943965\n",
      "Iteration is: 526 and loss is: 0.006070711184293032\n",
      "Iteration is: 527 and loss is: 0.006053579039871693\n",
      "Iteration is: 528 and loss is: 0.006036534905433655\n",
      "Iteration is: 529 and loss is: 0.006019565276801586\n",
      "Iteration is: 530 and loss is: 0.006002673879265785\n",
      "Iteration is: 531 and loss is: 0.0059858583845198154\n",
      "Iteration is: 532 and loss is: 0.005969117395579815\n",
      "Iteration is: 533 and loss is: 0.00595246534794569\n",
      "Iteration is: 534 and loss is: 0.005935885012149811\n",
      "Iteration is: 535 and loss is: 0.00591937405988574\n",
      "Iteration is: 536 and loss is: 0.005902945529669523\n",
      "Iteration is: 537 and loss is: 0.00588659243658185\n",
      "Iteration is: 538 and loss is: 0.00587030965834856\n",
      "Iteration is: 539 and loss is: 0.005854103714227676\n",
      "Iteration is: 540 and loss is: 0.005837967153638601\n",
      "Iteration is: 541 and loss is: 0.005821908824145794\n",
      "Iteration is: 542 and loss is: 0.005805928725749254\n",
      "Iteration is: 543 and loss is: 0.005790013819932938\n",
      "Iteration is: 544 and loss is: 0.005774162709712982\n",
      "Iteration is: 545 and loss is: 0.005758387036621571\n",
      "Iteration is: 546 and loss is: 0.005742695182561874\n",
      "Iteration is: 547 and loss is: 0.005727057345211506\n",
      "Iteration is: 548 and loss is: 0.005711502395570278\n",
      "Iteration is: 549 and loss is: 0.005696004256606102\n",
      "Iteration is: 550 and loss is: 0.0056805904023349285\n",
      "Iteration is: 551 and loss is: 0.005665232893079519\n",
      "Iteration is: 552 and loss is: 0.005649937782436609\n",
      "Iteration is: 553 and loss is: 0.005634726025164127\n",
      "Iteration is: 554 and loss is: 0.005619584117084742\n",
      "Iteration is: 555 and loss is: 0.005604485981166363\n",
      "Iteration is: 556 and loss is: 0.005589467938989401\n",
      "Iteration is: 557 and loss is: 0.005574518349021673\n",
      "Iteration is: 558 and loss is: 0.005559634417295456\n",
      "Iteration is: 559 and loss is: 0.005544811487197876\n",
      "Iteration is: 560 and loss is: 0.005530048161745071\n",
      "Iteration is: 561 and loss is: 0.005515358876436949\n",
      "Iteration is: 562 and loss is: 0.005500727333128452\n",
      "Iteration is: 563 and loss is: 0.005486154463142157\n",
      "Iteration is: 564 and loss is: 0.005471654701977968\n",
      "Iteration is: 565 and loss is: 0.005457204766571522\n",
      "Iteration is: 566 and loss is: 0.005442838184535503\n",
      "Iteration is: 567 and loss is: 0.005428512580692768\n",
      "Iteration is: 568 and loss is: 0.005414267536252737\n",
      "Iteration is: 569 and loss is: 0.005400070454925299\n",
      "Iteration is: 570 and loss is: 0.005385934375226498\n",
      "Iteration is: 571 and loss is: 0.005371860694140196\n",
      "Iteration is: 572 and loss is: 0.005357841495424509\n",
      "Iteration is: 573 and loss is: 0.005343887954950333\n",
      "Iteration is: 574 and loss is: 0.0053299833089113235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 575 and loss is: 0.005316153634339571\n",
      "Iteration is: 576 and loss is: 0.005302362609654665\n",
      "Iteration is: 577 and loss is: 0.005288653075695038\n",
      "Iteration is: 578 and loss is: 0.005274982191622257\n",
      "Iteration is: 579 and loss is: 0.005261377431452274\n",
      "Iteration is: 580 and loss is: 0.005247823428362608\n",
      "Iteration is: 581 and loss is: 0.0052343374118208885\n",
      "Iteration is: 582 and loss is: 0.005220897030085325\n",
      "Iteration is: 583 and loss is: 0.005207512527704239\n",
      "Iteration is: 584 and loss is: 0.005194184836000204\n",
      "Iteration is: 585 and loss is: 0.005180913023650646\n",
      "Iteration is: 586 and loss is: 0.005167696624994278\n",
      "Iteration is: 587 and loss is: 0.005154538433998823\n",
      "Iteration is: 588 and loss is: 0.005141428206115961\n",
      "Iteration is: 589 and loss is: 0.005128375254571438\n",
      "Iteration is: 590 and loss is: 0.005115382838994265\n",
      "Iteration is: 591 and loss is: 0.005102428607642651\n",
      "Iteration is: 592 and loss is: 0.00508953956887126\n",
      "Iteration is: 593 and loss is: 0.005076690576970577\n",
      "Iteration is: 594 and loss is: 0.005063896998763084\n",
      "Iteration is: 595 and loss is: 0.00505116768181324\n",
      "Iteration is: 596 and loss is: 0.005038474220782518\n",
      "Iteration is: 597 and loss is: 0.005025842692703009\n",
      "Iteration is: 598 and loss is: 0.0050132558681070805\n",
      "Iteration is: 599 and loss is: 0.005000720731914043\n",
      "Iteration is: 600 and loss is: 0.004988245200365782\n",
      "Iteration is: 601 and loss is: 0.0049758050590753555\n",
      "Iteration is: 602 and loss is: 0.004963421728461981\n",
      "Iteration is: 603 and loss is: 0.004951083101332188\n",
      "Iteration is: 604 and loss is: 0.004938797559589148\n",
      "Iteration is: 605 and loss is: 0.004926562774926424\n",
      "Iteration is: 606 and loss is: 0.004914376884698868\n",
      "Iteration is: 607 and loss is: 0.004902240354567766\n",
      "Iteration is: 608 and loss is: 0.004890146665275097\n",
      "Iteration is: 609 and loss is: 0.004878104664385319\n",
      "Iteration is: 610 and loss is: 0.0048661078326404095\n",
      "Iteration is: 611 and loss is: 0.0048541598953306675\n",
      "Iteration is: 612 and loss is: 0.00484224921092391\n",
      "Iteration is: 613 and loss is: 0.00483039952814579\n",
      "Iteration is: 614 and loss is: 0.004818590823560953\n",
      "Iteration is: 615 and loss is: 0.004806823562830687\n",
      "Iteration is: 616 and loss is: 0.00479509774595499\n",
      "Iteration is: 617 and loss is: 0.004783442243933678\n",
      "Iteration is: 618 and loss is: 0.004771808627992868\n",
      "Iteration is: 619 and loss is: 0.004760225303471088\n",
      "Iteration is: 620 and loss is: 0.004748690407723188\n",
      "Iteration is: 621 and loss is: 0.0047371950931847095\n",
      "Iteration is: 622 and loss is: 0.004725749138742685\n",
      "Iteration is: 623 and loss is: 0.004714346490800381\n",
      "Iteration is: 624 and loss is: 0.004702980630099773\n",
      "Iteration is: 625 and loss is: 0.004691666457802057\n",
      "Iteration is: 626 and loss is: 0.004680396523326635\n",
      "Iteration is: 627 and loss is: 0.00466915313154459\n",
      "Iteration is: 628 and loss is: 0.004657972604036331\n",
      "Iteration is: 629 and loss is: 0.004646827932447195\n",
      "Iteration is: 630 and loss is: 0.004635732155293226\n",
      "Iteration is: 631 and loss is: 0.0046246666461229324\n",
      "Iteration is: 632 and loss is: 0.004613642580807209\n",
      "Iteration is: 633 and loss is: 0.004602675326168537\n",
      "Iteration is: 634 and loss is: 0.004591735079884529\n",
      "Iteration is: 635 and loss is: 0.004580841399729252\n",
      "Iteration is: 636 and loss is: 0.0045699868351221085\n",
      "Iteration is: 637 and loss is: 0.004559173248708248\n",
      "Iteration is: 638 and loss is: 0.004548399243503809\n",
      "Iteration is: 639 and loss is: 0.004537667613476515\n",
      "Iteration is: 640 and loss is: 0.004526978824287653\n",
      "Iteration is: 641 and loss is: 0.004516320303082466\n",
      "Iteration is: 642 and loss is: 0.004505716729909182\n",
      "Iteration is: 643 and loss is: 0.0044951410964131355\n",
      "Iteration is: 644 and loss is: 0.004484603647142649\n",
      "Iteration is: 645 and loss is: 0.0044741141609847546\n",
      "Iteration is: 646 and loss is: 0.004463654011487961\n",
      "Iteration is: 647 and loss is: 0.004453232977539301\n",
      "Iteration is: 648 and loss is: 0.004442851524800062\n",
      "Iteration is: 649 and loss is: 0.004432507790625095\n",
      "Iteration is: 650 and loss is: 0.004422204103320837\n",
      "Iteration is: 651 and loss is: 0.004411939997226\n",
      "Iteration is: 652 and loss is: 0.004401709418743849\n",
      "Iteration is: 653 and loss is: 0.0043915193527936935\n",
      "Iteration is: 654 and loss is: 0.004381365142762661\n",
      "Iteration is: 655 and loss is: 0.00437125051394105\n",
      "Iteration is: 656 and loss is: 0.004361160099506378\n",
      "Iteration is: 657 and loss is: 0.004351125098764896\n",
      "Iteration is: 658 and loss is: 0.004341116640716791\n",
      "Iteration is: 659 and loss is: 0.004331147763878107\n",
      "Iteration is: 660 and loss is: 0.0043212054297327995\n",
      "Iteration is: 661 and loss is: 0.004311306867748499\n",
      "Iteration is: 662 and loss is: 0.0043014404363930225\n",
      "Iteration is: 663 and loss is: 0.0042916107922792435\n",
      "Iteration is: 664 and loss is: 0.0042818165384233\n",
      "Iteration is: 665 and loss is: 0.004272051155567169\n",
      "Iteration is: 666 and loss is: 0.0042623295448720455\n",
      "Iteration is: 667 and loss is: 0.004252641461789608\n",
      "Iteration is: 668 and loss is: 0.004242978058755398\n",
      "Iteration is: 669 and loss is: 0.004233362153172493\n",
      "Iteration is: 670 and loss is: 0.0042237769812345505\n",
      "Iteration is: 671 and loss is: 0.0042142183519899845\n",
      "Iteration is: 672 and loss is: 0.004204699769616127\n",
      "Iteration is: 673 and loss is: 0.0041952249594032764\n",
      "Iteration is: 674 and loss is: 0.004185762256383896\n",
      "Iteration is: 675 and loss is: 0.004176343325525522\n",
      "Iteration is: 676 and loss is: 0.004166954196989536\n",
      "Iteration is: 677 and loss is: 0.004157600924372673\n",
      "Iteration is: 678 and loss is: 0.004148280713707209\n",
      "Iteration is: 679 and loss is: 0.0041389986872673035\n",
      "Iteration is: 680 and loss is: 0.0041297380812466145\n",
      "Iteration is: 681 and loss is: 0.0041205123998224735\n",
      "Iteration is: 682 and loss is: 0.004111322108656168\n",
      "Iteration is: 683 and loss is: 0.004102158825844526\n",
      "Iteration is: 684 and loss is: 0.004093038383871317\n",
      "Iteration is: 685 and loss is: 0.004083933774381876\n",
      "Iteration is: 686 and loss is: 0.004074869677424431\n",
      "Iteration is: 687 and loss is: 0.0040658386424183846\n",
      "Iteration is: 688 and loss is: 0.004056835081428289\n",
      "Iteration is: 689 and loss is: 0.004047860391438007\n",
      "Iteration is: 690 and loss is: 0.004038920160382986\n",
      "Iteration is: 691 and loss is: 0.004030007869005203\n",
      "Iteration is: 692 and loss is: 0.004021125845611095\n",
      "Iteration is: 693 and loss is: 0.004012279212474823\n",
      "Iteration is: 694 and loss is: 0.004003453068435192\n",
      "Iteration is: 695 and loss is: 0.00399466697126627\n",
      "Iteration is: 696 and loss is: 0.003985904622823\n",
      "Iteration is: 697 and loss is: 0.003977175801992416\n",
      "Iteration is: 698 and loss is: 0.003968478180468082\n",
      "Iteration is: 699 and loss is: 0.003959798254072666\n",
      "Iteration is: 700 and loss is: 0.003951159305870533\n",
      "Iteration is: 701 and loss is: 0.003942551091313362\n",
      "Iteration is: 702 and loss is: 0.0039339615032076836\n",
      "Iteration is: 703 and loss is: 0.003925404977053404\n",
      "Iteration is: 704 and loss is: 0.003916876390576363\n",
      "Iteration is: 705 and loss is: 0.00390838086605072\n",
      "Iteration is: 706 and loss is: 0.003899909555912018\n",
      "Iteration is: 707 and loss is: 0.0038914710748940706\n",
      "Iteration is: 708 and loss is: 0.0038830633275210857\n",
      "Iteration is: 709 and loss is: 0.0038746697828173637\n",
      "Iteration is: 710 and loss is: 0.003866319078952074\n",
      "Iteration is: 711 and loss is: 0.0038579823449254036\n",
      "Iteration is: 712 and loss is: 0.003849686123430729\n",
      "Iteration is: 713 and loss is: 0.0038414103910326958\n",
      "Iteration is: 714 and loss is: 0.003833157243207097\n",
      "Iteration is: 715 and loss is: 0.003824938088655472\n",
      "Iteration is: 716 and loss is: 0.003816739423200488\n",
      "Iteration is: 717 and loss is: 0.003808581968769431\n",
      "Iteration is: 718 and loss is: 0.003800447564572096\n",
      "Iteration is: 719 and loss is: 0.00379232713021338\n",
      "Iteration is: 720 and loss is: 0.0037842420861124992\n",
      "Iteration is: 721 and loss is: 0.0037761875428259373\n",
      "Iteration is: 722 and loss is: 0.003768153488636017\n",
      "Iteration is: 723 and loss is: 0.003760139225050807\n",
      "Iteration is: 724 and loss is: 0.0037521577905863523\n",
      "Iteration is: 725 and loss is: 0.003744206391274929\n",
      "Iteration is: 726 and loss is: 0.0037362745497375727\n",
      "Iteration is: 727 and loss is: 0.0037283715792000294\n",
      "Iteration is: 728 and loss is: 0.003720490727573633\n",
      "Iteration is: 729 and loss is: 0.003712643636390567\n",
      "Iteration is: 730 and loss is: 0.0037048121448606253\n",
      "Iteration is: 731 and loss is: 0.0036970162764191628\n",
      "Iteration is: 732 and loss is: 0.0036892371717840433\n",
      "Iteration is: 733 and loss is: 0.0036814871709793806\n",
      "Iteration is: 734 and loss is: 0.003673756029456854\n",
      "Iteration is: 735 and loss is: 0.0036660553887486458\n",
      "Iteration is: 736 and loss is: 0.0036583752371370792\n",
      "Iteration is: 737 and loss is: 0.0036507288459688425\n",
      "Iteration is: 738 and loss is: 0.0036431015469133854\n",
      "Iteration is: 739 and loss is: 0.003635494038462639\n",
      "Iteration is: 740 and loss is: 0.003627909580245614\n",
      "Iteration is: 741 and loss is: 0.0036203544586896896\n",
      "Iteration is: 742 and loss is: 0.00361282704398036\n",
      "Iteration is: 743 and loss is: 0.003605313366279006\n",
      "Iteration is: 744 and loss is: 0.0035978290252387524\n",
      "Iteration is: 745 and loss is: 0.0035903695970773697\n",
      "Iteration is: 746 and loss is: 0.003582932287827134\n",
      "Iteration is: 747 and loss is: 0.0035755198914557695\n",
      "Iteration is: 748 and loss is: 0.003568129613995552\n",
      "Iteration is: 749 and loss is: 0.0035607602912932634\n",
      "Iteration is: 750 and loss is: 0.0035534121561795473\n",
      "Iteration is: 751 and loss is: 0.0035460919607430696\n",
      "Iteration is: 752 and loss is: 0.0035387997049838305\n",
      "Iteration is: 753 and loss is: 0.0035315232817083597\n",
      "Iteration is: 754 and loss is: 0.0035242706071585417\n",
      "Iteration is: 755 and loss is: 0.0035170421469956636\n",
      "Iteration is: 756 and loss is: 0.0035098290536552668\n",
      "Iteration is: 757 and loss is: 0.0035026443656533957\n",
      "Iteration is: 758 and loss is: 0.003495484823361039\n",
      "Iteration is: 759 and loss is: 0.0034883394837379456\n",
      "Iteration is: 760 and loss is: 0.0034812239464372396\n",
      "Iteration is: 761 and loss is: 0.003474124474450946\n",
      "Iteration is: 762 and loss is: 0.003467054106295109\n",
      "Iteration is: 763 and loss is: 0.0034600007347762585\n",
      "Iteration is: 764 and loss is: 0.0034529685508459806\n",
      "Iteration is: 765 and loss is: 0.00344596104696393\n",
      "Iteration is: 766 and loss is: 0.003438967978581786\n",
      "Iteration is: 767 and loss is: 0.0034320016857236624\n",
      "Iteration is: 768 and loss is: 0.0034250610042363405\n",
      "Iteration is: 769 and loss is: 0.003418138949200511\n",
      "Iteration is: 770 and loss is: 0.003411237383261323\n",
      "Iteration is: 771 and loss is: 0.0034043516498059034\n",
      "Iteration is: 772 and loss is: 0.0033974938560277224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 773 and loss is: 0.003390655620023608\n",
      "Iteration is: 774 and loss is: 0.0033838392700999975\n",
      "Iteration is: 775 and loss is: 0.003377031534910202\n",
      "Iteration is: 776 and loss is: 0.0033702568616718054\n",
      "Iteration is: 777 and loss is: 0.0033634991850703955\n",
      "Iteration is: 778 and loss is: 0.0033567636273801327\n",
      "Iteration is: 779 and loss is: 0.0033500511199235916\n",
      "Iteration is: 780 and loss is: 0.003343353047966957\n",
      "Iteration is: 781 and loss is: 0.003336676163598895\n",
      "Iteration is: 782 and loss is: 0.0033300211653113365\n",
      "Iteration is: 783 and loss is: 0.0033233824651688337\n",
      "Iteration is: 784 and loss is: 0.003316770540550351\n",
      "Iteration is: 785 and loss is: 0.00331016443669796\n",
      "Iteration is: 786 and loss is: 0.0033035981468856335\n",
      "Iteration is: 787 and loss is: 0.0032970367465168238\n",
      "Iteration is: 788 and loss is: 0.00329050631262362\n",
      "Iteration is: 789 and loss is: 0.0032839796040207148\n",
      "Iteration is: 790 and loss is: 0.003277481533586979\n",
      "Iteration is: 791 and loss is: 0.0032710032537579536\n",
      "Iteration is: 792 and loss is: 0.003264546627178788\n",
      "Iteration is: 793 and loss is: 0.003258103970438242\n",
      "Iteration is: 794 and loss is: 0.0032516804058104753\n",
      "Iteration is: 795 and loss is: 0.0032452819868922234\n",
      "Iteration is: 796 and loss is: 0.003238898003473878\n",
      "Iteration is: 797 and loss is: 0.003232533112168312\n",
      "Iteration is: 798 and loss is: 0.0032261854503303766\n",
      "Iteration is: 799 and loss is: 0.003219857346266508\n",
      "Iteration is: 800 and loss is: 0.003213551128283143\n",
      "Iteration is: 801 and loss is: 0.0032072626054286957\n",
      "Iteration is: 802 and loss is: 0.003200986422598362\n",
      "Iteration is: 803 and loss is: 0.00319473585113883\n",
      "Iteration is: 804 and loss is: 0.003188505070284009\n",
      "Iteration is: 805 and loss is: 0.003182285465300083\n",
      "Iteration is: 806 and loss is: 0.0031760830897837877\n",
      "Iteration is: 807 and loss is: 0.003169907722622156\n",
      "Iteration is: 808 and loss is: 0.003163746325299144\n",
      "Iteration is: 809 and loss is: 0.003157597966492176\n",
      "Iteration is: 810 and loss is: 0.0031514682341367006\n",
      "Iteration is: 811 and loss is: 0.0031453650444746017\n",
      "Iteration is: 812 and loss is: 0.003139273962005973\n",
      "Iteration is: 813 and loss is: 0.0031331987120211124\n",
      "Iteration is: 814 and loss is: 0.003127140225842595\n",
      "Iteration is: 815 and loss is: 0.0031211036257445812\n",
      "Iteration is: 816 and loss is: 0.0031150784343481064\n",
      "Iteration is: 817 and loss is: 0.003109079785645008\n",
      "Iteration is: 818 and loss is: 0.003103091614320874\n",
      "Iteration is: 819 and loss is: 0.0030971206724643707\n",
      "Iteration is: 820 and loss is: 0.0030911676585674286\n",
      "Iteration is: 821 and loss is: 0.0030852374620735645\n",
      "Iteration is: 822 and loss is: 0.003079312853515148\n",
      "Iteration is: 823 and loss is: 0.0030734133906662464\n",
      "Iteration is: 824 and loss is: 0.003067529294639826\n",
      "Iteration is: 825 and loss is: 0.003061661496758461\n",
      "Iteration is: 826 and loss is: 0.0030558116268366575\n",
      "Iteration is: 827 and loss is: 0.0030499775893986225\n",
      "Iteration is: 828 and loss is: 0.0030441628769040108\n",
      "Iteration is: 829 and loss is: 0.0030383607372641563\n",
      "Iteration is: 830 and loss is: 0.0030325809493660927\n",
      "Iteration is: 831 and loss is: 0.0030268083792179823\n",
      "Iteration is: 832 and loss is: 0.00302105862647295\n",
      "Iteration is: 833 and loss is: 0.0030153270345181227\n",
      "Iteration is: 834 and loss is: 0.003009603125974536\n",
      "Iteration is: 835 and loss is: 0.003003898309543729\n",
      "Iteration is: 836 and loss is: 0.0029982130508869886\n",
      "Iteration is: 837 and loss is: 0.0029925380367785692\n",
      "Iteration is: 838 and loss is: 0.002986886305734515\n",
      "Iteration is: 839 and loss is: 0.00298124342225492\n",
      "Iteration is: 840 and loss is: 0.002975620562210679\n",
      "Iteration is: 841 and loss is: 0.00297002075240016\n",
      "Iteration is: 842 and loss is: 0.0029644223395735025\n",
      "Iteration is: 843 and loss is: 0.0029588481411337852\n",
      "Iteration is: 844 and loss is: 0.0029532895423471928\n",
      "Iteration is: 845 and loss is: 0.002947742585092783\n",
      "Iteration is: 846 and loss is: 0.002942214021459222\n",
      "Iteration is: 847 and loss is: 0.002936698729172349\n",
      "Iteration is: 848 and loss is: 0.0029312046244740486\n",
      "Iteration is: 849 and loss is: 0.0029257149435579777\n",
      "Iteration is: 850 and loss is: 0.0029202469158917665\n",
      "Iteration is: 851 and loss is: 0.002914794720709324\n",
      "Iteration is: 852 and loss is: 0.002909361617639661\n",
      "Iteration is: 853 and loss is: 0.0029039352666586637\n",
      "Iteration is: 854 and loss is: 0.0028985259123146534\n",
      "Iteration is: 855 and loss is: 0.0028931342530995607\n",
      "Iteration is: 856 and loss is: 0.0028877523727715015\n",
      "Iteration is: 857 and loss is: 0.0028823851607739925\n",
      "Iteration is: 858 and loss is: 0.002877043792977929\n",
      "Iteration is: 859 and loss is: 0.0028717052191495895\n",
      "Iteration is: 860 and loss is: 0.0028663906268775463\n",
      "Iteration is: 861 and loss is: 0.002861078828573227\n",
      "Iteration is: 862 and loss is: 0.002855790313333273\n",
      "Iteration is: 863 and loss is: 0.002850519260391593\n",
      "Iteration is: 864 and loss is: 0.00284525565803051\n",
      "Iteration is: 865 and loss is: 0.0028400085866451263\n",
      "Iteration is: 866 and loss is: 0.0028347745537757874\n",
      "Iteration is: 867 and loss is: 0.0028295558877289295\n",
      "Iteration is: 868 and loss is: 0.0028243560809642076\n",
      "Iteration is: 869 and loss is: 0.0028191653545945883\n",
      "Iteration is: 870 and loss is: 0.0028139869682490826\n",
      "Iteration is: 871 and loss is: 0.0028088255785405636\n",
      "Iteration is: 872 and loss is: 0.0028036811854690313\n",
      "Iteration is: 873 and loss is: 0.0027985440101474524\n",
      "Iteration is: 874 and loss is: 0.002793426625430584\n",
      "Iteration is: 875 and loss is: 0.0027883220463991165\n",
      "Iteration is: 876 and loss is: 0.0027832246851176023\n",
      "Iteration is: 877 and loss is: 0.0027781506069004536\n",
      "Iteration is: 878 and loss is: 0.00277308258228004\n",
      "Iteration is: 879 and loss is: 0.002768035512417555\n",
      "Iteration is: 880 and loss is: 0.0027629940304905176\n",
      "Iteration is: 881 and loss is: 0.0027579674497246742\n",
      "Iteration is: 882 and loss is: 0.002752959728240967\n",
      "Iteration is: 883 and loss is: 0.002747962484136224\n",
      "Iteration is: 884 and loss is: 0.0027429761830717325\n",
      "Iteration is: 885 and loss is: 0.0027380071114748716\n",
      "Iteration is: 886 and loss is: 0.002733048517256975\n",
      "Iteration is: 887 and loss is: 0.0027281043585389853\n",
      "Iteration is: 888 and loss is: 0.00272317323833704\n",
      "Iteration is: 889 and loss is: 0.0027182584162801504\n",
      "Iteration is: 890 and loss is: 0.002713350346311927\n",
      "Iteration is: 891 and loss is: 0.00270845927298069\n",
      "Iteration is: 892 and loss is: 0.002703582402318716\n",
      "Iteration is: 893 and loss is: 0.002698717638850212\n",
      "Iteration is: 894 and loss is: 0.002693865681067109\n",
      "Iteration is: 895 and loss is: 0.0026890221051871777\n",
      "Iteration is: 896 and loss is: 0.002684197621420026\n",
      "Iteration is: 897 and loss is: 0.0026793822180479765\n",
      "Iteration is: 898 and loss is: 0.0026745765935629606\n",
      "Iteration is: 899 and loss is: 0.0026697933208197355\n",
      "Iteration is: 900 and loss is: 0.0026650168001651764\n",
      "Iteration is: 901 and loss is: 0.002660248661413789\n",
      "Iteration is: 902 and loss is: 0.0026555005460977554\n",
      "Iteration is: 903 and loss is: 0.002650759182870388\n",
      "Iteration is: 904 and loss is: 0.002646032487973571\n",
      "Iteration is: 905 and loss is: 0.002641319762915373\n",
      "Iteration is: 906 and loss is: 0.0026366200763732195\n",
      "Iteration is: 907 and loss is: 0.0026319329626858234\n",
      "Iteration is: 908 and loss is: 0.002627254230901599\n",
      "Iteration is: 909 and loss is: 0.002622586442157626\n",
      "Iteration is: 910 and loss is: 0.0026179368142038584\n",
      "Iteration is: 911 and loss is: 0.002613296266645193\n",
      "Iteration is: 912 and loss is: 0.002608666894957423\n",
      "Iteration is: 913 and loss is: 0.0026040489319711924\n",
      "Iteration is: 914 and loss is: 0.0025994437746703625\n",
      "Iteration is: 915 and loss is: 0.0025948542170226574\n",
      "Iteration is: 916 and loss is: 0.002590272342786193\n",
      "Iteration is: 917 and loss is: 0.002585704904049635\n",
      "Iteration is: 918 and loss is: 0.0025811437517404556\n",
      "Iteration is: 919 and loss is: 0.002576606348156929\n",
      "Iteration is: 920 and loss is: 0.0025720675475895405\n",
      "Iteration is: 921 and loss is: 0.0025675480719655752\n",
      "Iteration is: 922 and loss is: 0.002563037909567356\n",
      "Iteration is: 923 and loss is: 0.002558534499257803\n",
      "Iteration is: 924 and loss is: 0.0025540536735206842\n",
      "Iteration is: 925 and loss is: 0.002549577271565795\n",
      "Iteration is: 926 and loss is: 0.002545112045481801\n",
      "Iteration is: 927 and loss is: 0.0025406593922525644\n",
      "Iteration is: 928 and loss is: 0.0025362202432006598\n",
      "Iteration is: 929 and loss is: 0.0025317941326647997\n",
      "Iteration is: 930 and loss is: 0.002527375239878893\n",
      "Iteration is: 931 and loss is: 0.002522968454286456\n",
      "Iteration is: 932 and loss is: 0.0025185737758874893\n",
      "Iteration is: 933 and loss is: 0.002514183521270752\n",
      "Iteration is: 934 and loss is: 0.0025098116602748632\n",
      "Iteration is: 935 and loss is: 0.0025054486468434334\n",
      "Iteration is: 936 and loss is: 0.00250110006891191\n",
      "Iteration is: 937 and loss is: 0.002496757311746478\n",
      "Iteration is: 938 and loss is: 0.002492430154234171\n",
      "Iteration is: 939 and loss is: 0.0024881146382540464\n",
      "Iteration is: 940 and loss is: 0.0024838061071932316\n",
      "Iteration is: 941 and loss is: 0.0024795085191726685\n",
      "Iteration is: 942 and loss is: 0.0024752221070230007\n",
      "Iteration is: 943 and loss is: 0.002470947103574872\n",
      "Iteration is: 944 and loss is: 0.00246668653562665\n",
      "Iteration is: 945 and loss is: 0.002462434582412243\n",
      "Iteration is: 946 and loss is: 0.002458193339407444\n",
      "Iteration is: 947 and loss is: 0.0024539607111364603\n",
      "Iteration is: 948 and loss is: 0.002449741354212165\n",
      "Iteration is: 949 and loss is: 0.002445533638820052\n",
      "Iteration is: 950 and loss is: 0.002441328950226307\n",
      "Iteration is: 951 and loss is: 0.0024371405597776175\n",
      "Iteration is: 952 and loss is: 0.0024329619482159615\n",
      "Iteration is: 953 and loss is: 0.0024287928827106953\n",
      "Iteration is: 954 and loss is: 0.0024246361572295427\n",
      "Iteration is: 955 and loss is: 0.0024204892106354237\n",
      "Iteration is: 956 and loss is: 0.0024163550697267056\n",
      "Iteration is: 957 and loss is: 0.0024122281465679407\n",
      "Iteration is: 958 and loss is: 0.002408107277005911\n",
      "Iteration is: 959 and loss is: 0.0024040034040808678\n",
      "Iteration is: 960 and loss is: 0.002399908611550927\n",
      "Iteration is: 961 and loss is: 0.0023958187084645033\n",
      "Iteration is: 962 and loss is: 0.002391749294474721\n",
      "Iteration is: 963 and loss is: 0.002387681510299444\n",
      "Iteration is: 964 and loss is: 0.002383628860116005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 965 and loss is: 0.0023795810993760824\n",
      "Iteration is: 966 and loss is: 0.0023755475413054228\n",
      "Iteration is: 967 and loss is: 0.0023715237621217966\n",
      "Iteration is: 968 and loss is: 0.0023675053380429745\n",
      "Iteration is: 969 and loss is: 0.0023635001853108406\n",
      "Iteration is: 970 and loss is: 0.0023595052771270275\n",
      "Iteration is: 971 and loss is: 0.0023555192165076733\n",
      "Iteration is: 972 and loss is: 0.002351542469114065\n",
      "Iteration is: 973 and loss is: 0.0023475773632526398\n",
      "Iteration is: 974 and loss is: 0.0023436229676008224\n",
      "Iteration is: 975 and loss is: 0.0023396757896989584\n",
      "Iteration is: 976 and loss is: 0.0023357386235147715\n",
      "Iteration is: 977 and loss is: 0.0023318100720643997\n",
      "Iteration is: 978 and loss is: 0.002327891532331705\n",
      "Iteration is: 979 and loss is: 0.0023239862639456987\n",
      "Iteration is: 980 and loss is: 0.002320085186511278\n",
      "Iteration is: 981 and loss is: 0.0023162022698670626\n",
      "Iteration is: 982 and loss is: 0.0023123202845454216\n",
      "Iteration is: 983 and loss is: 0.0023084513377398252\n",
      "Iteration is: 984 and loss is: 0.0023045942652970552\n",
      "Iteration is: 985 and loss is: 0.0023007383570075035\n",
      "Iteration is: 986 and loss is: 0.002296897117048502\n",
      "Iteration is: 987 and loss is: 0.002293060766533017\n",
      "Iteration is: 988 and loss is: 0.002289242809638381\n",
      "Iteration is: 989 and loss is: 0.002285429509356618\n",
      "Iteration is: 990 and loss is: 0.002281621564179659\n",
      "Iteration is: 991 and loss is: 0.002277826424688101\n",
      "Iteration is: 992 and loss is: 0.0022740408312529325\n",
      "Iteration is: 993 and loss is: 0.0022702650167047977\n",
      "Iteration is: 994 and loss is: 0.002266497118398547\n",
      "Iteration is: 995 and loss is: 0.0022627366706728935\n",
      "Iteration is: 996 and loss is: 0.0022589880973100662\n",
      "Iteration is: 997 and loss is: 0.0022552432492375374\n",
      "Iteration is: 998 and loss is: 0.002251515630632639\n",
      "Iteration is: 999 and loss is: 0.002247791038826108\n",
      "Iteration is: 1000 and loss is: 0.0022440783213824034\n",
      "Iteration is: 1001 and loss is: 0.0022403772454708815\n",
      "Iteration is: 1002 and loss is: 0.00223668129183352\n",
      "Iteration is: 1003 and loss is: 0.0022329906933009624\n",
      "Iteration is: 1004 and loss is: 0.002229314064607024\n",
      "Iteration is: 1005 and loss is: 0.0022256469819694757\n",
      "Iteration is: 1006 and loss is: 0.0022219843231141567\n",
      "Iteration is: 1007 and loss is: 0.002218330977484584\n",
      "Iteration is: 1008 and loss is: 0.0022146895062178373\n",
      "Iteration is: 1009 and loss is: 0.0022110557183623314\n",
      "Iteration is: 1010 and loss is: 0.002207428915426135\n",
      "Iteration is: 1011 and loss is: 0.0022038088645786047\n",
      "Iteration is: 1012 and loss is: 0.0022002016194164753\n",
      "Iteration is: 1013 and loss is: 0.0021966027561575174\n",
      "Iteration is: 1014 and loss is: 0.002193013671785593\n",
      "Iteration is: 1015 and loss is: 0.0021894252859055996\n",
      "Iteration is: 1016 and loss is: 0.0021858548279851675\n",
      "Iteration is: 1017 and loss is: 0.002182289259508252\n",
      "Iteration is: 1018 and loss is: 0.0021787292789667845\n",
      "Iteration is: 1019 and loss is: 0.0021751851309090853\n",
      "Iteration is: 1020 and loss is: 0.002171645639464259\n",
      "Iteration is: 1021 and loss is: 0.002168109640479088\n",
      "Iteration is: 1022 and loss is: 0.00216458342038095\n",
      "Iteration is: 1023 and loss is: 0.0021610716357827187\n",
      "Iteration is: 1024 and loss is: 0.0021575612481683493\n",
      "Iteration is: 1025 and loss is: 0.002154062269255519\n",
      "Iteration is: 1026 and loss is: 0.0021505714394152164\n",
      "Iteration is: 1027 and loss is: 0.0021470878273248672\n",
      "Iteration is: 1028 and loss is: 0.002143612364307046\n",
      "Iteration is: 1029 and loss is: 0.0021401471458375454\n",
      "Iteration is: 1030 and loss is: 0.0021366921719163656\n",
      "Iteration is: 1031 and loss is: 0.002133236965164542\n",
      "Iteration is: 1032 and loss is: 0.0021297968924045563\n",
      "Iteration is: 1033 and loss is: 0.002126364503055811\n",
      "Iteration is: 1034 and loss is: 0.0021229402627795935\n",
      "Iteration is: 1035 and loss is: 0.0021195232402533293\n",
      "Iteration is: 1036 and loss is: 0.0021161145996302366\n",
      "Iteration is: 1037 and loss is: 0.0021127115469425917\n",
      "Iteration is: 1038 and loss is: 0.0021093147806823254\n",
      "Iteration is: 1039 and loss is: 0.0021059291902929544\n",
      "Iteration is: 1040 and loss is: 0.0021025475580245256\n",
      "Iteration is: 1041 and loss is: 0.002099178731441498\n",
      "Iteration is: 1042 and loss is: 0.002095816656947136\n",
      "Iteration is: 1043 and loss is: 0.002092462731525302\n",
      "Iteration is: 1044 and loss is: 0.002089111600071192\n",
      "Iteration is: 1045 and loss is: 0.002085775835439563\n",
      "Iteration is: 1046 and loss is: 0.0020824449602514505\n",
      "Iteration is: 1047 and loss is: 0.0020791187416762114\n",
      "Iteration is: 1048 and loss is: 0.0020758050959557295\n",
      "Iteration is: 1049 and loss is: 0.0020724949426949024\n",
      "Iteration is: 1050 and loss is: 0.0020691948011517525\n",
      "Iteration is: 1051 and loss is: 0.0020659032743424177\n",
      "Iteration is: 1052 and loss is: 0.002062617102637887\n",
      "Iteration is: 1053 and loss is: 0.002059341175481677\n",
      "Iteration is: 1054 and loss is: 0.0020560682751238346\n",
      "Iteration is: 1055 and loss is: 0.002052804920822382\n",
      "Iteration is: 1056 and loss is: 0.0020495490171015263\n",
      "Iteration is: 1057 and loss is: 0.0020463010296225548\n",
      "Iteration is: 1058 and loss is: 0.002043058630079031\n",
      "Iteration is: 1059 and loss is: 0.002039828570559621\n",
      "Iteration is: 1060 and loss is: 0.0020366027019917965\n",
      "Iteration is: 1061 and loss is: 0.0020333826541900635\n",
      "Iteration is: 1062 and loss is: 0.002030171686783433\n",
      "Iteration is: 1063 and loss is: 0.0020269728265702724\n",
      "Iteration is: 1064 and loss is: 0.0020237769931554794\n",
      "Iteration is: 1065 and loss is: 0.0020205883774906397\n",
      "Iteration is: 1066 and loss is: 0.0020174081437289715\n",
      "Iteration is: 1067 and loss is: 0.0020142351277172565\n",
      "Iteration is: 1068 and loss is: 0.0020110744517296553\n",
      "Iteration is: 1069 and loss is: 0.002007923787459731\n",
      "Iteration is: 1070 and loss is: 0.002004781039431691\n",
      "Iteration is: 1071 and loss is: 0.002001657150685787\n",
      "Iteration is: 1072 and loss is: 0.0019985500257462263\n",
      "Iteration is: 1073 and loss is: 0.0019954654853791\n",
      "Iteration is: 1074 and loss is: 0.0019924158696085215\n",
      "Iteration is: 1075 and loss is: 0.0019894216675311327\n",
      "Iteration is: 1076 and loss is: 0.0019865117501467466\n",
      "Iteration is: 1077 and loss is: 0.0019837471190840006\n",
      "Iteration is: 1078 and loss is: 0.0019812160171568394\n",
      "Iteration is: 1079 and loss is: 0.001979096094146371\n",
      "Iteration is: 1080 and loss is: 0.001977690728381276\n",
      "Iteration is: 1081 and loss is: 0.0019775494001805782\n",
      "Iteration is: 1082 and loss is: 0.001979687949642539\n",
      "Iteration is: 1083 and loss is: 0.001985932467505336\n",
      "Iteration is: 1084 and loss is: 0.001999728148803115\n",
      "Iteration is: 1085 and loss is: 0.002027227310463786\n",
      "Iteration is: 1086 and loss is: 0.0020801573991775513\n",
      "Iteration is: 1087 and loss is: 0.00217811088077724\n",
      "Iteration is: 1088 and loss is: 0.0023565711453557014\n",
      "Iteration is: 1089 and loss is: 0.0026612700894474983\n",
      "Iteration is: 1090 and loss is: 0.0031544151715934277\n",
      "Iteration is: 1091 and loss is: 0.0038246200419962406\n",
      "Iteration is: 1092 and loss is: 0.004572742618620396\n",
      "Iteration is: 1093 and loss is: 0.004985874984413385\n",
      "Iteration is: 1094 and loss is: 0.004725117236375809\n",
      "Iteration is: 1095 and loss is: 0.003669972065836191\n",
      "Iteration is: 1096 and loss is: 0.0024854429066181183\n",
      "Iteration is: 1097 and loss is: 0.0019294554367661476\n",
      "Iteration is: 1098 and loss is: 0.0022293743677437305\n",
      "Iteration is: 1099 and loss is: 0.0029136070515960455\n",
      "Iteration is: 1100 and loss is: 0.003277755342423916\n",
      "Iteration is: 1101 and loss is: 0.003003964200615883\n",
      "Iteration is: 1102 and loss is: 0.002353065414354205\n",
      "Iteration is: 1103 and loss is: 0.001931648119352758\n",
      "Iteration is: 1104 and loss is: 0.0020305528305470943\n",
      "Iteration is: 1105 and loss is: 0.0024112174287438393\n",
      "Iteration is: 1106 and loss is: 0.0026249627117067575\n",
      "Iteration is: 1107 and loss is: 0.002453011926263571\n",
      "Iteration is: 1108 and loss is: 0.0020962378475815058\n",
      "Iteration is: 1109 and loss is: 0.0018963818438351154\n",
      "Iteration is: 1110 and loss is: 0.001987476833164692\n",
      "Iteration is: 1111 and loss is: 0.0021998449228703976\n",
      "Iteration is: 1112 and loss is: 0.0022808087524026632\n",
      "Iteration is: 1113 and loss is: 0.0021560753230005503\n",
      "Iteration is: 1114 and loss is: 0.001960586756467819\n",
      "Iteration is: 1115 and loss is: 0.0018769977614283562\n",
      "Iteration is: 1116 and loss is: 0.0019462167983874679\n",
      "Iteration is: 1117 and loss is: 0.0020580552518367767\n",
      "Iteration is: 1118 and loss is: 0.0020860934164375067\n",
      "Iteration is: 1119 and loss is: 0.0020059708040207624\n",
      "Iteration is: 1120 and loss is: 0.001901008770801127\n",
      "Iteration is: 1121 and loss is: 0.0018610538681969047\n",
      "Iteration is: 1122 and loss is: 0.001900105969980359\n",
      "Iteration is: 1123 and loss is: 0.0019588114228099585\n",
      "Iteration is: 1124 and loss is: 0.001972428523004055\n",
      "Iteration is: 1125 and loss is: 0.0019300744170323014\n",
      "Iteration is: 1126 and loss is: 0.0018718173960223794\n",
      "Iteration is: 1127 and loss is: 0.0018445816822350025\n",
      "Iteration is: 1128 and loss is: 0.0018595104338601232\n",
      "Iteration is: 1129 and loss is: 0.0018902102019637823\n",
      "Iteration is: 1130 and loss is: 0.0019028977258130908\n",
      "Iteration is: 1131 and loss is: 0.0018854406662285328\n",
      "Iteration is: 1132 and loss is: 0.0018532348331063986\n",
      "Iteration is: 1133 and loss is: 0.0018304504919797182\n",
      "Iteration is: 1134 and loss is: 0.0018292187014594674\n",
      "Iteration is: 1135 and loss is: 0.0018425165908411145\n",
      "Iteration is: 1136 and loss is: 0.0018537562573328614\n",
      "Iteration is: 1137 and loss is: 0.0018514852272346616\n",
      "Iteration is: 1138 and loss is: 0.0018366770818829536\n",
      "Iteration is: 1139 and loss is: 0.0018195341108366847\n",
      "Iteration is: 1140 and loss is: 0.001809893874451518\n",
      "Iteration is: 1141 and loss is: 0.0018103583715856075\n",
      "Iteration is: 1142 and loss is: 0.0018159991595894098\n",
      "Iteration is: 1143 and loss is: 0.0018194486619904637\n",
      "Iteration is: 1144 and loss is: 0.0018164372304454446\n",
      "Iteration is: 1145 and loss is: 0.0018078653374686837\n",
      "Iteration is: 1146 and loss is: 0.00179816666059196\n",
      "Iteration is: 1147 and loss is: 0.0017915816279128194\n",
      "Iteration is: 1148 and loss is: 0.0017895597266033292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 1149 and loss is: 0.0017905031563714147\n",
      "Iteration is: 1150 and loss is: 0.001791414339095354\n",
      "Iteration is: 1151 and loss is: 0.0017899756785482168\n",
      "Iteration is: 1152 and loss is: 0.0017857140628620982\n",
      "Iteration is: 1153 and loss is: 0.0017798958579078317\n",
      "Iteration is: 1154 and loss is: 0.0017743908101692796\n",
      "Iteration is: 1155 and loss is: 0.001770503236912191\n",
      "Iteration is: 1156 and loss is: 0.0017684327904134989\n",
      "Iteration is: 1157 and loss is: 0.0017674226546660066\n",
      "Iteration is: 1158 and loss is: 0.0017663714243099093\n",
      "Iteration is: 1159 and loss is: 0.0017644520848989487\n",
      "Iteration is: 1160 and loss is: 0.0017614526441320777\n",
      "Iteration is: 1161 and loss is: 0.0017577136168256402\n",
      "Iteration is: 1162 and loss is: 0.0017538157990202308\n",
      "Iteration is: 1163 and loss is: 0.0017503001727163792\n",
      "Iteration is: 1164 and loss is: 0.0017474230844527483\n",
      "Iteration is: 1165 and loss is: 0.0017451416933909059\n",
      "Iteration is: 1166 and loss is: 0.0017431927844882011\n",
      "Iteration is: 1167 and loss is: 0.001741263666190207\n",
      "Iteration is: 1168 and loss is: 0.0017391119617968798\n",
      "Iteration is: 1169 and loss is: 0.0017366254469379783\n",
      "Iteration is: 1170 and loss is: 0.0017338418401777744\n",
      "Iteration is: 1171 and loss is: 0.001730888499878347\n",
      "Iteration is: 1172 and loss is: 0.001727909897454083\n",
      "Iteration is: 1173 and loss is: 0.0017250376986339688\n",
      "Iteration is: 1174 and loss is: 0.0017223391914740205\n",
      "Iteration is: 1175 and loss is: 0.001719816355034709\n",
      "Iteration is: 1176 and loss is: 0.0017174356617033482\n",
      "Iteration is: 1177 and loss is: 0.0017151387874037027\n",
      "Iteration is: 1178 and loss is: 0.0017128594918176532\n",
      "Iteration is: 1179 and loss is: 0.0017105616861954331\n",
      "Iteration is: 1180 and loss is: 0.001708203344605863\n",
      "Iteration is: 1181 and loss is: 0.0017057813238352537\n",
      "Iteration is: 1182 and loss is: 0.0017032981850206852\n",
      "Iteration is: 1183 and loss is: 0.001700780470855534\n",
      "Iteration is: 1184 and loss is: 0.0016982379602268338\n",
      "Iteration is: 1185 and loss is: 0.0016957017360255122\n",
      "Iteration is: 1186 and loss is: 0.0016931683057919145\n",
      "Iteration is: 1187 and loss is: 0.0016906579257920384\n",
      "Iteration is: 1188 and loss is: 0.0016881643095985055\n",
      "Iteration is: 1189 and loss is: 0.001685688621364534\n",
      "Iteration is: 1190 and loss is: 0.0016832315595820546\n",
      "Iteration is: 1191 and loss is: 0.0016807965002954006\n",
      "Iteration is: 1192 and loss is: 0.0016783701721578836\n",
      "Iteration is: 1193 and loss is: 0.0016759639838710427\n",
      "Iteration is: 1194 and loss is: 0.0016735662939026952\n",
      "Iteration is: 1195 and loss is: 0.0016711740754544735\n",
      "Iteration is: 1196 and loss is: 0.0016687919851392508\n",
      "Iteration is: 1197 and loss is: 0.0016664211871102452\n",
      "Iteration is: 1198 and loss is: 0.0016640539979562163\n",
      "Iteration is: 1199 and loss is: 0.0016616961220279336\n",
      "Iteration is: 1200 and loss is: 0.0016593376640230417\n",
      "Iteration is: 1201 and loss is: 0.0016569935251027346\n",
      "Iteration is: 1202 and loss is: 0.0016546561382710934\n",
      "Iteration is: 1203 and loss is: 0.0016523270169273019\n",
      "Iteration is: 1204 and loss is: 0.0016500134952366352\n",
      "Iteration is: 1205 and loss is: 0.0016477181343361735\n",
      "Iteration is: 1206 and loss is: 0.001645437441766262\n",
      "Iteration is: 1207 and loss is: 0.0016431969124823809\n",
      "Iteration is: 1208 and loss is: 0.001641009352169931\n",
      "Iteration is: 1209 and loss is: 0.0016388980438932776\n",
      "Iteration is: 1210 and loss is: 0.0016369279474020004\n",
      "Iteration is: 1211 and loss is: 0.0016351636731997132\n",
      "Iteration is: 1212 and loss is: 0.0016337399138137698\n",
      "Iteration is: 1213 and loss is: 0.0016328844940289855\n",
      "Iteration is: 1214 and loss is: 0.001632992411032319\n",
      "Iteration is: 1215 and loss is: 0.0016347561031579971\n",
      "Iteration is: 1216 and loss is: 0.0016393933910876513\n",
      "Iteration is: 1217 and loss is: 0.0016491246642544866\n",
      "Iteration is: 1218 and loss is: 0.001667844713665545\n",
      "Iteration is: 1219 and loss is: 0.0017027687281370163\n",
      "Iteration is: 1220 and loss is: 0.0017664696788415313\n",
      "Iteration is: 1221 and loss is: 0.0018827419262379408\n",
      "Iteration is: 1222 and loss is: 0.002091245725750923\n",
      "Iteration is: 1223 and loss is: 0.00246563833206892\n",
      "Iteration is: 1224 and loss is: 0.0031072874553501606\n",
      "Iteration is: 1225 and loss is: 0.0041742585599422455\n",
      "Iteration is: 1226 and loss is: 0.0057052383199334145\n",
      "Iteration is: 1227 and loss is: 0.00756043940782547\n",
      "Iteration is: 1228 and loss is: 0.008726130239665508\n",
      "Iteration is: 1229 and loss is: 0.008154870942234993\n",
      "Iteration is: 1230 and loss is: 0.0053798239678144455\n",
      "Iteration is: 1231 and loss is: 0.0024620986077934504\n",
      "Iteration is: 1232 and loss is: 0.0016424866626039147\n",
      "Iteration is: 1233 and loss is: 0.0030531336087733507\n",
      "Iteration is: 1234 and loss is: 0.004700361285358667\n",
      "Iteration is: 1235 and loss is: 0.004591182805597782\n",
      "Iteration is: 1236 and loss is: 0.0029382987413555384\n",
      "Iteration is: 1237 and loss is: 0.0016512839356437325\n",
      "Iteration is: 1238 and loss is: 0.0020000014919787645\n",
      "Iteration is: 1239 and loss is: 0.003117891261354089\n",
      "Iteration is: 1240 and loss is: 0.0033740485087037086\n",
      "Iteration is: 1241 and loss is: 0.0024922366719692945\n",
      "Iteration is: 1242 and loss is: 0.0016441077459603548\n",
      "Iteration is: 1243 and loss is: 0.0017903372645378113\n",
      "Iteration is: 1244 and loss is: 0.0024660746566951275\n",
      "Iteration is: 1245 and loss is: 0.0026254397816956043\n",
      "Iteration is: 1246 and loss is: 0.002088098553940654\n",
      "Iteration is: 1247 and loss is: 0.0015970910899341106\n",
      "Iteration is: 1248 and loss is: 0.0017154983943328261\n",
      "Iteration is: 1249 and loss is: 0.002119804499670863\n",
      "Iteration is: 1250 and loss is: 0.0021830315236002207\n",
      "Iteration is: 1251 and loss is: 0.0018435537349432707\n",
      "Iteration is: 1252 and loss is: 0.0015682055382058024\n",
      "Iteration is: 1253 and loss is: 0.0016612035688012838\n",
      "Iteration is: 1254 and loss is: 0.0019002011977136135\n",
      "Iteration is: 1255 and loss is: 0.0019211722537875175\n",
      "Iteration is: 1256 and loss is: 0.0017114011570811272\n",
      "Iteration is: 1257 and loss is: 0.0015527778305113316\n",
      "Iteration is: 1258 and loss is: 0.0016125190304592252\n",
      "Iteration is: 1259 and loss is: 0.0017536982195451856\n",
      "Iteration is: 1260 and loss is: 0.0017656413838267326\n",
      "Iteration is: 1261 and loss is: 0.0016411154065281153\n",
      "Iteration is: 1262 and loss is: 0.0015426191966980696\n",
      "Iteration is: 1263 and loss is: 0.0015713981119915843\n",
      "Iteration is: 1264 and loss is: 0.0016552056185901165\n",
      "Iteration is: 1265 and loss is: 0.0016702709253877401\n",
      "Iteration is: 1266 and loss is: 0.001600791234523058\n",
      "Iteration is: 1267 and loss is: 0.0015348640736192465\n",
      "Iteration is: 1268 and loss is: 0.0015402872813865542\n",
      "Iteration is: 1269 and loss is: 0.0015885080210864544\n",
      "Iteration is: 1270 and loss is: 0.0016073223669081926\n",
      "Iteration is: 1271 and loss is: 0.0015734299086034298\n",
      "Iteration is: 1272 and loss is: 0.0015285845147445798\n",
      "Iteration is: 1273 and loss is: 0.0015194750158116221\n",
      "Iteration is: 1274 and loss is: 0.0015436764806509018\n",
      "Iteration is: 1275 and loss is: 0.0015623996732756495\n",
      "Iteration is: 1276 and loss is: 0.0015509232180193067\n",
      "Iteration is: 1277 and loss is: 0.0015226544346660376\n",
      "Iteration is: 1278 and loss is: 0.0015069806249812245\n",
      "Iteration is: 1279 and loss is: 0.0015140967443585396\n",
      "Iteration is: 1280 and loss is: 0.0015280707739293575\n",
      "Iteration is: 1281 and loss is: 0.0015291094314306974\n",
      "Iteration is: 1282 and loss is: 0.0015149798709899187\n",
      "Iteration is: 1283 and loss is: 0.001499784179031849\n",
      "Iteration is: 1284 and loss is: 0.0014960019616410136\n",
      "Iteration is: 1285 and loss is: 0.0015021878061816096\n",
      "Iteration is: 1286 and loss is: 0.0015073223039507866\n",
      "Iteration is: 1287 and loss is: 0.0015037136618047953\n",
      "Iteration is: 1288 and loss is: 0.0014938721433281898\n",
      "Iteration is: 1289 and loss is: 0.0014858981594443321\n",
      "Iteration is: 1290 and loss is: 0.0014845605473965406\n",
      "Iteration is: 1291 and loss is: 0.001487450092099607\n",
      "Iteration is: 1292 and loss is: 0.0014887440484017134\n",
      "Iteration is: 1293 and loss is: 0.0014853207394480705\n",
      "Iteration is: 1294 and loss is: 0.0014790212735533714\n",
      "Iteration is: 1295 and loss is: 0.0014739681500941515\n",
      "Iteration is: 1296 and loss is: 0.0014723659260198474\n",
      "Iteration is: 1297 and loss is: 0.0014729817630723119\n",
      "Iteration is: 1298 and loss is: 0.0014729303075000644\n",
      "Iteration is: 1299 and loss is: 0.0014705087523907423\n",
      "Iteration is: 1300 and loss is: 0.001466390211135149\n",
      "Iteration is: 1301 and loss is: 0.0014625468757003546\n",
      "Iteration is: 1302 and loss is: 0.0014603415038436651\n",
      "Iteration is: 1303 and loss is: 0.0014595452230423689\n",
      "Iteration is: 1304 and loss is: 0.0014588846825063229\n",
      "Iteration is: 1305 and loss is: 0.0014572381041944027\n",
      "Iteration is: 1306 and loss is: 0.0014544980367645621\n",
      "Iteration is: 1307 and loss is: 0.0014514239737764\n",
      "Iteration is: 1308 and loss is: 0.0014488761080428958\n",
      "Iteration is: 1309 and loss is: 0.0014471603790298104\n",
      "Iteration is: 1310 and loss is: 0.001445922302082181\n",
      "Iteration is: 1311 and loss is: 0.0014445582637563348\n",
      "Iteration is: 1312 and loss is: 0.0014426801353693008\n",
      "Iteration is: 1313 and loss is: 0.0014403513632714748\n",
      "Iteration is: 1314 and loss is: 0.0014379166532307863\n",
      "Iteration is: 1315 and loss is: 0.001435731304809451\n",
      "Iteration is: 1316 and loss is: 0.0014339281478896737\n",
      "Iteration is: 1317 and loss is: 0.0014323637587949634\n",
      "Iteration is: 1318 and loss is: 0.0014307767851278186\n",
      "Iteration is: 1319 and loss is: 0.0014289924874901772\n",
      "Iteration is: 1320 and loss is: 0.0014269736129790545\n",
      "Iteration is: 1321 and loss is: 0.0014248598599806428\n",
      "Iteration is: 1322 and loss is: 0.001422806759364903\n",
      "Iteration is: 1323 and loss is: 0.0014209095388650894\n",
      "Iteration is: 1324 and loss is: 0.001419159583747387\n",
      "Iteration is: 1325 and loss is: 0.0014174696989357471\n",
      "Iteration is: 1326 and loss is: 0.0014157429104670882\n",
      "Iteration is: 1327 and loss is: 0.001413926132954657\n",
      "Iteration is: 1328 and loss is: 0.001412029261700809\n",
      "Iteration is: 1329 and loss is: 0.0014100914122536778\n",
      "Iteration is: 1330 and loss is: 0.0014081766130402684\n",
      "Iteration is: 1331 and loss is: 0.0014063218841329217\n",
      "Iteration is: 1332 and loss is: 0.0014045274583622813\n",
      "Iteration is: 1333 and loss is: 0.0014027755241841078\n",
      "Iteration is: 1334 and loss is: 0.0014010305749252439\n",
      "Iteration is: 1335 and loss is: 0.0013992649037390947\n",
      "Iteration is: 1336 and loss is: 0.0013974686153233051\n",
      "Iteration is: 1337 and loss is: 0.0013956441543996334\n",
      "Iteration is: 1338 and loss is: 0.001393806654959917\n",
      "Iteration is: 1339 and loss is: 0.0013919767225161195\n",
      "Iteration is: 1340 and loss is: 0.0013901658821851015\n",
      "Iteration is: 1341 and loss is: 0.001388378208503127\n",
      "Iteration is: 1342 and loss is: 0.0013866156805306673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 1343 and loss is: 0.0013848571106791496\n",
      "Iteration is: 1344 and loss is: 0.0013831069227308035\n",
      "Iteration is: 1345 and loss is: 0.0013813514960929751\n",
      "Iteration is: 1346 and loss is: 0.0013795942068099976\n",
      "Iteration is: 1347 and loss is: 0.0013778266729786992\n",
      "Iteration is: 1348 and loss is: 0.001376057625748217\n",
      "Iteration is: 1349 and loss is: 0.001374292653053999\n",
      "Iteration is: 1350 and loss is: 0.0013725294265896082\n",
      "Iteration is: 1351 and loss is: 0.0013707715552300215\n",
      "Iteration is: 1352 and loss is: 0.0013690198538824916\n",
      "Iteration is: 1353 and loss is: 0.0013672756031155586\n",
      "Iteration is: 1354 and loss is: 0.0013655386865139008\n",
      "Iteration is: 1355 and loss is: 0.0013638049131259322\n",
      "Iteration is: 1356 and loss is: 0.0013620788231492043\n",
      "Iteration is: 1357 and loss is: 0.0013603559928014874\n",
      "Iteration is: 1358 and loss is: 0.0013586393324658275\n",
      "Iteration is: 1359 and loss is: 0.001356922322884202\n",
      "Iteration is: 1360 and loss is: 0.0013552086893469095\n",
      "Iteration is: 1361 and loss is: 0.0013534959871321917\n",
      "Iteration is: 1362 and loss is: 0.0013517842162400484\n",
      "Iteration is: 1363 and loss is: 0.001350076519884169\n",
      "Iteration is: 1364 and loss is: 0.0013483701040968299\n",
      "Iteration is: 1365 and loss is: 0.0013466674135997891\n",
      "Iteration is: 1366 and loss is: 0.0013449684483930469\n",
      "Iteration is: 1367 and loss is: 0.0013432702980935574\n",
      "Iteration is: 1368 and loss is: 0.001341582858003676\n",
      "Iteration is: 1369 and loss is: 0.0013398934388533235\n",
      "Iteration is: 1370 and loss is: 0.0013382137985900044\n",
      "Iteration is: 1371 and loss is: 0.0013365366030484438\n",
      "Iteration is: 1372 and loss is: 0.0013348666252568364\n",
      "Iteration is: 1373 and loss is: 0.0013332079397514462\n",
      "Iteration is: 1374 and loss is: 0.001331556006334722\n",
      "Iteration is: 1375 and loss is: 0.00132992013823241\n",
      "Iteration is: 1376 and loss is: 0.0013283084845170379\n",
      "Iteration is: 1377 and loss is: 0.0013267226750031114\n",
      "Iteration is: 1378 and loss is: 0.0013251684140414\n",
      "Iteration is: 1379 and loss is: 0.00132366840261966\n",
      "Iteration is: 1380 and loss is: 0.0013222479028627276\n",
      "Iteration is: 1381 and loss is: 0.0013209570897743106\n",
      "Iteration is: 1382 and loss is: 0.001319863018579781\n",
      "Iteration is: 1383 and loss is: 0.0013190842000767589\n",
      "Iteration is: 1384 and loss is: 0.0013188208686187863\n",
      "Iteration is: 1385 and loss is: 0.0013193704653531313\n",
      "Iteration is: 1386 and loss is: 0.0013212605845183134\n",
      "Iteration is: 1387 and loss is: 0.0013253687648102641\n",
      "Iteration is: 1388 and loss is: 0.0013331797672435641\n",
      "Iteration is: 1389 and loss is: 0.0013473116559907794\n",
      "Iteration is: 1390 and loss is: 0.001372179831378162\n",
      "Iteration is: 1391 and loss is: 0.0014157115947455168\n",
      "Iteration is: 1392 and loss is: 0.0014911837643012404\n",
      "Iteration is: 1393 and loss is: 0.0016231186455115676\n",
      "Iteration is: 1394 and loss is: 0.0018508895300328732\n",
      "Iteration is: 1395 and loss is: 0.002246841788291931\n",
      "Iteration is: 1396 and loss is: 0.0029109795577824116\n",
      "Iteration is: 1397 and loss is: 0.004009820055216551\n",
      "Iteration is: 1398 and loss is: 0.005642968229949474\n",
      "Iteration is: 1399 and loss is: 0.007843413390219212\n",
      "Iteration is: 1400 and loss is: 0.009877271018922329\n",
      "Iteration is: 1401 and loss is: 0.010670176707208157\n",
      "Iteration is: 1402 and loss is: 0.008678024634718895\n",
      "Iteration is: 1403 and loss is: 0.0048401253297924995\n",
      "Iteration is: 1404 and loss is: 0.001753650838509202\n",
      "Iteration is: 1405 and loss is: 0.0016121502267196774\n",
      "Iteration is: 1406 and loss is: 0.0036963876336812973\n",
      "Iteration is: 1407 and loss is: 0.005377120338380337\n",
      "Iteration is: 1408 and loss is: 0.004846241325139999\n",
      "Iteration is: 1409 and loss is: 0.002689223736524582\n",
      "Iteration is: 1410 and loss is: 0.001318658236414194\n",
      "Iteration is: 1411 and loss is: 0.0018910878570750356\n",
      "Iteration is: 1412 and loss is: 0.0032207625918090343\n",
      "Iteration is: 1413 and loss is: 0.003510845825076103\n",
      "Iteration is: 1414 and loss is: 0.0024357617367058992\n",
      "Iteration is: 1415 and loss is: 0.0013901417842134833\n",
      "Iteration is: 1416 and loss is: 0.0014860929222777486\n",
      "Iteration is: 1417 and loss is: 0.0022843596525490284\n",
      "Iteration is: 1418 and loss is: 0.00260261632502079\n",
      "Iteration is: 1419 and loss is: 0.002044504741206765\n",
      "Iteration is: 1420 and loss is: 0.0013775925617665052\n",
      "Iteration is: 1421 and loss is: 0.0013598200166597962\n",
      "Iteration is: 1422 and loss is: 0.001825230661779642\n",
      "Iteration is: 1423 and loss is: 0.00205984222702682\n",
      "Iteration is: 1424 and loss is: 0.0017632716335356236\n",
      "Iteration is: 1425 and loss is: 0.0013511546421796083\n",
      "Iteration is: 1426 and loss is: 0.0013013541465625167\n",
      "Iteration is: 1427 and loss is: 0.0015669016866013408\n",
      "Iteration is: 1428 and loss is: 0.0017338598845526576\n",
      "Iteration is: 1429 and loss is: 0.0015863570151850581\n",
      "Iteration is: 1430 and loss is: 0.001332954503595829\n",
      "Iteration is: 1431 and loss is: 0.0012689976720139384\n",
      "Iteration is: 1432 and loss is: 0.0014110610354691744\n",
      "Iteration is: 1433 and loss is: 0.0015318641671910882\n",
      "Iteration is: 1434 and loss is: 0.0014737832825630903\n",
      "Iteration is: 1435 and loss is: 0.001321699470281601\n",
      "Iteration is: 1436 and loss is: 0.0012532819528132677\n",
      "Iteration is: 1437 and loss is: 0.0013171775499358773\n",
      "Iteration is: 1438 and loss is: 0.001402424299158156\n",
      "Iteration is: 1439 and loss is: 0.0013961470685899258\n",
      "Iteration is: 1440 and loss is: 0.0013123571407049894\n",
      "Iteration is: 1441 and loss is: 0.0012493390822783113\n",
      "Iteration is: 1442 and loss is: 0.0012642168439924717\n",
      "Iteration is: 1443 and loss is: 0.0013176175998523831\n",
      "Iteration is: 1444 and loss is: 0.0013362662866711617\n",
      "Iteration is: 1445 and loss is: 0.0012998352758586407\n",
      "Iteration is: 1446 and loss is: 0.0012514624977484345\n",
      "Iteration is: 1447 and loss is: 0.001239241799339652\n",
      "Iteration is: 1448 and loss is: 0.0012638767948374152\n",
      "Iteration is: 1449 and loss is: 0.0012876569526270032\n",
      "Iteration is: 1450 and loss is: 0.0012816025409847498\n",
      "Iteration is: 1451 and loss is: 0.0012532692635431886\n",
      "Iteration is: 1452 and loss is: 0.0012318051885813475\n",
      "Iteration is: 1453 and loss is: 0.0012337942607700825\n",
      "Iteration is: 1454 and loss is: 0.0012493145186454058\n",
      "Iteration is: 1455 and loss is: 0.0012572782579809427\n",
      "Iteration is: 1456 and loss is: 0.0012483212631195784\n",
      "Iteration is: 1457 and loss is: 0.0012315487256273627\n",
      "Iteration is: 1458 and loss is: 0.0012218279298394918\n",
      "Iteration is: 1459 and loss is: 0.0012245047837495804\n",
      "Iteration is: 1460 and loss is: 0.001232326845638454\n",
      "Iteration is: 1461 and loss is: 0.0012348289601504803\n",
      "Iteration is: 1462 and loss is: 0.0012286307755857706\n",
      "Iteration is: 1463 and loss is: 0.0012189635308459401\n",
      "Iteration is: 1464 and loss is: 0.0012132094707340002\n",
      "Iteration is: 1465 and loss is: 0.0012138610472902656\n",
      "Iteration is: 1466 and loss is: 0.0012173369759693742\n",
      "Iteration is: 1467 and loss is: 0.0012183993821963668\n",
      "Iteration is: 1468 and loss is: 0.0012149638496339321\n",
      "Iteration is: 1469 and loss is: 0.0012092331890016794\n",
      "Iteration is: 1470 and loss is: 0.0012048946227878332\n",
      "Iteration is: 1471 and loss is: 0.0012037834385409951\n",
      "Iteration is: 1472 and loss is: 0.0012047792552039027\n",
      "Iteration is: 1473 and loss is: 0.0012053325772285461\n",
      "Iteration is: 1474 and loss is: 0.0012037798296660185\n",
      "Iteration is: 1475 and loss is: 0.0012004394084215164\n",
      "Iteration is: 1476 and loss is: 0.0011969666229560971\n",
      "Iteration is: 1477 and loss is: 0.0011947930324822664\n",
      "Iteration is: 1478 and loss is: 0.0011940933763980865\n",
      "Iteration is: 1479 and loss is: 0.0011939611285924911\n",
      "Iteration is: 1480 and loss is: 0.0011932767229154706\n",
      "Iteration is: 1481 and loss is: 0.0011915509821847081\n",
      "Iteration is: 1482 and loss is: 0.0011891352478414774\n",
      "Iteration is: 1483 and loss is: 0.0011867847060784698\n",
      "Iteration is: 1484 and loss is: 0.00118505978025496\n",
      "Iteration is: 1485 and loss is: 0.0011840008664876223\n",
      "Iteration is: 1486 and loss is: 0.0011832158779725432\n",
      "Iteration is: 1487 and loss is: 0.0011822248343378305\n",
      "Iteration is: 1488 and loss is: 0.0011807898990809917\n",
      "Iteration is: 1489 and loss is: 0.0011789918644353747\n",
      "Iteration is: 1490 and loss is: 0.0011771293357014656\n",
      "Iteration is: 1491 and loss is: 0.0011754713486880064\n",
      "Iteration is: 1492 and loss is: 0.0011741196503862739\n",
      "Iteration is: 1493 and loss is: 0.0011729839025065303\n",
      "Iteration is: 1494 and loss is: 0.001171870855614543\n",
      "Iteration is: 1495 and loss is: 0.0011706255609169602\n",
      "Iteration is: 1496 and loss is: 0.001169202383607626\n",
      "Iteration is: 1497 and loss is: 0.0011676503345370293\n",
      "Iteration is: 1498 and loss is: 0.0011660909513011575\n",
      "Iteration is: 1499 and loss is: 0.0011646091006696224\n",
      "Iteration is: 1500 and loss is: 0.0011632411042228341\n",
      "Iteration is: 1501 and loss is: 0.0011619721772149205\n",
      "Iteration is: 1502 and loss is: 0.0011607372434809804\n",
      "Iteration is: 1503 and loss is: 0.0011594805400818586\n",
      "Iteration is: 1504 and loss is: 0.0011581628350540996\n",
      "Iteration is: 1505 and loss is: 0.0011567802866920829\n",
      "Iteration is: 1506 and loss is: 0.0011553611839190125\n",
      "Iteration is: 1507 and loss is: 0.0011539426632225513\n",
      "Iteration is: 1508 and loss is: 0.0011525520822033286\n",
      "Iteration is: 1509 and loss is: 0.001151206553913653\n",
      "Iteration is: 1510 and loss is: 0.001149899559095502\n",
      "Iteration is: 1511 and loss is: 0.001148613984696567\n",
      "Iteration is: 1512 and loss is: 0.0011473316699266434\n",
      "Iteration is: 1513 and loss is: 0.0011460366658866405\n",
      "Iteration is: 1514 and loss is: 0.0011447217548266053\n",
      "Iteration is: 1515 and loss is: 0.0011433911276981235\n",
      "Iteration is: 1516 and loss is: 0.00114205211866647\n",
      "Iteration is: 1517 and loss is: 0.0011407103156670928\n",
      "Iteration is: 1518 and loss is: 0.0011393761960789561\n",
      "Iteration is: 1519 and loss is: 0.0011380509240552783\n",
      "Iteration is: 1520 and loss is: 0.0011367431143298745\n",
      "Iteration is: 1521 and loss is: 0.0011354427551850677\n",
      "Iteration is: 1522 and loss is: 0.0011341506615281105\n",
      "Iteration is: 1523 and loss is: 0.0011328627588227391\n",
      "Iteration is: 1524 and loss is: 0.001131574739702046\n",
      "Iteration is: 1525 and loss is: 0.0011302884668111801\n",
      "Iteration is: 1526 and loss is: 0.0011290006805211306\n",
      "Iteration is: 1527 and loss is: 0.001127711497247219\n",
      "Iteration is: 1528 and loss is: 0.001126422081142664\n",
      "Iteration is: 1529 and loss is: 0.0011251328978687525\n",
      "Iteration is: 1530 and loss is: 0.0011238433653488755\n",
      "Iteration is: 1531 and loss is: 0.0011225567432120442\n",
      "Iteration is: 1532 and loss is: 0.0011212744284421206\n",
      "Iteration is: 1533 and loss is: 0.0011199896689504385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 1534 and loss is: 0.0011187110794708133\n",
      "Iteration is: 1535 and loss is: 0.0011174340033903718\n",
      "Iteration is: 1536 and loss is: 0.0011161596048623323\n",
      "Iteration is: 1537 and loss is: 0.0011148902121931314\n",
      "Iteration is: 1538 and loss is: 0.0011136200046166778\n",
      "Iteration is: 1539 and loss is: 0.0011123535223305225\n",
      "Iteration is: 1540 and loss is: 0.0011110901832580566\n",
      "Iteration is: 1541 and loss is: 0.001109828008338809\n",
      "Iteration is: 1542 and loss is: 0.0011085697915405035\n",
      "Iteration is: 1543 and loss is: 0.0011073099449276924\n",
      "Iteration is: 1544 and loss is: 0.0011060547549277544\n",
      "Iteration is: 1545 and loss is: 0.0011048036394640803\n",
      "Iteration is: 1546 and loss is: 0.00110355275683105\n",
      "Iteration is: 1547 and loss is: 0.0011023051338270307\n",
      "Iteration is: 1548 and loss is: 0.001101059140637517\n",
      "Iteration is: 1549 and loss is: 0.0010998162906616926\n",
      "Iteration is: 1550 and loss is: 0.0010985761182382703\n",
      "Iteration is: 1551 and loss is: 0.00109733990393579\n",
      "Iteration is: 1552 and loss is: 0.0010961059015244246\n",
      "Iteration is: 1553 and loss is: 0.0010948767885565758\n",
      "Iteration is: 1554 and loss is: 0.0010936562903225422\n",
      "Iteration is: 1555 and loss is: 0.0010924440575763583\n",
      "Iteration is: 1556 and loss is: 0.0010912427678704262\n",
      "Iteration is: 1557 and loss is: 0.0010900604538619518\n",
      "Iteration is: 1558 and loss is: 0.0010889063123613596\n",
      "Iteration is: 1559 and loss is: 0.0010877982713282108\n",
      "Iteration is: 1560 and loss is: 0.0010867653181776404\n",
      "Iteration is: 1561 and loss is: 0.0010858543682843447\n",
      "Iteration is: 1562 and loss is: 0.0010851393453776836\n",
      "Iteration is: 1563 and loss is: 0.001084768446162343\n",
      "Iteration is: 1564 and loss is: 0.0010849747341126204\n",
      "Iteration is: 1565 and loss is: 0.001086186384782195\n",
      "Iteration is: 1566 and loss is: 0.0010891305282711983\n",
      "Iteration is: 1567 and loss is: 0.00109516317024827\n",
      "Iteration is: 1568 and loss is: 0.0011067045852541924\n",
      "Iteration is: 1569 and loss is: 0.0011282070772722363\n",
      "Iteration is: 1570 and loss is: 0.0011676696594804525\n",
      "Iteration is: 1571 and loss is: 0.0012402853462845087\n",
      "Iteration is: 1572 and loss is: 0.0013731198851019144\n",
      "Iteration is: 1573 and loss is: 0.0016183035913854837\n",
      "Iteration is: 1574 and loss is: 0.0020647025667130947\n",
      "Iteration is: 1575 and loss is: 0.002881196793168783\n",
      "Iteration is: 1576 and loss is: 0.004306436516344547\n",
      "Iteration is: 1577 and loss is: 0.006724834907799959\n",
      "Iteration is: 1578 and loss is: 0.010237012058496475\n",
      "Iteration is: 1579 and loss is: 0.01451412495225668\n",
      "Iteration is: 1580 and loss is: 0.016964664682745934\n",
      "Iteration is: 1581 and loss is: 0.01509666908532381\n",
      "Iteration is: 1582 and loss is: 0.008234378881752491\n",
      "Iteration is: 1583 and loss is: 0.0021001098211854696\n",
      "Iteration is: 1584 and loss is: 0.0016628766898065805\n",
      "Iteration is: 1585 and loss is: 0.005700285546481609\n",
      "Iteration is: 1586 and loss is: 0.008552911691367626\n",
      "Iteration is: 1587 and loss is: 0.006455213762819767\n",
      "Iteration is: 1588 and loss is: 0.0023251210805028677\n",
      "Iteration is: 1589 and loss is: 0.0012175183510407805\n",
      "Iteration is: 1590 and loss is: 0.0036020921543240547\n",
      "Iteration is: 1591 and loss is: 0.005347998812794685\n",
      "Iteration is: 1592 and loss is: 0.003825972555205226\n",
      "Iteration is: 1593 and loss is: 0.0014504370046779513\n",
      "Iteration is: 1594 and loss is: 0.0014074589125812054\n",
      "Iteration is: 1595 and loss is: 0.0030707032419741154\n",
      "Iteration is: 1596 and loss is: 0.0035414788872003555\n",
      "Iteration is: 1597 and loss is: 0.0021369298920035362\n",
      "Iteration is: 1598 and loss is: 0.0010783154284581542\n",
      "Iteration is: 1599 and loss is: 0.0016749539645388722\n",
      "Iteration is: 1600 and loss is: 0.0025891237892210484\n",
      "Iteration is: 1601 and loss is: 0.0022928735706955194\n",
      "Iteration is: 1602 and loss is: 0.001318600494414568\n",
      "Iteration is: 1603 and loss is: 0.0011308301473036408\n",
      "Iteration is: 1604 and loss is: 0.0017594574019312859\n",
      "Iteration is: 1605 and loss is: 0.002030903473496437\n",
      "Iteration is: 1606 and loss is: 0.0015360487159341574\n",
      "Iteration is: 1607 and loss is: 0.0010735808173194528\n",
      "Iteration is: 1608 and loss is: 0.0012563451891764998\n",
      "Iteration is: 1609 and loss is: 0.0016367207281291485\n",
      "Iteration is: 1610 and loss is: 0.0015655478928238153\n",
      "Iteration is: 1611 and loss is: 0.001186808804050088\n",
      "Iteration is: 1612 and loss is: 0.001066623255610466\n",
      "Iteration is: 1613 and loss is: 0.0012928347568958998\n",
      "Iteration is: 1614 and loss is: 0.0014390095602720976\n",
      "Iteration is: 1615 and loss is: 0.0012773789931088686\n",
      "Iteration is: 1616 and loss is: 0.0010708932531997561\n",
      "Iteration is: 1617 and loss is: 0.001097151660360396\n",
      "Iteration is: 1618 and loss is: 0.0012517758877947927\n",
      "Iteration is: 1619 and loss is: 0.0012713602045550942\n",
      "Iteration is: 1620 and loss is: 0.0011344425147399306\n",
      "Iteration is: 1621 and loss is: 0.0010459565091878176\n",
      "Iteration is: 1622 and loss is: 0.0011045504361391068\n",
      "Iteration is: 1623 and loss is: 0.0011869525769725442\n",
      "Iteration is: 1624 and loss is: 0.0011624280596151948\n",
      "Iteration is: 1625 and loss is: 0.0010729794157668948\n",
      "Iteration is: 1626 and loss is: 0.0010425201617181301\n",
      "Iteration is: 1627 and loss is: 0.0010914470767602324\n",
      "Iteration is: 1628 and loss is: 0.0011298994068056345\n",
      "Iteration is: 1629 and loss is: 0.0011004026746377349\n",
      "Iteration is: 1630 and loss is: 0.0010475795716047287\n",
      "Iteration is: 1631 and loss is: 0.001039112452417612\n",
      "Iteration is: 1632 and loss is: 0.0010714932577684522\n",
      "Iteration is: 1633 and loss is: 0.0010888829128816724\n",
      "Iteration is: 1634 and loss is: 0.0010663357097655535\n",
      "Iteration is: 1635 and loss is: 0.001035577617585659\n",
      "Iteration is: 1636 and loss is: 0.0010331074008718133\n",
      "Iteration is: 1637 and loss is: 0.0010525251273065805\n",
      "Iteration is: 1638 and loss is: 0.0010612523183226585\n",
      "Iteration is: 1639 and loss is: 0.0010467593092471361\n",
      "Iteration is: 1640 and loss is: 0.0010281240101903677\n",
      "Iteration is: 1641 and loss is: 0.0010260897688567638\n",
      "Iteration is: 1642 and loss is: 0.001037055510096252\n",
      "Iteration is: 1643 and loss is: 0.001042420044541359\n",
      "Iteration is: 1644 and loss is: 0.0010341681772843003\n",
      "Iteration is: 1645 and loss is: 0.0010222925338894129\n",
      "Iteration is: 1646 and loss is: 0.0010192693443968892\n",
      "Iteration is: 1647 and loss is: 0.0010247586760669947\n",
      "Iteration is: 1648 and loss is: 0.0010285885073244572\n",
      "Iteration is: 1649 and loss is: 0.0010246213059872389\n",
      "Iteration is: 1650 and loss is: 0.0010169816669076681\n",
      "Iteration is: 1651 and loss is: 0.0010132606839761138\n",
      "Iteration is: 1652 and loss is: 0.0010151674505323172\n",
      "Iteration is: 1653 and loss is: 0.0010177820222452283\n",
      "Iteration is: 1654 and loss is: 0.0010164245031774044\n",
      "Iteration is: 1655 and loss is: 0.0010117702186107635\n",
      "Iteration is: 1656 and loss is: 0.0010080160573124886\n",
      "Iteration is: 1657 and loss is: 0.0010075800819322467\n",
      "Iteration is: 1658 and loss is: 0.0010088655399158597\n",
      "Iteration is: 1659 and loss is: 0.0010087867267429829\n",
      "Iteration is: 1660 and loss is: 0.0010063342051580548\n",
      "Iteration is: 1661 and loss is: 0.0010032085701823235\n",
      "Iteration is: 1662 and loss is: 0.0010014937724918127\n",
      "Iteration is: 1663 and loss is: 0.0010014172876253724\n",
      "Iteration is: 1664 and loss is: 0.0010015201987698674\n",
      "Iteration is: 1665 and loss is: 0.001000475836917758\n",
      "Iteration is: 1666 and loss is: 0.000998373026959598\n",
      "Iteration is: 1667 and loss is: 0.0009963435586541891\n",
      "Iteration is: 1668 and loss is: 0.000995233072899282\n",
      "Iteration is: 1669 and loss is: 0.000994846341200173\n",
      "Iteration is: 1670 and loss is: 0.0009943345794454217\n",
      "Iteration is: 1671 and loss is: 0.0009931494714692235\n",
      "Iteration is: 1672 and loss is: 0.0009914870606735349\n",
      "Iteration is: 1673 and loss is: 0.0009899622527882457\n",
      "Iteration is: 1674 and loss is: 0.0009889400098472834\n",
      "Iteration is: 1675 and loss is: 0.00098826689645648\n",
      "Iteration is: 1676 and loss is: 0.0009875086834654212\n",
      "Iteration is: 1677 and loss is: 0.0009864023886620998\n",
      "Iteration is: 1678 and loss is: 0.0009850444039329886\n",
      "Iteration is: 1679 and loss is: 0.0009837335674092174\n",
      "Iteration is: 1680 and loss is: 0.0009826751193031669\n",
      "Iteration is: 1681 and loss is: 0.0009818251710385084\n",
      "Iteration is: 1682 and loss is: 0.0009809762705117464\n",
      "Iteration is: 1683 and loss is: 0.0009799626423045993\n",
      "Iteration is: 1684 and loss is: 0.0009787895251065493\n",
      "Iteration is: 1685 and loss is: 0.0009775913786143064\n",
      "Iteration is: 1686 and loss is: 0.0009764981223270297\n",
      "Iteration is: 1687 and loss is: 0.0009755333885550499\n",
      "Iteration is: 1688 and loss is: 0.0009746180730871856\n",
      "Iteration is: 1689 and loss is: 0.0009736550273373723\n",
      "Iteration is: 1690 and loss is: 0.0009726048447191715\n",
      "Iteration is: 1691 and loss is: 0.0009715023916214705\n",
      "Iteration is: 1692 and loss is: 0.0009704157127998769\n",
      "Iteration is: 1693 and loss is: 0.0009693892789073288\n",
      "Iteration is: 1694 and loss is: 0.0009684186661615968\n",
      "Iteration is: 1695 and loss is: 0.0009674580069258809\n",
      "Iteration is: 1696 and loss is: 0.0009664716781117022\n",
      "Iteration is: 1697 and loss is: 0.0009654474561102688\n",
      "Iteration is: 1698 and loss is: 0.0009644069941714406\n",
      "Iteration is: 1699 and loss is: 0.0009633667068555951\n",
      "Iteration is: 1700 and loss is: 0.0009623516816645861\n",
      "Iteration is: 1701 and loss is: 0.0009613646543584764\n",
      "Iteration is: 1702 and loss is: 0.0009603862999938428\n",
      "Iteration is: 1703 and loss is: 0.0009594071307219565\n",
      "Iteration is: 1704 and loss is: 0.0009584128274582326\n",
      "Iteration is: 1705 and loss is: 0.0009574059513397515\n",
      "Iteration is: 1706 and loss is: 0.0009563911589793861\n",
      "Iteration is: 1707 and loss is: 0.0009553894633427262\n",
      "Iteration is: 1708 and loss is: 0.0009543972555547953\n",
      "Iteration is: 1709 and loss is: 0.0009534167475067079\n",
      "Iteration is: 1710 and loss is: 0.000952437927480787\n",
      "Iteration is: 1711 and loss is: 0.0009514577686786652\n",
      "Iteration is: 1712 and loss is: 0.0009504728368483484\n",
      "Iteration is: 1713 and loss is: 0.0009494830155745149\n",
      "Iteration is: 1714 and loss is: 0.0009484977927058935\n",
      "Iteration is: 1715 and loss is: 0.0009475156548433006\n",
      "Iteration is: 1716 and loss is: 0.0009465364273637533\n",
      "Iteration is: 1717 and loss is: 0.0009455617400817573\n",
      "Iteration is: 1718 and loss is: 0.0009445920004509389\n",
      "Iteration is: 1719 and loss is: 0.0009436191176064312\n",
      "Iteration is: 1720 and loss is: 0.0009426463511772454\n",
      "Iteration is: 1721 and loss is: 0.0009416727116331458\n",
      "Iteration is: 1722 and loss is: 0.0009407004690729082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 1723 and loss is: 0.0009397318353876472\n",
      "Iteration is: 1724 and loss is: 0.0009387630852870643\n",
      "Iteration is: 1725 and loss is: 0.0009377985843457282\n",
      "Iteration is: 1726 and loss is: 0.0009368364117108285\n",
      "Iteration is: 1727 and loss is: 0.0009358745883218944\n",
      "Iteration is: 1728 and loss is: 0.0009349136962555349\n",
      "Iteration is: 1729 and loss is: 0.0009339530952274799\n",
      "Iteration is: 1730 and loss is: 0.000932994531467557\n",
      "Iteration is: 1731 and loss is: 0.0009320357930846512\n",
      "Iteration is: 1732 and loss is: 0.0009310804307460785\n",
      "Iteration is: 1733 and loss is: 0.0009301250684075058\n",
      "Iteration is: 1734 and loss is: 0.0009291702299378812\n",
      "Iteration is: 1735 and loss is: 0.0009282188839279115\n",
      "Iteration is: 1736 and loss is: 0.0009272683528251946\n",
      "Iteration is: 1737 and loss is: 0.0009263203828595579\n",
      "Iteration is: 1738 and loss is: 0.0009253709577023983\n",
      "Iteration is: 1739 and loss is: 0.0009244266548193991\n",
      "Iteration is: 1740 and loss is: 0.0009234820026904345\n",
      "Iteration is: 1741 and loss is: 0.0009225366520695388\n",
      "Iteration is: 1742 and loss is: 0.0009215943864546716\n",
      "Iteration is: 1743 and loss is: 0.0009206538670696318\n",
      "Iteration is: 1744 and loss is: 0.0009197141625918448\n",
      "Iteration is: 1745 and loss is: 0.0009187747491523623\n",
      "Iteration is: 1746 and loss is: 0.0009178349282592535\n",
      "Iteration is: 1747 and loss is: 0.0009169006953015924\n",
      "Iteration is: 1748 and loss is: 0.0009159668115898967\n",
      "Iteration is: 1749 and loss is: 0.0009150344412773848\n",
      "Iteration is: 1750 and loss is: 0.0009141014306806028\n",
      "Iteration is: 1751 and loss is: 0.0009131726110354066\n",
      "Iteration is: 1752 and loss is: 0.0009122430346906185\n",
      "Iteration is: 1753 and loss is: 0.000911315786652267\n",
      "Iteration is: 1754 and loss is: 0.0009103878401219845\n",
      "Iteration is: 1755 and loss is: 0.0009094656561501324\n",
      "Iteration is: 1756 and loss is: 0.0009085427154786885\n",
      "Iteration is: 1757 and loss is: 0.0009076222777366638\n",
      "Iteration is: 1758 and loss is: 0.0009067023056559265\n",
      "Iteration is: 1759 and loss is: 0.0009057852439582348\n",
      "Iteration is: 1760 and loss is: 0.0009048687643371522\n",
      "Iteration is: 1761 and loss is: 0.0009039558353833854\n",
      "Iteration is: 1762 and loss is: 0.0009030438377521932\n",
      "Iteration is: 1763 and loss is: 0.0009021396981552243\n",
      "Iteration is: 1764 and loss is: 0.0009012368391267955\n",
      "Iteration is: 1765 and loss is: 0.0009003388113342226\n",
      "Iteration is: 1766 and loss is: 0.0008994491072371602\n",
      "Iteration is: 1767 and loss is: 0.0008985683089122176\n",
      "Iteration is: 1768 and loss is: 0.0008977043908089399\n",
      "Iteration is: 1769 and loss is: 0.0008968631154857576\n",
      "Iteration is: 1770 and loss is: 0.0008960539707913995\n",
      "Iteration is: 1771 and loss is: 0.0008952884236350656\n",
      "Iteration is: 1772 and loss is: 0.0008945964509621263\n",
      "Iteration is: 1773 and loss is: 0.0008940204861573875\n",
      "Iteration is: 1774 and loss is: 0.0008936183294281363\n",
      "Iteration is: 1775 and loss is: 0.0008934878278523684\n",
      "Iteration is: 1776 and loss is: 0.0008937910897657275\n",
      "Iteration is: 1777 and loss is: 0.000894806522410363\n",
      "Iteration is: 1778 and loss is: 0.0008969932096078992\n",
      "Iteration is: 1779 and loss is: 0.0009011240908876061\n",
      "Iteration is: 1780 and loss is: 0.0009085298515856266\n",
      "Iteration is: 1781 and loss is: 0.0009214386227540672\n",
      "Iteration is: 1782 and loss is: 0.0009437757544219494\n",
      "Iteration is: 1783 and loss is: 0.000982183963060379\n",
      "Iteration is: 1784 and loss is: 0.0010486558312550187\n",
      "Iteration is: 1785 and loss is: 0.0011633329559117556\n",
      "Iteration is: 1786 and loss is: 0.0013637139927595854\n",
      "Iteration is: 1787 and loss is: 0.001710401033051312\n",
      "Iteration is: 1788 and loss is: 0.002316675614565611\n",
      "Iteration is: 1789 and loss is: 0.003343592630699277\n",
      "Iteration is: 1790 and loss is: 0.005075608845800161\n",
      "Iteration is: 1791 and loss is: 0.007734481245279312\n",
      "Iteration is: 1792 and loss is: 0.011546488851308823\n",
      "Iteration is: 1793 and loss is: 0.01556013897061348\n",
      "Iteration is: 1794 and loss is: 0.018177060410380363\n",
      "Iteration is: 1795 and loss is: 0.016041196882724762\n",
      "Iteration is: 1796 and loss is: 0.009631053544580936\n",
      "Iteration is: 1797 and loss is: 0.002865314483642578\n",
      "Iteration is: 1798 and loss is: 0.0009954796405509114\n",
      "Iteration is: 1799 and loss is: 0.004200738389045\n",
      "Iteration is: 1800 and loss is: 0.0079507352784276\n",
      "Iteration is: 1801 and loss is: 0.00797558669000864\n",
      "Iteration is: 1802 and loss is: 0.004208423662930727\n",
      "Iteration is: 1803 and loss is: 0.0011140159331262112\n",
      "Iteration is: 1804 and loss is: 0.0016803736798465252\n",
      "Iteration is: 1805 and loss is: 0.00420741643756628\n",
      "Iteration is: 1806 and loss is: 0.0050538768991827965\n",
      "Iteration is: 1807 and loss is: 0.0031497515738010406\n",
      "Iteration is: 1808 and loss is: 0.0011165891773998737\n",
      "Iteration is: 1809 and loss is: 0.0012699024518951774\n",
      "Iteration is: 1810 and loss is: 0.0027904948219656944\n",
      "Iteration is: 1811 and loss is: 0.0033128871582448483\n",
      "Iteration is: 1812 and loss is: 0.0021559821907430887\n",
      "Iteration is: 1813 and loss is: 0.0009841201826930046\n",
      "Iteration is: 1814 and loss is: 0.001172538148239255\n",
      "Iteration is: 1815 and loss is: 0.0020914357155561447\n",
      "Iteration is: 1816 and loss is: 0.0022995765320956707\n",
      "Iteration is: 1817 and loss is: 0.0015452010557055473\n",
      "Iteration is: 1818 and loss is: 0.000909381837118417\n",
      "Iteration is: 1819 and loss is: 0.0011145771713927388\n",
      "Iteration is: 1820 and loss is: 0.0016579049406573176\n",
      "Iteration is: 1821 and loss is: 0.0017032993491739035\n",
      "Iteration is: 1822 and loss is: 0.0012200715718790889\n",
      "Iteration is: 1823 and loss is: 0.0008841462549753487\n",
      "Iteration is: 1824 and loss is: 0.0010523940436542034\n",
      "Iteration is: 1825 and loss is: 0.0013672260101884604\n",
      "Iteration is: 1826 and loss is: 0.0013578522484749556\n",
      "Iteration is: 1827 and loss is: 0.0010595179628580809\n",
      "Iteration is: 1828 and loss is: 0.000876806618180126\n",
      "Iteration is: 1829 and loss is: 0.000990349450148642\n",
      "Iteration is: 1830 and loss is: 0.001173733384348452\n",
      "Iteration is: 1831 and loss is: 0.0011600053403526545\n",
      "Iteration is: 1832 and loss is: 0.0009813265642151237\n",
      "Iteration is: 1833 and loss is: 0.0008730723056942225\n",
      "Iteration is: 1834 and loss is: 0.0009384181466884911\n",
      "Iteration is: 1835 and loss is: 0.0010480425553396344\n",
      "Iteration is: 1836 and loss is: 0.001045455108396709\n",
      "Iteration is: 1837 and loss is: 0.0009417980909347534\n",
      "Iteration is: 1838 and loss is: 0.0008703095954842865\n",
      "Iteration is: 1839 and loss is: 0.0009000676218420267\n",
      "Iteration is: 1840 and loss is: 0.0009666726691648364\n",
      "Iteration is: 1841 and loss is: 0.0009756290237419307\n",
      "Iteration is: 1842 and loss is: 0.0009192248107865453\n",
      "Iteration is: 1843 and loss is: 0.0008688592351973057\n",
      "Iteration is: 1844 and loss is: 0.0008752972353249788\n",
      "Iteration is: 1845 and loss is: 0.0009143538773059845\n",
      "Iteration is: 1846 and loss is: 0.0009297272772528231\n",
      "Iteration is: 1847 and loss is: 0.000903407926671207\n",
      "Iteration is: 1848 and loss is: 0.0008685470093041658\n",
      "Iteration is: 1849 and loss is: 0.0008615425322204828\n",
      "Iteration is: 1850 and loss is: 0.0008810281869955361\n",
      "Iteration is: 1851 and loss is: 0.0008970044436864555\n",
      "Iteration is: 1852 and loss is: 0.0008894235361367464\n",
      "Iteration is: 1853 and loss is: 0.0008680278551764786\n",
      "Iteration is: 1854 and loss is: 0.0008556576212868094\n",
      "Iteration is: 1855 and loss is: 0.0008612293750047684\n",
      "Iteration is: 1856 and loss is: 0.0008730678237043321\n",
      "Iteration is: 1857 and loss is: 0.0008753769216127694\n",
      "Iteration is: 1858 and loss is: 0.0008655327837914228\n",
      "Iteration is: 1859 and loss is: 0.0008540098206140101\n",
      "Iteration is: 1860 and loss is: 0.0008510752231813967\n",
      "Iteration is: 1861 and loss is: 0.000856350059621036\n",
      "Iteration is: 1862 and loss is: 0.00086137157632038\n",
      "Iteration is: 1863 and loss is: 0.0008596948464401066\n",
      "Iteration is: 1864 and loss is: 0.0008528102771379054\n",
      "Iteration is: 1865 and loss is: 0.0008470526081509888\n",
      "Iteration is: 1866 and loss is: 0.0008464587153866887\n",
      "Iteration is: 1867 and loss is: 0.0008493852219544351\n",
      "Iteration is: 1868 and loss is: 0.0008512315107509494\n",
      "Iteration is: 1869 and loss is: 0.000849362404551357\n",
      "Iteration is: 1870 and loss is: 0.0008451325702480972\n",
      "Iteration is: 1871 and loss is: 0.0008418172365054488\n",
      "Iteration is: 1872 and loss is: 0.000841237953864038\n",
      "Iteration is: 1873 and loss is: 0.0008424237603321671\n",
      "Iteration is: 1874 and loss is: 0.0008430641028098762\n",
      "Iteration is: 1875 and loss is: 0.0008418071665801108\n",
      "Iteration is: 1876 and loss is: 0.0008392367162741721\n",
      "Iteration is: 1877 and loss is: 0.0008369614370167255\n",
      "Iteration is: 1878 and loss is: 0.0008360305218957365\n",
      "Iteration is: 1879 and loss is: 0.000836153922136873\n",
      "Iteration is: 1880 and loss is: 0.0008362555527128279\n",
      "Iteration is: 1881 and loss is: 0.0008354985038749874\n",
      "Iteration is: 1882 and loss is: 0.0008339295745827258\n",
      "Iteration is: 1883 and loss is: 0.0008322427165694535\n",
      "Iteration is: 1884 and loss is: 0.0008310930570587516\n",
      "Iteration is: 1885 and loss is: 0.0008305905503220856\n",
      "Iteration is: 1886 and loss is: 0.0008303329814225435\n",
      "Iteration is: 1887 and loss is: 0.0008298208122141659\n",
      "Iteration is: 1888 and loss is: 0.0008288543904200196\n",
      "Iteration is: 1889 and loss is: 0.0008276079897768795\n",
      "Iteration is: 1890 and loss is: 0.0008264272473752499\n",
      "Iteration is: 1891 and loss is: 0.0008255369612015784\n",
      "Iteration is: 1892 and loss is: 0.0008249262464232743\n",
      "Iteration is: 1893 and loss is: 0.0008243829943239689\n",
      "Iteration is: 1894 and loss is: 0.0008237091242335737\n",
      "Iteration is: 1895 and loss is: 0.0008228324586525559\n",
      "Iteration is: 1896 and loss is: 0.000821836234536022\n",
      "Iteration is: 1897 and loss is: 0.0008208674844354391\n",
      "Iteration is: 1898 and loss is: 0.0008200268493965268\n",
      "Iteration is: 1899 and loss is: 0.000819312990643084\n",
      "Iteration is: 1900 and loss is: 0.000818650412838906\n",
      "Iteration is: 1901 and loss is: 0.0008179463911801577\n",
      "Iteration is: 1902 and loss is: 0.0008171563968062401\n",
      "Iteration is: 1903 and loss is: 0.0008162946323864162\n",
      "Iteration is: 1904 and loss is: 0.0008154188981279731\n",
      "Iteration is: 1905 and loss is: 0.0008145825122483075\n",
      "Iteration is: 1906 and loss is: 0.0008138052653521299\n",
      "Iteration is: 1907 and loss is: 0.0008130764472298324\n",
      "Iteration is: 1908 and loss is: 0.0008123557199724019\n",
      "Iteration is: 1909 and loss is: 0.000811613688711077\n",
      "Iteration is: 1910 and loss is: 0.0008108330075629056\n",
      "Iteration is: 1911 and loss is: 0.0008100247941911221\n",
      "Iteration is: 1912 and loss is: 0.0008092102943919599\n",
      "Iteration is: 1913 and loss is: 0.0008084147702902555\n",
      "Iteration is: 1914 and loss is: 0.000807642936706543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 1915 and loss is: 0.0008068914758041501\n",
      "Iteration is: 1916 and loss is: 0.0008061467669904232\n",
      "Iteration is: 1917 and loss is: 0.0008053987985476851\n",
      "Iteration is: 1918 and loss is: 0.0008046419825404882\n",
      "Iteration is: 1919 and loss is: 0.0008038730593398213\n",
      "Iteration is: 1920 and loss is: 0.0008030968019738793\n",
      "Iteration is: 1921 and loss is: 0.0008023224654607475\n",
      "Iteration is: 1922 and loss is: 0.0008015534840524197\n",
      "Iteration is: 1923 and loss is: 0.0008007925935089588\n",
      "Iteration is: 1924 and loss is: 0.0008000386878848076\n",
      "Iteration is: 1925 and loss is: 0.0007992908940650523\n",
      "Iteration is: 1926 and loss is: 0.0007985425763763487\n",
      "Iteration is: 1927 and loss is: 0.0007977942004799843\n",
      "Iteration is: 1928 and loss is: 0.0007970432634465396\n",
      "Iteration is: 1929 and loss is: 0.0007962904637679458\n",
      "Iteration is: 1930 and loss is: 0.0007955341134220362\n",
      "Iteration is: 1931 and loss is: 0.0007947820122353733\n",
      "Iteration is: 1932 and loss is: 0.0007940287468954921\n",
      "Iteration is: 1933 and loss is: 0.0007932789158076048\n",
      "Iteration is: 1934 and loss is: 0.0007925312384031713\n",
      "Iteration is: 1935 and loss is: 0.0007917855400592089\n",
      "Iteration is: 1936 and loss is: 0.0007910415297374129\n",
      "Iteration is: 1937 and loss is: 0.0007903025834821165\n",
      "Iteration is: 1938 and loss is: 0.0007895607268437743\n",
      "Iteration is: 1939 and loss is: 0.0007888188702054322\n",
      "Iteration is: 1940 and loss is: 0.0007880788180045784\n",
      "Iteration is: 1941 and loss is: 0.0007873397553339601\n",
      "Iteration is: 1942 and loss is: 0.0007866017986088991\n",
      "Iteration is: 1943 and loss is: 0.0007858613971620798\n",
      "Iteration is: 1944 and loss is: 0.0007851238478906453\n",
      "Iteration is: 1945 and loss is: 0.0007843862404115498\n",
      "Iteration is: 1946 and loss is: 0.0007836503791622818\n",
      "Iteration is: 1947 and loss is: 0.0007829166133888066\n",
      "Iteration is: 1948 and loss is: 0.0007821830222383142\n",
      "Iteration is: 1949 and loss is: 0.0007814522250555456\n",
      "Iteration is: 1950 and loss is: 0.0007807204383425415\n",
      "Iteration is: 1951 and loss is: 0.0007799906888976693\n",
      "Iteration is: 1952 and loss is: 0.0007792626856826246\n",
      "Iteration is: 1953 and loss is: 0.0007785364869050682\n",
      "Iteration is: 1954 and loss is: 0.0007778106373734772\n",
      "Iteration is: 1955 and loss is: 0.0007770840893499553\n",
      "Iteration is: 1956 and loss is: 0.0007763595203869045\n",
      "Iteration is: 1957 and loss is: 0.0007756364066153765\n",
      "Iteration is: 1958 and loss is: 0.0007749128271825612\n",
      "Iteration is: 1959 and loss is: 0.0007741923327557743\n",
      "Iteration is: 1960 and loss is: 0.0007734709070064127\n",
      "Iteration is: 1961 and loss is: 0.0007727540214546025\n",
      "Iteration is: 1962 and loss is: 0.0007720362627878785\n",
      "Iteration is: 1963 and loss is: 0.0007713238592259586\n",
      "Iteration is: 1964 and loss is: 0.0007706111646257341\n",
      "Iteration is: 1965 and loss is: 0.0007699051639065146\n",
      "Iteration is: 1966 and loss is: 0.0007692007347941399\n",
      "Iteration is: 1967 and loss is: 0.0007685021846555173\n",
      "Iteration is: 1968 and loss is: 0.0007678117253817618\n",
      "Iteration is: 1969 and loss is: 0.0007671285420656204\n",
      "Iteration is: 1970 and loss is: 0.0007664617733098567\n",
      "Iteration is: 1971 and loss is: 0.0007658139802515507\n",
      "Iteration is: 1972 and loss is: 0.0007651972118765116\n",
      "Iteration is: 1973 and loss is: 0.0007646344020031393\n",
      "Iteration is: 1974 and loss is: 0.0007641458651050925\n",
      "Iteration is: 1975 and loss is: 0.0007637751987203956\n",
      "Iteration is: 1976 and loss is: 0.0007635874208062887\n",
      "Iteration is: 1977 and loss is: 0.0007636912632733583\n",
      "Iteration is: 1978 and loss is: 0.0007642620475962758\n",
      "Iteration is: 1979 and loss is: 0.0007655887748114765\n",
      "Iteration is: 1980 and loss is: 0.0007681517745368183\n",
      "Iteration is: 1981 and loss is: 0.0007727494230493903\n",
      "Iteration is: 1982 and loss is: 0.0007807861547917128\n",
      "Iteration is: 1983 and loss is: 0.0007946340483613312\n",
      "Iteration is: 1984 and loss is: 0.0008184691541828215\n",
      "Iteration is: 1985 and loss is: 0.0008593230741098523\n",
      "Iteration is: 1986 and loss is: 0.0009299636585637927\n",
      "Iteration is: 1987 and loss is: 0.0010519210482016206\n",
      "Iteration is: 1988 and loss is: 0.0012653119629248977\n",
      "Iteration is: 1989 and loss is: 0.0016355435363948345\n",
      "Iteration is: 1990 and loss is: 0.002285800874233246\n",
      "Iteration is: 1991 and loss is: 0.003395896404981613\n",
      "Iteration is: 1992 and loss is: 0.005292120389640331\n",
      "Iteration is: 1993 and loss is: 0.008273344486951828\n",
      "Iteration is: 1994 and loss is: 0.012727520428597927\n",
      "Iteration is: 1995 and loss is: 0.01785285212099552\n",
      "Iteration is: 1996 and loss is: 0.022089531645178795\n",
      "Iteration is: 1997 and loss is: 0.021112866699695587\n",
      "Iteration is: 1998 and loss is: 0.014265037141740322\n",
      "Iteration is: 1999 and loss is: 0.00501615833491087\n",
      "Iteration is: 2000 and loss is: 0.0007660137489438057\n",
      "Iteration is: 2001 and loss is: 0.0036169816739857197\n",
      "Iteration is: 2002 and loss is: 0.00881973560899496\n",
      "Iteration is: 2003 and loss is: 0.010202751494944096\n",
      "Iteration is: 2004 and loss is: 0.006073057651519775\n",
      "Iteration is: 2005 and loss is: 0.0015071553643792868\n",
      "Iteration is: 2006 and loss is: 0.0012942042667418718\n",
      "Iteration is: 2007 and loss is: 0.004411115311086178\n",
      "Iteration is: 2008 and loss is: 0.0061079831793904305\n",
      "Iteration is: 2009 and loss is: 0.004096959251910448\n",
      "Iteration is: 2010 and loss is: 0.0012802507262676954\n",
      "Iteration is: 2011 and loss is: 0.0010701215360313654\n",
      "Iteration is: 2012 and loss is: 0.0029680707957595587\n",
      "Iteration is: 2013 and loss is: 0.003874792717397213\n",
      "Iteration is: 2014 and loss is: 0.0025249221362173557\n",
      "Iteration is: 2015 and loss is: 0.0009414047817699611\n",
      "Iteration is: 2016 and loss is: 0.001084250514395535\n",
      "Iteration is: 2017 and loss is: 0.002268719719722867\n",
      "Iteration is: 2018 and loss is: 0.00256915669888258\n",
      "Iteration is: 2019 and loss is: 0.0015982133336365223\n",
      "Iteration is: 2020 and loss is: 0.0007922957884147763\n",
      "Iteration is: 2021 and loss is: 0.0010950451251119375\n",
      "Iteration is: 2022 and loss is: 0.001788137829862535\n",
      "Iteration is: 2023 and loss is: 0.001778844278305769\n",
      "Iteration is: 2024 and loss is: 0.001126117305830121\n",
      "Iteration is: 2025 and loss is: 0.0007633331115357578\n",
      "Iteration is: 2026 and loss is: 0.001057442743331194\n",
      "Iteration is: 2027 and loss is: 0.0014304318465292454\n",
      "Iteration is: 2028 and loss is: 0.0013186114374548197\n",
      "Iteration is: 2029 and loss is: 0.0009129412937909365\n",
      "Iteration is: 2030 and loss is: 0.0007681355345994234\n",
      "Iteration is: 2031 and loss is: 0.0009881474543362856\n",
      "Iteration is: 2032 and loss is: 0.0011787721887230873\n",
      "Iteration is: 2033 and loss is: 0.0010654344223439693\n",
      "Iteration is: 2034 and loss is: 0.0008248931844718754\n",
      "Iteration is: 2035 and loss is: 0.000770285667385906\n",
      "Iteration is: 2036 and loss is: 0.0009144399664364755\n",
      "Iteration is: 2037 and loss is: 0.0010130561422556639\n",
      "Iteration is: 2038 and loss is: 0.0009304035338573158\n",
      "Iteration is: 2039 and loss is: 0.0007891947752796113\n",
      "Iteration is: 2040 and loss is: 0.0007652157219126821\n",
      "Iteration is: 2041 and loss is: 0.0008529145270586014\n",
      "Iteration is: 2042 and loss is: 0.0009086641948670149\n",
      "Iteration is: 2043 and loss is: 0.0008582060690969229\n",
      "Iteration is: 2044 and loss is: 0.0007739450084045529\n",
      "Iteration is: 2045 and loss is: 0.0007572247995994985\n",
      "Iteration is: 2046 and loss is: 0.0008074134238995612\n",
      "Iteration is: 2047 and loss is: 0.0008431800524704158\n",
      "Iteration is: 2048 and loss is: 0.0008173412643373013\n",
      "Iteration is: 2049 and loss is: 0.0007663515280000865\n",
      "Iteration is: 2050 and loss is: 0.0007502827211283147\n",
      "Iteration is: 2051 and loss is: 0.0007763351895846426\n",
      "Iteration is: 2052 and loss is: 0.0008010690798982978\n",
      "Iteration is: 2053 and loss is: 0.000791605853009969\n",
      "Iteration is: 2054 and loss is: 0.0007615083013661206\n",
      "Iteration is: 2055 and loss is: 0.0007458399632014334\n",
      "Iteration is: 2056 and loss is: 0.0007564669358544052\n",
      "Iteration is: 2057 and loss is: 0.0007733082165941596\n",
      "Iteration is: 2058 and loss is: 0.0007734710816293955\n",
      "Iteration is: 2059 and loss is: 0.0007573561160825193\n",
      "Iteration is: 2060 and loss is: 0.0007436022278852761\n",
      "Iteration is: 2061 and loss is: 0.0007446871604770422\n",
      "Iteration is: 2062 and loss is: 0.000754549284465611\n",
      "Iteration is: 2063 and loss is: 0.0007589609012939036\n",
      "Iteration is: 2064 and loss is: 0.0007524467655457556\n",
      "Iteration is: 2065 and loss is: 0.0007423895294778049\n",
      "Iteration is: 2066 and loss is: 0.0007385265198536217\n",
      "Iteration is: 2067 and loss is: 0.000742242846172303\n",
      "Iteration is: 2068 and loss is: 0.0007468684925697744\n",
      "Iteration is: 2069 and loss is: 0.0007461791974492371\n",
      "Iteration is: 2070 and loss is: 0.0007406793884001672\n",
      "Iteration is: 2071 and loss is: 0.0007356831920333207\n",
      "Iteration is: 2072 and loss is: 0.0007349834777414799\n",
      "Iteration is: 2073 and loss is: 0.0007373923435807228\n",
      "Iteration is: 2074 and loss is: 0.0007389414822682738\n",
      "Iteration is: 2075 and loss is: 0.0007373355911113322\n",
      "Iteration is: 2076 and loss is: 0.000733806227799505\n",
      "Iteration is: 2077 and loss is: 0.0007312166271731257\n",
      "Iteration is: 2078 and loss is: 0.0007309808861464262\n",
      "Iteration is: 2079 and loss is: 0.0007320587756112218\n",
      "Iteration is: 2080 and loss is: 0.0007324066245928407\n",
      "Iteration is: 2081 and loss is: 0.0007310813525691628\n",
      "Iteration is: 2082 and loss is: 0.0007288679480552673\n",
      "Iteration is: 2083 and loss is: 0.0007272105431184173\n",
      "Iteration is: 2084 and loss is: 0.0007267811452038586\n",
      "Iteration is: 2085 and loss is: 0.0007270455243997276\n",
      "Iteration is: 2086 and loss is: 0.0007269759080372751\n",
      "Iteration is: 2087 and loss is: 0.000726055062841624\n",
      "Iteration is: 2088 and loss is: 0.0007246047607623041\n",
      "Iteration is: 2089 and loss is: 0.0007233331562019885\n",
      "Iteration is: 2090 and loss is: 0.0007226610323414207\n",
      "Iteration is: 2091 and loss is: 0.0007224424043670297\n",
      "Iteration is: 2092 and loss is: 0.000722196651622653\n",
      "Iteration is: 2093 and loss is: 0.0007215639925561845\n",
      "Iteration is: 2094 and loss is: 0.0007205744623206556\n",
      "Iteration is: 2095 and loss is: 0.0007195318466983736\n",
      "Iteration is: 2096 and loss is: 0.0007187242154031992\n",
      "Iteration is: 2097 and loss is: 0.0007182073895819485\n",
      "Iteration is: 2098 and loss is: 0.0007178066880442202\n",
      "Iteration is: 2099 and loss is: 0.0007173033081926405\n",
      "Iteration is: 2100 and loss is: 0.0007165981223806739\n",
      "Iteration is: 2101 and loss is: 0.0007157584768719971\n",
      "Iteration is: 2102 and loss is: 0.0007149395532906055\n",
      "Iteration is: 2103 and loss is: 0.0007142549147829413\n",
      "Iteration is: 2104 and loss is: 0.0007136989152058959\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 2105 and loss is: 0.0007131872698664665\n",
      "Iteration is: 2106 and loss is: 0.0007126171258278191\n",
      "Iteration is: 2107 and loss is: 0.0007119475048966706\n",
      "Iteration is: 2108 and loss is: 0.0007112130988389254\n",
      "Iteration is: 2109 and loss is: 0.0007104843971319497\n",
      "Iteration is: 2110 and loss is: 0.0007098217029124498\n",
      "Iteration is: 2111 and loss is: 0.0007092267042025924\n",
      "Iteration is: 2112 and loss is: 0.0007086597033776343\n",
      "Iteration is: 2113 and loss is: 0.0007080745999701321\n",
      "Iteration is: 2114 and loss is: 0.0007074435125105083\n",
      "Iteration is: 2115 and loss is: 0.0007067742990329862\n",
      "Iteration is: 2116 and loss is: 0.0007060962379910052\n",
      "Iteration is: 2117 and loss is: 0.0007054387242533267\n",
      "Iteration is: 2118 and loss is: 0.0007048120605759323\n",
      "Iteration is: 2119 and loss is: 0.0007042099605314434\n",
      "Iteration is: 2120 and loss is: 0.0007036137976683676\n",
      "Iteration is: 2121 and loss is: 0.0007030071574263275\n",
      "Iteration is: 2122 and loss is: 0.0007023807847872376\n",
      "Iteration is: 2123 and loss is: 0.0007017444586381316\n",
      "Iteration is: 2124 and loss is: 0.000701103825122118\n",
      "Iteration is: 2125 and loss is: 0.0007004704675637186\n",
      "Iteration is: 2126 and loss is: 0.0006998488097451627\n",
      "Iteration is: 2127 and loss is: 0.0006992361159063876\n",
      "Iteration is: 2128 and loss is: 0.0006986302905716002\n",
      "Iteration is: 2129 and loss is: 0.0006980234174989164\n",
      "Iteration is: 2130 and loss is: 0.0006974134012125432\n",
      "Iteration is: 2131 and loss is: 0.0006967959343455732\n",
      "Iteration is: 2132 and loss is: 0.0006961756735108793\n",
      "Iteration is: 2133 and loss is: 0.0006955544813536108\n",
      "Iteration is: 2134 and loss is: 0.0006949382368475199\n",
      "Iteration is: 2135 and loss is: 0.0006943246116861701\n",
      "Iteration is: 2136 and loss is: 0.0006937162834219635\n",
      "Iteration is: 2137 and loss is: 0.0006931105745024979\n",
      "Iteration is: 2138 and loss is: 0.0006925062043592334\n",
      "Iteration is: 2139 and loss is: 0.000691899680532515\n",
      "Iteration is: 2140 and loss is: 0.0006912924582138658\n",
      "Iteration is: 2141 and loss is: 0.0006906832568347454\n",
      "Iteration is: 2142 and loss is: 0.0006900760345160961\n",
      "Iteration is: 2143 and loss is: 0.000689468695782125\n",
      "Iteration is: 2144 and loss is: 0.0006888624047860503\n",
      "Iteration is: 2145 and loss is: 0.0006882586749270558\n",
      "Iteration is: 2146 and loss is: 0.0006876555271446705\n",
      "Iteration is: 2147 and loss is: 0.0006870559882372618\n",
      "Iteration is: 2148 and loss is: 0.0006864572060294449\n",
      "Iteration is: 2149 and loss is: 0.0006858558626845479\n",
      "Iteration is: 2150 and loss is: 0.0006852601654827595\n",
      "Iteration is: 2151 and loss is: 0.0006846601609140635\n",
      "Iteration is: 2152 and loss is: 0.0006840609130449593\n",
      "Iteration is: 2153 and loss is: 0.0006834625382907689\n",
      "Iteration is: 2154 and loss is: 0.0006828638142906129\n",
      "Iteration is: 2155 and loss is: 0.0006822662544436753\n",
      "Iteration is: 2156 and loss is: 0.0006816696259193122\n",
      "Iteration is: 2157 and loss is: 0.0006810739869251847\n",
      "Iteration is: 2158 and loss is: 0.0006804803851991892\n",
      "Iteration is: 2159 and loss is: 0.0006798873073421419\n",
      "Iteration is: 2160 and loss is: 0.0006792949861846864\n",
      "Iteration is: 2161 and loss is: 0.0006787020829506218\n",
      "Iteration is: 2162 and loss is: 0.0006781105767004192\n",
      "Iteration is: 2163 and loss is: 0.0006775219808332622\n",
      "Iteration is: 2164 and loss is: 0.0006769320461899042\n",
      "Iteration is: 2165 and loss is: 0.0006763429846614599\n",
      "Iteration is: 2166 and loss is: 0.0006757544470019639\n",
      "Iteration is: 2167 and loss is: 0.0006751669570803642\n",
      "Iteration is: 2168 and loss is: 0.0006745792925357819\n",
      "Iteration is: 2169 and loss is: 0.0006739950622431934\n",
      "Iteration is: 2170 and loss is: 0.0006734092603437603\n",
      "Iteration is: 2171 and loss is: 0.0006728238658979535\n",
      "Iteration is: 2172 and loss is: 0.0006722409161739051\n",
      "Iteration is: 2173 and loss is: 0.0006716563948430121\n",
      "Iteration is: 2174 and loss is: 0.0006710741436108947\n",
      "Iteration is: 2175 and loss is: 0.0006704918341711164\n",
      "Iteration is: 2176 and loss is: 0.0006699125515297055\n",
      "Iteration is: 2177 and loss is: 0.0006693324539810419\n",
      "Iteration is: 2178 and loss is: 0.0006687527056783438\n",
      "Iteration is: 2179 and loss is: 0.000668174703605473\n",
      "Iteration is: 2180 and loss is: 0.0006675985059700906\n",
      "Iteration is: 2181 and loss is: 0.0006670238799415529\n",
      "Iteration is: 2182 and loss is: 0.0006664503016509116\n",
      "Iteration is: 2183 and loss is: 0.000665876897983253\n",
      "Iteration is: 2184 and loss is: 0.0006653078598901629\n",
      "Iteration is: 2185 and loss is: 0.0006647432455793023\n",
      "Iteration is: 2186 and loss is: 0.0006641843938268721\n",
      "Iteration is: 2187 and loss is: 0.0006636336911469698\n",
      "Iteration is: 2188 and loss is: 0.0006630973075516522\n",
      "Iteration is: 2189 and loss is: 0.0006625835667364299\n",
      "Iteration is: 2190 and loss is: 0.0006620972999371588\n",
      "Iteration is: 2191 and loss is: 0.0006616666214540601\n",
      "Iteration is: 2192 and loss is: 0.0006613157456740737\n",
      "Iteration is: 2193 and loss is: 0.0006610935088247061\n",
      "Iteration is: 2194 and loss is: 0.0006610816344618797\n",
      "Iteration is: 2195 and loss is: 0.0006614191806875169\n",
      "Iteration is: 2196 and loss is: 0.0006623337394557893\n",
      "Iteration is: 2197 and loss is: 0.0006642069201916456\n",
      "Iteration is: 2198 and loss is: 0.0006676990888081491\n",
      "Iteration is: 2199 and loss is: 0.0006739477394148707\n",
      "Iteration is: 2200 and loss is: 0.0006849794299341738\n",
      "Iteration is: 2201 and loss is: 0.000704306410625577\n",
      "Iteration is: 2202 and loss is: 0.0007382983458228409\n",
      "Iteration is: 2203 and loss is: 0.0007980068912729621\n",
      "Iteration is: 2204 and loss is: 0.0009039317374117672\n",
      "Iteration is: 2205 and loss is: 0.0010913894511759281\n",
      "Iteration is: 2206 and loss is: 0.0014275121502578259\n",
      "Iteration is: 2207 and loss is: 0.002023540437221527\n",
      "Iteration is: 2208 and loss is: 0.0030921371653676033\n",
      "Iteration is: 2209 and loss is: 0.004939449485391378\n",
      "Iteration is: 2210 and loss is: 0.00811447948217392\n",
      "Iteration is: 2211 and loss is: 0.013003109022974968\n",
      "Iteration is: 2212 and loss is: 0.019939783960580826\n",
      "Iteration is: 2213 and loss is: 0.026635324582457542\n",
      "Iteration is: 2214 and loss is: 0.029704559594392776\n",
      "Iteration is: 2215 and loss is: 0.02330148033797741\n",
      "Iteration is: 2216 and loss is: 0.010854605585336685\n",
      "Iteration is: 2217 and loss is: 0.0015271867159754038\n",
      "Iteration is: 2218 and loss is: 0.002514968626201153\n",
      "Iteration is: 2219 and loss is: 0.009879494085907936\n",
      "Iteration is: 2220 and loss is: 0.01363139133900404\n",
      "Iteration is: 2221 and loss is: 0.009386511519551277\n",
      "Iteration is: 2222 and loss is: 0.0023861376103013754\n",
      "Iteration is: 2223 and loss is: 0.001083459472283721\n",
      "Iteration is: 2224 and loss is: 0.0053449892438948154\n",
      "Iteration is: 2225 and loss is: 0.007954058237373829\n",
      "Iteration is: 2226 and loss is: 0.005251717753708363\n",
      "Iteration is: 2227 and loss is: 0.001257086405530572\n",
      "Iteration is: 2228 and loss is: 0.0012960226740688086\n",
      "Iteration is: 2229 and loss is: 0.0041320244781672955\n",
      "Iteration is: 2230 and loss is: 0.004834414925426245\n",
      "Iteration is: 2231 and loss is: 0.00248518493026495\n",
      "Iteration is: 2232 and loss is: 0.0006961325998418033\n",
      "Iteration is: 2233 and loss is: 0.001693147700279951\n",
      "Iteration is: 2234 and loss is: 0.0032545276917517185\n",
      "Iteration is: 2235 and loss is: 0.0027453568764030933\n",
      "Iteration is: 2236 and loss is: 0.0011140796123072505\n",
      "Iteration is: 2237 and loss is: 0.000786110816989094\n",
      "Iteration is: 2238 and loss is: 0.0018377774395048618\n",
      "Iteration is: 2239 and loss is: 0.0023105107247829437\n",
      "Iteration is: 2240 and loss is: 0.0014703599736094475\n",
      "Iteration is: 2241 and loss is: 0.0006988909444771707\n",
      "Iteration is: 2242 and loss is: 0.0010161628015339375\n",
      "Iteration is: 2243 and loss is: 0.0016477855388075113\n",
      "Iteration is: 2244 and loss is: 0.0015164960641413927\n",
      "Iteration is: 2245 and loss is: 0.0008778804913163185\n",
      "Iteration is: 2246 and loss is: 0.0007032047724351287\n",
      "Iteration is: 2247 and loss is: 0.0010977976489812136\n",
      "Iteration is: 2248 and loss is: 0.0013157054781913757\n",
      "Iteration is: 2249 and loss is: 0.0010217332746833563\n",
      "Iteration is: 2250 and loss is: 0.0006942761829122901\n",
      "Iteration is: 2251 and loss is: 0.0007763566100038588\n",
      "Iteration is: 2252 and loss is: 0.0010341060115024447\n",
      "Iteration is: 2253 and loss is: 0.001026920392177999\n",
      "Iteration is: 2254 and loss is: 0.000785927870310843\n",
      "Iteration is: 2255 and loss is: 0.0006732501788064837\n",
      "Iteration is: 2256 and loss is: 0.0008013575570657849\n",
      "Iteration is: 2257 and loss is: 0.0009204222005791962\n",
      "Iteration is: 2258 and loss is: 0.0008428841829299927\n",
      "Iteration is: 2259 and loss is: 0.0006977807497605681\n",
      "Iteration is: 2260 and loss is: 0.0006850806530565023\n",
      "Iteration is: 2261 and loss is: 0.0007827028748579323\n",
      "Iteration is: 2262 and loss is: 0.0008208518847823143\n",
      "Iteration is: 2263 and loss is: 0.0007457428728230298\n",
      "Iteration is: 2264 and loss is: 0.0006715537747368217\n",
      "Iteration is: 2265 and loss is: 0.0006897306302562356\n",
      "Iteration is: 2266 and loss is: 0.0007490417920053005\n",
      "Iteration is: 2267 and loss is: 0.000753329077269882\n",
      "Iteration is: 2268 and loss is: 0.0006997956661507487\n",
      "Iteration is: 2269 and loss is: 0.0006645388202741742\n",
      "Iteration is: 2270 and loss is: 0.0006849186029285192\n",
      "Iteration is: 2271 and loss is: 0.0007178056985139847\n",
      "Iteration is: 2272 and loss is: 0.000712525681592524\n",
      "Iteration is: 2273 and loss is: 0.0006785765872336924\n",
      "Iteration is: 2274 and loss is: 0.0006614635931327939\n",
      "Iteration is: 2275 and loss is: 0.0006762799457646906\n",
      "Iteration is: 2276 and loss is: 0.000694369780831039\n",
      "Iteration is: 2277 and loss is: 0.0006887841736897826\n",
      "Iteration is: 2278 and loss is: 0.0006681241211481392\n",
      "Iteration is: 2279 and loss is: 0.0006586231174878776\n",
      "Iteration is: 2280 and loss is: 0.0006675695185549557\n",
      "Iteration is: 2281 and loss is: 0.0006780229741707444\n",
      "Iteration is: 2282 and loss is: 0.0006746057188138366\n",
      "Iteration is: 2283 and loss is: 0.0006621264037676156\n",
      "Iteration is: 2284 and loss is: 0.0006556923035532236\n",
      "Iteration is: 2285 and loss is: 0.0006602301727980375\n",
      "Iteration is: 2286 and loss is: 0.000666567764710635\n",
      "Iteration is: 2287 and loss is: 0.0006652554147876799\n",
      "Iteration is: 2288 and loss is: 0.0006578557658940554\n",
      "Iteration is: 2289 and loss is: 0.000652909919153899\n",
      "Iteration is: 2290 and loss is: 0.0006544431671500206\n",
      "Iteration is: 2291 and loss is: 0.0006582186906598508\n",
      "Iteration is: 2292 and loss is: 0.0006583184585906565\n",
      "Iteration is: 2293 and loss is: 0.0006542404880747199\n",
      "Iteration is: 2294 and loss is: 0.0006503724143840373\n",
      "Iteration is: 2295 and loss is: 0.0006500370218418539\n",
      "Iteration is: 2296 and loss is: 0.000651987676974386\n",
      "Iteration is: 2297 and loss is: 0.0006527137011289597\n",
      "Iteration is: 2298 and loss is: 0.0006507794605568051\n",
      "Iteration is: 2299 and loss is: 0.0006479623261839151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 2300 and loss is: 0.0006466756458394229\n",
      "Iteration is: 2301 and loss is: 0.0006472351378761232\n",
      "Iteration is: 2302 and loss is: 0.0006479303119704127\n",
      "Iteration is: 2303 and loss is: 0.0006472595268860459\n",
      "Iteration is: 2304 and loss is: 0.0006454722606576979\n",
      "Iteration is: 2305 and loss is: 0.0006439478020183742\n",
      "Iteration is: 2306 and loss is: 0.0006435419782064855\n",
      "Iteration is: 2307 and loss is: 0.0006437969277612865\n",
      "Iteration is: 2308 and loss is: 0.0006436798139475286\n",
      "Iteration is: 2309 and loss is: 0.000642751925624907\n",
      "Iteration is: 2310 and loss is: 0.0006414724048227072\n",
      "Iteration is: 2311 and loss is: 0.0006405745516531169\n",
      "Iteration is: 2312 and loss is: 0.0006402716971933842\n",
      "Iteration is: 2313 and loss is: 0.0006401666905730963\n",
      "Iteration is: 2314 and loss is: 0.0006397500983439386\n",
      "Iteration is: 2315 and loss is: 0.0006389197078533471\n",
      "Iteration is: 2316 and loss is: 0.00063799083000049\n",
      "Iteration is: 2317 and loss is: 0.0006373121868818998\n",
      "Iteration is: 2318 and loss is: 0.000636935408692807\n",
      "Iteration is: 2319 and loss is: 0.0006366271409206092\n",
      "Iteration is: 2320 and loss is: 0.0006361407577060163\n",
      "Iteration is: 2321 and loss is: 0.0006354372599162161\n",
      "Iteration is: 2322 and loss is: 0.0006346923182718456\n",
      "Iteration is: 2323 and loss is: 0.0006340766558423638\n",
      "Iteration is: 2324 and loss is: 0.0006336190272122622\n",
      "Iteration is: 2325 and loss is: 0.0006332066259346902\n",
      "Iteration is: 2326 and loss is: 0.0006327143055386841\n",
      "Iteration is: 2327 and loss is: 0.0006321118562482297\n",
      "Iteration is: 2328 and loss is: 0.0006314675556495786\n",
      "Iteration is: 2329 and loss is: 0.0006308745360001922\n",
      "Iteration is: 2330 and loss is: 0.0006303591653704643\n",
      "Iteration is: 2331 and loss is: 0.0006298850639723241\n",
      "Iteration is: 2332 and loss is: 0.0006293922779150307\n",
      "Iteration is: 2333 and loss is: 0.0006288500735536218\n",
      "Iteration is: 2334 and loss is: 0.000628273410256952\n",
      "Iteration is: 2335 and loss is: 0.0006277009379118681\n",
      "Iteration is: 2336 and loss is: 0.000627166882622987\n",
      "Iteration is: 2337 and loss is: 0.0006266627460718155\n",
      "Iteration is: 2338 and loss is: 0.0006261614034883678\n",
      "Iteration is: 2339 and loss is: 0.0006256443448364735\n",
      "Iteration is: 2340 and loss is: 0.0006251056911423802\n",
      "Iteration is: 2341 and loss is: 0.000624556269031018\n",
      "Iteration is: 2342 and loss is: 0.000624016800429672\n",
      "Iteration is: 2343 and loss is: 0.0006234926404431462\n",
      "Iteration is: 2344 and loss is: 0.0006229833234101534\n",
      "Iteration is: 2345 and loss is: 0.0006224740645848215\n",
      "Iteration is: 2346 and loss is: 0.0006219577044248581\n",
      "Iteration is: 2347 and loss is: 0.000621431041508913\n",
      "Iteration is: 2348 and loss is: 0.0006208993145264685\n",
      "Iteration is: 2349 and loss is: 0.000620368926320225\n",
      "Iteration is: 2350 and loss is: 0.0006198505870997906\n",
      "Iteration is: 2351 and loss is: 0.000619335740339011\n",
      "Iteration is: 2352 and loss is: 0.0006188233965076506\n",
      "Iteration is: 2353 and loss is: 0.0006183106452226639\n",
      "Iteration is: 2354 and loss is: 0.0006177941104397178\n",
      "Iteration is: 2355 and loss is: 0.000617272628005594\n",
      "Iteration is: 2356 and loss is: 0.0006167545798234642\n",
      "Iteration is: 2357 and loss is: 0.0006162393838167191\n",
      "Iteration is: 2358 and loss is: 0.0006157245952636003\n",
      "Iteration is: 2359 and loss is: 0.0006152153946459293\n",
      "Iteration is: 2360 and loss is: 0.0006147054373286664\n",
      "Iteration is: 2361 and loss is: 0.0006141942576505244\n",
      "Iteration is: 2362 and loss is: 0.0006136830779723823\n",
      "Iteration is: 2363 and loss is: 0.0006131699192337692\n",
      "Iteration is: 2364 and loss is: 0.0006126573425717652\n",
      "Iteration is: 2365 and loss is: 0.000612147618085146\n",
      "Iteration is: 2366 and loss is: 0.0006116387667134404\n",
      "Iteration is: 2367 and loss is: 0.0006111311377026141\n",
      "Iteration is: 2368 and loss is: 0.0006106252549216151\n",
      "Iteration is: 2369 and loss is: 0.0006101189646869898\n",
      "Iteration is: 2370 and loss is: 0.0006096128490753472\n",
      "Iteration is: 2371 and loss is: 0.00060910580214113\n",
      "Iteration is: 2372 and loss is: 0.0006086002103984356\n",
      "Iteration is: 2373 and loss is: 0.0006080944440327585\n",
      "Iteration is: 2374 and loss is: 0.0006075890269130468\n",
      "Iteration is: 2375 and loss is: 0.0006070867530070245\n",
      "Iteration is: 2376 and loss is: 0.0006065841298550367\n",
      "Iteration is: 2377 and loss is: 0.0006060810410417616\n",
      "Iteration is: 2378 and loss is: 0.0006055791163817048\n",
      "Iteration is: 2379 and loss is: 0.0006050788215361536\n",
      "Iteration is: 2380 and loss is: 0.0006045783520676196\n",
      "Iteration is: 2381 and loss is: 0.000604078231845051\n",
      "Iteration is: 2382 and loss is: 0.0006035767146386206\n",
      "Iteration is: 2383 and loss is: 0.0006030779331922531\n",
      "Iteration is: 2384 and loss is: 0.0006025799666531384\n",
      "Iteration is: 2385 and loss is: 0.0006020815344527364\n",
      "Iteration is: 2386 and loss is: 0.0006015836843289435\n",
      "Iteration is: 2387 and loss is: 0.0006010875222273171\n",
      "Iteration is: 2388 and loss is: 0.0006005920586176217\n",
      "Iteration is: 2389 and loss is: 0.0006000970606692135\n",
      "Iteration is: 2390 and loss is: 0.0005996008403599262\n",
      "Iteration is: 2391 and loss is: 0.0005991074140183628\n",
      "Iteration is: 2392 and loss is: 0.0005986141040921211\n",
      "Iteration is: 2393 and loss is: 0.0005981203285045922\n",
      "Iteration is: 2394 and loss is: 0.0005976264365017414\n",
      "Iteration is: 2395 and loss is: 0.0005971339414827526\n",
      "Iteration is: 2396 and loss is: 0.0005966408061794937\n",
      "Iteration is: 2397 and loss is: 0.0005961486021988094\n",
      "Iteration is: 2398 and loss is: 0.0005956584354862571\n",
      "Iteration is: 2399 and loss is: 0.0005951685016043484\n",
      "Iteration is: 2400 and loss is: 0.0005946785677224398\n",
      "Iteration is: 2401 and loss is: 0.0005941890995018184\n",
      "Iteration is: 2402 and loss is: 0.000593700329773128\n",
      "Iteration is: 2403 and loss is: 0.0005932148196734488\n",
      "Iteration is: 2404 and loss is: 0.0005927266320213675\n",
      "Iteration is: 2405 and loss is: 0.0005922394338995218\n",
      "Iteration is: 2406 and loss is: 0.0005917534581385553\n",
      "Iteration is: 2407 and loss is: 0.000591266667470336\n",
      "Iteration is: 2408 and loss is: 0.0005907823797315359\n",
      "Iteration is: 2409 and loss is: 0.0005902977427467704\n",
      "Iteration is: 2410 and loss is: 0.0005898142699152231\n",
      "Iteration is: 2411 and loss is: 0.000589331379160285\n",
      "Iteration is: 2412 and loss is: 0.0005888484884053469\n",
      "Iteration is: 2413 and loss is: 0.000588364724535495\n",
      "Iteration is: 2414 and loss is: 0.0005878832889720798\n",
      "Iteration is: 2415 and loss is: 0.0005874010967090726\n",
      "Iteration is: 2416 and loss is: 0.0005869233864359558\n",
      "Iteration is: 2417 and loss is: 0.0005864431150257587\n",
      "Iteration is: 2418 and loss is: 0.0005859656375832856\n",
      "Iteration is: 2419 and loss is: 0.0005854889750480652\n",
      "Iteration is: 2420 and loss is: 0.0005850141169503331\n",
      "Iteration is: 2421 and loss is: 0.0005845414125360548\n",
      "Iteration is: 2422 and loss is: 0.0005840698140673339\n",
      "Iteration is: 2423 and loss is: 0.0005836007185280323\n",
      "Iteration is: 2424 and loss is: 0.000583138782531023\n",
      "Iteration is: 2425 and loss is: 0.0005826830165460706\n",
      "Iteration is: 2426 and loss is: 0.0005822365637868643\n",
      "Iteration is: 2427 and loss is: 0.0005818011704832315\n",
      "Iteration is: 2428 and loss is: 0.0005813876632601023\n",
      "Iteration is: 2429 and loss is: 0.0005810025613754988\n",
      "Iteration is: 2430 and loss is: 0.0005806612898595631\n",
      "Iteration is: 2431 and loss is: 0.0005803898675367236\n",
      "Iteration is: 2432 and loss is: 0.0005802233354188502\n",
      "Iteration is: 2433 and loss is: 0.0005802236264571548\n",
      "Iteration is: 2434 and loss is: 0.0005804935353808105\n",
      "Iteration is: 2435 and loss is: 0.0005811955779790878\n",
      "Iteration is: 2436 and loss is: 0.0005825931439176202\n",
      "Iteration is: 2437 and loss is: 0.0005851380992680788\n",
      "Iteration is: 2438 and loss is: 0.000589586328715086\n",
      "Iteration is: 2439 and loss is: 0.0005972557701170444\n",
      "Iteration is: 2440 and loss is: 0.0006103548221290112\n",
      "Iteration is: 2441 and loss is: 0.000632729206699878\n",
      "Iteration is: 2442 and loss is: 0.0006708804285153747\n",
      "Iteration is: 2443 and loss is: 0.0007366566569544375\n",
      "Iteration is: 2444 and loss is: 0.0008501000120304525\n",
      "Iteration is: 2445 and loss is: 0.0010482461657375097\n",
      "Iteration is: 2446 and loss is: 0.0013925228267908096\n",
      "Iteration is: 2447 and loss is: 0.0019991265144199133\n",
      "Iteration is: 2448 and loss is: 0.003047310747206211\n",
      "Iteration is: 2449 and loss is: 0.004875101149082184\n",
      "Iteration is: 2450 and loss is: 0.007886461913585663\n",
      "Iteration is: 2451 and loss is: 0.012755895033478737\n",
      "Iteration is: 2452 and loss is: 0.019421398639678955\n",
      "Iteration is: 2453 and loss is: 0.027327094227075577\n",
      "Iteration is: 2454 and loss is: 0.03180555999279022\n",
      "Iteration is: 2455 and loss is: 0.029076270759105682\n",
      "Iteration is: 2456 and loss is: 0.016904762014746666\n",
      "Iteration is: 2457 and loss is: 0.004361096303910017\n",
      "Iteration is: 2458 and loss is: 0.000787809956818819\n",
      "Iteration is: 2459 and loss is: 0.0067885871976614\n",
      "Iteration is: 2460 and loss is: 0.013751646503806114\n",
      "Iteration is: 2461 and loss is: 0.012976442463696003\n",
      "Iteration is: 2462 and loss is: 0.0058316499926149845\n",
      "Iteration is: 2463 and loss is: 0.0007349286461248994\n",
      "Iteration is: 2464 and loss is: 0.002768532605841756\n",
      "Iteration is: 2465 and loss is: 0.007430304307490587\n",
      "Iteration is: 2466 and loss is: 0.0076518733985722065\n",
      "Iteration is: 2467 and loss is: 0.003456223988905549\n",
      "Iteration is: 2468 and loss is: 0.0006214617751538754\n",
      "Iteration is: 2469 and loss is: 0.0022681066766381264\n",
      "Iteration is: 2470 and loss is: 0.004922228399664164\n",
      "Iteration is: 2471 and loss is: 0.004345593973994255\n",
      "Iteration is: 2472 and loss is: 0.0016345771728083491\n",
      "Iteration is: 2473 and loss is: 0.0006560266483575106\n",
      "Iteration is: 2474 and loss is: 0.002197038382291794\n",
      "Iteration is: 2475 and loss is: 0.003368994453921914\n",
      "Iteration is: 2476 and loss is: 0.0023348939139395952\n",
      "Iteration is: 2477 and loss is: 0.0008034443017095327\n",
      "Iteration is: 2478 and loss is: 0.0008756020106375217\n",
      "Iteration is: 2479 and loss is: 0.00198350939899683\n",
      "Iteration is: 2480 and loss is: 0.0022028968669474125\n",
      "Iteration is: 2481 and loss is: 0.0012464807368814945\n",
      "Iteration is: 2482 and loss is: 0.0006038769497536123\n",
      "Iteration is: 2483 and loss is: 0.0010329505894333124\n",
      "Iteration is: 2484 and loss is: 0.0016176740173250437\n",
      "Iteration is: 2485 and loss is: 0.0014080957043915987\n",
      "Iteration is: 2486 and loss is: 0.000774452812038362\n",
      "Iteration is: 2487 and loss is: 0.0006387768080458045\n",
      "Iteration is: 2488 and loss is: 0.001038447255268693\n",
      "Iteration is: 2489 and loss is: 0.0012456118129193783\n",
      "Iteration is: 2490 and loss is: 0.0009543477790430188\n",
      "Iteration is: 2491 and loss is: 0.0006250516744330525\n",
      "Iteration is: 2492 and loss is: 0.0006908677751198411\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 2493 and loss is: 0.000948121421970427\n",
      "Iteration is: 2494 and loss is: 0.0009661478106863797\n",
      "Iteration is: 2495 and loss is: 0.0007365534547716379\n",
      "Iteration is: 2496 and loss is: 0.0005985490861348808\n",
      "Iteration is: 2497 and loss is: 0.0007025435916148126\n",
      "Iteration is: 2498 and loss is: 0.0008378862403333187\n",
      "Iteration is: 2499 and loss is: 0.0007928568520583212\n",
      "Iteration is: 2500 and loss is: 0.00064616440795362\n",
      "Iteration is: 2501 and loss is: 0.0006006400799378753\n",
      "Iteration is: 2502 and loss is: 0.0006845479947514832\n",
      "Iteration is: 2503 and loss is: 0.000747480196878314\n",
      "Iteration is: 2504 and loss is: 0.000697161303833127\n",
      "Iteration is: 2505 and loss is: 0.0006119465688243508\n",
      "Iteration is: 2506 and loss is: 0.0006018973072059453\n",
      "Iteration is: 2507 and loss is: 0.0006574355065822601\n",
      "Iteration is: 2508 and loss is: 0.0006855642423033714\n",
      "Iteration is: 2509 and loss is: 0.0006476172711700201\n",
      "Iteration is: 2510 and loss is: 0.0005990792997181416\n",
      "Iteration is: 2511 and loss is: 0.0005987138720229268\n",
      "Iteration is: 2512 and loss is: 0.0006325930589810014\n",
      "Iteration is: 2513 and loss is: 0.0006462773308157921\n",
      "Iteration is: 2514 and loss is: 0.0006218315102159977\n",
      "Iteration is: 2515 and loss is: 0.0005933515494689345\n",
      "Iteration is: 2516 and loss is: 0.0005936151137575507\n",
      "Iteration is: 2517 and loss is: 0.0006135195726528764\n",
      "Iteration is: 2518 and loss is: 0.0006217087502591312\n",
      "Iteration is: 2519 and loss is: 0.0006075370474718511\n",
      "Iteration is: 2520 and loss is: 0.0005900016985833645\n",
      "Iteration is: 2521 and loss is: 0.0005886366707272828\n",
      "Iteration is: 2522 and loss is: 0.0005998439155519009\n",
      "Iteration is: 2523 and loss is: 0.0006058161961846054\n",
      "Iteration is: 2524 and loss is: 0.0005985699244774878\n",
      "Iteration is: 2525 and loss is: 0.0005874677444808185\n",
      "Iteration is: 2526 and loss is: 0.0005846609710715711\n",
      "Iteration is: 2527 and loss is: 0.0005903223645873368\n",
      "Iteration is: 2528 and loss is: 0.000594903773162514\n",
      "Iteration is: 2529 and loss is: 0.0005920567782595754\n",
      "Iteration is: 2530 and loss is: 0.0005852184258401394\n",
      "Iteration is: 2531 and loss is: 0.0005817299243062735\n",
      "Iteration is: 2532 and loss is: 0.0005837731878273189\n",
      "Iteration is: 2533 and loss is: 0.0005869855522178113\n",
      "Iteration is: 2534 and loss is: 0.0005866213468834758\n",
      "Iteration is: 2535 and loss is: 0.0005828486173413694\n",
      "Iteration is: 2536 and loss is: 0.000579552841372788\n",
      "Iteration is: 2537 and loss is: 0.0005793778691440821\n",
      "Iteration is: 2538 and loss is: 0.0005811233422718942\n",
      "Iteration is: 2539 and loss is: 0.0005818012868985534\n",
      "Iteration is: 2540 and loss is: 0.0005801711813546717\n",
      "Iteration is: 2541 and loss is: 0.0005776824546046555\n",
      "Iteration is: 2542 and loss is: 0.0005763919907622039\n",
      "Iteration is: 2543 and loss is: 0.0005767440306954086\n",
      "Iteration is: 2544 and loss is: 0.0005774328601546586\n",
      "Iteration is: 2545 and loss is: 0.0005770842544734478\n",
      "Iteration is: 2546 and loss is: 0.000575645943172276\n",
      "Iteration is: 2547 and loss is: 0.0005741911008954048\n",
      "Iteration is: 2548 and loss is: 0.0005735920276492834\n",
      "Iteration is: 2549 and loss is: 0.0005737227038480341\n",
      "Iteration is: 2550 and loss is: 0.0005737813771702349\n",
      "Iteration is: 2551 and loss is: 0.0005732069839723408\n",
      "Iteration is: 2552 and loss is: 0.0005721579655073583\n",
      "Iteration is: 2553 and loss is: 0.0005712179699912667\n",
      "Iteration is: 2554 and loss is: 0.0005707601085305214\n",
      "Iteration is: 2555 and loss is: 0.0005706362426280975\n",
      "Iteration is: 2556 and loss is: 0.0005704302457161248\n",
      "Iteration is: 2557 and loss is: 0.0005698867607861757\n",
      "Iteration is: 2558 and loss is: 0.0005691104452125728\n",
      "Iteration is: 2559 and loss is: 0.0005683904746547341\n",
      "Iteration is: 2560 and loss is: 0.0005679106106981635\n",
      "Iteration is: 2561 and loss is: 0.0005676135187968612\n",
      "Iteration is: 2562 and loss is: 0.0005673005362041295\n",
      "Iteration is: 2563 and loss is: 0.0005668295198120177\n",
      "Iteration is: 2564 and loss is: 0.0005662229377776384\n",
      "Iteration is: 2565 and loss is: 0.0005656133289448917\n",
      "Iteration is: 2566 and loss is: 0.0005651087849400938\n",
      "Iteration is: 2567 and loss is: 0.0005647126818075776\n",
      "Iteration is: 2568 and loss is: 0.0005643446929752827\n",
      "Iteration is: 2569 and loss is: 0.000563917972613126\n",
      "Iteration is: 2570 and loss is: 0.0005634105764329433\n",
      "Iteration is: 2571 and loss is: 0.0005628676735796034\n",
      "Iteration is: 2572 and loss is: 0.000562356086447835\n",
      "Iteration is: 2573 and loss is: 0.000561905384529382\n",
      "Iteration is: 2574 and loss is: 0.0005614929832518101\n",
      "Iteration is: 2575 and loss is: 0.0005610751104541123\n",
      "Iteration is: 2576 and loss is: 0.0005606201593764126\n",
      "Iteration is: 2577 and loss is: 0.0005601319717243314\n",
      "Iteration is: 2578 and loss is: 0.0005596379633061588\n",
      "Iteration is: 2579 and loss is: 0.0005591660155914724\n",
      "Iteration is: 2580 and loss is: 0.000558720959816128\n",
      "Iteration is: 2581 and loss is: 0.000558289757464081\n",
      "Iteration is: 2582 and loss is: 0.0005578559939749539\n",
      "Iteration is: 2583 and loss is: 0.0005574029055424035\n",
      "Iteration is: 2584 and loss is: 0.0005569364293478429\n",
      "Iteration is: 2585 and loss is: 0.0005564659368246794\n",
      "Iteration is: 2586 and loss is: 0.000556008133571595\n",
      "Iteration is: 2587 and loss is: 0.0005555627285502851\n",
      "Iteration is: 2588 and loss is: 0.0005551245994865894\n",
      "Iteration is: 2589 and loss is: 0.0005546854226849973\n",
      "Iteration is: 2590 and loss is: 0.0005542404833249748\n",
      "Iteration is: 2591 and loss is: 0.0005537873948924243\n",
      "Iteration is: 2592 and loss is: 0.0005533319199457765\n",
      "Iteration is: 2593 and loss is: 0.0005528781330212951\n",
      "Iteration is: 2594 and loss is: 0.0005524317384697497\n",
      "Iteration is: 2595 and loss is: 0.0005519906990230083\n",
      "Iteration is: 2596 and loss is: 0.0005515536176972091\n",
      "Iteration is: 2597 and loss is: 0.0005511139170266688\n",
      "Iteration is: 2598 and loss is: 0.0005506703164428473\n",
      "Iteration is: 2599 and loss is: 0.0005502243293449283\n",
      "Iteration is: 2600 and loss is: 0.0005497773527167737\n",
      "Iteration is: 2601 and loss is: 0.0005493316566571593\n",
      "Iteration is: 2602 and loss is: 0.0005488888709805906\n",
      "Iteration is: 2603 and loss is: 0.0005484491121023893\n",
      "Iteration is: 2604 and loss is: 0.0005480108084157109\n",
      "Iteration is: 2605 and loss is: 0.0005475720390677452\n",
      "Iteration is: 2606 and loss is: 0.000547133618965745\n",
      "Iteration is: 2607 and loss is: 0.000546694325748831\n",
      "Iteration is: 2608 and loss is: 0.000546254392247647\n",
      "Iteration is: 2609 and loss is: 0.0005458152736537158\n",
      "Iteration is: 2610 and loss is: 0.0005453777848742902\n",
      "Iteration is: 2611 and loss is: 0.0005449417512863874\n",
      "Iteration is: 2612 and loss is: 0.0005445049609988928\n",
      "Iteration is: 2613 and loss is: 0.0005440699751488864\n",
      "Iteration is: 2614 and loss is: 0.0005436351057142019\n",
      "Iteration is: 2615 and loss is: 0.0005432005855254829\n",
      "Iteration is: 2616 and loss is: 0.0005427670548669994\n",
      "Iteration is: 2617 and loss is: 0.0005423330585472286\n",
      "Iteration is: 2618 and loss is: 0.0005418997607193887\n",
      "Iteration is: 2619 and loss is: 0.0005414679180830717\n",
      "Iteration is: 2620 and loss is: 0.00054103450383991\n",
      "Iteration is: 2621 and loss is: 0.0005406017880886793\n",
      "Iteration is: 2622 and loss is: 0.000540170178283006\n",
      "Iteration is: 2623 and loss is: 0.0005397398490458727\n",
      "Iteration is: 2624 and loss is: 0.0005393092869780958\n",
      "Iteration is: 2625 and loss is: 0.000538880005478859\n",
      "Iteration is: 2626 and loss is: 0.0005384498508647084\n",
      "Iteration is: 2627 and loss is: 0.000538021617103368\n",
      "Iteration is: 2628 and loss is: 0.0005375918699428439\n",
      "Iteration is: 2629 and loss is: 0.0005371645675040781\n",
      "Iteration is: 2630 and loss is: 0.0005367376143112779\n",
      "Iteration is: 2631 and loss is: 0.0005363110103644431\n",
      "Iteration is: 2632 and loss is: 0.00053588388254866\n",
      "Iteration is: 2633 and loss is: 0.0005354578606784344\n",
      "Iteration is: 2634 and loss is: 0.0005350319552235305\n",
      "Iteration is: 2635 and loss is: 0.0005346061661839485\n",
      "Iteration is: 2636 and loss is: 0.0005341815995052457\n",
      "Iteration is: 2637 and loss is: 0.000533757614903152\n",
      "Iteration is: 2638 and loss is: 0.0005333332810550928\n",
      "Iteration is: 2639 and loss is: 0.0005329101695679128\n",
      "Iteration is: 2640 and loss is: 0.0005324876983650029\n",
      "Iteration is: 2641 and loss is: 0.0005320647032931447\n",
      "Iteration is: 2642 and loss is: 0.0005316425231285393\n",
      "Iteration is: 2643 and loss is: 0.000531221681740135\n",
      "Iteration is: 2644 and loss is: 0.0005307990941219032\n",
      "Iteration is: 2645 and loss is: 0.0005303797661326826\n",
      "Iteration is: 2646 and loss is: 0.0005299596232362092\n",
      "Iteration is: 2647 and loss is: 0.0005295398295857012\n",
      "Iteration is: 2648 and loss is: 0.0005291207344271243\n",
      "Iteration is: 2649 and loss is: 0.0005287021631374955\n",
      "Iteration is: 2650 and loss is: 0.0005282823112793267\n",
      "Iteration is: 2651 and loss is: 0.0005278652533888817\n",
      "Iteration is: 2652 and loss is: 0.0005274481954984367\n",
      "Iteration is: 2653 and loss is: 0.0005270300898700953\n",
      "Iteration is: 2654 and loss is: 0.000526614545378834\n",
      "Iteration is: 2655 and loss is: 0.0005261986516416073\n",
      "Iteration is: 2656 and loss is: 0.0005257828161120415\n",
      "Iteration is: 2657 and loss is: 0.000525368086528033\n",
      "Iteration is: 2658 and loss is: 0.000524954404681921\n",
      "Iteration is: 2659 and loss is: 0.0005245403153821826\n",
      "Iteration is: 2660 and loss is: 0.0005241277976892889\n",
      "Iteration is: 2661 and loss is: 0.0005237151053734124\n",
      "Iteration is: 2662 and loss is: 0.0005233047413639724\n",
      "Iteration is: 2663 and loss is: 0.0005228942027315497\n",
      "Iteration is: 2664 and loss is: 0.0005224847118370235\n",
      "Iteration is: 2665 and loss is: 0.0005220785969868302\n",
      "Iteration is: 2666 and loss is: 0.0005216735298745334\n",
      "Iteration is: 2667 and loss is: 0.0005212710238993168\n",
      "Iteration is: 2668 and loss is: 0.0005208750371821225\n",
      "Iteration is: 2669 and loss is: 0.0005204857443459332\n",
      "Iteration is: 2670 and loss is: 0.0005201061721891165\n",
      "Iteration is: 2671 and loss is: 0.0005197418504394591\n",
      "Iteration is: 2672 and loss is: 0.0005193985416553915\n",
      "Iteration is: 2673 and loss is: 0.0005190935917198658\n",
      "Iteration is: 2674 and loss is: 0.0005188502836972475\n",
      "Iteration is: 2675 and loss is: 0.0005187052302062511\n",
      "Iteration is: 2676 and loss is: 0.0005187204224057496\n",
      "Iteration is: 2677 and loss is: 0.0005190016236156225\n",
      "Iteration is: 2678 and loss is: 0.0005197270656935871\n",
      "Iteration is: 2679 and loss is: 0.0005211930838413537\n",
      "Iteration is: 2680 and loss is: 0.0005239085876382887\n",
      "Iteration is: 2681 and loss is: 0.0005287548410706222\n",
      "Iteration is: 2682 and loss is: 0.0005372592713683844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 2683 and loss is: 0.0005521580460481346\n",
      "Iteration is: 2684 and loss is: 0.0005781860090792179\n",
      "Iteration is: 2685 and loss is: 0.0006239385693334043\n",
      "Iteration is: 2686 and loss is: 0.00070444296579808\n",
      "Iteration is: 2687 and loss is: 0.0008477212395519018\n",
      "Iteration is: 2688 and loss is: 0.0011021662503480911\n",
      "Iteration is: 2689 and loss is: 0.0015603514621034265\n",
      "Iteration is: 2690 and loss is: 0.0023758907336741686\n",
      "Iteration is: 2691 and loss is: 0.0038447980768978596\n",
      "Iteration is: 2692 and loss is: 0.006392217241227627\n",
      "Iteration is: 2693 and loss is: 0.010793188586831093\n",
      "Iteration is: 2694 and loss is: 0.017587881535291672\n",
      "Iteration is: 2695 and loss is: 0.027294635772705078\n",
      "Iteration is: 2696 and loss is: 0.03664914891123772\n",
      "Iteration is: 2697 and loss is: 0.04094493389129639\n",
      "Iteration is: 2698 and loss is: 0.03174440562725067\n",
      "Iteration is: 2699 and loss is: 0.014191880822181702\n",
      "Iteration is: 2700 and loss is: 0.0014849747531116009\n",
      "Iteration is: 2701 and loss is: 0.003567593405023217\n",
      "Iteration is: 2702 and loss is: 0.01404573768377304\n",
      "Iteration is: 2703 and loss is: 0.018332211300730705\n",
      "Iteration is: 2704 and loss is: 0.011328388936817646\n",
      "Iteration is: 2705 and loss is: 0.001999013125896454\n",
      "Iteration is: 2706 and loss is: 0.0017920280806720257\n",
      "Iteration is: 2707 and loss is: 0.008254863321781158\n",
      "Iteration is: 2708 and loss is: 0.01038435474038124\n",
      "Iteration is: 2709 and loss is: 0.005273730494081974\n",
      "Iteration is: 2710 and loss is: 0.0006811066996306181\n",
      "Iteration is: 2711 and loss is: 0.0025565042160451412\n",
      "Iteration is: 2712 and loss is: 0.0063312421552836895\n",
      "Iteration is: 2713 and loss is: 0.005420302972197533\n",
      "Iteration is: 2714 and loss is: 0.0016228391323238611\n",
      "Iteration is: 2715 and loss is: 0.0007962172385305166\n",
      "Iteration is: 2716 and loss is: 0.0032566701993346214\n",
      "Iteration is: 2717 and loss is: 0.0042358264327049255\n",
      "Iteration is: 2718 and loss is: 0.0021339734084904194\n",
      "Iteration is: 2719 and loss is: 0.0005577048868872225\n",
      "Iteration is: 2720 and loss is: 0.0016166989225894213\n",
      "Iteration is: 2721 and loss is: 0.002897155936807394\n",
      "Iteration is: 2722 and loss is: 0.002129953820258379\n",
      "Iteration is: 2723 and loss is: 0.000716471578925848\n",
      "Iteration is: 2724 and loss is: 0.0008638539002276957\n",
      "Iteration is: 2725 and loss is: 0.001888528000563383\n",
      "Iteration is: 2726 and loss is: 0.0018487180350348353\n",
      "Iteration is: 2727 and loss is: 0.000889558345079422\n",
      "Iteration is: 2728 and loss is: 0.0005942924763076007\n",
      "Iteration is: 2729 and loss is: 0.0012073260731995106\n",
      "Iteration is: 2730 and loss is: 0.0014954686630517244\n",
      "Iteration is: 2731 and loss is: 0.00097829126752913\n",
      "Iteration is: 2732 and loss is: 0.0005563385202549398\n",
      "Iteration is: 2733 and loss is: 0.0008108050096780062\n",
      "Iteration is: 2734 and loss is: 0.0011520457919687033\n",
      "Iteration is: 2735 and loss is: 0.000975143862888217\n",
      "Iteration is: 2736 and loss is: 0.0006060774903744459\n",
      "Iteration is: 2737 and loss is: 0.0006164752412587404\n",
      "Iteration is: 2738 and loss is: 0.0008792763110250235\n",
      "Iteration is: 2739 and loss is: 0.0009022118174470961\n",
      "Iteration is: 2740 and loss is: 0.0006643532542511821\n",
      "Iteration is: 2741 and loss is: 0.0005533005460165441\n",
      "Iteration is: 2742 and loss is: 0.000690528133418411\n",
      "Iteration is: 2743 and loss is: 0.0007958011701703072\n",
      "Iteration is: 2744 and loss is: 0.0006931059760972857\n",
      "Iteration is: 2745 and loss is: 0.0005596644477918744\n",
      "Iteration is: 2746 and loss is: 0.0005862966645509005\n",
      "Iteration is: 2747 and loss is: 0.0006871561636216938\n",
      "Iteration is: 2748 and loss is: 0.0006829785997979343\n",
      "Iteration is: 2749 and loss is: 0.0005871744942851365\n",
      "Iteration is: 2750 and loss is: 0.0005487609887495637\n",
      "Iteration is: 2751 and loss is: 0.0006035921396687627\n",
      "Iteration is: 2752 and loss is: 0.0006436884286813438\n",
      "Iteration is: 2753 and loss is: 0.0006046071066521108\n",
      "Iteration is: 2754 and loss is: 0.0005508401081897318\n",
      "Iteration is: 2755 and loss is: 0.0005566509789787233\n",
      "Iteration is: 2756 and loss is: 0.0005960811395198107\n",
      "Iteration is: 2757 and loss is: 0.0006002042209729552\n",
      "Iteration is: 2758 and loss is: 0.0005646687350235879\n",
      "Iteration is: 2759 and loss is: 0.000542943540494889\n",
      "Iteration is: 2760 and loss is: 0.0005589724169112742\n",
      "Iteration is: 2761 and loss is: 0.0005790253053419292\n",
      "Iteration is: 2762 and loss is: 0.0005705561488866806\n",
      "Iteration is: 2763 and loss is: 0.0005475476500578225\n",
      "Iteration is: 2764 and loss is: 0.0005417350912466645\n",
      "Iteration is: 2765 and loss is: 0.000555178732611239\n",
      "Iteration is: 2766 and loss is: 0.0005632330430671573\n",
      "Iteration is: 2767 and loss is: 0.0005535492091439664\n",
      "Iteration is: 2768 and loss is: 0.0005404902622103691\n",
      "Iteration is: 2769 and loss is: 0.0005403510294854641\n",
      "Iteration is: 2770 and loss is: 0.0005490294424816966\n",
      "Iteration is: 2771 and loss is: 0.0005515092052519321\n",
      "Iteration is: 2772 and loss is: 0.0005440405220724642\n",
      "Iteration is: 2773 and loss is: 0.0005368931451812387\n",
      "Iteration is: 2774 and loss is: 0.0005380024667829275\n",
      "Iteration is: 2775 and loss is: 0.0005430446472018957\n",
      "Iteration is: 2776 and loss is: 0.000543489120900631\n",
      "Iteration is: 2777 and loss is: 0.000538461550604552\n",
      "Iteration is: 2778 and loss is: 0.0005343335797078907\n",
      "Iteration is: 2779 and loss is: 0.0005351422005333006\n",
      "Iteration is: 2780 and loss is: 0.0005379221402108669\n",
      "Iteration is: 2781 and loss is: 0.0005378482746891677\n",
      "Iteration is: 2782 and loss is: 0.0005346541875042021\n",
      "Iteration is: 2783 and loss is: 0.0005320376367308199\n",
      "Iteration is: 2784 and loss is: 0.000532297941390425\n",
      "Iteration is: 2785 and loss is: 0.0005337732727639377\n",
      "Iteration is: 2786 and loss is: 0.000533673504833132\n",
      "Iteration is: 2787 and loss is: 0.0005316811730153859\n",
      "Iteration is: 2788 and loss is: 0.0005298416363075376\n",
      "Iteration is: 2789 and loss is: 0.0005296427407301962\n",
      "Iteration is: 2790 and loss is: 0.0005303496145643294\n",
      "Iteration is: 2791 and loss is: 0.0005303138168528676\n",
      "Iteration is: 2792 and loss is: 0.0005290924455039203\n",
      "Iteration is: 2793 and loss is: 0.0005277126329019666\n",
      "Iteration is: 2794 and loss is: 0.0005271958070807159\n",
      "Iteration is: 2795 and loss is: 0.0005273853312246501\n",
      "Iteration is: 2796 and loss is: 0.0005273583228699863\n",
      "Iteration is: 2797 and loss is: 0.0005266288062557578\n",
      "Iteration is: 2798 and loss is: 0.0005256038857623935\n",
      "Iteration is: 2799 and loss is: 0.0005249480600468814\n",
      "Iteration is: 2800 and loss is: 0.0005247863009572029\n",
      "Iteration is: 2801 and loss is: 0.0005246833898127079\n",
      "Iteration is: 2802 and loss is: 0.0005242330371402204\n",
      "Iteration is: 2803 and loss is: 0.000523491355124861\n",
      "Iteration is: 2804 and loss is: 0.000522819347679615\n",
      "Iteration is: 2805 and loss is: 0.0005224401247687638\n",
      "Iteration is: 2806 and loss is: 0.0005222250474616885\n",
      "Iteration is: 2807 and loss is: 0.0005218947771936655\n",
      "Iteration is: 2808 and loss is: 0.0005213542026467621\n",
      "Iteration is: 2809 and loss is: 0.0005207455833442509\n",
      "Iteration is: 2810 and loss is: 0.0005202585598453879\n",
      "Iteration is: 2811 and loss is: 0.0005199222359806299\n",
      "Iteration is: 2812 and loss is: 0.0005196079146116972\n",
      "Iteration is: 2813 and loss is: 0.0005191948148421943\n",
      "Iteration is: 2814 and loss is: 0.0005186829366721213\n",
      "Iteration is: 2815 and loss is: 0.0005181736778467894\n",
      "Iteration is: 2816 and loss is: 0.0005177425337024033\n",
      "Iteration is: 2817 and loss is: 0.0005173826357349753\n",
      "Iteration is: 2818 and loss is: 0.0005170174408704042\n",
      "Iteration is: 2819 and loss is: 0.0005165926995687187\n",
      "Iteration is: 2820 and loss is: 0.0005161233129911125\n",
      "Iteration is: 2821 and loss is: 0.0005156603874638677\n",
      "Iteration is: 2822 and loss is: 0.0005152428639121354\n",
      "Iteration is: 2823 and loss is: 0.0005148597410880029\n",
      "Iteration is: 2824 and loss is: 0.0005144744063727558\n",
      "Iteration is: 2825 and loss is: 0.0005140574648976326\n",
      "Iteration is: 2826 and loss is: 0.0005136155523359776\n",
      "Iteration is: 2827 and loss is: 0.0005131762591190636\n",
      "Iteration is: 2828 and loss is: 0.0005127617041580379\n",
      "Iteration is: 2829 and loss is: 0.000512363447342068\n",
      "Iteration is: 2830 and loss is: 0.0005119683337397873\n",
      "Iteration is: 2831 and loss is: 0.0005115595413371921\n",
      "Iteration is: 2832 and loss is: 0.0005111385835334659\n",
      "Iteration is: 2833 and loss is: 0.0005107158212922513\n",
      "Iteration is: 2834 and loss is: 0.0005103023140691221\n",
      "Iteration is: 2835 and loss is: 0.0005098992842249572\n",
      "Iteration is: 2836 and loss is: 0.0005095001542940736\n",
      "Iteration is: 2837 and loss is: 0.0005090964259579778\n",
      "Iteration is: 2838 and loss is: 0.000508685305248946\n",
      "Iteration is: 2839 and loss is: 0.0005082730785943568\n",
      "Iteration is: 2840 and loss is: 0.000507861957885325\n",
      "Iteration is: 2841 and loss is: 0.0005074574728496373\n",
      "Iteration is: 2842 and loss is: 0.0005070577608421445\n",
      "Iteration is: 2843 and loss is: 0.0005066570010967553\n",
      "Iteration is: 2844 and loss is: 0.0005062546697445214\n",
      "Iteration is: 2845 and loss is: 0.000505848613101989\n",
      "Iteration is: 2846 and loss is: 0.0005054433131590486\n",
      "Iteration is: 2847 and loss is: 0.0005050393519923091\n",
      "Iteration is: 2848 and loss is: 0.0005046387086622417\n",
      "Iteration is: 2849 and loss is: 0.000504239636939019\n",
      "Iteration is: 2850 and loss is: 0.000503841380123049\n",
      "Iteration is: 2851 and loss is: 0.0005034406785853207\n",
      "Iteration is: 2852 and loss is: 0.0005030398606322706\n",
      "Iteration is: 2853 and loss is: 0.0005026384606026113\n",
      "Iteration is: 2854 and loss is: 0.0005022382247261703\n",
      "Iteration is: 2855 and loss is: 0.0005018413648940623\n",
      "Iteration is: 2856 and loss is: 0.0005014447961002588\n",
      "Iteration is: 2857 and loss is: 0.0005010479944758117\n",
      "Iteration is: 2858 and loss is: 0.0005006524734199047\n",
      "Iteration is: 2859 and loss is: 0.0005002558464184403\n",
      "Iteration is: 2860 and loss is: 0.0004998575313948095\n",
      "Iteration is: 2861 and loss is: 0.0004994604387320578\n",
      "Iteration is: 2862 and loss is: 0.0004990662564523518\n",
      "Iteration is: 2863 and loss is: 0.0004986718413420022\n",
      "Iteration is: 2864 and loss is: 0.0004982787650078535\n",
      "Iteration is: 2865 and loss is: 0.0004978870274499059\n",
      "Iteration is: 2866 and loss is: 0.0004974931362085044\n",
      "Iteration is: 2867 and loss is: 0.0004971004091203213\n",
      "Iteration is: 2868 and loss is: 0.0004967063432559371\n",
      "Iteration is: 2869 and loss is: 0.0004963137907907367\n",
      "Iteration is: 2870 and loss is: 0.0004959222860634327\n",
      "Iteration is: 2871 and loss is: 0.0004955324693582952\n",
      "Iteration is: 2872 and loss is: 0.0004951435839757323\n",
      "Iteration is: 2873 and loss is: 0.0004947540583088994\n",
      "Iteration is: 2874 and loss is: 0.0004943633684888482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 2875 and loss is: 0.0004939734353683889\n",
      "Iteration is: 2876 and loss is: 0.0004935833858326077\n",
      "Iteration is: 2877 and loss is: 0.0004931945586577058\n",
      "Iteration is: 2878 and loss is: 0.0004928067210130394\n",
      "Iteration is: 2879 and loss is: 0.0004924198728986084\n",
      "Iteration is: 2880 and loss is: 0.0004920341889373958\n",
      "Iteration is: 2881 and loss is: 0.0004916468751616776\n",
      "Iteration is: 2882 and loss is: 0.0004912613076157868\n",
      "Iteration is: 2883 and loss is: 0.0004908755072392523\n",
      "Iteration is: 2884 and loss is: 0.000490489648655057\n",
      "Iteration is: 2885 and loss is: 0.0004901057691313326\n",
      "Iteration is: 2886 and loss is: 0.000489721423946321\n",
      "Iteration is: 2887 and loss is: 0.0004893358564004302\n",
      "Iteration is: 2888 and loss is: 0.0004889528499916196\n",
      "Iteration is: 2889 and loss is: 0.0004885697271674871\n",
      "Iteration is: 2890 and loss is: 0.0004881859931629151\n",
      "Iteration is: 2891 and loss is: 0.00048780374345369637\n",
      "Iteration is: 2892 and loss is: 0.00048742166836746037\n",
      "Iteration is: 2893 and loss is: 0.00048704049549996853\n",
      "Iteration is: 2894 and loss is: 0.0004866582457907498\n",
      "Iteration is: 2895 and loss is: 0.0004862776549998671\n",
      "Iteration is: 2896 and loss is: 0.00048589767538942397\n",
      "Iteration is: 2897 and loss is: 0.000485517579363659\n",
      "Iteration is: 2898 and loss is: 0.00048513905494473875\n",
      "Iteration is: 2899 and loss is: 0.00048475907533429563\n",
      "Iteration is: 2900 and loss is: 0.0004843815404456109\n",
      "Iteration is: 2901 and loss is: 0.0004840028996113688\n",
      "Iteration is: 2902 and loss is: 0.000483624724438414\n",
      "Iteration is: 2903 and loss is: 0.00048324844101443887\n",
      "Iteration is: 2904 and loss is: 0.0004828707023989409\n",
      "Iteration is: 2905 and loss is: 0.0004824943607673049\n",
      "Iteration is: 2906 and loss is: 0.0004821195325348526\n",
      "Iteration is: 2907 and loss is: 0.00048174330731853843\n",
      "Iteration is: 2908 and loss is: 0.0004813677223864943\n",
      "Iteration is: 2909 and loss is: 0.0004809931560885161\n",
      "Iteration is: 2910 and loss is: 0.0004806185606867075\n",
      "Iteration is: 2911 and loss is: 0.00048024384886957705\n",
      "Iteration is: 2912 and loss is: 0.00047986983554437757\n",
      "Iteration is: 2913 and loss is: 0.00047949596773833036\n",
      "Iteration is: 2914 and loss is: 0.00047912224545143545\n",
      "Iteration is: 2915 and loss is: 0.0004787506186403334\n",
      "Iteration is: 2916 and loss is: 0.00047837907914072275\n",
      "Iteration is: 2917 and loss is: 0.00047800689935684204\n",
      "Iteration is: 2918 and loss is: 0.0004776359419338405\n",
      "Iteration is: 2919 and loss is: 0.00047726527554914355\n",
      "Iteration is: 2920 and loss is: 0.00047689530765637755\n",
      "Iteration is: 2921 and loss is: 0.0004765257181134075\n",
      "Iteration is: 2922 and loss is: 0.0004761562158819288\n",
      "Iteration is: 2923 and loss is: 0.00047578683006577194\n",
      "Iteration is: 2924 and loss is: 0.0004754196561407298\n",
      "Iteration is: 2925 and loss is: 0.00047505248221568763\n",
      "Iteration is: 2926 and loss is: 0.000474685279186815\n",
      "Iteration is: 2927 and loss is: 0.00047432034625671804\n",
      "Iteration is: 2928 and loss is: 0.0004739557043649256\n",
      "Iteration is: 2929 and loss is: 0.0004735918191727251\n",
      "Iteration is: 2930 and loss is: 0.0004732295637950301\n",
      "Iteration is: 2931 and loss is: 0.0004728688218165189\n",
      "Iteration is: 2932 and loss is: 0.0004725106409750879\n",
      "Iteration is: 2933 and loss is: 0.0004721579316537827\n",
      "Iteration is: 2934 and loss is: 0.0004718108393717557\n",
      "Iteration is: 2935 and loss is: 0.0004714711685664952\n",
      "Iteration is: 2936 and loss is: 0.0004711421497631818\n",
      "Iteration is: 2937 and loss is: 0.0004708297783508897\n",
      "Iteration is: 2938 and loss is: 0.0004705398459918797\n",
      "Iteration is: 2939 and loss is: 0.00047028620610944927\n",
      "Iteration is: 2940 and loss is: 0.0004700955469161272\n",
      "Iteration is: 2941 and loss is: 0.00046999938786029816\n",
      "Iteration is: 2942 and loss is: 0.0004700481949839741\n",
      "Iteration is: 2943 and loss is: 0.00047032738802954555\n",
      "Iteration is: 2944 and loss is: 0.0004709749191533774\n",
      "Iteration is: 2945 and loss is: 0.0004722288576886058\n",
      "Iteration is: 2946 and loss is: 0.0004744773614220321\n",
      "Iteration is: 2947 and loss is: 0.00047838303726166487\n",
      "Iteration is: 2948 and loss is: 0.00048505852464586496\n",
      "Iteration is: 2949 and loss is: 0.0004964452236890793\n",
      "Iteration is: 2950 and loss is: 0.0005158106214366853\n",
      "Iteration is: 2951 and loss is: 0.0005489400355145335\n",
      "Iteration is: 2952 and loss is: 0.0006057083955965936\n",
      "Iteration is: 2953 and loss is: 0.0007040717173367739\n",
      "Iteration is: 2954 and loss is: 0.0008743353537283838\n",
      "Iteration is: 2955 and loss is: 0.0011731323320418596\n",
      "Iteration is: 2956 and loss is: 0.0016940368805080652\n",
      "Iteration is: 2957 and loss is: 0.0026152327191084623\n",
      "Iteration is: 2958 and loss is: 0.004206354729831219\n",
      "Iteration is: 2959 and loss is: 0.00697722565382719\n",
      "Iteration is: 2960 and loss is: 0.01148837711662054\n",
      "Iteration is: 2961 and loss is: 0.018652940168976784\n",
      "Iteration is: 2962 and loss is: 0.02797159180045128\n",
      "Iteration is: 2963 and loss is: 0.03805378079414368\n",
      "Iteration is: 2964 and loss is: 0.04146232455968857\n",
      "Iteration is: 2965 and loss is: 0.033983003348112106\n",
      "Iteration is: 2966 and loss is: 0.01608695089817047\n",
      "Iteration is: 2967 and loss is: 0.0022279981058090925\n",
      "Iteration is: 2968 and loss is: 0.002437075600028038\n",
      "Iteration is: 2969 and loss is: 0.012373127974569798\n",
      "Iteration is: 2970 and loss is: 0.018735386431217194\n",
      "Iteration is: 2971 and loss is: 0.013300370424985886\n",
      "Iteration is: 2972 and loss is: 0.0034545997623354197\n",
      "Iteration is: 2973 and loss is: 0.0008306464878842235\n",
      "Iteration is: 2974 and loss is: 0.006442904472351074\n",
      "Iteration is: 2975 and loss is: 0.010626820847392082\n",
      "Iteration is: 2976 and loss is: 0.007088399492204189\n",
      "Iteration is: 2977 and loss is: 0.0014461260288953781\n",
      "Iteration is: 2978 and loss is: 0.001251646550372243\n",
      "Iteration is: 2979 and loss is: 0.005151489283889532\n",
      "Iteration is: 2980 and loss is: 0.006274238228797913\n",
      "Iteration is: 2981 and loss is: 0.002954126801341772\n",
      "Iteration is: 2982 and loss is: 0.0005199672887101769\n",
      "Iteration is: 2983 and loss is: 0.002007512142881751\n",
      "Iteration is: 2984 and loss is: 0.00407975260168314\n",
      "Iteration is: 2985 and loss is: 0.00320767005905509\n",
      "Iteration is: 2986 and loss is: 0.0009538822341710329\n",
      "Iteration is: 2987 and loss is: 0.0007713254308328032\n",
      "Iteration is: 2988 and loss is: 0.002311739372089505\n",
      "Iteration is: 2989 and loss is: 0.0026679823640733957\n",
      "Iteration is: 2990 and loss is: 0.001338050584308803\n",
      "Iteration is: 2991 and loss is: 0.0005083203432150185\n",
      "Iteration is: 2992 and loss is: 0.0012146285735070705\n",
      "Iteration is: 2993 and loss is: 0.0019435740541666746\n",
      "Iteration is: 2994 and loss is: 0.0014375762548297644\n",
      "Iteration is: 2995 and loss is: 0.0006043996545486152\n",
      "Iteration is: 2996 and loss is: 0.0006864234455861151\n",
      "Iteration is: 2997 and loss is: 0.0012943006586283445\n",
      "Iteration is: 2998 and loss is: 0.0013168654404580593\n",
      "Iteration is: 2999 and loss is: 0.0007547112181782722\n",
      "Iteration is: 3000 and loss is: 0.0005166635964997113\n",
      "Training time: 4.1337\n"
     ]
    }
   ],
   "source": [
    "#Using Vanilla PINN\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, 'Utilities/')\n",
    "import os\n",
    "\n",
    "from scipy.interpolate import griddata\n",
    "from pyDOE import lhs\n",
    "from plotting import newfig, savefig\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import scipy.io\n",
    "\n",
    "np.random.seed(seed=1234)\n",
    "tf.random.set_seed(1234)\n",
    "tf.config.experimental.enable_tensor_float_32_execution(False)\n",
    "lb = -1\n",
    "ub = 1   \n",
    "\n",
    "# Initalization of Network\n",
    "def hyper_initial(size):\n",
    "    in_dim = size[0]\n",
    "    out_dim = size[1]\n",
    "    std = np.sqrt(2.0/(in_dim + out_dim))\n",
    "    return tf.Variable(tf.random.truncated_normal(shape=size, stddev = std))\n",
    "\n",
    "# Neural Network \n",
    "def DNN(X, W, b):\n",
    "    A = 2.0*(X - lb)/(ub - lb) - 1.0\n",
    "    L = len(W)\n",
    "    for i in range(L-1):\n",
    "        A = tf.tanh(tf.add(tf.matmul(A, W[i]), b[i]))\n",
    "    Y = tf.add(tf.matmul(A, W[-1]), b[-1])\n",
    "    return Y\n",
    "\n",
    "def train_vars(W, b):\n",
    "    return W + b\n",
    "\n",
    "def net_u(x,w, b):\n",
    "    u = DNN(x, w, b)\n",
    "    return u\n",
    "\n",
    "\n",
    "#@tf.function(jit_compile=True)\n",
    "@tf.function\n",
    "def net_f(x,W, b, nu):\n",
    "    with tf.GradientTape(persistent=True) as tape1:\n",
    "        tape1.watch([x])\n",
    "        with tf.GradientTape(persistent=True) as tape2:\n",
    "            tape2.watch([x])\n",
    "            u=net_u(x, W, b)\n",
    "        u_x = tape2.gradient(u, x)\n",
    "        del tape2\n",
    "    u_xx = tape1.gradient(u_x, x)  \n",
    "    del tape1\n",
    "    f = u_xx - (1/nu)*u-(1/nu)*tf.exp(x)\n",
    "    return f\n",
    "\n",
    "\n",
    "\n",
    "#@tf.function(jit_compile=True)\n",
    "@tf.function\n",
    "def train_step(W, b, X_u_train_tf, u_train_tf, X_f_train_tf, opt, nu):\n",
    "    x_u = X_u_train_tf\n",
    "    x_f = X_f_train_tf\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch([W,b])\n",
    "        u_nn = net_u(x_u, W, b) \n",
    "        f_nn = net_f(x_f,W, b, nu)\n",
    "        loss =  100.0*tf.reduce_mean(tf.square(u_nn - u_train_tf)) + tf.reduce_mean(tf.square(f_nn)) \n",
    "    grads = tape.gradient(loss, train_vars(W,b))\n",
    "    opt.apply_gradients(zip(grads, train_vars(W,b)))\n",
    "    return loss\n",
    "\n",
    "def predict(X_star_tf, w, b):\n",
    "    u_pred = net_u(X_star_tf, w, b)\n",
    "    return u_pred\n",
    "    \n",
    "nu = 10**3\n",
    "noise = 0.0        \n",
    "N_f = 300\n",
    "Nmax=3000\n",
    "\n",
    "layers = [1, 4,4,4,4,4,4, 1]\n",
    "L = len(layers)\n",
    "W = [hyper_initial([layers[l-1], layers[l]]) for l in range(1, L)] \n",
    "b = [tf.Variable(tf.zeros([1, layers[l]])) for l in range(1, L)] \n",
    "\n",
    "x_0 = -1\n",
    "x_1 = 1\n",
    "u_0 = 1\n",
    "u_1 = 0\n",
    "\n",
    "X_u_train = np.vstack([x_0, x_1])\n",
    "u_train = np.vstack([u_0, u_1])\n",
    "\n",
    "X_f_train = lb + (ub-lb)*lhs(1, N_f)\n",
    "X_f_star =  np.linspace(-1,1,200)\n",
    "X_f_star = X_f_star.reshape((-1,1))\n",
    "\n",
    "\n",
    "X_u_train_tf = tf.convert_to_tensor(X_u_train, dtype=tf.float32)\n",
    "u_train_tf =   tf.convert_to_tensor(u_train, dtype=tf.float32)\n",
    "X_f_train_tf = tf.convert_to_tensor(X_f_train, dtype=tf.float32)\n",
    "X_star_tf = tf.convert_to_tensor(X_f_star, dtype=tf.float32)\n",
    "\n",
    "\n",
    "lr = 5e-3\n",
    "optimizer = tf.optimizers.Adam(learning_rate=lr)\n",
    "\n",
    "start_time = time.time()\n",
    "n=0\n",
    "loss = []\n",
    "while n <= Nmax:\n",
    "    loss_= train_step(W, b, X_u_train_tf, u_train_tf, X_f_train_tf, optimizer, nu)\n",
    "    loss.append(loss_)    \n",
    "    print(f\"Iteration is: {n} and loss is: {loss_}\")\n",
    "    n+=1\n",
    "\n",
    "elapsed = time.time() - start_time                \n",
    "print('Training time: %.4f' % (elapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a128727",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X_star_tf, w, b):\n",
    "    x_star = X_star_tf\n",
    "    u_pred = net_u(x_star, w, b)\n",
    "    return u_pred\n",
    "\n",
    "u_star = y_act\n",
    "    \n",
    "u_pred = predict(X_star_tf, W, b)\n",
    "error_u = np.linalg.norm(u_star-u_pred,2)/np.linalg.norm(u_star,2)\n",
    "print('Error u: %e' % (error_u))                     \n",
    "\n",
    "\n",
    "\n",
    "####### Row 1: u(t,x) slices ##################    \n",
    "gs1 = gridspec.GridSpec(1, 1)\n",
    "#gs1.update(top=1-1/3, bottom=0, left=0.1, right=0.9, wspace=0.5)\n",
    "\n",
    "ax = plt.subplot(gs1[0, 0])\n",
    "ax.plot(x,y_act, 'b-', linewidth = 2, label = 'Exact')       \n",
    "ax.plot(X_f_star, u_pred, '--r', linewidth = 2, label = 'Prediction')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$u(x)$') \n",
    "plt.legend()\n",
    "#ax.axis('square')\n",
    "ax.set_xlim([-1.1,1.1])\n",
    "ax.set_ylim([-3.1,1.1])\n",
    "plt.show()\n",
    "\n",
    "###############################################\n",
    "loss_list = [loss[i].numpy() for i in range(len(loss))]\n",
    "\n",
    "\n",
    "gs2 = gridspec.GridSpec(1, 1)\n",
    "ax = plt.subplot(gs2[0, 0])\n",
    "\n",
    "ep = np.arange(0,Nmax+1,1)\n",
    "ax.semilogy(ep,loss_list, 'g-', linewidth = 2)       \n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('Loss') \n",
    "plt.legend()\n",
    "#ax.axis('square')\n",
    "##ax.set_xlim([-1.1,1.1])\n",
    "##x.set_ylim([-3.1,1.1])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6a3b22f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 0 and loss is: 2177501.0\n",
      "Iteration is: 1 and loss is: 2144915.75\n",
      "Iteration is: 2 and loss is: 2120281.5\n",
      "Iteration is: 3 and loss is: 2095652.125\n",
      "Iteration is: 4 and loss is: 2070088.375\n",
      "Iteration is: 5 and loss is: 2043338.75\n",
      "Iteration is: 6 and loss is: 2015364.0\n",
      "Iteration is: 7 and loss is: 1986224.375\n",
      "Iteration is: 8 and loss is: 1956035.5\n",
      "Iteration is: 9 and loss is: 1924947.0\n",
      "Iteration is: 10 and loss is: 1893129.125\n",
      "Iteration is: 11 and loss is: 1860765.375\n",
      "Iteration is: 12 and loss is: 1828046.75\n",
      "Iteration is: 13 and loss is: 1795166.25\n",
      "Iteration is: 14 and loss is: 1762318.875\n",
      "Iteration is: 15 and loss is: 1729697.625\n",
      "Iteration is: 16 and loss is: 1697492.625\n",
      "Iteration is: 17 and loss is: 1665889.125\n",
      "Iteration is: 18 and loss is: 1635065.25\n",
      "Iteration is: 19 and loss is: 1605189.625\n",
      "Iteration is: 20 and loss is: 1576418.625\n",
      "Iteration is: 21 and loss is: 1548894.375\n",
      "Iteration is: 22 and loss is: 1522740.625\n",
      "Iteration is: 23 and loss is: 1498060.25\n",
      "Iteration is: 24 and loss is: 1474934.0\n",
      "Iteration is: 25 and loss is: 1453416.125\n",
      "Iteration is: 26 and loss is: 1433534.0\n",
      "Iteration is: 27 and loss is: 1415286.75\n",
      "Iteration is: 28 and loss is: 1398646.125\n",
      "Iteration is: 29 and loss is: 1383558.25\n",
      "Iteration is: 30 and loss is: 1369948.0\n",
      "Iteration is: 31 and loss is: 1357723.25\n",
      "Iteration is: 32 and loss is: 1346780.0\n",
      "Iteration is: 33 and loss is: 1337008.5\n",
      "Iteration is: 34 and loss is: 1328297.75\n",
      "Iteration is: 35 and loss is: 1320539.875\n",
      "Iteration is: 36 and loss is: 1313633.875\n",
      "Iteration is: 37 and loss is: 1307486.25\n",
      "Iteration is: 38 and loss is: 1302013.375\n",
      "Iteration is: 39 and loss is: 1297137.5\n",
      "Iteration is: 40 and loss is: 1292782.875\n",
      "Iteration is: 41 and loss is: 1288871.75\n",
      "Iteration is: 42 and loss is: 1285317.0\n",
      "Iteration is: 43 and loss is: 1282018.25\n",
      "Iteration is: 44 and loss is: 1278861.0\n",
      "Iteration is: 45 and loss is: 1275723.625\n",
      "Iteration is: 46 and loss is: 1272488.25\n",
      "Iteration is: 47 and loss is: 1269053.75\n",
      "Iteration is: 48 and loss is: 1265352.75\n",
      "Iteration is: 49 and loss is: 1261358.875\n",
      "Iteration is: 50 and loss is: 1257089.75\n",
      "Iteration is: 51 and loss is: 1252601.625\n",
      "Iteration is: 52 and loss is: 1247977.0\n",
      "Iteration is: 53 and loss is: 1243312.0\n",
      "Iteration is: 54 and loss is: 1238700.5\n",
      "Iteration is: 55 and loss is: 1234224.0\n",
      "Iteration is: 56 and loss is: 1229941.75\n",
      "Iteration is: 57 and loss is: 1225888.75\n",
      "Iteration is: 58 and loss is: 1222077.0\n",
      "Iteration is: 59 and loss is: 1218499.375\n",
      "Iteration is: 60 and loss is: 1215136.375\n",
      "Iteration is: 61 and loss is: 1211962.0\n",
      "Iteration is: 62 and loss is: 1208949.5\n",
      "Iteration is: 63 and loss is: 1206076.375\n",
      "Iteration is: 64 and loss is: 1203324.875\n",
      "Iteration is: 65 and loss is: 1200683.125\n",
      "Iteration is: 66 and loss is: 1198144.0\n",
      "Iteration is: 67 and loss is: 1195703.5\n",
      "Iteration is: 68 and loss is: 1193357.75\n",
      "Iteration is: 69 and loss is: 1191101.75\n",
      "Iteration is: 70 and loss is: 1188927.0\n",
      "Iteration is: 71 and loss is: 1186820.125\n",
      "Iteration is: 72 and loss is: 1184766.0\n",
      "Iteration is: 73 and loss is: 1182747.5\n",
      "Iteration is: 74 and loss is: 1180751.75\n",
      "Iteration is: 75 and loss is: 1178773.125\n",
      "Iteration is: 76 and loss is: 1176815.375\n",
      "Iteration is: 77 and loss is: 1174891.125\n",
      "Iteration is: 78 and loss is: 1173018.75\n",
      "Iteration is: 79 and loss is: 1171217.25\n",
      "Iteration is: 80 and loss is: 1169500.5\n",
      "Iteration is: 81 and loss is: 1167875.0\n",
      "Iteration is: 82 and loss is: 1166337.75\n",
      "Iteration is: 83 and loss is: 1164881.75\n",
      "Iteration is: 84 and loss is: 1163497.0\n",
      "Iteration is: 85 and loss is: 1162177.75\n",
      "Iteration is: 86 and loss is: 1160921.75\n",
      "Iteration is: 87 and loss is: 1159732.125\n",
      "Iteration is: 88 and loss is: 1158611.5\n",
      "Iteration is: 89 and loss is: 1157559.75\n",
      "Iteration is: 90 and loss is: 1156571.25\n",
      "Iteration is: 91 and loss is: 1155636.5\n",
      "Iteration is: 92 and loss is: 1154745.25\n",
      "Iteration is: 93 and loss is: 1153892.25\n",
      "Iteration is: 94 and loss is: 1153078.25\n",
      "Iteration is: 95 and loss is: 1152310.5\n",
      "Iteration is: 96 and loss is: 1151594.25\n",
      "Iteration is: 97 and loss is: 1150931.375\n",
      "Iteration is: 98 and loss is: 1150319.5\n",
      "Iteration is: 99 and loss is: 1149753.0\n",
      "Iteration is: 100 and loss is: 1149230.25\n",
      "Iteration is: 101 and loss is: 1148752.0\n",
      "Iteration is: 102 and loss is: 1148322.375\n",
      "Iteration is: 103 and loss is: 1147942.625\n",
      "Iteration is: 104 and loss is: 1147610.5\n",
      "Iteration is: 105 and loss is: 1147320.625\n",
      "Iteration is: 106 and loss is: 1147067.625\n",
      "Iteration is: 107 and loss is: 1146850.25\n",
      "Iteration is: 108 and loss is: 1146670.75\n",
      "Iteration is: 109 and loss is: 1146530.0\n",
      "Iteration is: 110 and loss is: 1146426.5\n",
      "Iteration is: 111 and loss is: 1146356.25\n",
      "Iteration is: 112 and loss is: 1146316.5\n",
      "Iteration is: 113 and loss is: 1146307.875\n",
      "Iteration is: 114 and loss is: 1146332.25\n",
      "Iteration is: 115 and loss is: 1146389.25\n",
      "Iteration is: 116 and loss is: 1146476.75\n",
      "Iteration is: 117 and loss is: 1146592.0\n",
      "Iteration is: 118 and loss is: 1146734.625\n",
      "Iteration is: 119 and loss is: 1146906.5\n",
      "Iteration is: 120 and loss is: 1147107.5\n",
      "Iteration is: 121 and loss is: 1147335.75\n",
      "Iteration is: 122 and loss is: 1147589.25\n",
      "Iteration is: 123 and loss is: 1147867.5\n",
      "Iteration is: 124 and loss is: 1148170.875\n",
      "Iteration is: 125 and loss is: 1148498.375\n",
      "Iteration is: 126 and loss is: 1148848.125\n",
      "Iteration is: 127 and loss is: 1149218.25\n",
      "Iteration is: 128 and loss is: 1149609.375\n",
      "Iteration is: 129 and loss is: 1150021.875\n",
      "Iteration is: 130 and loss is: 1150454.375\n",
      "Iteration is: 131 and loss is: 1150905.375\n",
      "Iteration is: 132 and loss is: 1151374.875\n",
      "Iteration is: 133 and loss is: 1151863.125\n",
      "Iteration is: 134 and loss is: 1152368.625\n",
      "Iteration is: 135 and loss is: 1152890.625\n",
      "Iteration is: 136 and loss is: 1153428.875\n",
      "Iteration is: 137 and loss is: 1153983.0\n",
      "Iteration is: 138 and loss is: 1154552.5\n",
      "Iteration is: 139 and loss is: 1155136.0\n",
      "Iteration is: 140 and loss is: 1155734.0\n",
      "Iteration is: 141 and loss is: 1156345.25\n",
      "Iteration is: 142 and loss is: 1156969.25\n",
      "Iteration is: 143 and loss is: 1157605.375\n",
      "Iteration is: 144 and loss is: 1158253.5\n",
      "Iteration is: 145 and loss is: 1158913.125\n",
      "Iteration is: 146 and loss is: 1159583.375\n",
      "Iteration is: 147 and loss is: 1160264.125\n",
      "Iteration is: 148 and loss is: 1160955.375\n",
      "Iteration is: 149 and loss is: 1161656.0\n",
      "Iteration is: 150 and loss is: 1162365.625\n",
      "Iteration is: 151 and loss is: 1163084.5\n",
      "Iteration is: 152 and loss is: 1163811.375\n",
      "Iteration is: 153 and loss is: 1164546.5\n",
      "Iteration is: 154 and loss is: 1165289.5\n",
      "Iteration is: 155 and loss is: 1166039.5\n",
      "Iteration is: 156 and loss is: 1166796.25\n",
      "Iteration is: 157 and loss is: 1167559.375\n",
      "Iteration is: 158 and loss is: 1168328.875\n",
      "Iteration is: 159 and loss is: 1169104.125\n",
      "Iteration is: 160 and loss is: 1169885.25\n",
      "Iteration is: 161 and loss is: 1170671.25\n",
      "Iteration is: 162 and loss is: 1171462.125\n",
      "Iteration is: 163 and loss is: 1172257.5\n",
      "Iteration is: 164 and loss is: 1173057.375\n",
      "Iteration is: 165 and loss is: 1173861.0\n",
      "Iteration is: 166 and loss is: 1174668.25\n",
      "Iteration is: 167 and loss is: 1175479.125\n",
      "Iteration is: 168 and loss is: 1176293.125\n",
      "Iteration is: 169 and loss is: 1177110.0\n",
      "Iteration is: 170 and loss is: 1177929.5\n",
      "Iteration is: 171 and loss is: 1178751.25\n",
      "Iteration is: 172 and loss is: 1179575.0\n",
      "Iteration is: 173 and loss is: 1180400.75\n",
      "Iteration is: 174 and loss is: 1181228.25\n",
      "Iteration is: 175 and loss is: 1182057.125\n",
      "Iteration is: 176 and loss is: 1182887.25\n",
      "Iteration is: 177 and loss is: 1183718.25\n",
      "Iteration is: 178 and loss is: 1184550.25\n",
      "Iteration is: 179 and loss is: 1185382.625\n",
      "Iteration is: 180 and loss is: 1186215.25\n",
      "Iteration is: 181 and loss is: 1187048.125\n",
      "Iteration is: 182 and loss is: 1187881.25\n",
      "Iteration is: 183 and loss is: 1188713.75\n",
      "Iteration is: 184 and loss is: 1189545.75\n",
      "Iteration is: 185 and loss is: 1190377.5\n",
      "Iteration is: 186 and loss is: 1191208.5\n",
      "Iteration is: 187 and loss is: 1192038.375\n",
      "Iteration is: 188 and loss is: 1192867.125\n",
      "Iteration is: 189 and loss is: 1193694.625\n",
      "Iteration is: 190 and loss is: 1194520.875\n",
      "Iteration is: 191 and loss is: 1195345.25\n",
      "Iteration is: 192 and loss is: 1196167.875\n",
      "Iteration is: 193 and loss is: 1196989.0\n",
      "Iteration is: 194 and loss is: 1197807.5\n",
      "Iteration is: 195 and loss is: 1198624.25\n",
      "Iteration is: 196 and loss is: 1199438.125\n",
      "Iteration is: 197 and loss is: 1200249.5\n",
      "Iteration is: 198 and loss is: 1201058.625\n",
      "Iteration is: 199 and loss is: 1201864.625\n",
      "Iteration is: 200 and loss is: 1202667.625\n",
      "Iteration is: 201 and loss is: 1203467.75\n",
      "Iteration is: 202 and loss is: 1204264.625\n",
      "Iteration is: 203 and loss is: 1205058.375\n",
      "Iteration is: 204 and loss is: 1205848.625\n",
      "Iteration is: 205 and loss is: 1206635.0\n",
      "Iteration is: 206 and loss is: 1207417.75\n",
      "Iteration is: 207 and loss is: 1208197.125\n",
      "Iteration is: 208 and loss is: 1208972.25\n",
      "Iteration is: 209 and loss is: 1209743.375\n",
      "Iteration is: 210 and loss is: 1210510.625\n",
      "Iteration is: 211 and loss is: 1211273.5\n",
      "Iteration is: 212 and loss is: 1212032.125\n",
      "Iteration is: 213 and loss is: 1212786.25\n",
      "Iteration is: 214 and loss is: 1213536.0\n",
      "Iteration is: 215 and loss is: 1214281.125\n",
      "Iteration is: 216 and loss is: 1215021.5\n",
      "Iteration is: 217 and loss is: 1215757.375\n",
      "Iteration is: 218 and loss is: 1216488.375\n",
      "Iteration is: 219 and loss is: 1217214.375\n",
      "Iteration is: 220 and loss is: 1217935.5\n",
      "Iteration is: 221 and loss is: 1218651.75\n",
      "Iteration is: 222 and loss is: 1219362.875\n",
      "Iteration is: 223 and loss is: 1220068.75\n",
      "Iteration is: 224 and loss is: 1220769.5\n",
      "Iteration is: 225 and loss is: 1221465.25\n",
      "Iteration is: 226 and loss is: 1222155.75\n",
      "Iteration is: 227 and loss is: 1222840.625\n",
      "Iteration is: 228 and loss is: 1223520.5\n",
      "Iteration is: 229 and loss is: 1224195.0\n",
      "Iteration is: 230 and loss is: 1224864.0\n",
      "Iteration is: 231 and loss is: 1225527.75\n",
      "Iteration is: 232 and loss is: 1226186.0\n",
      "Iteration is: 233 and loss is: 1226838.75\n",
      "Iteration is: 234 and loss is: 1227486.375\n",
      "Iteration is: 235 and loss is: 1228128.875\n",
      "Iteration is: 236 and loss is: 1228765.5\n",
      "Iteration is: 237 and loss is: 1229397.125\n",
      "Iteration is: 238 and loss is: 1230026.5\n",
      "Iteration is: 239 and loss is: 1230661.75\n",
      "Iteration is: 240 and loss is: 1231357.25\n",
      "Iteration is: 241 and loss is: 1232262.75\n",
      "Iteration is: 242 and loss is: 1233479.875\n",
      "Iteration is: 243 and loss is: 1233748.25\n",
      "Iteration is: 244 and loss is: 1233773.75\n",
      "Iteration is: 245 and loss is: 1235036.25\n",
      "Iteration is: 246 and loss is: 1235440.75\n",
      "Iteration is: 247 and loss is: 1235715.25\n",
      "Iteration is: 248 and loss is: 1236864.375\n",
      "Iteration is: 249 and loss is: 1237084.75\n",
      "Iteration is: 250 and loss is: 1237716.25\n",
      "Iteration is: 251 and loss is: 1238602.75\n",
      "Iteration is: 252 and loss is: 1238826.5\n",
      "Iteration is: 253 and loss is: 1239671.875\n",
      "Iteration is: 254 and loss is: 1240280.75\n",
      "Iteration is: 255 and loss is: 1240665.875\n",
      "Iteration is: 256 and loss is: 1241518.0\n",
      "Iteration is: 257 and loss is: 1241988.375\n",
      "Iteration is: 258 and loss is: 1242512.75\n",
      "Iteration is: 259 and loss is: 1243288.5\n",
      "Iteration is: 260 and loss is: 1243736.625\n",
      "Iteration is: 261 and loss is: 1244310.25\n",
      "Iteration is: 262 and loss is: 1245026.75\n",
      "Iteration is: 263 and loss is: 1245497.5\n",
      "Iteration is: 264 and loss is: 1246053.75\n",
      "Iteration is: 265 and loss is: 1246739.125\n",
      "Iteration is: 266 and loss is: 1247255.625\n",
      "Iteration is: 267 and loss is: 1247763.375\n",
      "Iteration is: 268 and loss is: 1248406.75\n",
      "Iteration is: 269 and loss is: 1248993.75\n",
      "Iteration is: 270 and loss is: 1249483.375\n",
      "Iteration is: 271 and loss is: 1250032.25\n",
      "Iteration is: 272 and loss is: 1250648.375\n",
      "Iteration is: 273 and loss is: 1251216.25\n",
      "Iteration is: 274 and loss is: 1251720.375\n",
      "Iteration is: 275 and loss is: 1252237.0\n",
      "Iteration is: 276 and loss is: 1252803.25\n",
      "Iteration is: 277 and loss is: 1253387.25\n",
      "Iteration is: 278 and loss is: 1253949.75\n",
      "Iteration is: 279 and loss is: 1254481.75\n",
      "Iteration is: 280 and loss is: 1254994.375\n",
      "Iteration is: 281 and loss is: 1255505.5\n",
      "Iteration is: 282 and loss is: 1256021.0\n",
      "Iteration is: 283 and loss is: 1256542.75\n",
      "Iteration is: 284 and loss is: 1257066.5\n",
      "Iteration is: 285 and loss is: 1257590.625\n",
      "Iteration is: 286 and loss is: 1258114.125\n",
      "Iteration is: 287 and loss is: 1258638.25\n",
      "Iteration is: 288 and loss is: 1259169.0\n",
      "Iteration is: 289 and loss is: 1259733.125\n",
      "Iteration is: 290 and loss is: 1260445.0\n",
      "Iteration is: 291 and loss is: 1261761.5\n",
      "Iteration is: 292 and loss is: 1264741.25\n",
      "Iteration is: 293 and loss is: 1267197.25\n",
      "Iteration is: 294 and loss is: 1263677.5\n",
      "Iteration is: 295 and loss is: 1263889.5\n",
      "Iteration is: 296 and loss is: 1267220.5\n",
      "Iteration is: 297 and loss is: 1264640.5\n",
      "Iteration is: 298 and loss is: 1266253.0\n",
      "Iteration is: 299 and loss is: 1267627.25\n",
      "Iteration is: 300 and loss is: 1265999.25\n",
      "Iteration is: 301 and loss is: 1268606.25\n",
      "Iteration is: 302 and loss is: 1267973.5\n",
      "Iteration is: 303 and loss is: 1268346.5\n",
      "Iteration is: 304 and loss is: 1270039.25\n",
      "Iteration is: 305 and loss is: 1269099.75\n",
      "Iteration is: 306 and loss is: 1270756.5\n",
      "Iteration is: 307 and loss is: 1271041.25\n",
      "Iteration is: 308 and loss is: 1271043.375\n",
      "Iteration is: 309 and loss is: 1272606.25\n",
      "Iteration is: 310 and loss is: 1272336.0\n",
      "Iteration is: 311 and loss is: 1273158.375\n",
      "Iteration is: 312 and loss is: 1274135.625\n",
      "Iteration is: 313 and loss is: 1274012.0\n",
      "Iteration is: 314 and loss is: 1275099.125\n",
      "Iteration is: 315 and loss is: 1275699.875\n",
      "Iteration is: 316 and loss is: 1275844.875\n",
      "Iteration is: 317 and loss is: 1276893.75\n",
      "Iteration is: 318 and loss is: 1277403.0\n",
      "Iteration is: 319 and loss is: 1277692.125\n",
      "Iteration is: 320 and loss is: 1278631.25\n",
      "Iteration is: 321 and loss is: 1279211.75\n",
      "Iteration is: 322 and loss is: 1279540.5\n",
      "Iteration is: 323 and loss is: 1280340.0\n",
      "Iteration is: 324 and loss is: 1281061.0\n",
      "Iteration is: 325 and loss is: 1281449.0\n",
      "Iteration is: 326 and loss is: 1282046.375\n",
      "Iteration is: 327 and loss is: 1282846.75\n",
      "Iteration is: 328 and loss is: 1283434.0\n",
      "Iteration is: 329 and loss is: 1283888.5\n",
      "Iteration is: 330 and loss is: 1284527.5\n",
      "Iteration is: 331 and loss is: 1285281.75\n",
      "Iteration is: 332 and loss is: 1285911.25\n",
      "Iteration is: 333 and loss is: 1286427.875\n",
      "Iteration is: 334 and loss is: 1286988.125\n",
      "Iteration is: 335 and loss is: 1287660.75\n",
      "Iteration is: 336 and loss is: 1288377.0\n",
      "Iteration is: 337 and loss is: 1289046.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 338 and loss is: 1289658.125\n",
      "Iteration is: 339 and loss is: 1290233.0\n",
      "Iteration is: 340 and loss is: 1290816.5\n",
      "Iteration is: 341 and loss is: 1291423.875\n",
      "Iteration is: 342 and loss is: 1292057.0\n",
      "Iteration is: 343 and loss is: 1292710.5\n",
      "Iteration is: 344 and loss is: 1293384.0\n",
      "Iteration is: 345 and loss is: 1294092.75\n",
      "Iteration is: 346 and loss is: 1294877.25\n",
      "Iteration is: 347 and loss is: 1295871.25\n",
      "Iteration is: 348 and loss is: 1297324.25\n",
      "Iteration is: 349 and loss is: 1299889.875\n",
      "Iteration is: 350 and loss is: 1302745.875\n",
      "Iteration is: 351 and loss is: 1304083.75\n",
      "Iteration is: 352 and loss is: 1300213.75\n",
      "Iteration is: 353 and loss is: 1299637.5\n",
      "Iteration is: 354 and loss is: 1303376.25\n",
      "Iteration is: 355 and loss is: 1303804.875\n",
      "Iteration is: 356 and loss is: 1301848.625\n",
      "Iteration is: 357 and loss is: 1302966.5\n",
      "Iteration is: 358 and loss is: 1305381.25\n",
      "Iteration is: 359 and loss is: 1305012.75\n",
      "Iteration is: 360 and loss is: 1304403.5\n",
      "Iteration is: 361 and loss is: 1306548.5\n",
      "Iteration is: 362 and loss is: 1307709.25\n",
      "Iteration is: 363 and loss is: 1306766.625\n",
      "Iteration is: 364 and loss is: 1307865.25\n",
      "Iteration is: 365 and loss is: 1309694.5\n",
      "Iteration is: 366 and loss is: 1309384.25\n",
      "Iteration is: 367 and loss is: 1309582.375\n",
      "Iteration is: 368 and loss is: 1311208.0\n",
      "Iteration is: 369 and loss is: 1311884.25\n",
      "Iteration is: 370 and loss is: 1311852.0\n",
      "Iteration is: 371 and loss is: 1312718.5\n",
      "Iteration is: 372 and loss is: 1313992.5\n",
      "Iteration is: 373 and loss is: 1314520.75\n",
      "Iteration is: 374 and loss is: 1314766.625\n",
      "Iteration is: 375 and loss is: 1315733.375\n",
      "Iteration is: 376 and loss is: 1316864.25\n",
      "Iteration is: 377 and loss is: 1317364.0\n",
      "Iteration is: 378 and loss is: 1317781.75\n",
      "Iteration is: 379 and loss is: 1318645.75\n",
      "Iteration is: 380 and loss is: 1319666.875\n",
      "Iteration is: 381 and loss is: 1320407.625\n",
      "Iteration is: 382 and loss is: 1320897.875\n",
      "Iteration is: 383 and loss is: 1321573.25\n",
      "Iteration is: 384 and loss is: 1322496.375\n",
      "Iteration is: 385 and loss is: 1323395.25\n",
      "Iteration is: 386 and loss is: 1324121.75\n",
      "Iteration is: 387 and loss is: 1324722.25\n",
      "Iteration is: 388 and loss is: 1325405.5\n",
      "Iteration is: 389 and loss is: 1326228.375\n",
      "Iteration is: 390 and loss is: 1327109.5\n",
      "Iteration is: 391 and loss is: 1327955.75\n",
      "Iteration is: 392 and loss is: 1328700.875\n",
      "Iteration is: 393 and loss is: 1329402.25\n",
      "Iteration is: 394 and loss is: 1330097.5\n",
      "Iteration is: 395 and loss is: 1330835.75\n",
      "Iteration is: 396 and loss is: 1331622.0\n",
      "Iteration is: 397 and loss is: 1332442.25\n",
      "Iteration is: 398 and loss is: 1333282.375\n",
      "Iteration is: 399 and loss is: 1334126.0\n",
      "Iteration is: 400 and loss is: 1334986.875\n",
      "Iteration is: 401 and loss is: 1335843.75\n",
      "Iteration is: 402 and loss is: 1336749.5\n",
      "Iteration is: 403 and loss is: 1337653.75\n",
      "Iteration is: 404 and loss is: 1338672.375\n",
      "Iteration is: 405 and loss is: 1339659.0\n",
      "Iteration is: 406 and loss is: 1340837.75\n",
      "Iteration is: 407 and loss is: 1341776.25\n",
      "Iteration is: 408 and loss is: 1342840.75\n",
      "Iteration is: 409 and loss is: 1343285.125\n",
      "Iteration is: 410 and loss is: 1343712.75\n",
      "Iteration is: 411 and loss is: 1343893.75\n",
      "Iteration is: 412 and loss is: 1344325.75\n",
      "Iteration is: 413 and loss is: 1345067.375\n",
      "Iteration is: 414 and loss is: 1346068.5\n",
      "Iteration is: 415 and loss is: 1347206.0\n",
      "Iteration is: 416 and loss is: 1348236.75\n",
      "Iteration is: 417 and loss is: 1349217.0\n",
      "Iteration is: 418 and loss is: 1349838.25\n",
      "Iteration is: 419 and loss is: 1350415.0\n",
      "Iteration is: 420 and loss is: 1350890.75\n",
      "Iteration is: 421 and loss is: 1351493.75\n",
      "Iteration is: 422 and loss is: 1352248.5\n",
      "Iteration is: 423 and loss is: 1353133.25\n",
      "Iteration is: 424 and loss is: 1354095.5\n",
      "Iteration is: 425 and loss is: 1355045.75\n",
      "Iteration is: 426 and loss is: 1356002.0\n",
      "Iteration is: 427 and loss is: 1356820.75\n",
      "Iteration is: 428 and loss is: 1357636.0\n",
      "Iteration is: 429 and loss is: 1358301.25\n",
      "Iteration is: 430 and loss is: 1358982.0\n",
      "Iteration is: 431 and loss is: 1359619.875\n",
      "Iteration is: 432 and loss is: 1360302.75\n",
      "Iteration is: 433 and loss is: 1361022.125\n",
      "Iteration is: 434 and loss is: 1361786.25\n",
      "Iteration is: 435 and loss is: 1362585.5\n",
      "Iteration is: 436 and loss is: 1363409.5\n",
      "Iteration is: 437 and loss is: 1364255.5\n",
      "Iteration is: 438 and loss is: 1365112.625\n",
      "Iteration is: 439 and loss is: 1366007.125\n",
      "Iteration is: 440 and loss is: 1366907.0\n",
      "Iteration is: 441 and loss is: 1367898.0\n",
      "Iteration is: 442 and loss is: 1368855.875\n",
      "Iteration is: 443 and loss is: 1369986.0\n",
      "Iteration is: 444 and loss is: 1370896.75\n",
      "Iteration is: 445 and loss is: 1371975.5\n",
      "Iteration is: 446 and loss is: 1372494.375\n",
      "Iteration is: 447 and loss is: 1373056.875\n",
      "Iteration is: 448 and loss is: 1373268.125\n",
      "Iteration is: 449 and loss is: 1373652.75\n",
      "Iteration is: 450 and loss is: 1374253.875\n",
      "Iteration is: 451 and loss is: 1375104.5\n",
      "Iteration is: 452 and loss is: 1376108.0\n",
      "Iteration is: 453 and loss is: 1377070.0\n",
      "Iteration is: 454 and loss is: 1377983.25\n",
      "Iteration is: 455 and loss is: 1378615.0\n",
      "Iteration is: 456 and loss is: 1379198.25\n",
      "Iteration is: 457 and loss is: 1379708.0\n",
      "Iteration is: 458 and loss is: 1380316.5\n",
      "Iteration is: 459 and loss is: 1381046.25\n",
      "Iteration is: 460 and loss is: 1381870.0\n",
      "Iteration is: 461 and loss is: 1382732.0\n",
      "Iteration is: 462 and loss is: 1383540.125\n",
      "Iteration is: 463 and loss is: 1384309.5\n",
      "Iteration is: 464 and loss is: 1384966.125\n",
      "Iteration is: 465 and loss is: 1385604.5\n",
      "Iteration is: 466 and loss is: 1386217.75\n",
      "Iteration is: 467 and loss is: 1386863.625\n",
      "Iteration is: 468 and loss is: 1387548.25\n",
      "Iteration is: 469 and loss is: 1388268.25\n",
      "Iteration is: 470 and loss is: 1389008.875\n",
      "Iteration is: 471 and loss is: 1389747.0\n",
      "Iteration is: 472 and loss is: 1390479.0\n",
      "Iteration is: 473 and loss is: 1391177.25\n",
      "Iteration is: 474 and loss is: 1391864.5\n",
      "Iteration is: 475 and loss is: 1392513.625\n",
      "Iteration is: 476 and loss is: 1393157.875\n",
      "Iteration is: 477 and loss is: 1393781.125\n",
      "Iteration is: 478 and loss is: 1394404.875\n",
      "Iteration is: 479 and loss is: 1395023.0\n",
      "Iteration is: 480 and loss is: 1395644.0\n",
      "Iteration is: 481 and loss is: 1396263.75\n",
      "Iteration is: 482 and loss is: 1396883.5\n",
      "Iteration is: 483 and loss is: 1397500.25\n",
      "Iteration is: 484 and loss is: 1398113.375\n",
      "Iteration is: 485 and loss is: 1398721.5\n",
      "Iteration is: 486 and loss is: 1399323.25\n",
      "Iteration is: 487 and loss is: 1399919.0\n",
      "Iteration is: 488 and loss is: 1400507.875\n",
      "Iteration is: 489 and loss is: 1401093.25\n",
      "Iteration is: 490 and loss is: 1401671.25\n",
      "Iteration is: 491 and loss is: 1402252.0\n",
      "Iteration is: 492 and loss is: 1402827.0\n",
      "Iteration is: 493 and loss is: 1403421.0\n",
      "Iteration is: 494 and loss is: 1404007.25\n",
      "Iteration is: 495 and loss is: 1404646.625\n",
      "Iteration is: 496 and loss is: 1405257.5\n",
      "Iteration is: 497 and loss is: 1405979.5\n",
      "Iteration is: 498 and loss is: 1406572.0\n",
      "Iteration is: 499 and loss is: 1407316.0\n",
      "Iteration is: 500 and loss is: 1407698.5\n",
      "Iteration is: 501 and loss is: 1408174.5\n",
      "Iteration is: 502 and loss is: 1408221.25\n",
      "Iteration is: 503 and loss is: 1408334.875\n",
      "Iteration is: 504 and loss is: 1408416.25\n",
      "Iteration is: 505 and loss is: 1408661.0\n",
      "Iteration is: 506 and loss is: 1409064.25\n",
      "Iteration is: 507 and loss is: 1409560.0\n",
      "Iteration is: 508 and loss is: 1410074.5\n",
      "Iteration is: 509 and loss is: 1410452.25\n",
      "Iteration is: 510 and loss is: 1410762.5\n",
      "Iteration is: 511 and loss is: 1410900.125\n",
      "Iteration is: 512 and loss is: 1411029.125\n",
      "Iteration is: 513 and loss is: 1411169.625\n",
      "Iteration is: 514 and loss is: 1411375.25\n",
      "Iteration is: 515 and loss is: 1411632.875\n",
      "Iteration is: 516 and loss is: 1411899.25\n",
      "Iteration is: 517 and loss is: 1412142.25\n",
      "Iteration is: 518 and loss is: 1412302.75\n",
      "Iteration is: 519 and loss is: 1412414.875\n",
      "Iteration is: 520 and loss is: 1412459.0\n",
      "Iteration is: 521 and loss is: 1412488.75\n",
      "Iteration is: 522 and loss is: 1412513.75\n",
      "Iteration is: 523 and loss is: 1412547.25\n",
      "Iteration is: 524 and loss is: 1412583.5\n",
      "Iteration is: 525 and loss is: 1412606.875\n",
      "Iteration is: 526 and loss is: 1412606.125\n",
      "Iteration is: 527 and loss is: 1412562.75\n",
      "Iteration is: 528 and loss is: 1412483.0\n",
      "Iteration is: 529 and loss is: 1412357.0\n",
      "Iteration is: 530 and loss is: 1412201.75\n",
      "Iteration is: 531 and loss is: 1412015.375\n",
      "Iteration is: 532 and loss is: 1411808.5\n",
      "Iteration is: 533 and loss is: 1411580.25\n",
      "Iteration is: 534 and loss is: 1411330.75\n",
      "Iteration is: 535 and loss is: 1411058.0\n",
      "Iteration is: 536 and loss is: 1410757.25\n",
      "Iteration is: 537 and loss is: 1410426.125\n",
      "Iteration is: 538 and loss is: 1410061.0\n",
      "Iteration is: 539 and loss is: 1409662.0\n",
      "Iteration is: 540 and loss is: 1409225.0\n",
      "Iteration is: 541 and loss is: 1408753.25\n",
      "Iteration is: 542 and loss is: 1408243.0\n",
      "Iteration is: 543 and loss is: 1407697.75\n",
      "Iteration is: 544 and loss is: 1407113.0\n",
      "Iteration is: 545 and loss is: 1406493.0\n",
      "Iteration is: 546 and loss is: 1405832.75\n",
      "Iteration is: 547 and loss is: 1405135.0\n",
      "Iteration is: 548 and loss is: 1404396.5\n",
      "Iteration is: 549 and loss is: 1403619.0\n",
      "Iteration is: 550 and loss is: 1402798.25\n",
      "Iteration is: 551 and loss is: 1401937.75\n",
      "Iteration is: 552 and loss is: 1401033.25\n",
      "Iteration is: 553 and loss is: 1400090.125\n",
      "Iteration is: 554 and loss is: 1399103.0\n",
      "Iteration is: 555 and loss is: 1398087.75\n",
      "Iteration is: 556 and loss is: 1397033.375\n",
      "Iteration is: 557 and loss is: 1395987.0\n",
      "Iteration is: 558 and loss is: 1394909.75\n",
      "Iteration is: 559 and loss is: 1393952.75\n",
      "Iteration is: 560 and loss is: 1392924.5\n",
      "Iteration is: 561 and loss is: 1392269.875\n",
      "Iteration is: 562 and loss is: 1391091.625\n",
      "Iteration is: 563 and loss is: 1390421.875\n",
      "Iteration is: 564 and loss is: 1388103.5\n",
      "Iteration is: 565 and loss is: 1385978.625\n",
      "Iteration is: 566 and loss is: 1383159.5\n",
      "Iteration is: 567 and loss is: 1380894.0\n",
      "Iteration is: 568 and loss is: 1379205.875\n",
      "Iteration is: 569 and loss is: 1377792.25\n",
      "Iteration is: 570 and loss is: 1376378.5\n",
      "Iteration is: 571 and loss is: 1374131.0\n",
      "Iteration is: 572 and loss is: 1371665.75\n",
      "Iteration is: 573 and loss is: 1368833.5\n",
      "Iteration is: 574 and loss is: 1366227.125\n",
      "Iteration is: 575 and loss is: 1363878.5\n",
      "Iteration is: 576 and loss is: 1361604.0\n",
      "Iteration is: 577 and loss is: 1359248.5\n",
      "Iteration is: 578 and loss is: 1356435.75\n",
      "Iteration is: 579 and loss is: 1353435.125\n",
      "Iteration is: 580 and loss is: 1350112.5\n",
      "Iteration is: 581 and loss is: 1346768.5\n",
      "Iteration is: 582 and loss is: 1343422.0\n",
      "Iteration is: 583 and loss is: 1340075.0\n",
      "Iteration is: 584 and loss is: 1336681.5\n",
      "Iteration is: 585 and loss is: 1333147.75\n",
      "Iteration is: 586 and loss is: 1329509.5\n",
      "Iteration is: 587 and loss is: 1325605.125\n",
      "Iteration is: 588 and loss is: 1321620.0\n",
      "Iteration is: 589 and loss is: 1317309.75\n",
      "Iteration is: 590 and loss is: 1312942.875\n",
      "Iteration is: 591 and loss is: 1308250.75\n",
      "Iteration is: 592 and loss is: 1303500.5\n",
      "Iteration is: 593 and loss is: 1298460.875\n",
      "Iteration is: 594 and loss is: 1293345.5\n",
      "Iteration is: 595 and loss is: 1287983.125\n",
      "Iteration is: 596 and loss is: 1282524.75\n",
      "Iteration is: 597 and loss is: 1276849.0\n",
      "Iteration is: 598 and loss is: 1271062.625\n",
      "Iteration is: 599 and loss is: 1265074.75\n",
      "Iteration is: 600 and loss is: 1258969.75\n",
      "Iteration is: 601 and loss is: 1252670.75\n",
      "Iteration is: 602 and loss is: 1246264.0\n",
      "Iteration is: 603 and loss is: 1239665.25\n",
      "Iteration is: 604 and loss is: 1232987.25\n",
      "Iteration is: 605 and loss is: 1226100.75\n",
      "Iteration is: 606 and loss is: 1219170.0\n",
      "Iteration is: 607 and loss is: 1211966.25\n",
      "Iteration is: 608 and loss is: 1204714.75\n",
      "Iteration is: 609 and loss is: 1197065.5\n",
      "Iteration is: 610 and loss is: 1189301.625\n",
      "Iteration is: 611 and loss is: 1181144.75\n",
      "Iteration is: 612 and loss is: 1172886.0\n",
      "Iteration is: 613 and loss is: 1164500.25\n",
      "Iteration is: 614 and loss is: 1156130.0\n",
      "Iteration is: 615 and loss is: 1147772.0\n",
      "Iteration is: 616 and loss is: 1139372.25\n",
      "Iteration is: 617 and loss is: 1130863.625\n",
      "Iteration is: 618 and loss is: 1122135.5\n",
      "Iteration is: 619 and loss is: 1113227.0\n",
      "Iteration is: 620 and loss is: 1104151.125\n",
      "Iteration is: 621 and loss is: 1095028.125\n",
      "Iteration is: 622 and loss is: 1085916.75\n",
      "Iteration is: 623 and loss is: 1076807.0\n",
      "Iteration is: 624 and loss is: 1067694.25\n",
      "Iteration is: 625 and loss is: 1058452.0\n",
      "Iteration is: 626 and loss is: 1049056.5\n",
      "Iteration is: 627 and loss is: 1039525.6875\n",
      "Iteration is: 628 and loss is: 1030008.5\n",
      "Iteration is: 629 and loss is: 1020570.3125\n",
      "Iteration is: 630 and loss is: 1011128.875\n",
      "Iteration is: 631 and loss is: 1001682.1875\n",
      "Iteration is: 632 and loss is: 992351.25\n",
      "Iteration is: 633 and loss is: 983148.625\n",
      "Iteration is: 634 and loss is: 973980.125\n",
      "Iteration is: 635 and loss is: 964408.5\n",
      "Iteration is: 636 and loss is: 954556.75\n",
      "Iteration is: 637 and loss is: 944800.625\n",
      "Iteration is: 638 and loss is: 935494.6875\n",
      "Iteration is: 639 and loss is: 926542.5\n",
      "Iteration is: 640 and loss is: 917660.5\n",
      "Iteration is: 641 and loss is: 908707.1875\n",
      "Iteration is: 642 and loss is: 899280.75\n",
      "Iteration is: 643 and loss is: 889679.875\n",
      "Iteration is: 644 and loss is: 880268.4375\n",
      "Iteration is: 645 and loss is: 871335.5\n",
      "Iteration is: 646 and loss is: 862784.625\n",
      "Iteration is: 647 and loss is: 854409.125\n",
      "Iteration is: 648 and loss is: 846161.75\n",
      "Iteration is: 649 and loss is: 837428.125\n",
      "Iteration is: 650 and loss is: 828282.375\n",
      "Iteration is: 651 and loss is: 819050.125\n",
      "Iteration is: 652 and loss is: 810432.8125\n",
      "Iteration is: 653 and loss is: 802435.375\n",
      "Iteration is: 654 and loss is: 794781.375\n",
      "Iteration is: 655 and loss is: 787352.5625\n",
      "Iteration is: 656 and loss is: 779537.5\n",
      "Iteration is: 657 and loss is: 771227.625\n",
      "Iteration is: 658 and loss is: 762413.0625\n",
      "Iteration is: 659 and loss is: 754009.4375\n",
      "Iteration is: 660 and loss is: 746352.0\n",
      "Iteration is: 661 and loss is: 739308.0625\n",
      "Iteration is: 662 and loss is: 732719.1875\n",
      "Iteration is: 663 and loss is: 726203.25\n",
      "Iteration is: 664 and loss is: 719511.25\n",
      "Iteration is: 665 and loss is: 711677.0625\n",
      "Iteration is: 666 and loss is: 703266.25\n",
      "Iteration is: 667 and loss is: 695341.125\n",
      "Iteration is: 668 and loss is: 688616.5\n",
      "Iteration is: 669 and loss is: 682775.5625\n",
      "Iteration is: 670 and loss is: 677187.8125\n",
      "Iteration is: 671 and loss is: 671422.625\n",
      "Iteration is: 672 and loss is: 664646.9375\n",
      "Iteration is: 673 and loss is: 657152.5\n",
      "Iteration is: 674 and loss is: 649629.25\n",
      "Iteration is: 675 and loss is: 642960.8125\n",
      "Iteration is: 676 and loss is: 637192.8125\n",
      "Iteration is: 677 and loss is: 632061.75\n",
      "Iteration is: 678 and loss is: 627419.375\n",
      "Iteration is: 679 and loss is: 622760.5\n",
      "Iteration is: 680 and loss is: 617663.9375\n",
      "Iteration is: 681 and loss is: 611050.9375\n",
      "Iteration is: 682 and loss is: 603566.125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 683 and loss is: 596495.5625\n",
      "Iteration is: 684 and loss is: 590894.6875\n",
      "Iteration is: 685 and loss is: 586535.875\n",
      "Iteration is: 686 and loss is: 582740.1875\n",
      "Iteration is: 687 and loss is: 578963.5\n",
      "Iteration is: 688 and loss is: 574225.1875\n",
      "Iteration is: 689 and loss is: 568314.3125\n",
      "Iteration is: 690 and loss is: 561490.0\n",
      "Iteration is: 691 and loss is: 555216.9375\n",
      "Iteration is: 692 and loss is: 550065.75\n",
      "Iteration is: 693 and loss is: 545948.3125\n",
      "Iteration is: 694 and loss is: 542529.875\n",
      "Iteration is: 695 and loss is: 539394.0625\n",
      "Iteration is: 696 and loss is: 536183.8125\n",
      "Iteration is: 697 and loss is: 531846.1875\n",
      "Iteration is: 698 and loss is: 526232.75\n",
      "Iteration is: 699 and loss is: 519629.09375\n",
      "Iteration is: 700 and loss is: 513710.0625\n",
      "Iteration is: 701 and loss is: 509119.0625\n",
      "Iteration is: 702 and loss is: 505739.90625\n",
      "Iteration is: 703 and loss is: 503129.96875\n",
      "Iteration is: 704 and loss is: 500731.71875\n",
      "Iteration is: 705 and loss is: 497994.75\n",
      "Iteration is: 706 and loss is: 493771.28125\n",
      "Iteration is: 707 and loss is: 488074.03125\n",
      "Iteration is: 708 and loss is: 481753.625\n",
      "Iteration is: 709 and loss is: 476560.4375\n",
      "Iteration is: 710 and loss is: 472961.65625\n",
      "Iteration is: 711 and loss is: 470520.875\n",
      "Iteration is: 712 and loss is: 468618.09375\n",
      "Iteration is: 713 and loss is: 466453.0625\n",
      "Iteration is: 714 and loss is: 463305.625\n",
      "Iteration is: 715 and loss is: 458383.59375\n",
      "Iteration is: 716 and loss is: 452565.59375\n",
      "Iteration is: 717 and loss is: 447282.65625\n",
      "Iteration is: 718 and loss is: 443549.8125\n",
      "Iteration is: 719 and loss is: 441150.0625\n",
      "Iteration is: 720 and loss is: 439458.8125\n",
      "Iteration is: 721 and loss is: 437915.125\n",
      "Iteration is: 722 and loss is: 435593.0625\n",
      "Iteration is: 723 and loss is: 431870.75\n",
      "Iteration is: 724 and loss is: 426511.3125\n",
      "Iteration is: 725 and loss is: 421018.1875\n",
      "Iteration is: 726 and loss is: 416819.8125\n",
      "Iteration is: 727 and loss is: 414253.875\n",
      "Iteration is: 728 and loss is: 412865.75\n",
      "Iteration is: 729 and loss is: 411885.21875\n",
      "Iteration is: 730 and loss is: 410921.75\n",
      "Iteration is: 731 and loss is: 408610.84375\n",
      "Iteration is: 732 and loss is: 405065.8125\n",
      "Iteration is: 733 and loss is: 399525.75\n",
      "Iteration is: 734 and loss is: 394542.96875\n",
      "Iteration is: 735 and loss is: 390273.59375\n",
      "Iteration is: 736 and loss is: 387861.65625\n",
      "Iteration is: 737 and loss is: 386693.03125\n",
      "Iteration is: 738 and loss is: 385920.09375\n",
      "Iteration is: 739 and loss is: 384574.1875\n",
      "Iteration is: 740 and loss is: 380994.0\n",
      "Iteration is: 741 and loss is: 375758.5625\n",
      "Iteration is: 742 and loss is: 370189.6875\n",
      "Iteration is: 743 and loss is: 366283.75\n",
      "Iteration is: 744 and loss is: 364322.0625\n",
      "Iteration is: 745 and loss is: 363525.375\n",
      "Iteration is: 746 and loss is: 363015.71875\n",
      "Iteration is: 747 and loss is: 361599.78125\n",
      "Iteration is: 748 and loss is: 358928.96875\n",
      "Iteration is: 749 and loss is: 354405.65625\n",
      "Iteration is: 750 and loss is: 349464.21875\n",
      "Iteration is: 751 and loss is: 345250.53125\n",
      "Iteration is: 752 and loss is: 342471.1875\n",
      "Iteration is: 753 and loss is: 340856.15625\n",
      "Iteration is: 754 and loss is: 339777.5625\n",
      "Iteration is: 755 and loss is: 338961.375\n",
      "Iteration is: 756 and loss is: 337748.9375\n",
      "Iteration is: 757 and loss is: 336213.5625\n",
      "Iteration is: 758 and loss is: 333340.65625\n",
      "Iteration is: 759 and loss is: 329715.96875\n",
      "Iteration is: 760 and loss is: 325305.5\n",
      "Iteration is: 761 and loss is: 321403.0\n",
      "Iteration is: 762 and loss is: 318575.375\n",
      "Iteration is: 763 and loss is: 316401.96875\n",
      "Iteration is: 764 and loss is: 314952.75\n",
      "Iteration is: 765 and loss is: 312861.5625\n",
      "Iteration is: 766 and loss is: 311018.125\n",
      "Iteration is: 767 and loss is: 309330.84375\n",
      "Iteration is: 768 and loss is: 308854.8125\n",
      "Iteration is: 769 and loss is: 309047.71875\n",
      "Iteration is: 770 and loss is: 309628.625\n",
      "Iteration is: 771 and loss is: 307520.90625\n",
      "Iteration is: 772 and loss is: 301883.5\n",
      "Iteration is: 773 and loss is: 294263.125\n",
      "Iteration is: 774 and loss is: 289057.875\n",
      "Iteration is: 775 and loss is: 287735.9375\n",
      "Iteration is: 776 and loss is: 288614.375\n",
      "Iteration is: 777 and loss is: 288817.75\n",
      "Iteration is: 778 and loss is: 286835.40625\n",
      "Iteration is: 779 and loss is: 283428.125\n",
      "Iteration is: 780 and loss is: 279220.21875\n",
      "Iteration is: 781 and loss is: 275844.53125\n",
      "Iteration is: 782 and loss is: 272611.125\n",
      "Iteration is: 783 and loss is: 270047.875\n",
      "Iteration is: 784 and loss is: 268088.71875\n",
      "Iteration is: 785 and loss is: 267269.03125\n",
      "Iteration is: 786 and loss is: 267357.8125\n",
      "Iteration is: 787 and loss is: 266796.5\n",
      "Iteration is: 788 and loss is: 265838.4375\n",
      "Iteration is: 789 and loss is: 263237.5625\n",
      "Iteration is: 790 and loss is: 260677.96875\n",
      "Iteration is: 791 and loss is: 257057.375\n",
      "Iteration is: 792 and loss is: 253193.75\n",
      "Iteration is: 793 and loss is: 249108.171875\n",
      "Iteration is: 794 and loss is: 245771.421875\n",
      "Iteration is: 795 and loss is: 243651.78125\n",
      "Iteration is: 796 and loss is: 242504.78125\n",
      "Iteration is: 797 and loss is: 241811.5625\n",
      "Iteration is: 798 and loss is: 241288.671875\n",
      "Iteration is: 799 and loss is: 241597.125\n",
      "Iteration is: 800 and loss is: 242260.125\n",
      "Iteration is: 801 and loss is: 244340.890625\n",
      "Iteration is: 802 and loss is: 243533.515625\n",
      "Iteration is: 803 and loss is: 240941.96875\n",
      "Iteration is: 804 and loss is: 232806.328125\n",
      "Iteration is: 805 and loss is: 225361.3125\n",
      "Iteration is: 806 and loss is: 221399.4375\n",
      "Iteration is: 807 and loss is: 221535.46875\n",
      "Iteration is: 808 and loss is: 223567.875\n",
      "Iteration is: 809 and loss is: 223907.28125\n",
      "Iteration is: 810 and loss is: 222199.203125\n",
      "Iteration is: 811 and loss is: 216786.078125\n",
      "Iteration is: 812 and loss is: 211477.671875\n",
      "Iteration is: 813 and loss is: 207966.71875\n",
      "Iteration is: 814 and loss is: 206931.84375\n",
      "Iteration is: 815 and loss is: 207407.078125\n",
      "Iteration is: 816 and loss is: 207593.71875\n",
      "Iteration is: 817 and loss is: 207092.59375\n",
      "Iteration is: 818 and loss is: 204126.3125\n",
      "Iteration is: 819 and loss is: 200430.703125\n",
      "Iteration is: 820 and loss is: 196211.90625\n",
      "Iteration is: 821 and loss is: 193012.390625\n",
      "Iteration is: 822 and loss is: 191042.625\n",
      "Iteration is: 823 and loss is: 190030.390625\n",
      "Iteration is: 824 and loss is: 189602.46875\n",
      "Iteration is: 825 and loss is: 189232.53125\n",
      "Iteration is: 826 and loss is: 189250.625\n",
      "Iteration is: 827 and loss is: 188461.671875\n",
      "Iteration is: 828 and loss is: 187906.65625\n",
      "Iteration is: 829 and loss is: 185419.703125\n",
      "Iteration is: 830 and loss is: 182834.546875\n",
      "Iteration is: 831 and loss is: 178778.25\n",
      "Iteration is: 832 and loss is: 175121.765625\n",
      "Iteration is: 833 and loss is: 171755.65625\n",
      "Iteration is: 834 and loss is: 169194.15625\n",
      "Iteration is: 835 and loss is: 167316.828125\n",
      "Iteration is: 836 and loss is: 165979.25\n",
      "Iteration is: 837 and loss is: 165107.5625\n",
      "Iteration is: 838 and loss is: 164647.96875\n",
      "Iteration is: 839 and loss is: 165157.1875\n",
      "Iteration is: 840 and loss is: 166243.1875\n",
      "Iteration is: 841 and loss is: 169911.65625\n",
      "Iteration is: 842 and loss is: 172055.125\n",
      "Iteration is: 843 and loss is: 176135.3125\n",
      "Iteration is: 844 and loss is: 169113.890625\n",
      "Iteration is: 845 and loss is: 160404.328125\n",
      "Iteration is: 846 and loss is: 150234.03125\n",
      "Iteration is: 847 and loss is: 147191.09375\n",
      "Iteration is: 848 and loss is: 150404.890625\n",
      "Iteration is: 849 and loss is: 153609.84375\n",
      "Iteration is: 850 and loss is: 154240.390625\n",
      "Iteration is: 851 and loss is: 147247.75\n",
      "Iteration is: 852 and loss is: 140503.25\n",
      "Iteration is: 853 and loss is: 137212.515625\n",
      "Iteration is: 854 and loss is: 138073.84375\n",
      "Iteration is: 855 and loss is: 140419.421875\n",
      "Iteration is: 856 and loss is: 139742.078125\n",
      "Iteration is: 857 and loss is: 137053.203125\n",
      "Iteration is: 858 and loss is: 131877.03125\n",
      "Iteration is: 859 and loss is: 128025.65625\n",
      "Iteration is: 860 and loss is: 126435.125\n",
      "Iteration is: 861 and loss is: 126574.25\n",
      "Iteration is: 862 and loss is: 127238.9765625\n",
      "Iteration is: 863 and loss is: 126479.53125\n",
      "Iteration is: 864 and loss is: 124929.390625\n",
      "Iteration is: 865 and loss is: 121676.25\n",
      "Iteration is: 866 and loss is: 118583.2421875\n",
      "Iteration is: 867 and loss is: 115920.1015625\n",
      "Iteration is: 868 and loss is: 114149.7734375\n",
      "Iteration is: 869 and loss is: 113119.421875\n",
      "Iteration is: 870 and loss is: 112499.109375\n",
      "Iteration is: 871 and loss is: 112171.5390625\n",
      "Iteration is: 872 and loss is: 111653.515625\n",
      "Iteration is: 873 and loss is: 111526.03125\n",
      "Iteration is: 874 and loss is: 110699.4921875\n",
      "Iteration is: 875 and loss is: 110529.6640625\n",
      "Iteration is: 876 and loss is: 109074.8515625\n",
      "Iteration is: 877 and loss is: 108353.5625\n",
      "Iteration is: 878 and loss is: 105984.0625\n",
      "Iteration is: 879 and loss is: 104237.609375\n",
      "Iteration is: 880 and loss is: 101305.484375\n",
      "Iteration is: 881 and loss is: 98911.328125\n",
      "Iteration is: 882 and loss is: 96285.3203125\n",
      "Iteration is: 883 and loss is: 94128.6796875\n",
      "Iteration is: 884 and loss is: 92216.703125\n",
      "Iteration is: 885 and loss is: 90611.625\n",
      "Iteration is: 886 and loss is: 89211.875\n",
      "Iteration is: 887 and loss is: 87958.375\n",
      "Iteration is: 888 and loss is: 86821.1171875\n",
      "Iteration is: 889 and loss is: 85811.78125\n",
      "Iteration is: 890 and loss is: 85056.9453125\n",
      "Iteration is: 891 and loss is: 84697.7109375\n",
      "Iteration is: 892 and loss is: 85480.1484375\n",
      "Iteration is: 893 and loss is: 87659.546875\n",
      "Iteration is: 894 and loss is: 95017.53125\n",
      "Iteration is: 895 and loss is: 103126.171875\n",
      "Iteration is: 896 and loss is: 123564.9921875\n",
      "Iteration is: 897 and loss is: 111875.6171875\n",
      "Iteration is: 898 and loss is: 100072.8125\n",
      "Iteration is: 899 and loss is: 76739.40625\n",
      "Iteration is: 900 and loss is: 76512.125\n",
      "Iteration is: 901 and loss is: 92312.7109375\n",
      "Iteration is: 902 and loss is: 90863.9296875\n",
      "Iteration is: 903 and loss is: 80635.390625\n",
      "Iteration is: 904 and loss is: 69112.953125\n",
      "Iteration is: 905 and loss is: 73638.671875\n",
      "Iteration is: 906 and loss is: 83344.0625\n",
      "Iteration is: 907 and loss is: 75742.3984375\n",
      "Iteration is: 908 and loss is: 66403.5625\n",
      "Iteration is: 909 and loss is: 65282.1953125\n",
      "Iteration is: 910 and loss is: 70386.140625\n",
      "Iteration is: 911 and loss is: 72440.09375\n",
      "Iteration is: 912 and loss is: 64822.8359375\n",
      "Iteration is: 913 and loss is: 60410.0625\n",
      "Iteration is: 914 and loss is: 62333.7890625\n",
      "Iteration is: 915 and loss is: 64451.06640625\n",
      "Iteration is: 916 and loss is: 63104.2265625\n",
      "Iteration is: 917 and loss is: 58166.05078125\n",
      "Iteration is: 918 and loss is: 56382.02734375\n",
      "Iteration is: 919 and loss is: 57834.82421875\n",
      "Iteration is: 920 and loss is: 58253.5859375\n",
      "Iteration is: 921 and loss is: 56560.34375\n",
      "Iteration is: 922 and loss is: 53530.9296875\n",
      "Iteration is: 923 and loss is: 52407.359375\n",
      "Iteration is: 924 and loss is: 52910.953125\n",
      "Iteration is: 925 and loss is: 52877.390625\n",
      "Iteration is: 926 and loss is: 51694.078125\n",
      "Iteration is: 927 and loss is: 49701.3984375\n",
      "Iteration is: 928 and loss is: 48588.53515625\n",
      "Iteration is: 929 and loss is: 48376.1796875\n",
      "Iteration is: 930 and loss is: 48234.765625\n",
      "Iteration is: 931 and loss is: 47611.46875\n",
      "Iteration is: 932 and loss is: 46364.3359375\n",
      "Iteration is: 933 and loss is: 45258.72265625\n",
      "Iteration is: 934 and loss is: 44490.9921875\n",
      "Iteration is: 935 and loss is: 44057.73828125\n",
      "Iteration is: 936 and loss is: 43687.90625\n",
      "Iteration is: 937 and loss is: 43144.5625\n",
      "Iteration is: 938 and loss is: 42468.1015625\n",
      "Iteration is: 939 and loss is: 41582.06640625\n",
      "Iteration is: 940 and loss is: 40738.65234375\n",
      "Iteration is: 941 and loss is: 40068.0234375\n",
      "Iteration is: 942 and loss is: 39648.4296875\n",
      "Iteration is: 943 and loss is: 39348.96875\n",
      "Iteration is: 944 and loss is: 38933.48828125\n",
      "Iteration is: 945 and loss is: 38374.24609375\n",
      "Iteration is: 946 and loss is: 37676.04296875\n",
      "Iteration is: 947 and loss is: 37023.40234375\n",
      "Iteration is: 948 and loss is: 36454.046875\n",
      "Iteration is: 949 and loss is: 35951.7109375\n",
      "Iteration is: 950 and loss is: 35484.69921875\n",
      "Iteration is: 951 and loss is: 35046.3046875\n",
      "Iteration is: 952 and loss is: 34656.2890625\n",
      "Iteration is: 953 and loss is: 34293.25\n",
      "Iteration is: 954 and loss is: 33932.296875\n",
      "Iteration is: 955 and loss is: 33525.17578125\n",
      "Iteration is: 956 and loss is: 33102.4375\n",
      "Iteration is: 957 and loss is: 32665.36328125\n",
      "Iteration is: 958 and loss is: 32260.708984375\n",
      "Iteration is: 959 and loss is: 31859.90625\n",
      "Iteration is: 960 and loss is: 31472.939453125\n",
      "Iteration is: 961 and loss is: 31078.642578125\n",
      "Iteration is: 962 and loss is: 30703.7578125\n",
      "Iteration is: 963 and loss is: 30347.6015625\n",
      "Iteration is: 964 and loss is: 30023.65234375\n",
      "Iteration is: 965 and loss is: 29712.779296875\n",
      "Iteration is: 966 and loss is: 29427.203125\n",
      "Iteration is: 967 and loss is: 29157.916015625\n",
      "Iteration is: 968 and loss is: 28949.34375\n",
      "Iteration is: 969 and loss is: 28788.333984375\n",
      "Iteration is: 970 and loss is: 28769.73828125\n",
      "Iteration is: 971 and loss is: 28838.791015625\n",
      "Iteration is: 972 and loss is: 29280.896484375\n",
      "Iteration is: 973 and loss is: 29868.31640625\n",
      "Iteration is: 974 and loss is: 31516.9765625\n",
      "Iteration is: 975 and loss is: 33001.3515625\n",
      "Iteration is: 976 and loss is: 37067.2734375\n",
      "Iteration is: 977 and loss is: 38309.10546875\n",
      "Iteration is: 978 and loss is: 43373.46484375\n",
      "Iteration is: 979 and loss is: 39252.79296875\n",
      "Iteration is: 980 and loss is: 37508.37109375\n",
      "Iteration is: 981 and loss is: 30172.837890625\n",
      "Iteration is: 982 and loss is: 25929.97265625\n",
      "Iteration is: 983 and loss is: 24870.392578125\n",
      "Iteration is: 984 and loss is: 26719.07421875\n",
      "Iteration is: 985 and loss is: 29926.86328125\n",
      "Iteration is: 986 and loss is: 30465.580078125\n",
      "Iteration is: 987 and loss is: 30206.662109375\n",
      "Iteration is: 988 and loss is: 26721.55078125\n",
      "Iteration is: 989 and loss is: 24224.849609375\n",
      "Iteration is: 990 and loss is: 23396.2578125\n",
      "Iteration is: 991 and loss is: 24282.541015625\n",
      "Iteration is: 992 and loss is: 25843.806640625\n",
      "Iteration is: 993 and loss is: 26104.689453125\n",
      "Iteration is: 994 and loss is: 25667.7421875\n",
      "Iteration is: 995 and loss is: 23913.109375\n",
      "Iteration is: 996 and loss is: 22617.09375\n",
      "Iteration is: 997 and loss is: 22173.7265625\n",
      "Iteration is: 998 and loss is: 22549.646484375\n",
      "Iteration is: 999 and loss is: 23221.125\n",
      "Iteration is: 1000 and loss is: 23345.8125\n",
      "Iteration is: 1001 and loss is: 23074.1953125\n",
      "Iteration is: 1002 and loss is: 22196.8671875\n",
      "Iteration is: 1003 and loss is: 21460.650390625\n",
      "Iteration is: 1004 and loss is: 21091.91015625\n",
      "Iteration is: 1005 and loss is: 21141.72265625\n",
      "Iteration is: 1006 and loss is: 21391.486328125\n",
      "Iteration is: 1007 and loss is: 21480.80859375\n",
      "Iteration is: 1008 and loss is: 21367.2890625\n",
      "Iteration is: 1009 and loss is: 20935.638671875\n",
      "Iteration is: 1010 and loss is: 20496.51171875\n",
      "Iteration is: 1011 and loss is: 20173.3046875\n",
      "Iteration is: 1012 and loss is: 20055.537109375\n",
      "Iteration is: 1013 and loss is: 20080.669921875\n",
      "Iteration is: 1014 and loss is: 20111.5703125\n",
      "Iteration is: 1015 and loss is: 20072.052734375\n",
      "Iteration is: 1016 and loss is: 19882.009765625\n",
      "Iteration is: 1017 and loss is: 19636.373046875\n",
      "Iteration is: 1018 and loss is: 19383.392578125\n",
      "Iteration is: 1019 and loss is: 19200.484375\n",
      "Iteration is: 1020 and loss is: 19100.375\n",
      "Iteration is: 1021 and loss is: 19056.48046875\n",
      "Iteration is: 1022 and loss is: 19019.095703125\n",
      "Iteration is: 1023 and loss is: 18939.41796875\n",
      "Iteration is: 1024 and loss is: 18818.3671875\n",
      "Iteration is: 1025 and loss is: 18654.9296875\n",
      "Iteration is: 1026 and loss is: 18491.453125\n",
      "Iteration is: 1027 and loss is: 18348.986328125\n",
      "Iteration is: 1028 and loss is: 18238.892578125\n",
      "Iteration is: 1029 and loss is: 18158.291015625\n",
      "Iteration is: 1030 and loss is: 18088.552734375\n",
      "Iteration is: 1031 and loss is: 18016.005859375\n",
      "Iteration is: 1032 and loss is: 17925.03125\n",
      "Iteration is: 1033 and loss is: 17818.955078125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 1034 and loss is: 17701.49609375\n",
      "Iteration is: 1035 and loss is: 17584.28125\n",
      "Iteration is: 1036 and loss is: 17475.802734375\n",
      "Iteration is: 1037 and loss is: 17379.765625\n",
      "Iteration is: 1038 and loss is: 17295.21875\n",
      "Iteration is: 1039 and loss is: 17217.37109375\n",
      "Iteration is: 1040 and loss is: 17139.451171875\n",
      "Iteration is: 1041 and loss is: 17057.12109375\n",
      "Iteration is: 1042 and loss is: 16969.30078125\n",
      "Iteration is: 1043 and loss is: 16876.87890625\n",
      "Iteration is: 1044 and loss is: 16782.1328125\n",
      "Iteration is: 1045 and loss is: 16689.619140625\n",
      "Iteration is: 1046 and loss is: 16600.875\n",
      "Iteration is: 1047 and loss is: 16517.26171875\n",
      "Iteration is: 1048 and loss is: 16437.912109375\n",
      "Iteration is: 1049 and loss is: 16360.720703125\n",
      "Iteration is: 1050 and loss is: 16284.2900390625\n",
      "Iteration is: 1051 and loss is: 16207.568359375\n",
      "Iteration is: 1052 and loss is: 16129.1201171875\n",
      "Iteration is: 1053 and loss is: 16049.115234375\n",
      "Iteration is: 1054 and loss is: 15968.86328125\n",
      "Iteration is: 1055 and loss is: 15889.189453125\n",
      "Iteration is: 1056 and loss is: 15810.5380859375\n",
      "Iteration is: 1057 and loss is: 15733.341796875\n",
      "Iteration is: 1058 and loss is: 15657.951171875\n",
      "Iteration is: 1059 and loss is: 15584.2275390625\n",
      "Iteration is: 1060 and loss is: 15511.388671875\n",
      "Iteration is: 1061 and loss is: 15439.955078125\n",
      "Iteration is: 1062 and loss is: 15368.9931640625\n",
      "Iteration is: 1063 and loss is: 15297.87109375\n",
      "Iteration is: 1064 and loss is: 15226.0751953125\n",
      "Iteration is: 1065 and loss is: 15155.421875\n",
      "Iteration is: 1066 and loss is: 15084.3076171875\n",
      "Iteration is: 1067 and loss is: 15013.7236328125\n",
      "Iteration is: 1068 and loss is: 14944.236328125\n",
      "Iteration is: 1069 and loss is: 14875.1005859375\n",
      "Iteration is: 1070 and loss is: 14806.98828125\n",
      "Iteration is: 1071 and loss is: 14739.0419921875\n",
      "Iteration is: 1072 and loss is: 14672.6845703125\n",
      "Iteration is: 1073 and loss is: 14606.1416015625\n",
      "Iteration is: 1074 and loss is: 14541.09375\n",
      "Iteration is: 1075 and loss is: 14476.171875\n",
      "Iteration is: 1076 and loss is: 14410.9580078125\n",
      "Iteration is: 1077 and loss is: 14346.712890625\n",
      "Iteration is: 1078 and loss is: 14282.865234375\n",
      "Iteration is: 1079 and loss is: 14219.0302734375\n",
      "Iteration is: 1080 and loss is: 14155.841796875\n",
      "Iteration is: 1081 and loss is: 14092.962890625\n",
      "Iteration is: 1082 and loss is: 14030.5712890625\n",
      "Iteration is: 1083 and loss is: 13969.0068359375\n",
      "Iteration is: 1084 and loss is: 13907.7783203125\n",
      "Iteration is: 1085 and loss is: 13846.892578125\n",
      "Iteration is: 1086 and loss is: 13786.1171875\n",
      "Iteration is: 1087 and loss is: 13726.0556640625\n",
      "Iteration is: 1088 and loss is: 13666.396484375\n",
      "Iteration is: 1089 and loss is: 13607.2783203125\n",
      "Iteration is: 1090 and loss is: 13548.2666015625\n",
      "Iteration is: 1091 and loss is: 13490.0908203125\n",
      "Iteration is: 1092 and loss is: 13432.46484375\n",
      "Iteration is: 1093 and loss is: 13374.658203125\n",
      "Iteration is: 1094 and loss is: 13317.69921875\n",
      "Iteration is: 1095 and loss is: 13260.75\n",
      "Iteration is: 1096 and loss is: 13204.552734375\n",
      "Iteration is: 1097 and loss is: 13148.2822265625\n",
      "Iteration is: 1098 and loss is: 13092.9296875\n",
      "Iteration is: 1099 and loss is: 13037.357421875\n",
      "Iteration is: 1100 and loss is: 12982.697265625\n",
      "Iteration is: 1101 and loss is: 12927.8642578125\n",
      "Iteration is: 1102 and loss is: 12873.6181640625\n",
      "Iteration is: 1103 and loss is: 12819.7197265625\n",
      "Iteration is: 1104 and loss is: 12765.9501953125\n",
      "Iteration is: 1105 and loss is: 12712.955078125\n",
      "Iteration is: 1106 and loss is: 12659.908203125\n",
      "Iteration is: 1107 and loss is: 12607.529296875\n",
      "Iteration is: 1108 and loss is: 12555.185546875\n",
      "Iteration is: 1109 and loss is: 12503.224609375\n",
      "Iteration is: 1110 and loss is: 12451.748046875\n",
      "Iteration is: 1111 and loss is: 12399.984375\n",
      "Iteration is: 1112 and loss is: 12349.3125\n",
      "Iteration is: 1113 and loss is: 12299.076171875\n",
      "Iteration is: 1114 and loss is: 12248.478515625\n",
      "Iteration is: 1115 and loss is: 12198.345703125\n",
      "Iteration is: 1116 and loss is: 12148.982421875\n",
      "Iteration is: 1117 and loss is: 12099.599609375\n",
      "Iteration is: 1118 and loss is: 12050.380859375\n",
      "Iteration is: 1119 and loss is: 12001.4638671875\n",
      "Iteration is: 1120 and loss is: 11953.34375\n",
      "Iteration is: 1121 and loss is: 11905.3955078125\n",
      "Iteration is: 1122 and loss is: 11857.5595703125\n",
      "Iteration is: 1123 and loss is: 11810.6396484375\n",
      "Iteration is: 1124 and loss is: 11763.3544921875\n",
      "Iteration is: 1125 and loss is: 11717.123046875\n",
      "Iteration is: 1126 and loss is: 11670.96484375\n",
      "Iteration is: 1127 and loss is: 11625.1826171875\n",
      "Iteration is: 1128 and loss is: 11580.671875\n",
      "Iteration is: 1129 and loss is: 11536.83203125\n",
      "Iteration is: 1130 and loss is: 11493.916015625\n",
      "Iteration is: 1131 and loss is: 11452.072265625\n",
      "Iteration is: 1132 and loss is: 11412.15234375\n",
      "Iteration is: 1133 and loss is: 11374.43359375\n",
      "Iteration is: 1134 and loss is: 11339.03125\n",
      "Iteration is: 1135 and loss is: 11307.115234375\n",
      "Iteration is: 1136 and loss is: 11281.552734375\n",
      "Iteration is: 1137 and loss is: 11261.4755859375\n",
      "Iteration is: 1138 and loss is: 11255.865234375\n",
      "Iteration is: 1139 and loss is: 11259.6103515625\n",
      "Iteration is: 1140 and loss is: 11292.568359375\n",
      "Iteration is: 1141 and loss is: 11341.0341796875\n",
      "Iteration is: 1142 and loss is: 11454.8740234375\n",
      "Iteration is: 1143 and loss is: 11586.40625\n",
      "Iteration is: 1144 and loss is: 11863.1259765625\n",
      "Iteration is: 1145 and loss is: 12135.083984375\n",
      "Iteration is: 1146 and loss is: 12717.970703125\n",
      "Iteration is: 1147 and loss is: 13161.384765625\n",
      "Iteration is: 1148 and loss is: 14214.7919921875\n",
      "Iteration is: 1149 and loss is: 14673.1904296875\n",
      "Iteration is: 1150 and loss is: 16105.1982421875\n",
      "Iteration is: 1151 and loss is: 15970.7734375\n",
      "Iteration is: 1152 and loss is: 16917.15234375\n",
      "Iteration is: 1153 and loss is: 15532.8671875\n",
      "Iteration is: 1154 and loss is: 14943.201171875\n",
      "Iteration is: 1155 and loss is: 12990.806640625\n",
      "Iteration is: 1156 and loss is: 11681.7822265625\n",
      "Iteration is: 1157 and loss is: 10655.873046875\n",
      "Iteration is: 1158 and loss is: 10304.912109375\n",
      "Iteration is: 1159 and loss is: 10516.6611328125\n",
      "Iteration is: 1160 and loss is: 11031.314453125\n",
      "Iteration is: 1161 and loss is: 11673.2236328125\n",
      "Iteration is: 1162 and loss is: 11895.990234375\n",
      "Iteration is: 1163 and loss is: 12034.845703125\n",
      "Iteration is: 1164 and loss is: 11537.0458984375\n",
      "Iteration is: 1165 and loss is: 11052.37109375\n",
      "Iteration is: 1166 and loss is: 10448.90625\n",
      "Iteration is: 1167 and loss is: 10063.2470703125\n",
      "Iteration is: 1168 and loss is: 9919.0654296875\n",
      "Iteration is: 1169 and loss is: 9993.6494140625\n",
      "Iteration is: 1170 and loss is: 10192.7197265625\n",
      "Iteration is: 1171 and loss is: 10366.2978515625\n",
      "Iteration is: 1172 and loss is: 10492.8173828125\n",
      "Iteration is: 1173 and loss is: 10413.5771484375\n",
      "Iteration is: 1174 and loss is: 10275.740234375\n",
      "Iteration is: 1175 and loss is: 10022.41796875\n",
      "Iteration is: 1176 and loss is: 9802.140625\n",
      "Iteration is: 1177 and loss is: 9635.7578125\n",
      "Iteration is: 1178 and loss is: 9559.44140625\n",
      "Iteration is: 1179 and loss is: 9561.2431640625\n",
      "Iteration is: 1180 and loss is: 9605.560546875\n",
      "Iteration is: 1181 and loss is: 9657.8046875\n",
      "Iteration is: 1182 and loss is: 9669.4931640625\n",
      "Iteration is: 1183 and loss is: 9648.8857421875\n",
      "Iteration is: 1184 and loss is: 9569.0595703125\n",
      "Iteration is: 1185 and loss is: 9474.8056640625\n",
      "Iteration is: 1186 and loss is: 9368.8994140625\n",
      "Iteration is: 1187 and loss is: 9281.671875\n",
      "Iteration is: 1188 and loss is: 9220.9404296875\n",
      "Iteration is: 1189 and loss is: 9188.6572265625\n",
      "Iteration is: 1190 and loss is: 9176.658203125\n",
      "Iteration is: 1191 and loss is: 9172.2978515625\n",
      "Iteration is: 1192 and loss is: 9166.0244140625\n",
      "Iteration is: 1193 and loss is: 9145.2939453125\n",
      "Iteration is: 1194 and loss is: 9113.6904296875\n",
      "Iteration is: 1195 and loss is: 9066.462890625\n",
      "Iteration is: 1196 and loss is: 9014.1044921875\n",
      "Iteration is: 1197 and loss is: 8958.544921875\n",
      "Iteration is: 1198 and loss is: 8908.40625\n",
      "Iteration is: 1199 and loss is: 8865.4208984375\n",
      "Iteration is: 1200 and loss is: 8830.6875\n",
      "Iteration is: 1201 and loss is: 8803.0791015625\n",
      "Iteration is: 1202 and loss is: 8779.322265625\n",
      "Iteration is: 1203 and loss is: 8756.7470703125\n",
      "Iteration is: 1204 and loss is: 8732.6513671875\n",
      "Iteration is: 1205 and loss is: 8705.4287109375\n",
      "Iteration is: 1206 and loss is: 8674.1484375\n",
      "Iteration is: 1207 and loss is: 8640.060546875\n",
      "Iteration is: 1208 and loss is: 8603.5625\n",
      "Iteration is: 1209 and loss is: 8567.234375\n",
      "Iteration is: 1210 and loss is: 8531.3603515625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 1211 and loss is: 8497.2978515625\n",
      "Iteration is: 1212 and loss is: 8465.27734375\n",
      "Iteration is: 1213 and loss is: 8435.337890625\n",
      "Iteration is: 1214 and loss is: 8406.576171875\n",
      "Iteration is: 1215 and loss is: 8379.0205078125\n",
      "Iteration is: 1216 and loss is: 8352.09375\n",
      "Iteration is: 1217 and loss is: 8325.1181640625\n",
      "Iteration is: 1218 and loss is: 8297.2548828125\n",
      "Iteration is: 1219 and loss is: 8268.958984375\n",
      "Iteration is: 1220 and loss is: 8240.08203125\n",
      "Iteration is: 1221 and loss is: 8210.845703125\n",
      "Iteration is: 1222 and loss is: 8181.20263671875\n",
      "Iteration is: 1223 and loss is: 8151.69287109375\n",
      "Iteration is: 1224 and loss is: 8121.90576171875\n",
      "Iteration is: 1225 and loss is: 8092.62158203125\n",
      "Iteration is: 1226 and loss is: 8063.70751953125\n",
      "Iteration is: 1227 and loss is: 8035.6953125\n",
      "Iteration is: 1228 and loss is: 8007.16455078125\n",
      "Iteration is: 1229 and loss is: 7979.71044921875\n",
      "Iteration is: 1230 and loss is: 7952.4296875\n",
      "Iteration is: 1231 and loss is: 7925.5400390625\n",
      "Iteration is: 1232 and loss is: 7898.80810546875\n",
      "Iteration is: 1233 and loss is: 7872.23046875\n",
      "Iteration is: 1234 and loss is: 7845.564453125\n",
      "Iteration is: 1235 and loss is: 7819.125\n",
      "Iteration is: 1236 and loss is: 7792.57275390625\n",
      "Iteration is: 1237 and loss is: 7765.9609375\n",
      "Iteration is: 1238 and loss is: 7739.537109375\n",
      "Iteration is: 1239 and loss is: 7713.20263671875\n",
      "Iteration is: 1240 and loss is: 7686.9404296875\n",
      "Iteration is: 1241 and loss is: 7660.76123046875\n",
      "Iteration is: 1242 and loss is: 7634.3251953125\n",
      "Iteration is: 1243 and loss is: 7608.4365234375\n",
      "Iteration is: 1244 and loss is: 7582.3564453125\n",
      "Iteration is: 1245 and loss is: 7557.0146484375\n",
      "Iteration is: 1246 and loss is: 7530.75390625\n",
      "Iteration is: 1247 and loss is: 7505.48681640625\n",
      "Iteration is: 1248 and loss is: 7480.03271484375\n",
      "Iteration is: 1249 and loss is: 7454.939453125\n",
      "Iteration is: 1250 and loss is: 7429.8466796875\n",
      "Iteration is: 1251 and loss is: 7404.59375\n",
      "Iteration is: 1252 and loss is: 7379.77587890625\n",
      "Iteration is: 1253 and loss is: 7354.6259765625\n",
      "Iteration is: 1254 and loss is: 7330.01171875\n",
      "Iteration is: 1255 and loss is: 7305.3203125\n",
      "Iteration is: 1256 and loss is: 7281.02880859375\n",
      "Iteration is: 1257 and loss is: 7256.55419921875\n",
      "Iteration is: 1258 and loss is: 7232.01025390625\n",
      "Iteration is: 1259 and loss is: 7208.0322265625\n",
      "Iteration is: 1260 and loss is: 7183.65869140625\n",
      "Iteration is: 1261 and loss is: 7159.9560546875\n",
      "Iteration is: 1262 and loss is: 7136.28955078125\n",
      "Iteration is: 1263 and loss is: 7112.38671875\n",
      "Iteration is: 1264 and loss is: 7088.81005859375\n",
      "Iteration is: 1265 and loss is: 7065.25537109375\n",
      "Iteration is: 1266 and loss is: 7042.2001953125\n",
      "Iteration is: 1267 and loss is: 7018.74853515625\n",
      "Iteration is: 1268 and loss is: 6995.47265625\n",
      "Iteration is: 1269 and loss is: 6972.34765625\n",
      "Iteration is: 1270 and loss is: 6949.625\n",
      "Iteration is: 1271 and loss is: 6927.1435546875\n",
      "Iteration is: 1272 and loss is: 6904.52978515625\n",
      "Iteration is: 1273 and loss is: 6882.37841796875\n",
      "Iteration is: 1274 and loss is: 6860.49267578125\n",
      "Iteration is: 1275 and loss is: 6838.5283203125\n",
      "Iteration is: 1276 and loss is: 6817.06982421875\n",
      "Iteration is: 1277 and loss is: 6796.27294921875\n",
      "Iteration is: 1278 and loss is: 6776.07666015625\n",
      "Iteration is: 1279 and loss is: 6756.8447265625\n",
      "Iteration is: 1280 and loss is: 6738.68359375\n",
      "Iteration is: 1281 and loss is: 6722.0537109375\n",
      "Iteration is: 1282 and loss is: 6707.2216796875\n",
      "Iteration is: 1283 and loss is: 6696.2412109375\n",
      "Iteration is: 1284 and loss is: 6688.8193359375\n",
      "Iteration is: 1285 and loss is: 6688.82763671875\n",
      "Iteration is: 1286 and loss is: 6695.21533203125\n",
      "Iteration is: 1287 and loss is: 6718.697265625\n",
      "Iteration is: 1288 and loss is: 6753.70361328125\n",
      "Iteration is: 1289 and loss is: 6825.599609375\n",
      "Iteration is: 1290 and loss is: 6916.79541015625\n",
      "Iteration is: 1291 and loss is: 7092.53076171875\n",
      "Iteration is: 1292 and loss is: 7291.0732421875\n",
      "Iteration is: 1293 and loss is: 7680.75927734375\n",
      "Iteration is: 1294 and loss is: 8056.07373046875\n",
      "Iteration is: 1295 and loss is: 8848.9384765625\n",
      "Iteration is: 1296 and loss is: 9419.8046875\n",
      "Iteration is: 1297 and loss is: 10800.154296875\n",
      "Iteration is: 1298 and loss is: 11269.740234375\n",
      "Iteration is: 1299 and loss is: 12939.09375\n",
      "Iteration is: 1300 and loss is: 12422.177734375\n",
      "Iteration is: 1301 and loss is: 13049.10546875\n",
      "Iteration is: 1302 and loss is: 11067.37890625\n",
      "Iteration is: 1303 and loss is: 9867.521484375\n",
      "Iteration is: 1304 and loss is: 7880.43359375\n",
      "Iteration is: 1305 and loss is: 6685.1123046875\n",
      "Iteration is: 1306 and loss is: 6189.5654296875\n",
      "Iteration is: 1307 and loss is: 6409.62353515625\n",
      "Iteration is: 1308 and loss is: 7090.84765625\n",
      "Iteration is: 1309 and loss is: 7724.40673828125\n",
      "Iteration is: 1310 and loss is: 8331.3603515625\n",
      "Iteration is: 1311 and loss is: 8126.4072265625\n",
      "Iteration is: 1312 and loss is: 7829.94970703125\n",
      "Iteration is: 1313 and loss is: 7041.96435546875\n",
      "Iteration is: 1314 and loss is: 6435.08154296875\n",
      "Iteration is: 1315 and loss is: 6056.84765625\n",
      "Iteration is: 1316 and loss is: 6016.97265625\n",
      "Iteration is: 1317 and loss is: 6230.111328125\n",
      "Iteration is: 1318 and loss is: 6512.875\n",
      "Iteration is: 1319 and loss is: 6771.18798828125\n",
      "Iteration is: 1320 and loss is: 6770.734375\n",
      "Iteration is: 1321 and loss is: 6672.6044921875\n",
      "Iteration is: 1322 and loss is: 6375.26171875\n",
      "Iteration is: 1323 and loss is: 6106.22509765625\n",
      "Iteration is: 1324 and loss is: 5899.83837890625\n",
      "Iteration is: 1325 and loss is: 5822.93212890625\n",
      "Iteration is: 1326 and loss is: 5861.8828125\n",
      "Iteration is: 1327 and loss is: 5959.9951171875\n",
      "Iteration is: 1328 and loss is: 6063.01318359375\n",
      "Iteration is: 1329 and loss is: 6094.298828125\n",
      "Iteration is: 1330 and loss is: 6074.177734375\n",
      "Iteration is: 1331 and loss is: 5970.869140625\n",
      "Iteration is: 1332 and loss is: 5854.92578125\n",
      "Iteration is: 1333 and loss is: 5741.9833984375\n",
      "Iteration is: 1334 and loss is: 5669.6904296875\n",
      "Iteration is: 1335 and loss is: 5644.056640625\n",
      "Iteration is: 1336 and loss is: 5655.849609375\n",
      "Iteration is: 1337 and loss is: 5685.08837890625\n",
      "Iteration is: 1338 and loss is: 5706.52734375\n",
      "Iteration is: 1339 and loss is: 5711.22607421875\n",
      "Iteration is: 1340 and loss is: 5684.81787109375\n",
      "Iteration is: 1341 and loss is: 5643.02783203125\n",
      "Iteration is: 1342 and loss is: 5587.63134765625\n",
      "Iteration is: 1343 and loss is: 5536.587890625\n",
      "Iteration is: 1344 and loss is: 5496.171875\n",
      "Iteration is: 1345 and loss is: 5471.828125\n",
      "Iteration is: 1346 and loss is: 5461.2236328125\n",
      "Iteration is: 1347 and loss is: 5458.87451171875\n",
      "Iteration is: 1348 and loss is: 5457.8837890625\n",
      "Iteration is: 1349 and loss is: 5451.95654296875\n",
      "Iteration is: 1350 and loss is: 5439.787109375\n",
      "Iteration is: 1351 and loss is: 5418.7861328125\n",
      "Iteration is: 1352 and loss is: 5393.43359375\n",
      "Iteration is: 1353 and loss is: 5364.5595703125\n",
      "Iteration is: 1354 and loss is: 5337.298828125\n",
      "Iteration is: 1355 and loss is: 5313.09130859375\n",
      "Iteration is: 1356 and loss is: 5293.5693359375\n",
      "Iteration is: 1357 and loss is: 5278.341796875\n",
      "Iteration is: 1358 and loss is: 5265.64208984375\n",
      "Iteration is: 1359 and loss is: 5254.4296875\n",
      "Iteration is: 1360 and loss is: 5243.00341796875\n",
      "Iteration is: 1361 and loss is: 5230.04052734375\n",
      "Iteration is: 1362 and loss is: 5215.0498046875\n",
      "Iteration is: 1363 and loss is: 5198.216796875\n",
      "Iteration is: 1364 and loss is: 5180.23046875\n",
      "Iteration is: 1365 and loss is: 5161.57861328125\n",
      "Iteration is: 1366 and loss is: 5142.7412109375\n",
      "Iteration is: 1367 and loss is: 5125.08349609375\n",
      "Iteration is: 1368 and loss is: 5108.193359375\n",
      "Iteration is: 1369 and loss is: 5092.44287109375\n",
      "Iteration is: 1370 and loss is: 5077.41064453125\n",
      "Iteration is: 1371 and loss is: 5063.2294921875\n",
      "Iteration is: 1372 and loss is: 5049.291015625\n",
      "Iteration is: 1373 and loss is: 5035.3896484375\n",
      "Iteration is: 1374 and loss is: 5021.470703125\n",
      "Iteration is: 1375 and loss is: 5007.056640625\n",
      "Iteration is: 1376 and loss is: 4992.60791015625\n",
      "Iteration is: 1377 and loss is: 4977.80029296875\n",
      "Iteration is: 1378 and loss is: 4962.4306640625\n",
      "Iteration is: 1379 and loss is: 4947.265625\n",
      "Iteration is: 1380 and loss is: 4932.03466796875\n",
      "Iteration is: 1381 and loss is: 4916.9609375\n",
      "Iteration is: 1382 and loss is: 4902.16650390625\n",
      "Iteration is: 1383 and loss is: 4887.52783203125\n",
      "Iteration is: 1384 and loss is: 4872.87939453125\n",
      "Iteration is: 1385 and loss is: 4858.58056640625\n",
      "Iteration is: 1386 and loss is: 4844.4619140625\n",
      "Iteration is: 1387 and loss is: 4830.658203125\n",
      "Iteration is: 1388 and loss is: 4816.68408203125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 1389 and loss is: 4802.7939453125\n",
      "Iteration is: 1390 and loss is: 4789.3427734375\n",
      "Iteration is: 1391 and loss is: 4775.544921875\n",
      "Iteration is: 1392 and loss is: 4761.8671875\n",
      "Iteration is: 1393 and loss is: 4748.17041015625\n",
      "Iteration is: 1394 and loss is: 4734.58544921875\n",
      "Iteration is: 1395 and loss is: 4721.07080078125\n",
      "Iteration is: 1396 and loss is: 4707.36474609375\n",
      "Iteration is: 1397 and loss is: 4693.84326171875\n",
      "Iteration is: 1398 and loss is: 4680.5400390625\n",
      "Iteration is: 1399 and loss is: 4667.08642578125\n",
      "Iteration is: 1400 and loss is: 4653.78564453125\n",
      "Iteration is: 1401 and loss is: 4640.55712890625\n",
      "Iteration is: 1402 and loss is: 4627.70361328125\n",
      "Iteration is: 1403 and loss is: 4614.7744140625\n",
      "Iteration is: 1404 and loss is: 4602.43408203125\n",
      "Iteration is: 1405 and loss is: 4590.27099609375\n",
      "Iteration is: 1406 and loss is: 4578.68310546875\n",
      "Iteration is: 1407 and loss is: 4567.70654296875\n",
      "Iteration is: 1408 and loss is: 4557.95166015625\n",
      "Iteration is: 1409 and loss is: 4549.22705078125\n",
      "Iteration is: 1410 and loss is: 4542.830078125\n",
      "Iteration is: 1411 and loss is: 4538.94921875\n",
      "Iteration is: 1412 and loss is: 4540.41015625\n",
      "Iteration is: 1413 and loss is: 4545.39111328125\n",
      "Iteration is: 1414 and loss is: 4563.64404296875\n",
      "Iteration is: 1415 and loss is: 4587.00244140625\n",
      "Iteration is: 1416 and loss is: 4640.84521484375\n",
      "Iteration is: 1417 and loss is: 4695.87158203125\n",
      "Iteration is: 1418 and loss is: 4818.31787109375\n",
      "Iteration is: 1419 and loss is: 4908.98828125\n",
      "Iteration is: 1420 and loss is: 5127.22216796875\n",
      "Iteration is: 1421 and loss is: 5196.34716796875\n",
      "Iteration is: 1422 and loss is: 5434.2646484375\n",
      "Iteration is: 1423 and loss is: 5321.580078125\n",
      "Iteration is: 1424 and loss is: 5332.107421875\n",
      "Iteration is: 1425 and loss is: 5016.98681640625\n",
      "Iteration is: 1426 and loss is: 4798.0068359375\n",
      "Iteration is: 1427 and loss is: 4617.71484375\n",
      "Iteration is: 1428 and loss is: 4584.21142578125\n",
      "Iteration is: 1429 and loss is: 4716.17578125\n",
      "Iteration is: 1430 and loss is: 4905.99169921875\n",
      "Iteration is: 1431 and loss is: 5171.873046875\n",
      "Iteration is: 1432 and loss is: 5297.68896484375\n",
      "Iteration is: 1433 and loss is: 5484.30859375\n",
      "Iteration is: 1434 and loss is: 5471.21533203125\n",
      "Iteration is: 1435 and loss is: 5616.5234375\n",
      "Iteration is: 1436 and loss is: 5629.67333984375\n",
      "Iteration is: 1437 and loss is: 5928.9375\n",
      "Iteration is: 1438 and loss is: 6007.44775390625\n",
      "Iteration is: 1439 and loss is: 6386.85400390625\n",
      "Iteration is: 1440 and loss is: 6354.51953125\n",
      "Iteration is: 1441 and loss is: 6542.140625\n",
      "Iteration is: 1442 and loss is: 6221.5517578125\n",
      "Iteration is: 1443 and loss is: 6074.4912109375\n",
      "Iteration is: 1444 and loss is: 5551.9228515625\n",
      "Iteration is: 1445 and loss is: 5226.95361328125\n",
      "Iteration is: 1446 and loss is: 4806.4169921875\n",
      "Iteration is: 1447 and loss is: 4553.64306640625\n",
      "Iteration is: 1448 and loss is: 4351.68994140625\n",
      "Iteration is: 1449 and loss is: 4243.0439453125\n",
      "Iteration is: 1450 and loss is: 4171.67431640625\n",
      "Iteration is: 1451 and loss is: 4149.3212890625\n",
      "Iteration is: 1452 and loss is: 4150.0830078125\n",
      "Iteration is: 1453 and loss is: 4182.7763671875\n",
      "Iteration is: 1454 and loss is: 4249.8115234375\n",
      "Iteration is: 1455 and loss is: 4316.9150390625\n",
      "Iteration is: 1456 and loss is: 4402.1943359375\n",
      "Iteration is: 1457 and loss is: 4433.041015625\n",
      "Iteration is: 1458 and loss is: 4461.75048828125\n",
      "Iteration is: 1459 and loss is: 4412.37451171875\n",
      "Iteration is: 1460 and loss is: 4367.431640625\n",
      "Iteration is: 1461 and loss is: 4273.37841796875\n",
      "Iteration is: 1462 and loss is: 4199.35791015625\n",
      "Iteration is: 1463 and loss is: 4114.3271484375\n",
      "Iteration is: 1464 and loss is: 4053.19580078125\n",
      "Iteration is: 1465 and loss is: 3996.1806640625\n",
      "Iteration is: 1466 and loss is: 3952.721923828125\n",
      "Iteration is: 1467 and loss is: 3915.451904296875\n",
      "Iteration is: 1468 and loss is: 3884.309326171875\n",
      "Iteration is: 1469 and loss is: 3860.90380859375\n",
      "Iteration is: 1470 and loss is: 3844.78564453125\n",
      "Iteration is: 1471 and loss is: 3836.862060546875\n",
      "Iteration is: 1472 and loss is: 3835.66357421875\n",
      "Iteration is: 1473 and loss is: 3839.915771484375\n",
      "Iteration is: 1474 and loss is: 3845.405517578125\n",
      "Iteration is: 1475 and loss is: 3851.191162109375\n",
      "Iteration is: 1476 and loss is: 3852.451416015625\n",
      "Iteration is: 1477 and loss is: 3852.132080078125\n",
      "Iteration is: 1478 and loss is: 3845.802734375\n",
      "Iteration is: 1479 and loss is: 3839.846923828125\n",
      "Iteration is: 1480 and loss is: 3829.372802734375\n",
      "Iteration is: 1481 and loss is: 3821.8330078125\n",
      "Iteration is: 1482 and loss is: 3810.705810546875\n",
      "Iteration is: 1483 and loss is: 3803.415283203125\n",
      "Iteration is: 1484 and loss is: 3792.512939453125\n",
      "Iteration is: 1485 and loss is: 3785.29638671875\n",
      "Iteration is: 1486 and loss is: 3774.218017578125\n",
      "Iteration is: 1487 and loss is: 3765.851318359375\n",
      "Iteration is: 1488 and loss is: 3754.25927734375\n",
      "Iteration is: 1489 and loss is: 3746.6513671875\n",
      "Iteration is: 1490 and loss is: 3736.31640625\n",
      "Iteration is: 1491 and loss is: 3730.583251953125\n",
      "Iteration is: 1492 and loss is: 3722.559326171875\n",
      "Iteration is: 1493 and loss is: 3720.283935546875\n",
      "Iteration is: 1494 and loss is: 3715.361083984375\n",
      "Iteration is: 1495 and loss is: 3716.6396484375\n",
      "Iteration is: 1496 and loss is: 3714.871337890625\n",
      "Iteration is: 1497 and loss is: 3721.07568359375\n",
      "Iteration is: 1498 and loss is: 3723.40966796875\n",
      "Iteration is: 1499 and loss is: 3737.00146484375\n",
      "Iteration is: 1500 and loss is: 3745.725341796875\n",
      "Iteration is: 1501 and loss is: 3770.4423828125\n",
      "Iteration is: 1502 and loss is: 3788.12158203125\n",
      "Iteration is: 1503 and loss is: 3829.8212890625\n",
      "Iteration is: 1504 and loss is: 3860.341064453125\n",
      "Iteration is: 1505 and loss is: 3927.1787109375\n",
      "Iteration is: 1506 and loss is: 3973.896240234375\n",
      "Iteration is: 1507 and loss is: 4076.99462890625\n",
      "Iteration is: 1508 and loss is: 4142.06884765625\n",
      "Iteration is: 1509 and loss is: 4293.2177734375\n",
      "Iteration is: 1510 and loss is: 4373.83740234375\n",
      "Iteration is: 1511 and loss is: 4581.7900390625\n",
      "Iteration is: 1512 and loss is: 4660.49658203125\n",
      "Iteration is: 1513 and loss is: 4916.6943359375\n",
      "Iteration is: 1514 and loss is: 4956.2822265625\n",
      "Iteration is: 1515 and loss is: 5220.89990234375\n",
      "Iteration is: 1516 and loss is: 5163.0810546875\n",
      "Iteration is: 1517 and loss is: 5349.5322265625\n",
      "Iteration is: 1518 and loss is: 5146.14208984375\n",
      "Iteration is: 1519 and loss is: 5166.10205078125\n",
      "Iteration is: 1520 and loss is: 4832.69580078125\n",
      "Iteration is: 1521 and loss is: 4671.646484375\n",
      "Iteration is: 1522 and loss is: 4304.005859375\n",
      "Iteration is: 1523 and loss is: 4058.015869140625\n",
      "Iteration is: 1524 and loss is: 3771.23681640625\n",
      "Iteration is: 1525 and loss is: 3573.8388671875\n",
      "Iteration is: 1526 and loss is: 3425.248046875\n",
      "Iteration is: 1527 and loss is: 3344.580810546875\n",
      "Iteration is: 1528 and loss is: 3319.135986328125\n",
      "Iteration is: 1529 and loss is: 3336.62060546875\n",
      "Iteration is: 1530 and loss is: 3382.9052734375\n",
      "Iteration is: 1531 and loss is: 3439.695068359375\n",
      "Iteration is: 1532 and loss is: 3505.56201171875\n",
      "Iteration is: 1533 and loss is: 3551.1171875\n",
      "Iteration is: 1534 and loss is: 3601.30712890625\n",
      "Iteration is: 1535 and loss is: 3610.9609375\n",
      "Iteration is: 1536 and loss is: 3627.91357421875\n",
      "Iteration is: 1537 and loss is: 3600.11474609375\n",
      "Iteration is: 1538 and loss is: 3584.477294921875\n",
      "Iteration is: 1539 and loss is: 3533.1962890625\n",
      "Iteration is: 1540 and loss is: 3494.6328125\n",
      "Iteration is: 1541 and loss is: 3435.446533203125\n",
      "Iteration is: 1542 and loss is: 3389.019287109375\n",
      "Iteration is: 1543 and loss is: 3336.365234375\n",
      "Iteration is: 1544 and loss is: 3294.720703125\n",
      "Iteration is: 1545 and loss is: 3255.38427734375\n",
      "Iteration is: 1546 and loss is: 3224.659423828125\n",
      "Iteration is: 1547 and loss is: 3199.22802734375\n",
      "Iteration is: 1548 and loss is: 3180.42724609375\n",
      "Iteration is: 1549 and loss is: 3166.098388671875\n",
      "Iteration is: 1550 and loss is: 3155.9580078125\n",
      "Iteration is: 1551 and loss is: 3148.582763671875\n",
      "Iteration is: 1552 and loss is: 3143.437744140625\n",
      "Iteration is: 1553 and loss is: 3139.787841796875\n",
      "Iteration is: 1554 and loss is: 3137.05029296875\n",
      "Iteration is: 1555 and loss is: 3134.883544921875\n",
      "Iteration is: 1556 and loss is: 3132.864990234375\n",
      "Iteration is: 1557 and loss is: 3131.1337890625\n",
      "Iteration is: 1558 and loss is: 3129.12744140625\n",
      "Iteration is: 1559 and loss is: 3127.82373046875\n",
      "Iteration is: 1560 and loss is: 3125.79736328125\n",
      "Iteration is: 1561 and loss is: 3125.510009765625\n",
      "Iteration is: 1562 and loss is: 3124.411376953125\n",
      "Iteration is: 1563 and loss is: 3125.59912109375\n",
      "Iteration is: 1564 and loss is: 3125.893798828125\n",
      "Iteration is: 1565 and loss is: 3129.987060546875\n",
      "Iteration is: 1566 and loss is: 3132.7255859375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 1567 and loss is: 3141.451416015625\n",
      "Iteration is: 1568 and loss is: 3148.221435546875\n",
      "Iteration is: 1569 and loss is: 3163.546630859375\n",
      "Iteration is: 1570 and loss is: 3175.81884765625\n",
      "Iteration is: 1571 and loss is: 3201.99267578125\n",
      "Iteration is: 1572 and loss is: 3223.21875\n",
      "Iteration is: 1573 and loss is: 3265.9541015625\n",
      "Iteration is: 1574 and loss is: 3299.929931640625\n",
      "Iteration is: 1575 and loss is: 3368.447265625\n",
      "Iteration is: 1576 and loss is: 3420.18896484375\n",
      "Iteration is: 1577 and loss is: 3527.022216796875\n",
      "Iteration is: 1578 and loss is: 3600.684814453125\n",
      "Iteration is: 1579 and loss is: 3763.992919921875\n",
      "Iteration is: 1580 and loss is: 3860.70751953125\n",
      "Iteration is: 1581 and loss is: 4095.76318359375\n",
      "Iteration is: 1582 and loss is: 4198.3798828125\n",
      "Iteration is: 1583 and loss is: 4502.74658203125\n",
      "Iteration is: 1584 and loss is: 4567.56396484375\n",
      "Iteration is: 1585 and loss is: 4898.056640625\n",
      "Iteration is: 1586 and loss is: 4851.09375\n",
      "Iteration is: 1587 and loss is: 5102.96923828125\n",
      "Iteration is: 1588 and loss is: 4878.95654296875\n",
      "Iteration is: 1589 and loss is: 4934.7216796875\n",
      "Iteration is: 1590 and loss is: 4555.45458984375\n",
      "Iteration is: 1591 and loss is: 4397.66943359375\n",
      "Iteration is: 1592 and loss is: 3999.89208984375\n",
      "Iteration is: 1593 and loss is: 3775.50927734375\n",
      "Iteration is: 1594 and loss is: 3519.851318359375\n",
      "Iteration is: 1595 and loss is: 3430.57861328125\n",
      "Iteration is: 1596 and loss is: 3375.928466796875\n",
      "Iteration is: 1597 and loss is: 3507.112060546875\n",
      "Iteration is: 1598 and loss is: 3552.13671875\n",
      "Iteration is: 1599 and loss is: 3765.257080078125\n",
      "Iteration is: 1600 and loss is: 3695.976318359375\n",
      "Iteration is: 1601 and loss is: 3701.5732421875\n",
      "Iteration is: 1602 and loss is: 3471.23779296875\n",
      "Iteration is: 1603 and loss is: 3280.723388671875\n",
      "Iteration is: 1604 and loss is: 3138.39453125\n",
      "Iteration is: 1605 and loss is: 3080.53955078125\n",
      "Iteration is: 1606 and loss is: 3135.21435546875\n",
      "Iteration is: 1607 and loss is: 3208.953857421875\n",
      "Iteration is: 1608 and loss is: 3307.9384765625\n",
      "Iteration is: 1609 and loss is: 3285.193115234375\n",
      "Iteration is: 1610 and loss is: 3250.240234375\n",
      "Iteration is: 1611 and loss is: 3087.694580078125\n",
      "Iteration is: 1612 and loss is: 2944.91162109375\n",
      "Iteration is: 1613 and loss is: 2815.60888671875\n",
      "Iteration is: 1614 and loss is: 2752.463623046875\n",
      "Iteration is: 1615 and loss is: 2751.358154296875\n",
      "Iteration is: 1616 and loss is: 2789.63525390625\n",
      "Iteration is: 1617 and loss is: 2844.740966796875\n",
      "Iteration is: 1618 and loss is: 2874.416015625\n",
      "Iteration is: 1619 and loss is: 2894.532470703125\n",
      "Iteration is: 1620 and loss is: 2864.50048828125\n",
      "Iteration is: 1621 and loss is: 2829.4462890625\n",
      "Iteration is: 1622 and loss is: 2779.626708984375\n",
      "Iteration is: 1623 and loss is: 2747.2978515625\n",
      "Iteration is: 1624 and loss is: 2734.980224609375\n",
      "Iteration is: 1625 and loss is: 2745.456298828125\n",
      "Iteration is: 1626 and loss is: 2765.387939453125\n",
      "Iteration is: 1627 and loss is: 2786.272216796875\n",
      "Iteration is: 1628 and loss is: 2797.47607421875\n",
      "Iteration is: 1629 and loss is: 2794.15576171875\n",
      "Iteration is: 1630 and loss is: 2778.94970703125\n",
      "Iteration is: 1631 and loss is: 2756.3818359375\n",
      "Iteration is: 1632 and loss is: 2731.609375\n",
      "Iteration is: 1633 and loss is: 2715.4775390625\n",
      "Iteration is: 1634 and loss is: 2705.772216796875\n",
      "Iteration is: 1635 and loss is: 2708.0185546875\n",
      "Iteration is: 1636 and loss is: 2712.7392578125\n",
      "Iteration is: 1637 and loss is: 2722.1708984375\n",
      "Iteration is: 1638 and loss is: 2725.881591796875\n",
      "Iteration is: 1639 and loss is: 2731.207275390625\n",
      "Iteration is: 1640 and loss is: 2727.904052734375\n",
      "Iteration is: 1641 and loss is: 2728.2705078125\n",
      "Iteration is: 1642 and loss is: 2723.300537109375\n",
      "Iteration is: 1643 and loss is: 2727.48779296875\n",
      "Iteration is: 1644 and loss is: 2730.678466796875\n",
      "Iteration is: 1645 and loss is: 2748.816650390625\n",
      "Iteration is: 1646 and loss is: 2764.572998046875\n",
      "Iteration is: 1647 and loss is: 2799.331787109375\n",
      "Iteration is: 1648 and loss is: 2826.368896484375\n",
      "Iteration is: 1649 and loss is: 2879.257568359375\n",
      "Iteration is: 1650 and loss is: 2917.581298828125\n",
      "Iteration is: 1651 and loss is: 2995.85546875\n",
      "Iteration is: 1652 and loss is: 3048.59619140625\n",
      "Iteration is: 1653 and loss is: 3164.804443359375\n",
      "Iteration is: 1654 and loss is: 3236.522705078125\n",
      "Iteration is: 1655 and loss is: 3408.073486328125\n",
      "Iteration is: 1656 and loss is: 3498.76025390625\n",
      "Iteration is: 1657 and loss is: 3738.2353515625\n",
      "Iteration is: 1658 and loss is: 3829.993896484375\n",
      "Iteration is: 1659 and loss is: 4127.21728515625\n",
      "Iteration is: 1660 and loss is: 4170.939453125\n",
      "Iteration is: 1661 and loss is: 4468.40966796875\n",
      "Iteration is: 1662 and loss is: 4390.71630859375\n",
      "Iteration is: 1663 and loss is: 4581.39013671875\n",
      "Iteration is: 1664 and loss is: 4331.76123046875\n",
      "Iteration is: 1665 and loss is: 4319.8955078125\n",
      "Iteration is: 1666 and loss is: 3935.28369140625\n",
      "Iteration is: 1667 and loss is: 3727.55810546875\n",
      "Iteration is: 1668 and loss is: 3331.69677734375\n",
      "Iteration is: 1669 and loss is: 3060.89013671875\n",
      "Iteration is: 1670 and loss is: 2780.945556640625\n",
      "Iteration is: 1671 and loss is: 2596.849853515625\n",
      "Iteration is: 1672 and loss is: 2479.876708984375\n",
      "Iteration is: 1673 and loss is: 2434.869873046875\n",
      "Iteration is: 1674 and loss is: 2446.720947265625\n",
      "Iteration is: 1675 and loss is: 2497.552978515625\n",
      "Iteration is: 1676 and loss is: 2572.7900390625\n",
      "Iteration is: 1677 and loss is: 2646.191650390625\n",
      "Iteration is: 1678 and loss is: 2726.006103515625\n",
      "Iteration is: 1679 and loss is: 2768.369140625\n",
      "Iteration is: 1680 and loss is: 2816.2763671875\n",
      "Iteration is: 1681 and loss is: 2809.25439453125\n",
      "Iteration is: 1682 and loss is: 2812.8037109375\n",
      "Iteration is: 1683 and loss is: 2764.7919921875\n",
      "Iteration is: 1684 and loss is: 2731.306396484375\n",
      "Iteration is: 1685 and loss is: 2664.199462890625\n",
      "Iteration is: 1686 and loss is: 2612.439208984375\n",
      "Iteration is: 1687 and loss is: 2547.658447265625\n",
      "Iteration is: 1688 and loss is: 2496.983154296875\n",
      "Iteration is: 1689 and loss is: 2447.9951171875\n",
      "Iteration is: 1690 and loss is: 2411.094970703125\n",
      "Iteration is: 1691 and loss is: 2381.84228515625\n",
      "Iteration is: 1692 and loss is: 2361.764404296875\n",
      "Iteration is: 1693 and loss is: 2348.8603515625\n",
      "Iteration is: 1694 and loss is: 2341.945068359375\n",
      "Iteration is: 1695 and loss is: 2339.644287109375\n",
      "Iteration is: 1696 and loss is: 2340.466552734375\n",
      "Iteration is: 1697 and loss is: 2343.450439453125\n",
      "Iteration is: 1698 and loss is: 2347.428955078125\n",
      "Iteration is: 1699 and loss is: 2352.371337890625\n",
      "Iteration is: 1700 and loss is: 2356.834716796875\n",
      "Iteration is: 1701 and loss is: 2362.425537109375\n",
      "Iteration is: 1702 and loss is: 2366.603271484375\n",
      "Iteration is: 1703 and loss is: 2372.656494140625\n",
      "Iteration is: 1704 and loss is: 2376.690673828125\n",
      "Iteration is: 1705 and loss is: 2383.5029296875\n",
      "Iteration is: 1706 and loss is: 2387.704833984375\n",
      "Iteration is: 1707 and loss is: 2396.691162109375\n",
      "Iteration is: 1708 and loss is: 2402.81298828125\n",
      "Iteration is: 1709 and loss is: 2415.8271484375\n",
      "Iteration is: 1710 and loss is: 2425.04541015625\n",
      "Iteration is: 1711 and loss is: 2444.59912109375\n",
      "Iteration is: 1712 and loss is: 2459.20068359375\n",
      "Iteration is: 1713 and loss is: 2488.44482421875\n",
      "Iteration is: 1714 and loss is: 2510.5888671875\n",
      "Iteration is: 1715 and loss is: 2554.79248046875\n",
      "Iteration is: 1716 and loss is: 2587.882080078125\n",
      "Iteration is: 1717 and loss is: 2654.575927734375\n",
      "Iteration is: 1718 and loss is: 2702.78955078125\n",
      "Iteration is: 1719 and loss is: 2802.60205078125\n",
      "Iteration is: 1720 and loss is: 2869.608642578125\n",
      "Iteration is: 1721 and loss is: 3015.7978515625\n",
      "Iteration is: 1722 and loss is: 3101.5859375\n",
      "Iteration is: 1723 and loss is: 3305.037109375\n",
      "Iteration is: 1724 and loss is: 3398.00830078125\n",
      "Iteration is: 1725 and loss is: 3656.7412109375\n",
      "Iteration is: 1726 and loss is: 3725.6474609375\n",
      "Iteration is: 1727 and loss is: 4008.384033203125\n",
      "Iteration is: 1728 and loss is: 3998.24658203125\n",
      "Iteration is: 1729 and loss is: 4228.3251953125\n",
      "Iteration is: 1730 and loss is: 4077.04638671875\n",
      "Iteration is: 1731 and loss is: 4151.14453125\n",
      "Iteration is: 1732 and loss is: 3845.068359375\n",
      "Iteration is: 1733 and loss is: 3717.4541015625\n",
      "Iteration is: 1734 and loss is: 3333.409912109375\n",
      "Iteration is: 1735 and loss is: 3077.98291015625\n",
      "Iteration is: 1736 and loss is: 2745.219482421875\n",
      "Iteration is: 1737 and loss is: 2511.412353515625\n",
      "Iteration is: 1738 and loss is: 2320.544677734375\n",
      "Iteration is: 1739 and loss is: 2210.28466796875\n",
      "Iteration is: 1740 and loss is: 2166.086181640625\n",
      "Iteration is: 1741 and loss is: 2176.967529296875\n",
      "Iteration is: 1742 and loss is: 2226.59765625\n",
      "Iteration is: 1743 and loss is: 2294.9365234375\n",
      "Iteration is: 1744 and loss is: 2375.6904296875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 1745 and loss is: 2437.831787109375\n",
      "Iteration is: 1746 and loss is: 2502.7646484375\n",
      "Iteration is: 1747 and loss is: 2522.859375\n",
      "Iteration is: 1748 and loss is: 2548.240966796875\n",
      "Iteration is: 1749 and loss is: 2522.814697265625\n",
      "Iteration is: 1750 and loss is: 2508.38330078125\n",
      "Iteration is: 1751 and loss is: 2454.190185546875\n",
      "Iteration is: 1752 and loss is: 2414.53564453125\n",
      "Iteration is: 1753 and loss is: 2352.98046875\n",
      "Iteration is: 1754 and loss is: 2306.4072265625\n",
      "Iteration is: 1755 and loss is: 2254.032470703125\n",
      "Iteration is: 1756 and loss is: 2215.226806640625\n",
      "Iteration is: 1757 and loss is: 2179.960205078125\n",
      "Iteration is: 1758 and loss is: 2155.7587890625\n",
      "Iteration is: 1759 and loss is: 2138.21044921875\n",
      "Iteration is: 1760 and loss is: 2128.585205078125\n",
      "Iteration is: 1761 and loss is: 2125.7958984375\n",
      "Iteration is: 1762 and loss is: 2127.923828125\n",
      "Iteration is: 1763 and loss is: 2136.73046875\n",
      "Iteration is: 1764 and loss is: 2146.7353515625\n",
      "Iteration is: 1765 and loss is: 2165.0185546875\n",
      "Iteration is: 1766 and loss is: 2180.1005859375\n",
      "Iteration is: 1767 and loss is: 2207.386962890625\n",
      "Iteration is: 1768 and loss is: 2224.591064453125\n",
      "Iteration is: 1769 and loss is: 2260.31640625\n",
      "Iteration is: 1770 and loss is: 2275.077880859375\n",
      "Iteration is: 1771 and loss is: 2315.6845703125\n",
      "Iteration is: 1772 and loss is: 2320.26171875\n",
      "Iteration is: 1773 and loss is: 2355.969482421875\n",
      "Iteration is: 1774 and loss is: 2340.075927734375\n",
      "Iteration is: 1775 and loss is: 2355.60498046875\n",
      "Iteration is: 1776 and loss is: 2315.724609375\n",
      "Iteration is: 1777 and loss is: 2302.990478515625\n",
      "Iteration is: 1778 and loss is: 2250.777099609375\n",
      "Iteration is: 1779 and loss is: 2220.66455078125\n",
      "Iteration is: 1780 and loss is: 2180.126708984375\n",
      "Iteration is: 1781 and loss is: 2159.037109375\n",
      "Iteration is: 1782 and loss is: 2151.19384765625\n",
      "Iteration is: 1783 and loss is: 2159.8671875\n",
      "Iteration is: 1784 and loss is: 2193.48828125\n",
      "Iteration is: 1785 and loss is: 2237.558349609375\n",
      "Iteration is: 1786 and loss is: 2315.2294921875\n",
      "Iteration is: 1787 and loss is: 2393.53515625\n",
      "Iteration is: 1788 and loss is: 2526.36474609375\n",
      "Iteration is: 1789 and loss is: 2642.72998046875\n",
      "Iteration is: 1790 and loss is: 2858.5068359375\n",
      "Iteration is: 1791 and loss is: 3019.797119140625\n",
      "Iteration is: 1792 and loss is: 3361.865966796875\n",
      "Iteration is: 1793 and loss is: 3560.38818359375\n",
      "Iteration is: 1794 and loss is: 4066.5283203125\n",
      "Iteration is: 1795 and loss is: 4239.3046875\n",
      "Iteration is: 1796 and loss is: 4872.25146484375\n",
      "Iteration is: 1797 and loss is: 4860.8466796875\n",
      "Iteration is: 1798 and loss is: 5405.896484375\n",
      "Iteration is: 1799 and loss is: 5013.8564453125\n",
      "Iteration is: 1800 and loss is: 5126.52734375\n",
      "Iteration is: 1801 and loss is: 4378.47216796875\n",
      "Iteration is: 1802 and loss is: 3977.751220703125\n",
      "Iteration is: 1803 and loss is: 3234.59814453125\n",
      "Iteration is: 1804 and loss is: 2724.51025390625\n",
      "Iteration is: 1805 and loss is: 2331.833740234375\n",
      "Iteration is: 1806 and loss is: 2128.310791015625\n",
      "Iteration is: 1807 and loss is: 2119.40869140625\n",
      "Iteration is: 1808 and loss is: 2225.017822265625\n",
      "Iteration is: 1809 and loss is: 2417.01953125\n",
      "Iteration is: 1810 and loss is: 2576.514892578125\n",
      "Iteration is: 1811 and loss is: 2753.3017578125\n",
      "Iteration is: 1812 and loss is: 2764.05322265625\n",
      "Iteration is: 1813 and loss is: 2787.14794921875\n",
      "Iteration is: 1814 and loss is: 2634.6298828125\n",
      "Iteration is: 1815 and loss is: 2510.57861328125\n",
      "Iteration is: 1816 and loss is: 2314.106689453125\n",
      "Iteration is: 1817 and loss is: 2163.08935546875\n",
      "Iteration is: 1818 and loss is: 2038.795654296875\n",
      "Iteration is: 1819 and loss is: 1967.7381591796875\n",
      "Iteration is: 1820 and loss is: 1947.9044189453125\n",
      "Iteration is: 1821 and loss is: 1966.2513427734375\n",
      "Iteration is: 1822 and loss is: 2010.957275390625\n",
      "Iteration is: 1823 and loss is: 2059.535888671875\n",
      "Iteration is: 1824 and loss is: 2108.586181640625\n",
      "Iteration is: 1825 and loss is: 2130.469482421875\n",
      "Iteration is: 1826 and loss is: 2145.238525390625\n",
      "Iteration is: 1827 and loss is: 2124.39404296875\n",
      "Iteration is: 1828 and loss is: 2100.990234375\n",
      "Iteration is: 1829 and loss is: 2054.676513671875\n",
      "Iteration is: 1830 and loss is: 2013.12158203125\n",
      "Iteration is: 1831 and loss is: 1967.5047607421875\n",
      "Iteration is: 1832 and loss is: 1931.4833984375\n",
      "Iteration is: 1833 and loss is: 1902.9129638671875\n",
      "Iteration is: 1834 and loss is: 1884.615966796875\n",
      "Iteration is: 1835 and loss is: 1875.1478271484375\n",
      "Iteration is: 1836 and loss is: 1873.243896484375\n",
      "Iteration is: 1837 and loss is: 1876.8209228515625\n",
      "Iteration is: 1838 and loss is: 1883.5509033203125\n",
      "Iteration is: 1839 and loss is: 1891.9874267578125\n",
      "Iteration is: 1840 and loss is: 1899.7821044921875\n",
      "Iteration is: 1841 and loss is: 1907.3798828125\n",
      "Iteration is: 1842 and loss is: 1911.837890625\n",
      "Iteration is: 1843 and loss is: 1915.80322265625\n",
      "Iteration is: 1844 and loss is: 1915.6983642578125\n",
      "Iteration is: 1845 and loss is: 1915.6634521484375\n",
      "Iteration is: 1846 and loss is: 1911.6759033203125\n",
      "Iteration is: 1847 and loss is: 1908.287841796875\n",
      "Iteration is: 1848 and loss is: 1902.1356201171875\n",
      "Iteration is: 1849 and loss is: 1897.4586181640625\n",
      "Iteration is: 1850 and loss is: 1891.027099609375\n",
      "Iteration is: 1851 and loss is: 1886.3115234375\n",
      "Iteration is: 1852 and loss is: 1880.66015625\n",
      "Iteration is: 1853 and loss is: 1877.2481689453125\n",
      "Iteration is: 1854 and loss is: 1873.418701171875\n",
      "Iteration is: 1855 and loss is: 1872.0020751953125\n",
      "Iteration is: 1856 and loss is: 1870.6151123046875\n",
      "Iteration is: 1857 and loss is: 1872.195068359375\n",
      "Iteration is: 1858 and loss is: 1874.1470947265625\n",
      "Iteration is: 1859 and loss is: 1880.3018798828125\n",
      "Iteration is: 1860 and loss is: 1886.6934814453125\n",
      "Iteration is: 1861 and loss is: 1899.9571533203125\n",
      "Iteration is: 1862 and loss is: 1913.012939453125\n",
      "Iteration is: 1863 and loss is: 1938.0252685546875\n",
      "Iteration is: 1864 and loss is: 1961.049072265625\n",
      "Iteration is: 1865 and loss is: 2005.142333984375\n",
      "Iteration is: 1866 and loss is: 2041.0673828125\n",
      "Iteration is: 1867 and loss is: 2112.775634765625\n",
      "Iteration is: 1868 and loss is: 2159.185546875\n",
      "Iteration is: 1869 and loss is: 2260.65625\n",
      "Iteration is: 1870 and loss is: 2300.386962890625\n",
      "Iteration is: 1871 and loss is: 2411.58935546875\n",
      "Iteration is: 1872 and loss is: 2409.63134765625\n",
      "Iteration is: 1873 and loss is: 2478.770263671875\n",
      "Iteration is: 1874 and loss is: 2412.226806640625\n",
      "Iteration is: 1875 and loss is: 2407.62646484375\n",
      "Iteration is: 1876 and loss is: 2323.35888671875\n",
      "Iteration is: 1877 and loss is: 2312.319091796875\n",
      "Iteration is: 1878 and loss is: 2302.42041015625\n",
      "Iteration is: 1879 and loss is: 2404.710205078125\n",
      "Iteration is: 1880 and loss is: 2517.82763671875\n",
      "Iteration is: 1881 and loss is: 2789.180908203125\n",
      "Iteration is: 1882 and loss is: 2998.061767578125\n",
      "Iteration is: 1883 and loss is: 3419.91796875\n",
      "Iteration is: 1884 and loss is: 3637.645263671875\n",
      "Iteration is: 1885 and loss is: 4142.40234375\n",
      "Iteration is: 1886 and loss is: 4210.2958984375\n",
      "Iteration is: 1887 and loss is: 4648.517578125\n",
      "Iteration is: 1888 and loss is: 4401.92822265625\n",
      "Iteration is: 1889 and loss is: 4566.7890625\n",
      "Iteration is: 1890 and loss is: 4013.078857421875\n",
      "Iteration is: 1891 and loss is: 3818.37548828125\n",
      "Iteration is: 1892 and loss is: 3197.35498046875\n",
      "Iteration is: 1893 and loss is: 2832.33203125\n",
      "Iteration is: 1894 and loss is: 2402.9541015625\n",
      "Iteration is: 1895 and loss is: 2151.36865234375\n",
      "Iteration is: 1896 and loss is: 1977.1007080078125\n",
      "Iteration is: 1897 and loss is: 1924.2359619140625\n",
      "Iteration is: 1898 and loss is: 1918.30078125\n",
      "Iteration is: 1899 and loss is: 1970.638671875\n",
      "Iteration is: 1900 and loss is: 2045.815673828125\n",
      "Iteration is: 1901 and loss is: 2114.644775390625\n",
      "Iteration is: 1902 and loss is: 2212.4150390625\n",
      "Iteration is: 1903 and loss is: 2252.21630859375\n",
      "Iteration is: 1904 and loss is: 2317.262451171875\n",
      "Iteration is: 1905 and loss is: 2294.01611328125\n",
      "Iteration is: 1906 and loss is: 2283.782470703125\n",
      "Iteration is: 1907 and loss is: 2189.39990234375\n",
      "Iteration is: 1908 and loss is: 2107.201171875\n",
      "Iteration is: 1909 and loss is: 1983.9771728515625\n",
      "Iteration is: 1910 and loss is: 1883.047119140625\n",
      "Iteration is: 1911 and loss is: 1790.822021484375\n",
      "Iteration is: 1912 and loss is: 1729.5614013671875\n",
      "Iteration is: 1913 and loss is: 1696.7158203125\n",
      "Iteration is: 1914 and loss is: 1690.180908203125\n",
      "Iteration is: 1915 and loss is: 1702.916259765625\n",
      "Iteration is: 1916 and loss is: 1726.6368408203125\n",
      "Iteration is: 1917 and loss is: 1755.1099853515625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 1918 and loss is: 1778.8665771484375\n",
      "Iteration is: 1919 and loss is: 1800.1806640625\n",
      "Iteration is: 1920 and loss is: 1808.960205078125\n",
      "Iteration is: 1921 and loss is: 1815.7608642578125\n",
      "Iteration is: 1922 and loss is: 1810.8837890625\n",
      "Iteration is: 1923 and loss is: 1807.92724609375\n",
      "Iteration is: 1924 and loss is: 1797.2684326171875\n",
      "Iteration is: 1925 and loss is: 1791.4532470703125\n",
      "Iteration is: 1926 and loss is: 1780.108154296875\n",
      "Iteration is: 1927 and loss is: 1774.256591796875\n",
      "Iteration is: 1928 and loss is: 1763.5982666015625\n",
      "Iteration is: 1929 and loss is: 1756.939453125\n",
      "Iteration is: 1930 and loss is: 1745.9603271484375\n",
      "Iteration is: 1931 and loss is: 1737.857177734375\n",
      "Iteration is: 1932 and loss is: 1726.645751953125\n",
      "Iteration is: 1933 and loss is: 1717.7760009765625\n",
      "Iteration is: 1934 and loss is: 1707.4051513671875\n",
      "Iteration is: 1935 and loss is: 1699.792236328125\n",
      "Iteration is: 1936 and loss is: 1691.7867431640625\n",
      "Iteration is: 1937 and loss is: 1686.54345703125\n",
      "Iteration is: 1938 and loss is: 1681.40380859375\n",
      "Iteration is: 1939 and loss is: 1678.65966796875\n",
      "Iteration is: 1940 and loss is: 1676.1329345703125\n",
      "Iteration is: 1941 and loss is: 1675.559814453125\n",
      "Iteration is: 1942 and loss is: 1675.1947021484375\n",
      "Iteration is: 1943 and loss is: 1677.1094970703125\n",
      "Iteration is: 1944 and loss is: 1679.1473388671875\n",
      "Iteration is: 1945 and loss is: 1683.9312744140625\n",
      "Iteration is: 1946 and loss is: 1688.9561767578125\n",
      "Iteration is: 1947 and loss is: 1698.153564453125\n",
      "Iteration is: 1948 and loss is: 1707.9114990234375\n",
      "Iteration is: 1949 and loss is: 1724.349853515625\n",
      "Iteration is: 1950 and loss is: 1741.75146484375\n",
      "Iteration is: 1951 and loss is: 1770.8837890625\n",
      "Iteration is: 1952 and loss is: 1801.3349609375\n",
      "Iteration is: 1953 and loss is: 1852.84326171875\n",
      "Iteration is: 1954 and loss is: 1905.05322265625\n",
      "Iteration is: 1955 and loss is: 1996.2191162109375\n",
      "Iteration is: 1956 and loss is: 2083.359619140625\n",
      "Iteration is: 1957 and loss is: 2243.56689453125\n",
      "Iteration is: 1958 and loss is: 2382.49169921875\n",
      "Iteration is: 1959 and loss is: 2657.857421875\n",
      "Iteration is: 1960 and loss is: 2860.4013671875\n",
      "Iteration is: 1961 and loss is: 3307.30615234375\n",
      "Iteration is: 1962 and loss is: 3547.64892578125\n",
      "Iteration is: 1963 and loss is: 4185.20166015625\n",
      "Iteration is: 1964 and loss is: 4341.74755859375\n",
      "Iteration is: 1965 and loss is: 5042.98876953125\n",
      "Iteration is: 1966 and loss is: 4870.6201171875\n",
      "Iteration is: 1967 and loss is: 5270.6494140625\n",
      "Iteration is: 1968 and loss is: 4604.6376953125\n",
      "Iteration is: 1969 and loss is: 4386.89453125\n",
      "Iteration is: 1970 and loss is: 3469.38134765625\n",
      "Iteration is: 1971 and loss is: 2868.396484375\n",
      "Iteration is: 1972 and loss is: 2200.7705078125\n",
      "Iteration is: 1973 and loss is: 1803.5653076171875\n",
      "Iteration is: 1974 and loss is: 1623.2579345703125\n",
      "Iteration is: 1975 and loss is: 1653.9208984375\n",
      "Iteration is: 1976 and loss is: 1830.69384765625\n",
      "Iteration is: 1977 and loss is: 2055.133056640625\n",
      "Iteration is: 1978 and loss is: 2314.87158203125\n",
      "Iteration is: 1979 and loss is: 2423.841064453125\n",
      "Iteration is: 1980 and loss is: 2536.38818359375\n",
      "Iteration is: 1981 and loss is: 2420.88232421875\n",
      "Iteration is: 1982 and loss is: 2324.38720703125\n",
      "Iteration is: 1983 and loss is: 2093.2275390625\n",
      "Iteration is: 1984 and loss is: 1909.1102294921875\n",
      "Iteration is: 1985 and loss is: 1728.079345703125\n",
      "Iteration is: 1986 and loss is: 1612.83642578125\n",
      "Iteration is: 1987 and loss is: 1558.20947265625\n",
      "Iteration is: 1988 and loss is: 1560.75439453125\n",
      "Iteration is: 1989 and loss is: 1604.6026611328125\n",
      "Iteration is: 1990 and loss is: 1667.51416015625\n",
      "Iteration is: 1991 and loss is: 1737.49951171875\n",
      "Iteration is: 1992 and loss is: 1782.5391845703125\n",
      "Iteration is: 1993 and loss is: 1819.8992919921875\n",
      "Iteration is: 1994 and loss is: 1812.6300048828125\n",
      "Iteration is: 1995 and loss is: 1800.664306640625\n",
      "Iteration is: 1996 and loss is: 1753.903076171875\n",
      "Iteration is: 1997 and loss is: 1711.067626953125\n",
      "Iteration is: 1998 and loss is: 1657.0330810546875\n",
      "Iteration is: 1999 and loss is: 1613.4150390625\n",
      "Iteration is: 2000 and loss is: 1576.27392578125\n",
      "Iteration is: 2001 and loss is: 1551.613037109375\n",
      "Iteration is: 2002 and loss is: 1538.47802734375\n",
      "Iteration is: 2003 and loss is: 1535.0911865234375\n",
      "Iteration is: 2004 and loss is: 1539.73583984375\n",
      "Iteration is: 2005 and loss is: 1548.84716796875\n",
      "Iteration is: 2006 and loss is: 1561.3267822265625\n",
      "Iteration is: 2007 and loss is: 1573.3729248046875\n",
      "Iteration is: 2008 and loss is: 1586.5914306640625\n",
      "Iteration is: 2009 and loss is: 1596.17626953125\n",
      "Iteration is: 2010 and loss is: 1607.2724609375\n",
      "Iteration is: 2011 and loss is: 1613.369384765625\n",
      "Iteration is: 2012 and loss is: 1622.66748046875\n",
      "Iteration is: 2013 and loss is: 1626.9384765625\n",
      "Iteration is: 2014 and loss is: 1637.6954345703125\n",
      "Iteration is: 2015 and loss is: 1643.6143798828125\n",
      "Iteration is: 2016 and loss is: 1660.670654296875\n",
      "Iteration is: 2017 and loss is: 1671.45556640625\n",
      "Iteration is: 2018 and loss is: 1700.5194091796875\n",
      "Iteration is: 2019 and loss is: 1717.7877197265625\n",
      "Iteration is: 2020 and loss is: 1763.6790771484375\n",
      "Iteration is: 2021 and loss is: 1784.40185546875\n",
      "Iteration is: 2022 and loss is: 1845.6688232421875\n",
      "Iteration is: 2023 and loss is: 1856.586669921875\n",
      "Iteration is: 2024 and loss is: 1916.18408203125\n",
      "Iteration is: 2025 and loss is: 1894.012451171875\n",
      "Iteration is: 2026 and loss is: 1918.1396484375\n",
      "Iteration is: 2027 and loss is: 1847.492431640625\n",
      "Iteration is: 2028 and loss is: 1811.5482177734375\n",
      "Iteration is: 2029 and loss is: 1712.59716796875\n",
      "Iteration is: 2030 and loss is: 1641.0594482421875\n",
      "Iteration is: 2031 and loss is: 1562.5556640625\n",
      "Iteration is: 2032 and loss is: 1511.5543212890625\n",
      "Iteration is: 2033 and loss is: 1482.93017578125\n",
      "Iteration is: 2034 and loss is: 1477.5438232421875\n",
      "Iteration is: 2035 and loss is: 1489.9737548828125\n",
      "Iteration is: 2036 and loss is: 1512.771240234375\n",
      "Iteration is: 2037 and loss is: 1541.854736328125\n",
      "Iteration is: 2038 and loss is: 1565.783203125\n",
      "Iteration is: 2039 and loss is: 1591.596435546875\n",
      "Iteration is: 2040 and loss is: 1600.906494140625\n",
      "Iteration is: 2041 and loss is: 1613.16650390625\n",
      "Iteration is: 2042 and loss is: 1607.18017578125\n",
      "Iteration is: 2043 and loss is: 1607.1943359375\n",
      "Iteration is: 2044 and loss is: 1595.09375\n",
      "Iteration is: 2045 and loss is: 1592.2012939453125\n",
      "Iteration is: 2046 and loss is: 1587.00439453125\n",
      "Iteration is: 2047 and loss is: 1595.9964599609375\n",
      "Iteration is: 2048 and loss is: 1610.94287109375\n",
      "Iteration is: 2049 and loss is: 1649.892578125\n",
      "Iteration is: 2050 and loss is: 1700.2001953125\n",
      "Iteration is: 2051 and loss is: 1796.052734375\n",
      "Iteration is: 2052 and loss is: 1903.90380859375\n",
      "Iteration is: 2053 and loss is: 2103.396240234375\n",
      "Iteration is: 2054 and loss is: 2301.736083984375\n",
      "Iteration is: 2055 and loss is: 2690.572265625\n",
      "Iteration is: 2056 and loss is: 3012.6474609375\n",
      "Iteration is: 2057 and loss is: 3715.95458984375\n",
      "Iteration is: 2058 and loss is: 4120.38916015625\n",
      "Iteration is: 2059 and loss is: 5201.064453125\n",
      "Iteration is: 2060 and loss is: 5399.724609375\n",
      "Iteration is: 2061 and loss is: 6530.736328125\n",
      "Iteration is: 2062 and loss is: 5960.63671875\n",
      "Iteration is: 2063 and loss is: 6278.625\n",
      "Iteration is: 2064 and loss is: 4847.419921875\n",
      "Iteration is: 2065 and loss is: 4049.181640625\n",
      "Iteration is: 2066 and loss is: 2724.62353515625\n",
      "Iteration is: 2067 and loss is: 1929.650634765625\n",
      "Iteration is: 2068 and loss is: 1525.1072998046875\n",
      "Iteration is: 2069 and loss is: 1567.1064453125\n",
      "Iteration is: 2070 and loss is: 1912.759033203125\n",
      "Iteration is: 2071 and loss is: 2343.596435546875\n",
      "Iteration is: 2072 and loss is: 2793.0244140625\n",
      "Iteration is: 2073 and loss is: 2863.3525390625\n",
      "Iteration is: 2074 and loss is: 2890.68994140625\n",
      "Iteration is: 2075 and loss is: 2498.328857421875\n",
      "Iteration is: 2076 and loss is: 2164.67626953125\n",
      "Iteration is: 2077 and loss is: 1776.19189453125\n",
      "Iteration is: 2078 and loss is: 1547.92333984375\n",
      "Iteration is: 2079 and loss is: 1470.5146484375\n",
      "Iteration is: 2080 and loss is: 1533.6048583984375\n",
      "Iteration is: 2081 and loss is: 1667.4327392578125\n",
      "Iteration is: 2082 and loss is: 1810.2850341796875\n",
      "Iteration is: 2083 and loss is: 1911.7071533203125\n",
      "Iteration is: 2084 and loss is: 1909.518310546875\n",
      "Iteration is: 2085 and loss is: 1856.8330078125\n",
      "Iteration is: 2086 and loss is: 1722.4471435546875\n",
      "Iteration is: 2087 and loss is: 1598.802001953125\n",
      "Iteration is: 2088 and loss is: 1488.4827880859375\n",
      "Iteration is: 2089 and loss is: 1429.54541015625\n",
      "Iteration is: 2090 and loss is: 1419.6651611328125\n",
      "Iteration is: 2091 and loss is: 1450.8104248046875\n",
      "Iteration is: 2092 and loss is: 1499.3719482421875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 2093 and loss is: 1551.0267333984375\n",
      "Iteration is: 2094 and loss is: 1579.8433837890625\n",
      "Iteration is: 2095 and loss is: 1584.365478515625\n",
      "Iteration is: 2096 and loss is: 1560.6259765625\n",
      "Iteration is: 2097 and loss is: 1518.66259765625\n",
      "Iteration is: 2098 and loss is: 1470.908447265625\n",
      "Iteration is: 2099 and loss is: 1426.94384765625\n",
      "Iteration is: 2100 and loss is: 1395.849365234375\n",
      "Iteration is: 2101 and loss is: 1379.6728515625\n",
      "Iteration is: 2102 and loss is: 1378.0501708984375\n",
      "Iteration is: 2103 and loss is: 1386.830078125\n",
      "Iteration is: 2104 and loss is: 1401.275634765625\n",
      "Iteration is: 2105 and loss is: 1414.8773193359375\n",
      "Iteration is: 2106 and loss is: 1425.69921875\n",
      "Iteration is: 2107 and loss is: 1428.9461669921875\n",
      "Iteration is: 2108 and loss is: 1427.3994140625\n",
      "Iteration is: 2109 and loss is: 1419.0133056640625\n",
      "Iteration is: 2110 and loss is: 1408.282470703125\n",
      "Iteration is: 2111 and loss is: 1395.0828857421875\n",
      "Iteration is: 2112 and loss is: 1382.8565673828125\n",
      "Iteration is: 2113 and loss is: 1372.0211181640625\n",
      "Iteration is: 2114 and loss is: 1364.078857421875\n",
      "Iteration is: 2115 and loss is: 1358.881591796875\n",
      "Iteration is: 2116 and loss is: 1356.406982421875\n",
      "Iteration is: 2117 and loss is: 1356.076171875\n",
      "Iteration is: 2118 and loss is: 1357.2393798828125\n",
      "Iteration is: 2119 and loss is: 1359.0078125\n",
      "Iteration is: 2120 and loss is: 1360.87109375\n",
      "Iteration is: 2121 and loss is: 1362.22607421875\n",
      "Iteration is: 2122 and loss is: 1363.1307373046875\n",
      "Iteration is: 2123 and loss is: 1363.1072998046875\n",
      "Iteration is: 2124 and loss is: 1362.3470458984375\n",
      "Iteration is: 2125 and loss is: 1360.66650390625\n",
      "Iteration is: 2126 and loss is: 1358.6302490234375\n",
      "Iteration is: 2127 and loss is: 1356.009033203125\n",
      "Iteration is: 2128 and loss is: 1353.2239990234375\n",
      "Iteration is: 2129 and loss is: 1350.2442626953125\n",
      "Iteration is: 2130 and loss is: 1347.32421875\n",
      "Iteration is: 2131 and loss is: 1344.4613037109375\n",
      "Iteration is: 2132 and loss is: 1341.8851318359375\n",
      "Iteration is: 2133 and loss is: 1339.5472412109375\n",
      "Iteration is: 2134 and loss is: 1337.456787109375\n",
      "Iteration is: 2135 and loss is: 1335.644775390625\n",
      "Iteration is: 2136 and loss is: 1334.148193359375\n",
      "Iteration is: 2137 and loss is: 1332.98291015625\n",
      "Iteration is: 2138 and loss is: 1332.2423095703125\n",
      "Iteration is: 2139 and loss is: 1331.916259765625\n",
      "Iteration is: 2140 and loss is: 1332.181884765625\n",
      "Iteration is: 2141 and loss is: 1333.153564453125\n",
      "Iteration is: 2142 and loss is: 1335.29296875\n",
      "Iteration is: 2143 and loss is: 1338.640625\n",
      "Iteration is: 2144 and loss is: 1344.4744873046875\n",
      "Iteration is: 2145 and loss is: 1352.867431640625\n",
      "Iteration is: 2146 and loss is: 1366.655029296875\n",
      "Iteration is: 2147 and loss is: 1385.5972900390625\n",
      "Iteration is: 2148 and loss is: 1416.8956298828125\n",
      "Iteration is: 2149 and loss is: 1457.8525390625\n",
      "Iteration is: 2150 and loss is: 1528.144287109375\n",
      "Iteration is: 2151 and loss is: 1614.38427734375\n",
      "Iteration is: 2152 and loss is: 1769.714111328125\n",
      "Iteration is: 2153 and loss is: 1939.7950439453125\n",
      "Iteration is: 2154 and loss is: 2264.45703125\n",
      "Iteration is: 2155 and loss is: 2551.060546875\n",
      "Iteration is: 2156 and loss is: 3137.712158203125\n",
      "Iteration is: 2157 and loss is: 3479.46923828125\n",
      "Iteration is: 2158 and loss is: 4280.61572265625\n",
      "Iteration is: 2159 and loss is: 4475.58984375\n",
      "Iteration is: 2160 and loss is: 5304.1796875\n",
      "Iteration is: 2161 and loss is: 5285.80224609375\n",
      "Iteration is: 2162 and loss is: 6274.72509765625\n",
      "Iteration is: 2163 and loss is: 5930.29541015625\n",
      "Iteration is: 2164 and loss is: 6750.81640625\n",
      "Iteration is: 2165 and loss is: 5638.3427734375\n",
      "Iteration is: 2166 and loss is: 5190.05517578125\n",
      "Iteration is: 2167 and loss is: 3610.470703125\n",
      "Iteration is: 2168 and loss is: 2427.2080078125\n",
      "Iteration is: 2169 and loss is: 1557.26220703125\n",
      "Iteration is: 2170 and loss is: 1315.6419677734375\n",
      "Iteration is: 2171 and loss is: 1631.70751953125\n",
      "Iteration is: 2172 and loss is: 2207.9345703125\n",
      "Iteration is: 2173 and loss is: 2853.771484375\n",
      "Iteration is: 2174 and loss is: 3013.73388671875\n",
      "Iteration is: 2175 and loss is: 3041.12744140625\n",
      "Iteration is: 2176 and loss is: 2509.73828125\n",
      "Iteration is: 2177 and loss is: 2055.49365234375\n",
      "Iteration is: 2178 and loss is: 1611.1593017578125\n",
      "Iteration is: 2179 and loss is: 1412.1209716796875\n",
      "Iteration is: 2180 and loss is: 1408.65283203125\n",
      "Iteration is: 2181 and loss is: 1537.1075439453125\n",
      "Iteration is: 2182 and loss is: 1682.26318359375\n",
      "Iteration is: 2183 and loss is: 1773.4420166015625\n",
      "Iteration is: 2184 and loss is: 1836.2747802734375\n",
      "Iteration is: 2185 and loss is: 1789.280029296875\n",
      "Iteration is: 2186 and loss is: 1750.609130859375\n",
      "Iteration is: 2187 and loss is: 1647.1185302734375\n",
      "Iteration is: 2188 and loss is: 1557.9534912109375\n",
      "Iteration is: 2189 and loss is: 1448.670654296875\n",
      "Iteration is: 2190 and loss is: 1369.7451171875\n",
      "Iteration is: 2191 and loss is: 1315.458740234375\n",
      "Iteration is: 2192 and loss is: 1306.6292724609375\n",
      "Iteration is: 2193 and loss is: 1337.322998046875\n",
      "Iteration is: 2194 and loss is: 1388.3724365234375\n",
      "Iteration is: 2195 and loss is: 1444.451416015625\n",
      "Iteration is: 2196 and loss is: 1468.8818359375\n",
      "Iteration is: 2197 and loss is: 1469.34326171875\n",
      "Iteration is: 2198 and loss is: 1425.789794921875\n",
      "Iteration is: 2199 and loss is: 1372.553955078125\n",
      "Iteration is: 2200 and loss is: 1315.43212890625\n",
      "Iteration is: 2201 and loss is: 1276.4735107421875\n",
      "Iteration is: 2202 and loss is: 1258.987548828125\n",
      "Iteration is: 2203 and loss is: 1260.4849853515625\n",
      "Iteration is: 2204 and loss is: 1272.8466796875\n",
      "Iteration is: 2205 and loss is: 1286.301513671875\n",
      "Iteration is: 2206 and loss is: 1297.0728759765625\n",
      "Iteration is: 2207 and loss is: 1300.707275390625\n",
      "Iteration is: 2208 and loss is: 1301.487060546875\n",
      "Iteration is: 2209 and loss is: 1298.4219970703125\n",
      "Iteration is: 2210 and loss is: 1295.8326416015625\n",
      "Iteration is: 2211 and loss is: 1290.5177001953125\n",
      "Iteration is: 2212 and loss is: 1284.7548828125\n",
      "Iteration is: 2213 and loss is: 1275.5506591796875\n",
      "Iteration is: 2214 and loss is: 1265.2481689453125\n",
      "Iteration is: 2215 and loss is: 1254.13916015625\n",
      "Iteration is: 2216 and loss is: 1244.5408935546875\n",
      "Iteration is: 2217 and loss is: 1237.725341796875\n",
      "Iteration is: 2218 and loss is: 1234.33154296875\n",
      "Iteration is: 2219 and loss is: 1234.22119140625\n",
      "Iteration is: 2220 and loss is: 1236.241943359375\n",
      "Iteration is: 2221 and loss is: 1239.08056640625\n",
      "Iteration is: 2222 and loss is: 1241.499267578125\n",
      "Iteration is: 2223 and loss is: 1242.8935546875\n",
      "Iteration is: 2224 and loss is: 1242.940185546875\n",
      "Iteration is: 2225 and loss is: 1242.302490234375\n",
      "Iteration is: 2226 and loss is: 1241.0098876953125\n",
      "Iteration is: 2227 and loss is: 1240.0345458984375\n",
      "Iteration is: 2228 and loss is: 1238.9422607421875\n",
      "Iteration is: 2229 and loss is: 1238.219970703125\n",
      "Iteration is: 2230 and loss is: 1237.125\n",
      "Iteration is: 2231 and loss is: 1235.8648681640625\n",
      "Iteration is: 2232 and loss is: 1233.9970703125\n",
      "Iteration is: 2233 and loss is: 1231.9940185546875\n",
      "Iteration is: 2234 and loss is: 1229.573974609375\n",
      "Iteration is: 2235 and loss is: 1227.372802734375\n",
      "Iteration is: 2236 and loss is: 1225.1888427734375\n",
      "Iteration is: 2237 and loss is: 1223.3831787109375\n",
      "Iteration is: 2238 and loss is: 1221.7666015625\n",
      "Iteration is: 2239 and loss is: 1220.5196533203125\n",
      "Iteration is: 2240 and loss is: 1219.36328125\n",
      "Iteration is: 2241 and loss is: 1218.378662109375\n",
      "Iteration is: 2242 and loss is: 1217.3646240234375\n",
      "Iteration is: 2243 and loss is: 1216.48779296875\n",
      "Iteration is: 2244 and loss is: 1215.610595703125\n",
      "Iteration is: 2245 and loss is: 1215.056884765625\n",
      "Iteration is: 2246 and loss is: 1214.6312255859375\n",
      "Iteration is: 2247 and loss is: 1214.6295166015625\n",
      "Iteration is: 2248 and loss is: 1214.896484375\n",
      "Iteration is: 2249 and loss is: 1215.7705078125\n",
      "Iteration is: 2250 and loss is: 1217.0625\n",
      "Iteration is: 2251 and loss is: 1219.4549560546875\n",
      "Iteration is: 2252 and loss is: 1222.6409912109375\n",
      "Iteration is: 2253 and loss is: 1227.654296875\n",
      "Iteration is: 2254 and loss is: 1233.939208984375\n",
      "Iteration is: 2255 and loss is: 1243.7723388671875\n",
      "Iteration is: 2256 and loss is: 1256.104736328125\n",
      "Iteration is: 2257 and loss is: 1275.1436767578125\n",
      "Iteration is: 2258 and loss is: 1298.242431640625\n",
      "Iteration is: 2259 and loss is: 1334.9681396484375\n",
      "Iteration is: 2260 and loss is: 1378.8189697265625\n",
      "Iteration is: 2261 and loss is: 1451.669189453125\n",
      "Iteration is: 2262 and loss is: 1535.072265625\n",
      "Iteration is: 2263 and loss is: 1680.1290283203125\n",
      "Iteration is: 2264 and loss is: 1834.4873046875\n",
      "Iteration is: 2265 and loss is: 2122.301025390625\n",
      "Iteration is: 2266 and loss is: 2394.19482421875\n",
      "Iteration is: 2267 and loss is: 2948.08984375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 2268 and loss is: 3362.0224609375\n",
      "Iteration is: 2269 and loss is: 4323.544921875\n",
      "Iteration is: 2270 and loss is: 4754.1630859375\n",
      "Iteration is: 2271 and loss is: 6067.26806640625\n",
      "Iteration is: 2272 and loss is: 6019.74072265625\n",
      "Iteration is: 2273 and loss is: 6986.76220703125\n",
      "Iteration is: 2274 and loss is: 5873.1572265625\n",
      "Iteration is: 2275 and loss is: 5527.12548828125\n",
      "Iteration is: 2276 and loss is: 3811.76904296875\n",
      "Iteration is: 2277 and loss is: 2672.64453125\n",
      "Iteration is: 2278 and loss is: 1648.866943359375\n",
      "Iteration is: 2279 and loss is: 1213.5985107421875\n",
      "Iteration is: 2280 and loss is: 1290.6824951171875\n",
      "Iteration is: 2281 and loss is: 1708.9136962890625\n",
      "Iteration is: 2282 and loss is: 2305.6865234375\n",
      "Iteration is: 2283 and loss is: 2652.20849609375\n",
      "Iteration is: 2284 and loss is: 2965.21142578125\n",
      "Iteration is: 2285 and loss is: 2688.828125\n",
      "Iteration is: 2286 and loss is: 2402.26904296875\n",
      "Iteration is: 2287 and loss is: 1855.7789306640625\n",
      "Iteration is: 2288 and loss is: 1454.6873779296875\n",
      "Iteration is: 2289 and loss is: 1208.921875\n",
      "Iteration is: 2290 and loss is: 1173.3336181640625\n",
      "Iteration is: 2291 and loss is: 1300.4039306640625\n",
      "Iteration is: 2292 and loss is: 1495.690185546875\n",
      "Iteration is: 2293 and loss is: 1701.06201171875\n",
      "Iteration is: 2294 and loss is: 1776.661376953125\n",
      "Iteration is: 2295 and loss is: 1798.2451171875\n",
      "Iteration is: 2296 and loss is: 1659.3699951171875\n",
      "Iteration is: 2297 and loss is: 1508.85791015625\n",
      "Iteration is: 2298 and loss is: 1332.37841796875\n",
      "Iteration is: 2299 and loss is: 1211.9683837890625\n",
      "Iteration is: 2300 and loss is: 1156.016845703125\n",
      "Iteration is: 2301 and loss is: 1164.3704833984375\n",
      "Iteration is: 2302 and loss is: 1216.7706298828125\n",
      "Iteration is: 2303 and loss is: 1283.0687255859375\n",
      "Iteration is: 2304 and loss is: 1345.7757568359375\n",
      "Iteration is: 2305 and loss is: 1370.6397705078125\n",
      "Iteration is: 2306 and loss is: 1374.170654296875\n",
      "Iteration is: 2307 and loss is: 1335.15625\n",
      "Iteration is: 2308 and loss is: 1288.2705078125\n",
      "Iteration is: 2309 and loss is: 1230.855224609375\n",
      "Iteration is: 2310 and loss is: 1185.1793212890625\n",
      "Iteration is: 2311 and loss is: 1154.0330810546875\n",
      "Iteration is: 2312 and loss is: 1141.3017578125\n",
      "Iteration is: 2313 and loss is: 1144.3299560546875\n",
      "Iteration is: 2314 and loss is: 1157.8790283203125\n",
      "Iteration is: 2315 and loss is: 1175.8299560546875\n",
      "Iteration is: 2316 and loss is: 1191.2911376953125\n",
      "Iteration is: 2317 and loss is: 1202.6385498046875\n",
      "Iteration is: 2318 and loss is: 1204.7796630859375\n",
      "Iteration is: 2319 and loss is: 1202.0108642578125\n",
      "Iteration is: 2320 and loss is: 1191.5672607421875\n",
      "Iteration is: 2321 and loss is: 1179.348876953125\n",
      "Iteration is: 2322 and loss is: 1164.8756103515625\n",
      "Iteration is: 2323 and loss is: 1152.1685791015625\n",
      "Iteration is: 2324 and loss is: 1141.31640625\n",
      "Iteration is: 2325 and loss is: 1133.7216796875\n",
      "Iteration is: 2326 and loss is: 1129.26611328125\n",
      "Iteration is: 2327 and loss is: 1127.6866455078125\n",
      "Iteration is: 2328 and loss is: 1128.2745361328125\n",
      "Iteration is: 2329 and loss is: 1130.225341796875\n",
      "Iteration is: 2330 and loss is: 1132.95458984375\n",
      "Iteration is: 2331 and loss is: 1135.5810546875\n",
      "Iteration is: 2332 and loss is: 1137.97265625\n",
      "Iteration is: 2333 and loss is: 1139.4176025390625\n",
      "Iteration is: 2334 and loss is: 1140.3482666015625\n",
      "Iteration is: 2335 and loss is: 1140.090087890625\n",
      "Iteration is: 2336 and loss is: 1139.4071044921875\n",
      "Iteration is: 2337 and loss is: 1137.740966796875\n",
      "Iteration is: 2338 and loss is: 1135.8255615234375\n",
      "Iteration is: 2339 and loss is: 1133.275634765625\n",
      "Iteration is: 2340 and loss is: 1130.818359375\n",
      "Iteration is: 2341 and loss is: 1128.1339111328125\n",
      "Iteration is: 2342 and loss is: 1125.7420654296875\n",
      "Iteration is: 2343 and loss is: 1123.3016357421875\n",
      "Iteration is: 2344 and loss is: 1121.14453125\n",
      "Iteration is: 2345 and loss is: 1119.093505859375\n",
      "Iteration is: 2346 and loss is: 1117.325927734375\n",
      "Iteration is: 2347 and loss is: 1115.6273193359375\n",
      "Iteration is: 2348 and loss is: 1114.1243896484375\n",
      "Iteration is: 2349 and loss is: 1112.728515625\n",
      "Iteration is: 2350 and loss is: 1111.4913330078125\n",
      "Iteration is: 2351 and loss is: 1110.3519287109375\n",
      "Iteration is: 2352 and loss is: 1109.373779296875\n",
      "Iteration is: 2353 and loss is: 1108.484130859375\n",
      "Iteration is: 2354 and loss is: 1107.6895751953125\n",
      "Iteration is: 2355 and loss is: 1106.983154296875\n",
      "Iteration is: 2356 and loss is: 1106.3922119140625\n",
      "Iteration is: 2357 and loss is: 1105.8857421875\n",
      "Iteration is: 2358 and loss is: 1105.564208984375\n",
      "Iteration is: 2359 and loss is: 1105.3585205078125\n",
      "Iteration is: 2360 and loss is: 1105.4117431640625\n",
      "Iteration is: 2361 and loss is: 1105.739990234375\n",
      "Iteration is: 2362 and loss is: 1106.573974609375\n",
      "Iteration is: 2363 and loss is: 1107.8360595703125\n",
      "Iteration is: 2364 and loss is: 1110.06640625\n",
      "Iteration is: 2365 and loss is: 1113.15234375\n",
      "Iteration is: 2366 and loss is: 1117.9793701171875\n",
      "Iteration is: 2367 and loss is: 1124.3194580078125\n",
      "Iteration is: 2368 and loss is: 1134.0814208984375\n",
      "Iteration is: 2369 and loss is: 1146.6224365234375\n",
      "Iteration is: 2370 and loss is: 1165.99658203125\n",
      "Iteration is: 2371 and loss is: 1190.53515625\n",
      "Iteration is: 2372 and loss is: 1229.8067626953125\n",
      "Iteration is: 2373 and loss is: 1278.3447265625\n",
      "Iteration is: 2374 and loss is: 1359.0899658203125\n",
      "Iteration is: 2375 and loss is: 1455.00341796875\n",
      "Iteration is: 2376 and loss is: 1623.344482421875\n",
      "Iteration is: 2377 and loss is: 1809.982666015625\n",
      "Iteration is: 2378 and loss is: 2160.992919921875\n",
      "Iteration is: 2379 and loss is: 2502.18212890625\n",
      "Iteration is: 2380 and loss is: 3202.915771484375\n",
      "Iteration is: 2381 and loss is: 3730.673095703125\n",
      "Iteration is: 2382 and loss is: 4971.10107421875\n",
      "Iteration is: 2383 and loss is: 5477.87744140625\n",
      "Iteration is: 2384 and loss is: 7103.29736328125\n",
      "Iteration is: 2385 and loss is: 6834.91796875\n",
      "Iteration is: 2386 and loss is: 7726.36181640625\n",
      "Iteration is: 2387 and loss is: 6040.2333984375\n",
      "Iteration is: 2388 and loss is: 5149.8310546875\n",
      "Iteration is: 2389 and loss is: 3147.69775390625\n",
      "Iteration is: 2390 and loss is: 1877.031005859375\n",
      "Iteration is: 2391 and loss is: 1165.495849609375\n",
      "Iteration is: 2392 and loss is: 1183.8414306640625\n",
      "Iteration is: 2393 and loss is: 1732.615966796875\n",
      "Iteration is: 2394 and loss is: 2394.10595703125\n",
      "Iteration is: 2395 and loss is: 3112.642578125\n",
      "Iteration is: 2396 and loss is: 3114.77685546875\n",
      "Iteration is: 2397 and loss is: 3050.365234375\n",
      "Iteration is: 2398 and loss is: 2340.333251953125\n",
      "Iteration is: 2399 and loss is: 1763.525634765625\n",
      "Iteration is: 2400 and loss is: 1277.3284912109375\n",
      "Iteration is: 2401 and loss is: 1109.631591796875\n",
      "Iteration is: 2402 and loss is: 1223.384033203125\n",
      "Iteration is: 2403 and loss is: 1486.4677734375\n",
      "Iteration is: 2404 and loss is: 1791.2652587890625\n",
      "Iteration is: 2405 and loss is: 1910.572998046875\n",
      "Iteration is: 2406 and loss is: 1938.5625\n",
      "Iteration is: 2407 and loss is: 1717.4017333984375\n",
      "Iteration is: 2408 and loss is: 1477.6807861328125\n",
      "Iteration is: 2409 and loss is: 1228.1124267578125\n",
      "Iteration is: 2410 and loss is: 1087.757568359375\n",
      "Iteration is: 2411 and loss is: 1068.298828125\n",
      "Iteration is: 2412 and loss is: 1143.271728515625\n",
      "Iteration is: 2413 and loss is: 1262.037841796875\n",
      "Iteration is: 2414 and loss is: 1353.743408203125\n",
      "Iteration is: 2415 and loss is: 1410.1234130859375\n",
      "Iteration is: 2416 and loss is: 1376.213623046875\n",
      "Iteration is: 2417 and loss is: 1309.938232421875\n",
      "Iteration is: 2418 and loss is: 1205.2178955078125\n",
      "Iteration is: 2419 and loss is: 1118.348876953125\n",
      "Iteration is: 2420 and loss is: 1063.052490234375\n",
      "Iteration is: 2421 and loss is: 1049.494873046875\n",
      "Iteration is: 2422 and loss is: 1069.8272705078125\n",
      "Iteration is: 2423 and loss is: 1107.4937744140625\n",
      "Iteration is: 2424 and loss is: 1147.6434326171875\n",
      "Iteration is: 2425 and loss is: 1171.03955078125\n",
      "Iteration is: 2426 and loss is: 1179.5413818359375\n",
      "Iteration is: 2427 and loss is: 1162.34814453125\n",
      "Iteration is: 2428 and loss is: 1136.2291259765625\n",
      "Iteration is: 2429 and loss is: 1101.9749755859375\n",
      "Iteration is: 2430 and loss is: 1073.80419921875\n",
      "Iteration is: 2431 and loss is: 1054.58251953125\n",
      "Iteration is: 2432 and loss is: 1047.1888427734375\n",
      "Iteration is: 2433 and loss is: 1050.3045654296875\n",
      "Iteration is: 2434 and loss is: 1060.6004638671875\n",
      "Iteration is: 2435 and loss is: 1075.0687255859375\n",
      "Iteration is: 2436 and loss is: 1087.83935546875\n",
      "Iteration is: 2437 and loss is: 1099.473876953125\n",
      "Iteration is: 2438 and loss is: 1104.53076171875\n",
      "Iteration is: 2439 and loss is: 1109.719970703125\n",
      "Iteration is: 2440 and loss is: 1109.793701171875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 2441 and loss is: 1115.8353271484375\n",
      "Iteration is: 2442 and loss is: 1120.5184326171875\n",
      "Iteration is: 2443 and loss is: 1138.8092041015625\n",
      "Iteration is: 2444 and loss is: 1158.0118408203125\n",
      "Iteration is: 2445 and loss is: 1199.111572265625\n",
      "Iteration is: 2446 and loss is: 1239.93994140625\n",
      "Iteration is: 2447 and loss is: 1313.495361328125\n",
      "Iteration is: 2448 and loss is: 1381.54443359375\n",
      "Iteration is: 2449 and loss is: 1498.30419921875\n",
      "Iteration is: 2450 and loss is: 1593.9224853515625\n",
      "Iteration is: 2451 and loss is: 1754.888427734375\n",
      "Iteration is: 2452 and loss is: 1854.6717529296875\n",
      "Iteration is: 2453 and loss is: 2017.680419921875\n",
      "Iteration is: 2454 and loss is: 2061.711181640625\n",
      "Iteration is: 2455 and loss is: 2127.0029296875\n",
      "Iteration is: 2456 and loss is: 2072.352783203125\n",
      "Iteration is: 2457 and loss is: 1985.3837890625\n",
      "Iteration is: 2458 and loss is: 1885.66796875\n",
      "Iteration is: 2459 and loss is: 1737.731689453125\n",
      "Iteration is: 2460 and loss is: 1665.188232421875\n",
      "Iteration is: 2461 and loss is: 1548.369140625\n",
      "Iteration is: 2462 and loss is: 1493.0045166015625\n",
      "Iteration is: 2463 and loss is: 1398.23486328125\n",
      "Iteration is: 2464 and loss is: 1327.6229248046875\n",
      "Iteration is: 2465 and loss is: 1234.6922607421875\n",
      "Iteration is: 2466 and loss is: 1158.181396484375\n",
      "Iteration is: 2467 and loss is: 1088.8167724609375\n",
      "Iteration is: 2468 and loss is: 1044.4114990234375\n",
      "Iteration is: 2469 and loss is: 1024.57080078125\n",
      "Iteration is: 2470 and loss is: 1029.5472412109375\n",
      "Iteration is: 2471 and loss is: 1053.759033203125\n",
      "Iteration is: 2472 and loss is: 1087.9151611328125\n",
      "Iteration is: 2473 and loss is: 1127.8406982421875\n",
      "Iteration is: 2474 and loss is: 1159.56396484375\n",
      "Iteration is: 2475 and loss is: 1190.84228515625\n",
      "Iteration is: 2476 and loss is: 1203.854736328125\n",
      "Iteration is: 2477 and loss is: 1218.8284912109375\n",
      "Iteration is: 2478 and loss is: 1215.8055419921875\n",
      "Iteration is: 2479 and loss is: 1222.333984375\n",
      "Iteration is: 2480 and loss is: 1215.8486328125\n",
      "Iteration is: 2481 and loss is: 1223.681640625\n",
      "Iteration is: 2482 and loss is: 1221.9000244140625\n",
      "Iteration is: 2483 and loss is: 1235.271728515625\n",
      "Iteration is: 2484 and loss is: 1240.0653076171875\n",
      "Iteration is: 2485 and loss is: 1256.662109375\n",
      "Iteration is: 2486 and loss is: 1265.1356201171875\n",
      "Iteration is: 2487 and loss is: 1281.878173828125\n",
      "Iteration is: 2488 and loss is: 1290.745361328125\n",
      "Iteration is: 2489 and loss is: 1306.973388671875\n",
      "Iteration is: 2490 and loss is: 1315.0350341796875\n",
      "Iteration is: 2491 and loss is: 1334.17431640625\n",
      "Iteration is: 2492 and loss is: 1343.3424072265625\n",
      "Iteration is: 2493 and loss is: 1370.963134765625\n",
      "Iteration is: 2494 and loss is: 1383.445068359375\n",
      "Iteration is: 2495 and loss is: 1424.582763671875\n",
      "Iteration is: 2496 and loss is: 1442.516845703125\n",
      "Iteration is: 2497 and loss is: 1501.930908203125\n",
      "Iteration is: 2498 and loss is: 1527.038818359375\n",
      "Iteration is: 2499 and loss is: 1608.364013671875\n",
      "Iteration is: 2500 and loss is: 1637.3890380859375\n",
      "Iteration is: 2501 and loss is: 1738.797607421875\n",
      "Iteration is: 2502 and loss is: 1764.494384765625\n",
      "Iteration is: 2503 and loss is: 1881.4697265625\n",
      "Iteration is: 2504 and loss is: 1893.8603515625\n",
      "Iteration is: 2505 and loss is: 2015.5987548828125\n",
      "Iteration is: 2506 and loss is: 2002.29443359375\n",
      "Iteration is: 2507 and loss is: 2111.3935546875\n",
      "Iteration is: 2508 and loss is: 2060.252197265625\n",
      "Iteration is: 2509 and loss is: 2133.405029296875\n",
      "Iteration is: 2510 and loss is: 2037.60498046875\n",
      "Iteration is: 2511 and loss is: 2055.538330078125\n",
      "Iteration is: 2512 and loss is: 1921.610107421875\n",
      "Iteration is: 2513 and loss is: 1879.27978515625\n",
      "Iteration is: 2514 and loss is: 1725.494873046875\n",
      "Iteration is: 2515 and loss is: 1639.034912109375\n",
      "Iteration is: 2516 and loss is: 1492.7269287109375\n",
      "Iteration is: 2517 and loss is: 1393.7786865234375\n",
      "Iteration is: 2518 and loss is: 1277.266845703125\n",
      "Iteration is: 2519 and loss is: 1193.4127197265625\n",
      "Iteration is: 2520 and loss is: 1115.7769775390625\n",
      "Iteration is: 2521 and loss is: 1060.8331298828125\n",
      "Iteration is: 2522 and loss is: 1019.1785888671875\n",
      "Iteration is: 2523 and loss is: 992.8087158203125\n",
      "Iteration is: 2524 and loss is: 978.1354370117188\n",
      "Iteration is: 2525 and loss is: 973.0866088867188\n",
      "Iteration is: 2526 and loss is: 975.1381225585938\n",
      "Iteration is: 2527 and loss is: 982.1968994140625\n",
      "Iteration is: 2528 and loss is: 992.7973022460938\n",
      "Iteration is: 2529 and loss is: 1005.4886474609375\n",
      "Iteration is: 2530 and loss is: 1020.2819213867188\n",
      "Iteration is: 2531 and loss is: 1034.996337890625\n",
      "Iteration is: 2532 and loss is: 1052.195068359375\n",
      "Iteration is: 2533 and loss is: 1067.984375\n",
      "Iteration is: 2534 and loss is: 1088.1895751953125\n",
      "Iteration is: 2535 and loss is: 1105.871826171875\n",
      "Iteration is: 2536 and loss is: 1131.938720703125\n",
      "Iteration is: 2537 and loss is: 1154.13427734375\n",
      "Iteration is: 2538 and loss is: 1190.6571044921875\n",
      "Iteration is: 2539 and loss is: 1221.0235595703125\n",
      "Iteration is: 2540 and loss is: 1274.7711181640625\n",
      "Iteration is: 2541 and loss is: 1317.57763671875\n",
      "Iteration is: 2542 and loss is: 1398.359375\n",
      "Iteration is: 2543 and loss is: 1458.765869140625\n",
      "Iteration is: 2544 and loss is: 1580.400146484375\n",
      "Iteration is: 2545 and loss is: 1662.171875\n",
      "Iteration is: 2546 and loss is: 1841.962890625\n",
      "Iteration is: 2547 and loss is: 1943.552734375\n",
      "Iteration is: 2548 and loss is: 2195.908935546875\n",
      "Iteration is: 2549 and loss is: 2298.02001953125\n",
      "Iteration is: 2550 and loss is: 2616.4443359375\n",
      "Iteration is: 2551 and loss is: 2668.9521484375\n",
      "Iteration is: 2552 and loss is: 2998.75537109375\n",
      "Iteration is: 2553 and loss is: 2923.1015625\n",
      "Iteration is: 2554 and loss is: 3148.99365234375\n",
      "Iteration is: 2555 and loss is: 2884.628173828125\n",
      "Iteration is: 2556 and loss is: 2889.41357421875\n",
      "Iteration is: 2557 and loss is: 2477.27978515625\n",
      "Iteration is: 2558 and loss is: 2268.33935546875\n",
      "Iteration is: 2559 and loss is: 1854.586669921875\n",
      "Iteration is: 2560 and loss is: 1583.979248046875\n",
      "Iteration is: 2561 and loss is: 1309.2979736328125\n",
      "Iteration is: 2562 and loss is: 1142.785400390625\n",
      "Iteration is: 2563 and loss is: 1049.58349609375\n",
      "Iteration is: 2564 and loss is: 1037.4063720703125\n",
      "Iteration is: 2565 and loss is: 1076.86083984375\n",
      "Iteration is: 2566 and loss is: 1162.0103759765625\n",
      "Iteration is: 2567 and loss is: 1254.7955322265625\n",
      "Iteration is: 2568 and loss is: 1350.557373046875\n",
      "Iteration is: 2569 and loss is: 1428.093017578125\n",
      "Iteration is: 2570 and loss is: 1473.32421875\n",
      "Iteration is: 2571 and loss is: 1497.391845703125\n",
      "Iteration is: 2572 and loss is: 1473.3681640625\n",
      "Iteration is: 2573 and loss is: 1439.548828125\n",
      "Iteration is: 2574 and loss is: 1365.484375\n",
      "Iteration is: 2575 and loss is: 1297.3653564453125\n",
      "Iteration is: 2576 and loss is: 1209.7955322265625\n",
      "Iteration is: 2577 and loss is: 1139.1693115234375\n",
      "Iteration is: 2578 and loss is: 1070.3665771484375\n",
      "Iteration is: 2579 and loss is: 1021.4267578125\n",
      "Iteration is: 2580 and loss is: 984.6181640625\n",
      "Iteration is: 2581 and loss is: 964.0181274414062\n",
      "Iteration is: 2582 and loss is: 955.566650390625\n",
      "Iteration is: 2583 and loss is: 957.8609619140625\n",
      "Iteration is: 2584 and loss is: 967.0978393554688\n",
      "Iteration is: 2585 and loss is: 983.2367553710938\n",
      "Iteration is: 2586 and loss is: 999.970947265625\n",
      "Iteration is: 2587 and loss is: 1022.086669921875\n",
      "Iteration is: 2588 and loss is: 1038.4957275390625\n",
      "Iteration is: 2589 and loss is: 1060.701904296875\n",
      "Iteration is: 2590 and loss is: 1071.45458984375\n",
      "Iteration is: 2591 and loss is: 1089.4638671875\n",
      "Iteration is: 2592 and loss is: 1091.456298828125\n",
      "Iteration is: 2593 and loss is: 1102.093017578125\n",
      "Iteration is: 2594 and loss is: 1094.018310546875\n",
      "Iteration is: 2595 and loss is: 1095.3402099609375\n",
      "Iteration is: 2596 and loss is: 1078.49951171875\n",
      "Iteration is: 2597 and loss is: 1070.7183837890625\n",
      "Iteration is: 2598 and loss is: 1048.7545166015625\n",
      "Iteration is: 2599 and loss is: 1035.1031494140625\n",
      "Iteration is: 2600 and loss is: 1013.0579223632812\n",
      "Iteration is: 2601 and loss is: 998.007080078125\n",
      "Iteration is: 2602 and loss is: 979.721435546875\n",
      "Iteration is: 2603 and loss is: 966.9739379882812\n",
      "Iteration is: 2604 and loss is: 954.3467407226562\n",
      "Iteration is: 2605 and loss is: 945.8030395507812\n",
      "Iteration is: 2606 and loss is: 938.7850952148438\n",
      "Iteration is: 2607 and loss is: 934.8121948242188\n",
      "Iteration is: 2608 and loss is: 932.745361328125\n",
      "Iteration is: 2609 and loss is: 933.1883544921875\n",
      "Iteration is: 2610 and loss is: 935.7435913085938\n",
      "Iteration is: 2611 and loss is: 941.2268676757812\n",
      "Iteration is: 2612 and loss is: 949.530517578125\n",
      "Iteration is: 2613 and loss is: 962.847900390625\n",
      "Iteration is: 2614 and loss is: 980.7064208984375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 2615 and loss is: 1008.714599609375\n",
      "Iteration is: 2616 and loss is: 1045.1368408203125\n",
      "Iteration is: 2617 and loss is: 1105.4034423828125\n",
      "Iteration is: 2618 and loss is: 1181.55615234375\n",
      "Iteration is: 2619 and loss is: 1313.279052734375\n",
      "Iteration is: 2620 and loss is: 1470.3858642578125\n",
      "Iteration is: 2621 and loss is: 1761.089599609375\n",
      "Iteration is: 2622 and loss is: 2076.2626953125\n",
      "Iteration is: 2623 and loss is: 2709.04931640625\n",
      "Iteration is: 2624 and loss is: 3274.2978515625\n",
      "Iteration is: 2625 and loss is: 4535.849609375\n",
      "Iteration is: 2626 and loss is: 5265.1162109375\n",
      "Iteration is: 2627 and loss is: 7245.8662109375\n",
      "Iteration is: 2628 and loss is: 7377.3486328125\n",
      "Iteration is: 2629 and loss is: 8995.4892578125\n",
      "Iteration is: 2630 and loss is: 7294.05859375\n",
      "Iteration is: 2631 and loss is: 6616.1982421875\n",
      "Iteration is: 2632 and loss is: 3937.224365234375\n",
      "Iteration is: 2633 and loss is: 2179.007080078125\n",
      "Iteration is: 2634 and loss is: 1063.536865234375\n",
      "Iteration is: 2635 and loss is: 1007.7559814453125\n",
      "Iteration is: 2636 and loss is: 1743.849365234375\n",
      "Iteration is: 2637 and loss is: 2651.139892578125\n",
      "Iteration is: 2638 and loss is: 3590.7431640625\n",
      "Iteration is: 2639 and loss is: 3457.5673828125\n",
      "Iteration is: 2640 and loss is: 3169.4794921875\n",
      "Iteration is: 2641 and loss is: 2137.160888671875\n",
      "Iteration is: 2642 and loss is: 1376.598388671875\n",
      "Iteration is: 2643 and loss is: 973.0403442382812\n",
      "Iteration is: 2644 and loss is: 1047.760498046875\n",
      "Iteration is: 2645 and loss is: 1434.388916015625\n",
      "Iteration is: 2646 and loss is: 1800.66162109375\n",
      "Iteration is: 2647 and loss is: 2069.7783203125\n",
      "Iteration is: 2648 and loss is: 1921.934814453125\n",
      "Iteration is: 2649 and loss is: 1678.293212890625\n",
      "Iteration is: 2650 and loss is: 1294.0762939453125\n",
      "Iteration is: 2651 and loss is: 1050.8961181640625\n",
      "Iteration is: 2652 and loss is: 981.426025390625\n",
      "Iteration is: 2653 and loss is: 1063.8135986328125\n",
      "Iteration is: 2654 and loss is: 1226.523193359375\n",
      "Iteration is: 2655 and loss is: 1325.58544921875\n",
      "Iteration is: 2656 and loss is: 1363.1060791015625\n",
      "Iteration is: 2657 and loss is: 1267.02392578125\n",
      "Iteration is: 2658 and loss is: 1147.213623046875\n",
      "Iteration is: 2659 and loss is: 1023.1414184570312\n",
      "Iteration is: 2660 and loss is: 962.3357543945312\n",
      "Iteration is: 2661 and loss is: 964.1149291992188\n",
      "Iteration is: 2662 and loss is: 1000.3435668945312\n",
      "Iteration is: 2663 and loss is: 1048.6693115234375\n",
      "Iteration is: 2664 and loss is: 1058.3243408203125\n",
      "Iteration is: 2665 and loss is: 1044.84130859375\n",
      "Iteration is: 2666 and loss is: 999.5113525390625\n",
      "Iteration is: 2667 and loss is: 957.4188232421875\n",
      "Iteration is: 2668 and loss is: 929.404296875\n",
      "Iteration is: 2669 and loss is: 925.4688720703125\n",
      "Iteration is: 2670 and loss is: 938.6277465820312\n",
      "Iteration is: 2671 and loss is: 953.06787109375\n",
      "Iteration is: 2672 and loss is: 963.9159545898438\n",
      "Iteration is: 2673 and loss is: 954.3653564453125\n",
      "Iteration is: 2674 and loss is: 937.1336669921875\n",
      "Iteration is: 2675 and loss is: 911.9996337890625\n",
      "Iteration is: 2676 and loss is: 892.5065307617188\n",
      "Iteration is: 2677 and loss is: 883.0925903320312\n",
      "Iteration is: 2678 and loss is: 885.1253051757812\n",
      "Iteration is: 2679 and loss is: 894.9270629882812\n",
      "Iteration is: 2680 and loss is: 905.7877197265625\n",
      "Iteration is: 2681 and loss is: 914.1378173828125\n",
      "Iteration is: 2682 and loss is: 913.7269897460938\n",
      "Iteration is: 2683 and loss is: 908.415283203125\n",
      "Iteration is: 2684 and loss is: 897.09375\n",
      "Iteration is: 2685 and loss is: 885.9810791015625\n",
      "Iteration is: 2686 and loss is: 876.9014892578125\n",
      "Iteration is: 2687 and loss is: 872.1701049804688\n",
      "Iteration is: 2688 and loss is: 871.6893310546875\n",
      "Iteration is: 2689 and loss is: 874.1533813476562\n",
      "Iteration is: 2690 and loss is: 877.741455078125\n",
      "Iteration is: 2691 and loss is: 880.3843994140625\n",
      "Iteration is: 2692 and loss is: 881.4780883789062\n",
      "Iteration is: 2693 and loss is: 880.1459350585938\n",
      "Iteration is: 2694 and loss is: 877.6116943359375\n",
      "Iteration is: 2695 and loss is: 874.1984252929688\n",
      "Iteration is: 2696 and loss is: 871.19189453125\n",
      "Iteration is: 2697 and loss is: 868.9678344726562\n",
      "Iteration is: 2698 and loss is: 867.826416015625\n",
      "Iteration is: 2699 and loss is: 867.7105712890625\n",
      "Iteration is: 2700 and loss is: 868.1417236328125\n",
      "Iteration is: 2701 and loss is: 868.7042846679688\n",
      "Iteration is: 2702 and loss is: 868.8873901367188\n",
      "Iteration is: 2703 and loss is: 868.496337890625\n",
      "Iteration is: 2704 and loss is: 867.4442138671875\n",
      "Iteration is: 2705 and loss is: 865.982177734375\n",
      "Iteration is: 2706 and loss is: 864.2271118164062\n",
      "Iteration is: 2707 and loss is: 862.46875\n",
      "Iteration is: 2708 and loss is: 860.8825073242188\n",
      "Iteration is: 2709 and loss is: 859.6693725585938\n",
      "Iteration is: 2710 and loss is: 858.772705078125\n",
      "Iteration is: 2711 and loss is: 858.2255249023438\n",
      "Iteration is: 2712 and loss is: 857.8591918945312\n",
      "Iteration is: 2713 and loss is: 857.629638671875\n",
      "Iteration is: 2714 and loss is: 857.3949584960938\n",
      "Iteration is: 2715 and loss is: 857.0909423828125\n",
      "Iteration is: 2716 and loss is: 856.684326171875\n",
      "Iteration is: 2717 and loss is: 856.1466674804688\n",
      "Iteration is: 2718 and loss is: 855.5414428710938\n",
      "Iteration is: 2719 and loss is: 854.8587646484375\n",
      "Iteration is: 2720 and loss is: 854.16064453125\n",
      "Iteration is: 2721 and loss is: 853.4801025390625\n",
      "Iteration is: 2722 and loss is: 852.8817749023438\n",
      "Iteration is: 2723 and loss is: 852.3599853515625\n",
      "Iteration is: 2724 and loss is: 851.94580078125\n",
      "Iteration is: 2725 and loss is: 851.68017578125\n",
      "Iteration is: 2726 and loss is: 851.5770874023438\n",
      "Iteration is: 2727 and loss is: 851.6991577148438\n",
      "Iteration is: 2728 and loss is: 852.0443725585938\n",
      "Iteration is: 2729 and loss is: 852.7286376953125\n",
      "Iteration is: 2730 and loss is: 853.7709350585938\n",
      "Iteration is: 2731 and loss is: 855.4512939453125\n",
      "Iteration is: 2732 and loss is: 857.8976440429688\n",
      "Iteration is: 2733 and loss is: 861.7256469726562\n",
      "Iteration is: 2734 and loss is: 867.1031494140625\n",
      "Iteration is: 2735 and loss is: 875.45654296875\n",
      "Iteration is: 2736 and loss is: 887.0443725585938\n",
      "Iteration is: 2737 and loss is: 905.143310546875\n",
      "Iteration is: 2738 and loss is: 929.8267822265625\n",
      "Iteration is: 2739 and loss is: 968.7721557617188\n",
      "Iteration is: 2740 and loss is: 1020.8289794921875\n",
      "Iteration is: 2741 and loss is: 1105.5284423828125\n",
      "Iteration is: 2742 and loss is: 1216.384521484375\n",
      "Iteration is: 2743 and loss is: 1403.7308349609375\n",
      "Iteration is: 2744 and loss is: 1636.76708984375\n",
      "Iteration is: 2745 and loss is: 2045.779541015625\n",
      "Iteration is: 2746 and loss is: 2500.80859375\n",
      "Iteration is: 2747 and loss is: 3333.302490234375\n",
      "Iteration is: 2748 and loss is: 4055.348388671875\n",
      "Iteration is: 2749 and loss is: 5465.9853515625\n",
      "Iteration is: 2750 and loss is: 6054.7802734375\n",
      "Iteration is: 2751 and loss is: 7568.041015625\n",
      "Iteration is: 2752 and loss is: 6847.630859375\n",
      "Iteration is: 2753 and loss is: 7043.8125\n",
      "Iteration is: 2754 and loss is: 4862.859375\n",
      "Iteration is: 2755 and loss is: 3613.359130859375\n",
      "Iteration is: 2756 and loss is: 2084.6513671875\n",
      "Iteration is: 2757 and loss is: 1436.533203125\n",
      "Iteration is: 2758 and loss is: 1367.4974365234375\n",
      "Iteration is: 2759 and loss is: 1803.935791015625\n",
      "Iteration is: 2760 and loss is: 2230.146240234375\n",
      "Iteration is: 2761 and loss is: 2456.232177734375\n",
      "Iteration is: 2762 and loss is: 2612.83544921875\n",
      "Iteration is: 2763 and loss is: 2307.77294921875\n",
      "Iteration is: 2764 and loss is: 2160.976806640625\n",
      "Iteration is: 2765 and loss is: 1817.625732421875\n",
      "Iteration is: 2766 and loss is: 1597.673095703125\n",
      "Iteration is: 2767 and loss is: 1287.38037109375\n",
      "Iteration is: 2768 and loss is: 1108.8724365234375\n",
      "Iteration is: 2769 and loss is: 1019.769287109375\n",
      "Iteration is: 2770 and loss is: 1094.405517578125\n",
      "Iteration is: 2771 and loss is: 1300.825927734375\n",
      "Iteration is: 2772 and loss is: 1468.243896484375\n",
      "Iteration is: 2773 and loss is: 1578.8525390625\n",
      "Iteration is: 2774 and loss is: 1452.85498046875\n",
      "Iteration is: 2775 and loss is: 1268.08935546875\n",
      "Iteration is: 2776 and loss is: 1025.4158935546875\n",
      "Iteration is: 2777 and loss is: 872.053955078125\n",
      "Iteration is: 2778 and loss is: 833.4411010742188\n",
      "Iteration is: 2779 and loss is: 888.453857421875\n",
      "Iteration is: 2780 and loss is: 983.1136474609375\n",
      "Iteration is: 2781 and loss is: 1045.756103515625\n",
      "Iteration is: 2782 and loss is: 1075.109375\n",
      "Iteration is: 2783 and loss is: 1046.85595703125\n",
      "Iteration is: 2784 and loss is: 1012.5117797851562\n",
      "Iteration is: 2785 and loss is: 969.9437255859375\n",
      "Iteration is: 2786 and loss is: 943.593017578125\n",
      "Iteration is: 2787 and loss is: 915.8757934570312\n",
      "Iteration is: 2788 and loss is: 887.2124633789062\n",
      "Iteration is: 2789 and loss is: 858.6976928710938\n",
      "Iteration is: 2790 and loss is: 835.9321899414062\n",
      "Iteration is: 2791 and loss is: 830.3070678710938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 2792 and loss is: 842.7280883789062\n",
      "Iteration is: 2793 and loss is: 867.438232421875\n",
      "Iteration is: 2794 and loss is: 889.8770751953125\n",
      "Iteration is: 2795 and loss is: 902.6255493164062\n",
      "Iteration is: 2796 and loss is: 897.2821044921875\n",
      "Iteration is: 2797 and loss is: 881.6747436523438\n",
      "Iteration is: 2798 and loss is: 859.434326171875\n",
      "Iteration is: 2799 and loss is: 841.2564697265625\n",
      "Iteration is: 2800 and loss is: 828.9326782226562\n",
      "Iteration is: 2801 and loss is: 823.273193359375\n",
      "Iteration is: 2802 and loss is: 820.9389038085938\n",
      "Iteration is: 2803 and loss is: 819.6443481445312\n",
      "Iteration is: 2804 and loss is: 817.6317749023438\n",
      "Iteration is: 2805 and loss is: 815.8881225585938\n",
      "Iteration is: 2806 and loss is: 815.4413452148438\n",
      "Iteration is: 2807 and loss is: 817.04296875\n",
      "Iteration is: 2808 and loss is: 820.56396484375\n",
      "Iteration is: 2809 and loss is: 824.25048828125\n",
      "Iteration is: 2810 and loss is: 826.9254150390625\n",
      "Iteration is: 2811 and loss is: 826.9310913085938\n",
      "Iteration is: 2812 and loss is: 825.0459594726562\n",
      "Iteration is: 2813 and loss is: 821.2764892578125\n",
      "Iteration is: 2814 and loss is: 817.3086547851562\n",
      "Iteration is: 2815 and loss is: 813.6039428710938\n",
      "Iteration is: 2816 and loss is: 810.9610595703125\n",
      "Iteration is: 2817 and loss is: 809.0235595703125\n",
      "Iteration is: 2818 and loss is: 807.5287475585938\n",
      "Iteration is: 2819 and loss is: 805.9192504882812\n",
      "Iteration is: 2820 and loss is: 804.0535278320312\n",
      "Iteration is: 2821 and loss is: 802.0523681640625\n",
      "Iteration is: 2822 and loss is: 800.1807861328125\n",
      "Iteration is: 2823 and loss is: 798.6881103515625\n",
      "Iteration is: 2824 and loss is: 797.7178344726562\n",
      "Iteration is: 2825 and loss is: 797.275634765625\n",
      "Iteration is: 2826 and loss is: 797.17236328125\n",
      "Iteration is: 2827 and loss is: 797.129638671875\n",
      "Iteration is: 2828 and loss is: 797.0137329101562\n",
      "Iteration is: 2829 and loss is: 796.690185546875\n",
      "Iteration is: 2830 and loss is: 796.2451171875\n",
      "Iteration is: 2831 and loss is: 795.7479858398438\n",
      "Iteration is: 2832 and loss is: 795.3233642578125\n",
      "Iteration is: 2833 and loss is: 795.1044921875\n",
      "Iteration is: 2834 and loss is: 795.0822143554688\n",
      "Iteration is: 2835 and loss is: 795.2628173828125\n",
      "Iteration is: 2836 and loss is: 795.5806274414062\n",
      "Iteration is: 2837 and loss is: 796.0280151367188\n",
      "Iteration is: 2838 and loss is: 796.5481567382812\n",
      "Iteration is: 2839 and loss is: 797.2837524414062\n",
      "Iteration is: 2840 and loss is: 798.2108764648438\n",
      "Iteration is: 2841 and loss is: 799.6328125\n",
      "Iteration is: 2842 and loss is: 801.5084838867188\n",
      "Iteration is: 2843 and loss is: 804.4879760742188\n",
      "Iteration is: 2844 and loss is: 808.3888549804688\n",
      "Iteration is: 2845 and loss is: 814.1732177734375\n",
      "Iteration is: 2846 and loss is: 821.453857421875\n",
      "Iteration is: 2847 and loss is: 832.3555297851562\n",
      "Iteration is: 2848 and loss is: 845.9737548828125\n",
      "Iteration is: 2849 and loss is: 867.0953369140625\n",
      "Iteration is: 2850 and loss is: 893.342529296875\n",
      "Iteration is: 2851 and loss is: 935.5401000976562\n",
      "Iteration is: 2852 and loss is: 986.4041748046875\n",
      "Iteration is: 2853 and loss is: 1071.6134033203125\n",
      "Iteration is: 2854 and loss is: 1170.14697265625\n",
      "Iteration is: 2855 and loss is: 1345.2568359375\n",
      "Iteration is: 2856 and loss is: 1533.15380859375\n",
      "Iteration is: 2857 and loss is: 1892.222412109375\n",
      "Iteration is: 2858 and loss is: 2226.420654296875\n",
      "Iteration is: 2859 and loss is: 2930.78466796875\n",
      "Iteration is: 2860 and loss is: 3426.67724609375\n",
      "Iteration is: 2861 and loss is: 4639.0390625\n",
      "Iteration is: 2862 and loss is: 5055.1572265625\n",
      "Iteration is: 2863 and loss is: 6563.02587890625\n",
      "Iteration is: 2864 and loss is: 6178.6513671875\n",
      "Iteration is: 2865 and loss is: 6900.03662109375\n",
      "Iteration is: 2866 and loss is: 5252.72314453125\n",
      "Iteration is: 2867 and loss is: 4367.900390625\n",
      "Iteration is: 2868 and loss is: 2576.418212890625\n",
      "Iteration is: 2869 and loss is: 1461.093505859375\n",
      "Iteration is: 2870 and loss is: 862.772216796875\n",
      "Iteration is: 2871 and loss is: 896.788330078125\n",
      "Iteration is: 2872 and loss is: 1382.2991943359375\n",
      "Iteration is: 2873 and loss is: 1958.293701171875\n",
      "Iteration is: 2874 and loss is: 2590.192138671875\n",
      "Iteration is: 2875 and loss is: 2616.4111328125\n",
      "Iteration is: 2876 and loss is: 2592.654541015625\n",
      "Iteration is: 2877 and loss is: 1999.693115234375\n",
      "Iteration is: 2878 and loss is: 1495.108642578125\n",
      "Iteration is: 2879 and loss is: 1021.2243041992188\n",
      "Iteration is: 2880 and loss is: 799.9086303710938\n",
      "Iteration is: 2881 and loss is: 821.3671264648438\n",
      "Iteration is: 2882 and loss is: 1009.450927734375\n",
      "Iteration is: 2883 and loss is: 1269.30126953125\n",
      "Iteration is: 2884 and loss is: 1428.7679443359375\n",
      "Iteration is: 2885 and loss is: 1530.1202392578125\n",
      "Iteration is: 2886 and loss is: 1413.26806640625\n",
      "Iteration is: 2887 and loss is: 1261.8626708984375\n",
      "Iteration is: 2888 and loss is: 1038.8192138671875\n",
      "Iteration is: 2889 and loss is: 873.977783203125\n",
      "Iteration is: 2890 and loss is: 784.7606811523438\n",
      "Iteration is: 2891 and loss is: 782.373779296875\n",
      "Iteration is: 2892 and loss is: 842.8851318359375\n",
      "Iteration is: 2893 and loss is: 925.3316650390625\n",
      "Iteration is: 2894 and loss is: 1005.3287353515625\n",
      "Iteration is: 2895 and loss is: 1033.8277587890625\n",
      "Iteration is: 2896 and loss is: 1034.6300048828125\n",
      "Iteration is: 2897 and loss is: 979.5593872070312\n",
      "Iteration is: 2898 and loss is: 916.7985229492188\n",
      "Iteration is: 2899 and loss is: 846.283935546875\n",
      "Iteration is: 2900 and loss is: 795.181396484375\n",
      "Iteration is: 2901 and loss is: 767.5363159179688\n",
      "Iteration is: 2902 and loss is: 764.5186157226562\n",
      "Iteration is: 2903 and loss is: 779.796142578125\n",
      "Iteration is: 2904 and loss is: 803.76416015625\n",
      "Iteration is: 2905 and loss is: 828.9024658203125\n",
      "Iteration is: 2906 and loss is: 844.2557373046875\n",
      "Iteration is: 2907 and loss is: 851.8831176757812\n",
      "Iteration is: 2908 and loss is: 844.672119140625\n",
      "Iteration is: 2909 and loss is: 831.9625854492188\n",
      "Iteration is: 2910 and loss is: 811.6166381835938\n",
      "Iteration is: 2911 and loss is: 792.1163940429688\n",
      "Iteration is: 2912 and loss is: 774.4719848632812\n",
      "Iteration is: 2913 and loss is: 762.3099365234375\n",
      "Iteration is: 2914 and loss is: 755.9584350585938\n",
      "Iteration is: 2915 and loss is: 755.0211791992188\n",
      "Iteration is: 2916 and loss is: 757.92138671875\n",
      "Iteration is: 2917 and loss is: 762.8916625976562\n",
      "Iteration is: 2918 and loss is: 768.376220703125\n",
      "Iteration is: 2919 and loss is: 772.6895751953125\n",
      "Iteration is: 2920 and loss is: 775.7592163085938\n",
      "Iteration is: 2921 and loss is: 776.3594360351562\n",
      "Iteration is: 2922 and loss is: 775.6206665039062\n",
      "Iteration is: 2923 and loss is: 772.730712890625\n",
      "Iteration is: 2924 and loss is: 769.1682739257812\n",
      "Iteration is: 2925 and loss is: 764.7061767578125\n",
      "Iteration is: 2926 and loss is: 760.5104370117188\n",
      "Iteration is: 2927 and loss is: 756.4856567382812\n",
      "Iteration is: 2928 and loss is: 753.0978393554688\n",
      "Iteration is: 2929 and loss is: 750.2828979492188\n",
      "Iteration is: 2930 and loss is: 748.1585083007812\n",
      "Iteration is: 2931 and loss is: 746.6497192382812\n",
      "Iteration is: 2932 and loss is: 745.7130737304688\n",
      "Iteration is: 2933 and loss is: 745.247802734375\n",
      "Iteration is: 2934 and loss is: 745.1064453125\n",
      "Iteration is: 2935 and loss is: 745.1956787109375\n",
      "Iteration is: 2936 and loss is: 745.416748046875\n",
      "Iteration is: 2937 and loss is: 745.7069091796875\n",
      "Iteration is: 2938 and loss is: 746.005126953125\n",
      "Iteration is: 2939 and loss is: 746.271484375\n",
      "Iteration is: 2940 and loss is: 746.468017578125\n",
      "Iteration is: 2941 and loss is: 746.644775390625\n",
      "Iteration is: 2942 and loss is: 746.75048828125\n",
      "Iteration is: 2943 and loss is: 746.9253540039062\n",
      "Iteration is: 2944 and loss is: 747.1433715820312\n",
      "Iteration is: 2945 and loss is: 747.57470703125\n",
      "Iteration is: 2946 and loss is: 748.021484375\n",
      "Iteration is: 2947 and loss is: 748.6275634765625\n",
      "Iteration is: 2948 and loss is: 749.29833984375\n",
      "Iteration is: 2949 and loss is: 750.3882446289062\n",
      "Iteration is: 2950 and loss is: 751.7379150390625\n",
      "Iteration is: 2951 and loss is: 753.7813720703125\n",
      "Iteration is: 2952 and loss is: 756.3064575195312\n",
      "Iteration is: 2953 and loss is: 759.9757080078125\n",
      "Iteration is: 2954 and loss is: 764.5415649414062\n",
      "Iteration is: 2955 and loss is: 771.15966796875\n",
      "Iteration is: 2956 and loss is: 779.3186645507812\n",
      "Iteration is: 2957 and loss is: 791.2537841796875\n",
      "Iteration is: 2958 and loss is: 806.2614135742188\n",
      "Iteration is: 2959 and loss is: 828.2371826171875\n",
      "Iteration is: 2960 and loss is: 855.7866821289062\n",
      "Iteration is: 2961 and loss is: 896.526611328125\n",
      "Iteration is: 2962 and loss is: 947.7120971679688\n",
      "Iteration is: 2963 and loss is: 1024.30078125\n",
      "Iteration is: 2964 and loss is: 1119.605712890625\n",
      "Iteration is: 2965 and loss is: 1262.510986328125\n",
      "Iteration is: 2966 and loss is: 1435.07958984375\n",
      "Iteration is: 2967 and loss is: 1693.970703125\n",
      "Iteration is: 2968 and loss is: 1986.635986328125\n",
      "Iteration is: 2969 and loss is: 2416.65869140625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 2970 and loss is: 2822.48876953125\n",
      "Iteration is: 2971 and loss is: 3412.32666015625\n",
      "Iteration is: 2972 and loss is: 3741.890869140625\n",
      "Iteration is: 2973 and loss is: 4321.69921875\n",
      "Iteration is: 2974 and loss is: 4195.3330078125\n",
      "Iteration is: 2975 and loss is: 4522.28759765625\n",
      "Iteration is: 2976 and loss is: 3875.36328125\n",
      "Iteration is: 2977 and loss is: 3883.5048828125\n",
      "Iteration is: 2978 and loss is: 3206.247314453125\n",
      "Iteration is: 2979 and loss is: 3097.64892578125\n",
      "Iteration is: 2980 and loss is: 2678.190673828125\n",
      "Iteration is: 2981 and loss is: 2628.14111328125\n",
      "Iteration is: 2982 and loss is: 2147.12744140625\n",
      "Iteration is: 2983 and loss is: 1845.9324951171875\n",
      "Iteration is: 2984 and loss is: 1237.205078125\n",
      "Iteration is: 2985 and loss is: 875.7158203125\n",
      "Iteration is: 2986 and loss is: 798.5112915039062\n",
      "Iteration is: 2987 and loss is: 1019.6702880859375\n",
      "Iteration is: 2988 and loss is: 1402.225341796875\n",
      "Iteration is: 2989 and loss is: 1630.322021484375\n",
      "Iteration is: 2990 and loss is: 1772.77734375\n",
      "Iteration is: 2991 and loss is: 1571.28759765625\n",
      "Iteration is: 2992 and loss is: 1393.0657958984375\n",
      "Iteration is: 2993 and loss is: 1214.46875\n",
      "Iteration is: 2994 and loss is: 1188.99609375\n",
      "Iteration is: 2995 and loss is: 1178.7841796875\n",
      "Iteration is: 2996 and loss is: 1144.910888671875\n",
      "Iteration is: 2997 and loss is: 1046.2547607421875\n",
      "Iteration is: 2998 and loss is: 887.2129516601562\n",
      "Iteration is: 2999 and loss is: 764.9080200195312\n",
      "Iteration is: 3000 and loss is: 719.710205078125\n",
      "Training time: 5.4373\n"
     ]
    }
   ],
   "source": [
    "##Self Adaptive for residual minimzation\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, 'Utilities/')\n",
    "import os\n",
    "\n",
    "from scipy.interpolate import griddata\n",
    "from pyDOE import lhs\n",
    "from plotting import newfig, savefig\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import scipy.io\n",
    "\n",
    "np.random.seed(seed=1234)\n",
    "tf.random.set_seed(1234)\n",
    "tf.config.experimental.enable_tensor_float_32_execution(False)\n",
    "lb = -1\n",
    "ub = 1   \n",
    "\n",
    "# Initalization of Network\n",
    "def hyper_initial(size):\n",
    "    in_dim = size[0]\n",
    "    out_dim = size[1]\n",
    "    std = np.sqrt(2.0/(in_dim + out_dim))\n",
    "    return tf.Variable(tf.random.truncated_normal(shape=size, stddev = std))\n",
    "\n",
    "# Neural Network \n",
    "def DNN(X, W, b):\n",
    "    A = 2.0*(X - lb)/(ub - lb) - 1.0\n",
    "    L = len(W)\n",
    "    for i in range(L-1):\n",
    "        A = tf.tanh(tf.add(tf.matmul(A, W[i]), b[i]))\n",
    "    Y = tf.add(tf.matmul(A, W[-1]), b[-1])\n",
    "    return Y\n",
    "\n",
    "def train_vars_nn(W, b):\n",
    "    return W + b\n",
    "\n",
    "def train_vars_total(W, b, lambda_r, lambda_b):\n",
    "    return W + b + lambda_r + lambda_b\n",
    "\n",
    "def train_vars_sa(lambda_r, lambda_b):\n",
    "    return lambda_r + lambda_b\n",
    "\n",
    "def net_u(x,w, b):\n",
    "    u = DNN(x, w, b)\n",
    "    return u\n",
    "\n",
    "def loss_weight(N_r, N_b):\n",
    "    alpha_b = tf.Variable(tf.reshape(tf.repeat(1000.0, N_b), (N_b, -1))) \n",
    "    alpha_r = tf.Variable(tf.ones(shape=[N_r, 1]), dtype=tf.float32)\n",
    "    return alpha_r, alpha_b\n",
    "\n",
    "\n",
    "#@tf.function(jit_compile=True)\n",
    "@tf.function\n",
    "def net_f(x, W, b, nu):\n",
    "    with tf.GradientTape(persistent=True) as tape1:\n",
    "        tape1.watch([x])\n",
    "        with tf.GradientTape(persistent=True) as tape2:\n",
    "            tape2.watch([x])\n",
    "            u=net_u(x, W, b)\n",
    "        u_x = tape2.gradient(u, x)\n",
    "        del tape2\n",
    "    u_xx = tape1.gradient(u_x, x)  \n",
    "    del tape1\n",
    "    f = u_xx - (1/nu)*u-(1/nu)*tf.exp(x)\n",
    "    return f\n",
    "\n",
    "@tf.function\n",
    "def net_fd(x, W, b, nu):\n",
    "    h=0.1\n",
    "    u_xx= (1/(h**2))*(net_u(x + h , W, b) - net_u(x, W, b) + net_u(x-h, W, b))\n",
    "    f = u_xx - (1/nu)*u-(1/nu)*tf.exp(x)\n",
    "    return f\n",
    "\n",
    "\n",
    "\n",
    "#@tf.function(jit_compile=True)\n",
    "@tf.function\n",
    "def train_step(W, b, X_u_train_tf, u_train_tf, X_f_train_tf, opt, nu, lambda_r, lambda_b):\n",
    "    x_u = X_u_train_tf\n",
    "    x_f = X_f_train_tf\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        tape.watch([W,b,lambda_r,lambda_b])\n",
    "        u_nn = net_u(x_u, W, b) \n",
    "        f_nn = net_f(x_f,W, b, nu)\n",
    "        loss_r = tf.square(lambda_r*f_nn)\n",
    "        loss_b = tf.square(lambda_b*(u_nn-u_train_tf))\n",
    "        loss =    tf.reduce_mean(loss_b) + tf.reduce_mean(loss_r)  \n",
    "    grads = tape.gradient(loss, train_vars_nn(W, b))\n",
    "    grads_u = tape.gradient(loss, lambda_r)\n",
    "    grads_b = tape.gradient(loss, lambda_b)\n",
    "    opt.apply_gradients(zip(grads, train_vars_nn(W,b)))\n",
    "    opt.apply_gradients(zip([-grads_u], [lambda_r]))\n",
    "    opt.apply_gradients(zip([-grads_b], [lambda_b]))\n",
    "    return loss\n",
    "\n",
    "def predict(X_star_tf, w, b):\n",
    "    u_pred = net_u(X_star_tf, w, b)\n",
    "    return u_pred\n",
    "    \n",
    "nu = 10**-3\n",
    "Nmax= 3000\n",
    "N_f = 500\n",
    "N_b = 2\n",
    "\n",
    "layers = [1, 8, 8,8,8,8,8,1]\n",
    "L = len(layers)\n",
    "W = [hyper_initial([layers[l-1], layers[l]]) for l in range(1, L)] \n",
    "b = [tf.Variable(tf.zeros([1, layers[l]])) for l in range(1, L)] \n",
    "\n",
    "alpha_r, alpha_b = loss_weight(N_f, N_b)\n",
    "\n",
    "x_0 = -1\n",
    "x_1 = 1\n",
    "u_0 = 1\n",
    "u_1 = 0\n",
    "\n",
    "X_u_train = np.vstack([x_0, x_1])\n",
    "u_train = np.vstack([u_0, u_1])\n",
    "\n",
    "X_f_train = lb + (ub-lb)*lhs(1, N_f)\n",
    "X_f_star =  np.linspace(-1,1,200)\n",
    "X_f_star = X_f_star.reshape((-1,1))\n",
    "\n",
    "\n",
    "X_u_train_tf = tf.convert_to_tensor(X_u_train, dtype=tf.float32)\n",
    "u_train_tf =   tf.convert_to_tensor(u_train, dtype=tf.float32)\n",
    "X_f_train_tf = tf.convert_to_tensor(X_f_train, dtype=tf.float32)\n",
    "X_star_tf = tf.convert_to_tensor(X_f_star, dtype=tf.float32)\n",
    "\n",
    "\n",
    "lr = 1e-3\n",
    "optimizer = tf.optimizers.Adam(learning_rate=lr)\n",
    "\n",
    "start_time = time.time()\n",
    "n=0\n",
    "loss = []\n",
    "while n <= Nmax:\n",
    "    loss_= train_step(W, b, X_u_train_tf, u_train_tf, X_f_train_tf, optimizer, nu, alpha_r, alpha_b)\n",
    "    loss.append(loss_)    \n",
    "    print(f\"Iteration is: {n} and loss is: {loss_}\")\n",
    "    n+=1\n",
    "\n",
    "elapsed = time.time() - start_time                \n",
    "print('Training time: %.4f' % (elapsed))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189805db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X_star_tf, w, b):\n",
    "    x_star = X_star_tf\n",
    "    u_pred = net_u(x_star, w, b)\n",
    "    return u_pred\n",
    "\n",
    "u_star = y_act\n",
    "    \n",
    "u_pred = predict(X_star_tf, W, b)\n",
    "error_u = np.linalg.norm(u_star-u_pred,2)/np.linalg.norm(u_star,2)\n",
    "print('Error u: %e' % (error_u))                     \n",
    "\n",
    "\n",
    "\n",
    "####### Row 1: u(t,x) slices ##################    \n",
    "gs1 = gridspec.GridSpec(1, 1)\n",
    "#gs1.update(top=1-1/3, bottom=0, left=0.1, right=0.9, wspace=0.5)\n",
    "\n",
    "ax = plt.subplot(gs1[0, 0])\n",
    "ax.plot(x,y_act, 'b-', linewidth = 2, label = 'Exact')       \n",
    "ax.plot(X_f_star, u_pred, '--r', linewidth = 2, label = 'Prediction')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$u(x)$') \n",
    "plt.legend()\n",
    "#ax.axis('square')\n",
    "ax.set_xlim([-1.1,1.1])\n",
    "ax.set_ylim([-3.1,1.1])\n",
    "plt.show()\n",
    "\n",
    "###############################################\n",
    "loss_list = [loss[i].numpy() for i in range(len(loss))]\n",
    "\n",
    "\n",
    "gs2 = gridspec.GridSpec(1, 1)\n",
    "ax = plt.subplot(gs2[0, 0])\n",
    "\n",
    "ep = np.arange(0,Nmax+1,1)\n",
    "ax.semilogy(ep,loss_list/(np.max(loss_list)), 'g-', linewidth = 2)       \n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('Loss') \n",
    "plt.legend()\n",
    "#ax.axis('square')\n",
    "##ax.set_xlim([-1.1,1.1])\n",
    "##x.set_ylim([-3.1,1.1])\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13e8349",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, 'Utilities/')\n",
    "import os\n",
    "\n",
    "from scipy.interpolate import griddata\n",
    "from pyDOE import lhs\n",
    "from plotting import newfig, savefig\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import scipy.io\n",
    "\n",
    "np.random.seed(seed=1234)\n",
    "tf.random.set_seed(1234)\n",
    "tf.config.experimental.enable_tensor_float_32_execution(False)\n",
    "lb = -1\n",
    "ub = 1   \n",
    "\n",
    "# Initalization of Network\n",
    "def hyper_initial(size):\n",
    "    in_dim = size[0]\n",
    "    out_dim = size[1]\n",
    "    std = np.sqrt(2.0/(in_dim + out_dim))\n",
    "    return tf.Variable(tf.random.truncated_normal(shape=size, stddev = std))\n",
    "\n",
    "# Neural Network \n",
    "def DNN(X, W, b):\n",
    "    A = 2.0*(X - lb)/(ub - lb) - 1.0\n",
    "    L = len(W)\n",
    "    for i in range(L-1):\n",
    "        A = tf.tanh(tf.add(tf.matmul(A, W[i]), b[i]))\n",
    "    Y = tf.add(tf.matmul(A, W[-1]), b[-1])\n",
    "    return Y\n",
    "\n",
    "def train_vars(W, b):\n",
    "    return W + b\n",
    "\n",
    "def net_u(x,w, b):\n",
    "    u = DNN(x, w, b)\n",
    "    return u\n",
    "\n",
    "\n",
    "#@tf.function(jit_compile=True)\n",
    "@tf.function\n",
    "def net_f(x,W, b, nu):\n",
    "    with tf.GradientTape(persistent=True) as tape1:\n",
    "        tape1.watch([x])\n",
    "        with tf.GradientTape(persistent=True) as tape2:\n",
    "            tape2.watch([x])\n",
    "            u=net_u(x, W, b)\n",
    "        u_x = tape2.gradient(u, x)\n",
    "        del tape2\n",
    "    u_xx = tape1.gradient(u_x, x)  \n",
    "    del tape1\n",
    "    f = u_xx - (1/nu)*u-(1/nu)*tf.exp(x)\n",
    "    return f\n",
    "\n",
    "@tf.function\n",
    "def net_f(x,W, b, nu):\n",
    "    with tf.GradientTape(persistent=True) as tape1:\n",
    "        tape1.watch([x])\n",
    "        with tf.GradientTape(persistent=True) as tape2:\n",
    "            tape2.watch([x])\n",
    "            u=net_u(x, W, b)\n",
    "        u_x = tape2.gradient(u, x)\n",
    "        del tape2\n",
    "    u_xx = tape1.gradient(u_x, x)  \n",
    "    del tape1\n",
    "    f = u_xx - (1/nu)*u-(1/nu)*tf.exp(x)\n",
    "    return f\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#@tf.function(jit_compile=True)\n",
    "@tf.function\n",
    "def train_step(layers, W, b, X_u_train_tf, u_train_tf, X_f_train_tf, opt, nu, lambda_b, beta):\n",
    "    x_u = X_u_train_tf[:,0:1]\n",
    "    x_f = X_f_train_tf[:,0:1]\n",
    "    adpative_constant_bcs_list = []\n",
    "    lambda_b_list = []\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        tape.watch([W,b])\n",
    "        u_nn = net_u(x_u,W, b) \n",
    "        f_nn = net_f(x_f,W, b, nu)\n",
    "        bc_loss =  tf.reduce_mean(tf.square(u_nn - u_train_tf))\n",
    "        phys_loss = tf.reduce_mean(tf.square(f_nn)) \n",
    "        loss = lambda_b[-1]*bc_loss  + phys_loss\n",
    "#         loss = loss / (1 + lambda_b[-1])\n",
    "#         p_loss = phys_loss/(1 + lambda_b[-1])\n",
    "#         b_loss = lambda_b[-1]*bc_loss/(1 + lambda_b[-1])\n",
    "    grad_loss = tape.gradient(loss, train_vars(W,b))\n",
    "    opt.apply_gradients(zip(grad_loss, train_vars(W,b)))\n",
    "    grads_bc = tape.gradient(bc_loss, train_vars(W,b))\n",
    "    grads_phys = tape.gradient(phys_loss, train_vars(W,b))\n",
    "    for i in range(len(layers) - 1):\n",
    "        adpative_constant_bcs_list.append(\n",
    "                tf.reduce_mean(tf.abs(grads_phys[i])) / tf.reduce_mean(tf.abs(grads_bc[i])))\n",
    "        \n",
    "    lambda_b_new = tf.reduce_mean(tf.stack(adpative_constant_bcs_list))\n",
    "    lambda_b_new = (1-beta)*lambda_b[-1] + beta*lambda_b_new\n",
    "    lambda_b_list.append(lambda_b_new)\n",
    "                                      \n",
    "    return loss, lambda_b_list, p_loss, b_loss\n",
    "\n",
    "\n",
    "def predict(X_star_tf, w, b):\n",
    "    u_pred = net_u(X_star_tf, w, b)\n",
    "    return u_pred\n",
    "    \n",
    "nu = 10**-3\n",
    "noise = 0.0        \n",
    "N_f = 500\n",
    "Nmax=50000\n",
    "\n",
    "layers = [1, 8,8,8,8,8,8, 1]\n",
    "L = len(layers)\n",
    "W = [hyper_initial([layers[l-1], layers[l]]) for l in range(1, L)] \n",
    "b = [tf.Variable(tf.zeros([1, layers[l]])) for l in range(1, L)] \n",
    "\n",
    "x_0 = -1\n",
    "x_1 = 1\n",
    "u_0 = 1\n",
    "u_1 = 0\n",
    "\n",
    "X_u_train = np.vstack([x_0, x_1])\n",
    "u_train = np.vstack([u_0, u_1])\n",
    "X_f_train = lb + (ub-lb)*lhs(1, N_f)\n",
    "X_f_star =  np.linspace(-1,1,200)\n",
    "X_f_star = X_f_star.reshape((-1,1))\n",
    "X_u_train_tf = tf.convert_to_tensor(X_u_train, dtype=tf.float32)\n",
    "u_train_tf =   tf.convert_to_tensor(u_train, dtype=tf.float32)\n",
    "X_f_train_tf = tf.convert_to_tensor(X_f_train, dtype=tf.float32)\n",
    "X_star_tf = tf.convert_to_tensor(X_f_star, dtype=tf.float32)\n",
    "lam_b = np.array([1.0])\n",
    "lam_b_tf= tf.convert_to_tensor(lam_b, dtype=tf.float32)                                \n",
    "lambda_b_list = [lam_b_tf]                               \n",
    "lr = 1e-3\n",
    "optimizer = tf.optimizers.Adam(learning_rate=lr)\n",
    "start_time = time.time()\n",
    "n=0\n",
    "loss = []\n",
    "bc_loss = []\n",
    "phys_loss = []\n",
    "beta = 0.1\n",
    "\n",
    "while n <= Nmax:\n",
    "    loss_, lambda_b_out, phys_loss_, bc_loss_ = train_step(layers, W, b, X_u_train_tf,\\\n",
    "                                                u_train_tf, X_f_train_tf, optimizer, nu,lambda_b_list, beta)\n",
    "    lambda_b_list = lambda_b_out\n",
    "    loss.append(loss_) \n",
    "    bc_loss.append(bc_loss_)\n",
    "    phys_loss.append(phys_loss_)\n",
    "    \n",
    "    print(f\"Iteration is: {n} and loss is: {loss_}\")\n",
    "    n+=1\n",
    "\n",
    "elapsed = time.time() - start_time                \n",
    "print('Training time: %.4f' % (elapsed))               \n",
    "print('Training time: %.4f' % (elapsed))\n",
    "\n",
    "\n",
    "def predict(X_star_tf, w, b):\n",
    "    x_star = X_star_tf\n",
    "    u_pred = net_u(x_star, w, b)\n",
    "    return u_pred\n",
    "\n",
    "u_star = y_act\n",
    "    \n",
    "u_pred = predict(X_star_tf, W, b)\n",
    "error_u = np.linalg.norm(u_star-u_pred,2)/np.linalg.norm(u_star,2)\n",
    "print('Error u: %e' % (error_u))                     \n",
    "\n",
    "\n",
    "\n",
    "####### Row 1: u(t,x) slices ##################    \n",
    "gs1 = gridspec.GridSpec(1, 1)\n",
    "#gs1.update(top=1-1/3, bottom=0, left=0.1, right=0.9, wspace=0.5)\n",
    "\n",
    "ax = plt.subplot(gs1[0, 0])\n",
    "ax.plot(x,y_act, 'b-', linewidth = 2, label = 'Exact')       \n",
    "ax.plot(X_f_star, u_pred, '--r', linewidth = 2, label = 'Prediction')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$u(x)$') \n",
    "plt.legend()\n",
    "#ax.axis('square')\n",
    "ax.set_xlim([-1.1,1.1])\n",
    "ax.set_ylim([-3.1,1.1])\n",
    "plt.show()\n",
    "\n",
    "###############################################\n",
    "loss_list = [loss[i].numpy() for i in range(len(loss))]\n",
    "bc_loss_list = [bc_loss[i].numpy() for i in range(len(bc_loss))]\n",
    "phys_loss_list = [phys_loss[i].numpy() for i in range(len(phys_loss))]\n",
    "\n",
    "\n",
    "gs2 = gridspec.GridSpec(1, 1)\n",
    "ax = plt.subplot(gs2[0, 0])\n",
    "\n",
    "ep = np.arange(0,Nmax+1,1)\n",
    "ax.semilogy(ep,bc_loss_list, 'g-', linewidth = 2, label=\"Boundary Loss\")   \n",
    "ax.semilogy(ep,phys_loss_list, 'r-', linewidth = 2, label=\"Residual Loss\")\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('Loss') \n",
    "plt.legend()\n",
    "#ax.axis('square')\n",
    "##ax.set_xlim([-1.1,1.1])\n",
    "##x.set_ylim([-3.1,1.1])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907589ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X_star_tf, w, b):\n",
    "    x_star = X_star_tf\n",
    "    u_pred = net_u(x_star, w, b)\n",
    "    return u_pred\n",
    "\n",
    "u_star = y_act\n",
    "    \n",
    "u_pred = predict(X_star_tf, W, b)\n",
    "error_u = np.linalg.norm(u_star-u_pred,2)/np.linalg.norm(u_star,2)\n",
    "print('Error u: %e' % (error_u))                     \n",
    "\n",
    "\n",
    "\n",
    "####### Row 1: u(t,x) slices ##################    \n",
    "gs1 = gridspec.GridSpec(1, 1)\n",
    "#gs1.update(top=1-1/3, bottom=0, left=0.1, right=0.9, wspace=0.5)\n",
    "\n",
    "ax = plt.subplot(gs1[0, 0])\n",
    "ax.plot(x,y_act, 'b-', linewidth = 2, label = 'Exact')       \n",
    "ax.plot(X_f_star, u_pred, '--r', linewidth = 2, label = 'Prediction')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$u(x)$') \n",
    "plt.legend()\n",
    "#ax.axis('square')\n",
    "ax.set_xlim([-1.1,1.1])\n",
    "ax.set_ylim([-3.1,1.1])\n",
    "plt.show()\n",
    "\n",
    "###############################################\n",
    "loss_list = [loss[i].numpy() for i in range(len(loss))]\n",
    "bc_loss_list = [bc_loss[i].numpy() for i in range(len(bc_loss))]\n",
    "phys_loss_list = [phys_loss[i].numpy() for i in range(len(phys_loss))]\n",
    "\n",
    "\n",
    "gs2 = gridspec.GridSpec(1, 1)\n",
    "ax = plt.subplot(gs2[0, 0])\n",
    "\n",
    "ep = np.arange(0,Nmax+1,1)\n",
    "ax.semilogy(ep, bc_loss_list, 'g-', linewidth = 2, label=\"Boundary Loss\")   \n",
    "ax.semilogy(ep,phys_loss_list, 'r-', linewidth = 2, label=\"Residual Loss\")\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('Loss') \n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b2f706",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/jnh277/Linearly-Constrained-NN/blob/release/simulted_divergence_free.py\n",
    "\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils import data\n",
    "import numpy as np\n",
    "import torch.autograd as ag\n",
    "import torch.nn as nn\n",
    "torch.manual_seed(1234)\n",
    "epochs = 400\n",
    "n_data = 200\n",
    "### Input Data\n",
    "def vector_field(x, y, a=0.01):\n",
    "    v1 = torch.exp(-a*x*y)*(a*x*torch.sin(x*y) - x*torch.cos(x*y))\n",
    "    v2 = torch.exp(-a*x*y)*(y*torch.cos(x*y) - a*y*torch.sin(x*y))\n",
    "    return (v1, v2)\n",
    "\n",
    "\n",
    "## ------------------ set up models-------------------------- ##\n",
    "# set network size\n",
    "n_in = 2\n",
    "n_h1 = 100\n",
    "n_h2 = 50\n",
    "n_o = 1\n",
    "\n",
    "# two outputs for the unconstrained network\n",
    "n_o_uc = 2\n",
    "\n",
    "# define model class\n",
    "class DivFree2D(torch.nn.Module):\n",
    "    def __init__(self, base_net):\n",
    "        super(DivFree2D, self).__init__()\n",
    "        self.base_net = base_net\n",
    "\n",
    "    def forward(self, x):\n",
    "        x.requires_grad = True\n",
    "        y = self.base_net(x)\n",
    "        dydx = ag.grad(outputs=y, inputs=x, create_graph=True, grad_outputs=torch.ones(y.size()),\n",
    "                       retain_graph=True, only_inputs=True)[0]\n",
    "        return y, dydx[:,1].unsqueeze(1), -dydx[:,0].unsqueeze(1)\n",
    "\n",
    "model = DivFree2D(nn.Sequential(nn.Linear(n_in,n_h1),nn.Tanh(),nn.Linear(n_h1,n_h2),\n",
    "                                         nn.Tanh(),nn.Linear(n_h2,n_o)))\n",
    "\n",
    "\n",
    "model_uc = torch.nn.Sequential(\n",
    "    torch.nn.Linear(n_in, n_h1),\n",
    "    torch.nn.Tanh(),\n",
    "    torch.nn.Linear(n_h1, n_h2),\n",
    "    torch.nn.Tanh(),\n",
    "    torch.nn.Linear(n_h2, n_o_uc),\n",
    ")\n",
    "\n",
    "\n",
    "# pregenerate validation data\n",
    "x_val = 4.0 * torch.rand(2000, 2)\n",
    "x1_val = x_val[:, 0].unsqueeze(1)\n",
    "x2_val = x_val[:, 1].unsqueeze(1)\n",
    "\n",
    "(v1, v2) = vector_field(x1_val, x2_val)\n",
    "y1_val = v1 + 0.1 * torch.randn(x1_val.size())\n",
    "y2_val = v2 + 0.1 * torch.randn(x1_val.size())\n",
    "y_val = torch.cat((y1_val, y2_val), 1)\n",
    "\n",
    "\n",
    "# Get the true function values on a grid\n",
    "xv, yv = torch.meshgrid([torch.arange(0.0, 20.0) * 4.0 / 20.0, torch.arange(0.0, 20.0) * 4.0 / 20.0])\n",
    "(v1, v2) = vector_field(xv, yv)\n",
    "\n",
    "# generate training data\n",
    "x_train = 4.0 * torch.rand(n_data, 2)\n",
    "x1_train = x_train[:, 0].unsqueeze(1)\n",
    "x2_train = x_train[:, 1].unsqueeze(1)\n",
    "\n",
    "(v1_t, v2_t) = vector_field(x1_train, x2_train)\n",
    "y1_train = v1_t + 0.1 * torch.randn(x1_train.size())\n",
    "y2_train = v2_t + 0.1 * torch.randn(x1_train.size())\n",
    "\n",
    "\n",
    "# define Dataset class\n",
    "class Dataset(data.Dataset):\n",
    "    'Characterizes a dataset for PyTorch'\n",
    "\n",
    "    def __init__(self, x1, x2, y1, y2):\n",
    "        'Initialization'\n",
    "        self.x1 = x1\n",
    "        self.x2 = x2\n",
    "        self.y1 = y1\n",
    "        self.y2 = y2\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.x1)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        x1 = self.x1[index]\n",
    "        x2 = self.x2[index]\n",
    "        y1 = self.y1[index]\n",
    "        y2 = self.y2[index]\n",
    "\n",
    "        return x1, x2, y1, y2\n",
    "\n",
    "training_set = Dataset(x1_train, x2_train, y1_train, y2_train)\n",
    "\n",
    "# data loader Parameters\n",
    "DL_params = {'batch_size': 100,\n",
    "             'shuffle': True,\n",
    "             'num_workers': 0,\n",
    "             'pin_memory': False}\n",
    "training_generator = data.DataLoader(training_set, **DL_params)\n",
    "\n",
    "\n",
    "# ---------------  Set up and train the constrained model -------------------------------\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)  \n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau\\\n",
    "                                    (optimizer, patience=10,\n",
    "                                     min_lr=1e-10,\n",
    "                                     factor=0.5, cooldown=15)\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    n_batches = 0\n",
    "    for x1_train, x2_train, y1_train, y2_train in training_generator:\n",
    "        optimizer.zero_grad()\n",
    "        x_train = torch.cat((x1_train, x2_train), 1)\n",
    "        (yhat, v1hat, v2hat) = model(x_train)\n",
    "        loss = (criterion(y1_train, v1hat) + criterion(y2_train, v2hat)) / 2  # divide by 2 as it is a mean\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss\n",
    "        n_batches += 1\n",
    "    return total_loss / n_batches\n",
    "\n",
    "def eval(epoch):\n",
    "    model.eval()\n",
    "    # with torch.no_grad():\n",
    "    (yhat, v1hat, v2hat) = model(x_val)\n",
    "    loss = (criterion(y1_val, v1hat) + criterion(y2_val, v2hat)) / 2\n",
    "    return loss.cpu()\n",
    "\n",
    "\n",
    "train_loss = np.empty([epochs, 1])\n",
    "val_loss = np.empty([epochs, 1])\n",
    "\n",
    "print('Training Constrained NN')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss[epoch] = train(epoch).detach().numpy()\n",
    "    v_loss = eval(epoch)\n",
    "    scheduler.step(v_loss)\n",
    "    val_loss[epoch] = v_loss.detach().numpy()\n",
    "    print('Constrained NN: epoch: ', epoch, 'training loss ', train_loss[epoch], \\\n",
    "          'validation loss', val_loss[epoch])\n",
    "\n",
    "\n",
    "# work out the rms error for this one\n",
    "x_pred = torch.cat((xv.reshape(20 * 20, 1), yv.reshape(20 * 20, 1)), 1)\n",
    "(f_pred, v1_pred, v2_pred) = model(x_pred)\n",
    "error_new = torch.cat((v1.reshape(400, 1) - v1_pred.detach(), v2.reshape(400, 1) - v2_pred.detach()), 0)\n",
    "rms_error = torch.sqrt(sum(error_new * error_new) / 800)\n",
    "\n",
    "# ---------------  Set up and train the uncconstrained model -------------------------------\n",
    "optimizer_uc = torch.optim.Adam(model_uc.parameters(), lr=0.01)\n",
    "scheduler_uc = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_uc, patience=10,\n",
    "                                                     min_lr=1e-10,\n",
    "                                                    factor=0.5,\n",
    "                                                    cooldown=15)\n",
    "\n",
    "def train_uc(epoch):\n",
    "    model_uc.train()\n",
    "    total_loss = 0\n",
    "    n_batches = 0\n",
    "    for x1_train, x2_train, y1_train, y2_train in training_generator:\n",
    "        optimizer_uc.zero_grad()\n",
    "        x_train = torch.cat((x1_train, x2_train), 1)\n",
    "        vhat = model_uc(x_train)\n",
    "        y_train = torch.cat((y1_train, y2_train), 1)\n",
    "        loss = criterion(y_train, vhat)\n",
    "        loss.backward()\n",
    "        optimizer_uc.step()\n",
    "        total_loss += loss.cpu()\n",
    "        n_batches += 1\n",
    "    return total_loss / n_batches\n",
    "\n",
    "def eval_uc(epoch):\n",
    "    model_uc.eval()\n",
    "    with torch.no_grad():\n",
    "        (vhat) = model_uc(x_val)\n",
    "        loss = criterion(y_val, vhat)\n",
    "    return loss.cpu()\n",
    "\n",
    "\n",
    "train_loss_uc = np.empty([epochs, 1])\n",
    "val_loss_uc = np.empty([epochs, 1])\n",
    "\n",
    "\n",
    "print('Training standard NN')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss_uc[epoch] = train_uc(epoch).detach().numpy()\n",
    "    v_loss = eval_uc(epoch)\n",
    "    scheduler_uc.step(v_loss)\n",
    "    val_loss_uc[epoch] = v_loss.detach().numpy()\n",
    "    print('Standard NN: epoch: ', epoch, 'training loss ', \\\n",
    "          train_loss_uc[epoch], 'validation loss', val_loss_uc[epoch])\n",
    "\n",
    "\n",
    "# work out final rms error for unconstrainted net\n",
    "(v_pred_uc) = model_uc(x_pred)\n",
    "v1_pred_uc = v_pred_uc[:, 0]\n",
    "v2_pred_uc = v_pred_uc[:, 1]\n",
    "\n",
    "error_uc = torch.cat((v1.reshape(400) - v1_pred_uc.detach(), v2.reshape(400) - v2_pred_uc.detach()), 0)\n",
    "rms_uc = torch.sqrt(sum(error_uc * error_uc) / 800)\n",
    "\n",
    "\n",
    "\n",
    "print('Finished')\n",
    "print(\"Final RMSE for constrained neural network: \",rms_error.item())\n",
    "print(\"Final RMSE for standard neural network: \",rms_uc.item())\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Initialize plot\n",
    "    f, ax = plt.subplots(2, 2, figsize=(20, 16))\n",
    "    # ax.pcolor(xv,yv,f_scalar)\n",
    "    ax[0, 0].quiver(xv, yv, v1, v2)\n",
    "    ax[0, 0].quiver(xv, yv, v1_pred.reshape(20, 20).detach(), v2_pred.reshape(20, 20).detach(), color='r')\n",
    "    ax[0, 0].legend(['true', 'predicted'], fontsize=22)\n",
    "    ax[0, 0].set_title('Constrained NN ', fontsize=22)\n",
    "    ax[0, 0].tick_params(axis='x', labelsize=14 )\n",
    "    ax[0, 0].tick_params(axis='y', labelsize=14 )\n",
    "\n",
    "\n",
    "    ax[1, 0].plot(np.log(train_loss),lw=3.0)\n",
    "    ax[1, 0].plot(np.log(val_loss), lw=3.0)\n",
    "    # ax[1].plot(loss_save[1:epoch].log().detach().numpy())\n",
    "    ax[1, 0].set_xlabel('training epoch',  fontsize=22)\n",
    "    ax[1, 0].set_ylabel('Loss',  fontsize=22)\n",
    "    ax[1, 0].legend(['training loss', 'val loss'], fontsize=22)\n",
    "    ax[1, 0].tick_params(axis='x', labelsize=14 )\n",
    "    ax[1, 0].tick_params(axis='y', labelsize=14 )\n",
    "\n",
    "    ax[0, 1].quiver(xv, yv, v1, v2)\n",
    "    ax[0, 1].quiver(xv, yv, v1_pred_uc.reshape(20, 20).detach(), v2_pred_uc.reshape(20, 20).detach(), color='r')\n",
    "    ax[0, 1].legend(['true', 'predicted'], fontsize=22)\n",
    "    ax[0, 1].set_title('Standard NN ',fontsize=22)\n",
    "    ax[0, 1].tick_params(axis='x', labelsize=14 )\n",
    "    ax[0,1].tick_params(axis='y', labelsize=14 )\n",
    "\n",
    "    ax[1, 1].plot(np.log(train_loss_uc),lw=3.0)\n",
    "    ax[1, 1].plot(np.log(val_loss_uc),lw=3.0)\n",
    "    ax[1, 1].set_ylabel('Loss', fontsize=22)\n",
    "    ax[1, 1].set_xlabel('training epoch', fontsize=22)\n",
    "    ax[1, 1].legend(['training loss','val loss'], fontsize=22)\n",
    "    ax[1, 1].tick_params(axis='x', labelsize=14)\n",
    "    ax[1,1].tick_params(axis='y', labelsize=14 )\n",
    "    plt.savefig(\"Figure_1.png\", dpi=300)\n",
    "    plt.show()\n",
    "   \n",
    "    \n",
    "\n",
    "    # Initialize second plot\n",
    "    f2, ax2 = plt.subplots(1, 3, figsize=(48, 10))\n",
    "    Q = ax2[0].quiver(xv, yv, v1, v2, scale=None, scale_units='inches')\n",
    "    Q._init()\n",
    "    assert isinstance(Q.scale, float)\n",
    "    ax2[0].quiver(x1_train, x2_train, y1_train, y2_train, scale=Q.scale, scale_units='inches', color='r')\n",
    "    ax2[0].set_xlabel('$x_1$')\n",
    "    ax2[0].set_ylabel('$x_2$')\n",
    "    ax2[0].tick_params(axis='x', labelsize=36)\n",
    "    ax2[0].tick_params(axis='y', labelsize=36)\n",
    "    \n",
    "\n",
    "\n",
    "    error_new = torch.cat((v1.reshape(400, 1) - v1_pred.detach(), v2.reshape(400, 1) - v2_pred.detach()), 0)\n",
    "    rms_new = torch.sqrt(sum(error_new * error_new) / 800)\n",
    "\n",
    "    ax2[1].quiver(xv, yv, v1 - v1_pred.reshape(20, 20).detach(), v2 - v2_pred.reshape(20, 20).detach(),\n",
    "                  scale=Q.scale, scale_units='inches')\n",
    "    ax2[1].set_xlabel('$x_1$')\n",
    "    ax2[1].set_ylabel('$x_2$')\n",
    "    ax2[1].set_title('Constrained Approach RMSE={0:.2f}'.format(rms_new.item()), fontsize=36)\n",
    "    ax2[1].tick_params(axis='x', labelsize=36 )\n",
    "    ax2[1].tick_params(axis='y', labelsize=36 )\n",
    "    \n",
    "\n",
    "\n",
    "    error_uc = torch.cat((v1.reshape(400) - v1_pred_uc.detach(), v2.reshape(400) - v2_pred_uc.detach()), 0)\n",
    "    rms_uc = torch.sqrt(sum(error_uc * error_uc) / 800)\n",
    "\n",
    "    ax2[2].quiver(xv, yv, v1 - v1_pred_uc.reshape(20, 20).detach(), v2 - v2_pred_uc.reshape(20, 20).detach(),\n",
    "                  scale=Q.scale, scale_units='inches')\n",
    "    ax2[2].set_xlabel('$x_1$')\n",
    "    ax2[2].set_ylabel('$x_2$')\n",
    "    ax2[2].set_title('Unconstrained NN RMSE={0:.2f}'.format(rms_uc.item()), fontsize=36)\n",
    "    ax2[2].tick_params(axis='x', labelsize=36 )\n",
    "    ax2[2].tick_params(axis='y', labelsize=36 )\n",
    "    plt.savefig(\"Figure_2.png\", dpi=300)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97dbec4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
