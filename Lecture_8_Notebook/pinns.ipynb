{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a069bff",
   "metadata": {},
   "source": [
    "## Submodule-2.1 : Dynamical Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0792af43",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "#### 1. [PINN for Burger's Equation in TensorFlow](#PINN-for-Burger's-Equation-in-TensorFlow)\n",
    "#### 2. [PINN for Burger's Equation in JAX](#PINN-for-Burger's-Equation-in-JAX)\n",
    "#### 3. [PINN for a Boundary Layer Problem](#PINN-for-a-Boundary-Layer-Problem)\n",
    "#### 4. [Neural Network with Hard Constraints](#Neural-Network-with-Hard-Constraints)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd3f65f",
   "metadata": {},
   "source": [
    "## PINN-for-Burger's-Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "860b77dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/raj/opt/anaconda3/lib/python3.8/_collections_abc.py:832: MatplotlibDeprecationWarning: Support for setting the 'text.latex.preamble' or 'pgf.preamble' rcParam to a list of strings is deprecated since 3.3 and will be removed two minor releases later; set it to a single string instead.\n",
      "  self[key] = other[key]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, 'Utilities/')\n",
    "import os\n",
    "\n",
    "from scipy.interpolate import griddata\n",
    "from pyDOE import lhs\n",
    "from plotting import newfig, savefig\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import scipy.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c77d7858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 0 and loss is: 0.2566310167312622\n",
      "Iteration is: 1 and loss is: 0.23526228964328766\n",
      "Iteration is: 2 and loss is: 0.22125136852264404\n",
      "Iteration is: 3 and loss is: 0.2152591347694397\n",
      "Iteration is: 4 and loss is: 0.21643051505088806\n",
      "Iteration is: 5 and loss is: 0.22051557898521423\n",
      "Iteration is: 6 and loss is: 0.22254502773284912\n",
      "Iteration is: 7 and loss is: 0.22123509645462036\n",
      "Iteration is: 8 and loss is: 0.2179287075996399\n",
      "Iteration is: 9 and loss is: 0.21430082619190216\n",
      "Iteration is: 10 and loss is: 0.21143503487110138\n",
      "Iteration is: 11 and loss is: 0.2097298800945282\n",
      "Iteration is: 12 and loss is: 0.20907731354236603\n",
      "Iteration is: 13 and loss is: 0.20907141268253326\n",
      "Iteration is: 14 and loss is: 0.20919497311115265\n",
      "Iteration is: 15 and loss is: 0.2089935839176178\n",
      "Iteration is: 16 and loss is: 0.20821210741996765\n",
      "Iteration is: 17 and loss is: 0.2068447321653366\n",
      "Iteration is: 18 and loss is: 0.20509067177772522\n",
      "Iteration is: 19 and loss is: 0.2032514214515686\n",
      "Iteration is: 20 and loss is: 0.20160870254039764\n",
      "Iteration is: 21 and loss is: 0.20031368732452393\n",
      "Iteration is: 22 and loss is: 0.1993217170238495\n",
      "Iteration is: 23 and loss is: 0.19841761887073517\n",
      "Iteration is: 24 and loss is: 0.1973397135734558\n",
      "Iteration is: 25 and loss is: 0.19593293964862823\n",
      "Iteration is: 26 and loss is: 0.19422461092472076\n",
      "Iteration is: 27 and loss is: 0.19237802922725677\n",
      "Iteration is: 28 and loss is: 0.19056735932826996\n",
      "Iteration is: 29 and loss is: 0.1888543963432312\n",
      "Iteration is: 30 and loss is: 0.18714410066604614\n",
      "Iteration is: 31 and loss is: 0.1852571964263916\n",
      "Iteration is: 32 and loss is: 0.18307088315486908\n",
      "Iteration is: 33 and loss is: 0.18061403930187225\n",
      "Iteration is: 34 and loss is: 0.17804372310638428\n",
      "Iteration is: 35 and loss is: 0.17552711069583893\n",
      "Iteration is: 36 and loss is: 0.17311783134937286\n",
      "Iteration is: 37 and loss is: 0.17072910070419312\n",
      "Iteration is: 38 and loss is: 0.16824722290039062\n",
      "Iteration is: 39 and loss is: 0.16568174958229065\n",
      "Iteration is: 40 and loss is: 0.16318590939044952\n",
      "Iteration is: 41 and loss is: 0.16091670095920563\n",
      "Iteration is: 42 and loss is: 0.1588827222585678\n",
      "Iteration is: 43 and loss is: 0.1569649577140808\n",
      "Iteration is: 44 and loss is: 0.15510302782058716\n",
      "Iteration is: 45 and loss is: 0.15339994430541992\n",
      "Iteration is: 46 and loss is: 0.15198446810245514\n",
      "Iteration is: 47 and loss is: 0.1508234590291977\n",
      "Iteration is: 48 and loss is: 0.14979110658168793\n",
      "Iteration is: 49 and loss is: 0.14889764785766602\n",
      "Iteration is: 50 and loss is: 0.1482483148574829\n",
      "Iteration is: 51 and loss is: 0.14777912199497223\n",
      "Iteration is: 52 and loss is: 0.14729447662830353\n",
      "Iteration is: 53 and loss is: 0.1467658281326294\n",
      "Iteration is: 54 and loss is: 0.14626574516296387\n",
      "Iteration is: 55 and loss is: 0.14570298790931702\n",
      "Iteration is: 56 and loss is: 0.14497317373752594\n",
      "Iteration is: 57 and loss is: 0.14414356648921967\n",
      "Iteration is: 58 and loss is: 0.14321483671665192\n",
      "Iteration is: 59 and loss is: 0.1420833319425583\n",
      "Iteration is: 60 and loss is: 0.14080394804477692\n",
      "Iteration is: 61 and loss is: 0.1394791603088379\n",
      "Iteration is: 62 and loss is: 0.1380748599767685\n",
      "Iteration is: 63 and loss is: 0.13660776615142822\n",
      "Iteration is: 64 and loss is: 0.13515442609786987\n",
      "Iteration is: 65 and loss is: 0.13368332386016846\n",
      "Iteration is: 66 and loss is: 0.1321907341480255\n",
      "Iteration is: 67 and loss is: 0.1307348757982254\n",
      "Iteration is: 68 and loss is: 0.1292707622051239\n",
      "Iteration is: 69 and loss is: 0.1277618408203125\n",
      "Iteration is: 70 and loss is: 0.12623974680900574\n",
      "Iteration is: 71 and loss is: 0.1246735155582428\n",
      "Iteration is: 72 and loss is: 0.12307177484035492\n",
      "Iteration is: 73 and loss is: 0.12147055566310883\n",
      "Iteration is: 74 and loss is: 0.11983572691679001\n",
      "Iteration is: 75 and loss is: 0.11820924282073975\n",
      "Iteration is: 76 and loss is: 0.11662177741527557\n",
      "Iteration is: 77 and loss is: 0.11508794873952866\n",
      "Iteration is: 78 and loss is: 0.1136520504951477\n",
      "Iteration is: 79 and loss is: 0.11229774355888367\n",
      "Iteration is: 80 and loss is: 0.11108487844467163\n",
      "Iteration is: 81 and loss is: 0.11000023782253265\n",
      "Iteration is: 82 and loss is: 0.10906646400690079\n",
      "Iteration is: 83 and loss is: 0.10825975239276886\n",
      "Iteration is: 84 and loss is: 0.10760527849197388\n",
      "Iteration is: 85 and loss is: 0.10705451667308807\n",
      "Iteration is: 86 and loss is: 0.10659933090209961\n",
      "Iteration is: 87 and loss is: 0.10619945824146271\n",
      "Iteration is: 88 and loss is: 0.10579840838909149\n",
      "Iteration is: 89 and loss is: 0.10538260638713837\n",
      "Iteration is: 90 and loss is: 0.10491597652435303\n",
      "Iteration is: 91 and loss is: 0.1043953150510788\n",
      "Iteration is: 92 and loss is: 0.10384710878133774\n",
      "Iteration is: 93 and loss is: 0.10327647626399994\n",
      "Iteration is: 94 and loss is: 0.10270283371210098\n",
      "Iteration is: 95 and loss is: 0.10214076936244965\n",
      "Iteration is: 96 and loss is: 0.10159572958946228\n",
      "Iteration is: 97 and loss is: 0.10106892138719559\n",
      "Iteration is: 98 and loss is: 0.10055893659591675\n",
      "Iteration is: 99 and loss is: 0.10007699579000473\n",
      "Iteration is: 100 and loss is: 0.09962654858827591\n",
      "Training time: 5.0198\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(seed=1234)\n",
    "tf.random.set_seed(1234)\n",
    "tf.config.experimental.enable_tensor_float_32_execution(False)\n",
    "#os.environ[‘TF_ENABLE_AUTO_MIXED_PRECISION’] = ‘1’\n",
    "\n",
    "# Initalization of Network\n",
    "def hyper_initial(size):\n",
    "    in_dim = size[0]\n",
    "    out_dim = size[1]\n",
    "    std = np.sqrt(2.0/(in_dim + out_dim))\n",
    "    return tf.Variable(tf.random.truncated_normal(shape=size, stddev = std))\n",
    "\n",
    "# Neural Network \n",
    "def DNN(X, W, b):\n",
    "    A = X\n",
    "    L = len(W)\n",
    "    for i in range(L-1):\n",
    "        A = tf.tanh(tf.add(tf.matmul(A, W[i]), b[i]))\n",
    "    Y = tf.add(tf.matmul(A, W[-1]), b[-1])\n",
    "    return Y\n",
    "\n",
    "def train_vars(W, b):\n",
    "    return W + b\n",
    "\n",
    "def net_u(x, t, w, b):\n",
    "    u = DNN(tf.concat([x,t],1), w, b)\n",
    "    return u\n",
    "\n",
    "\n",
    "#@tf.function(jit_compile=True)\n",
    "@tf.function\n",
    "def net_f(x,t,W, b, nu):\n",
    "    with tf.GradientTape(persistent=True) as tape1:\n",
    "        tape1.watch([x, t])\n",
    "        with tf.GradientTape(persistent=True) as tape2:\n",
    "            tape2.watch([x, t])\n",
    "            u=net_u(x,t, W, b)\n",
    "        u_t = tape2.gradient(u, t)\n",
    "        u_x = tape2.gradient(u, x)\n",
    "    u_xx = tape1.gradient(u_x, x)  \n",
    "    del tape1\n",
    "    f = u_t + u*u_x - nu*u_xx\n",
    "    return f\n",
    "\n",
    "\n",
    "\n",
    "#@tf.function(jit_compile=True)\n",
    "@tf.function\n",
    "def train_step(W, b, X_u_train_tf, u_train_tf, X_f_train_tf, opt, nu):\n",
    "    x_u = X_u_train_tf[:,0:1]\n",
    "    t_u = X_u_train_tf[:,1:2]\n",
    "    x_f = X_f_train_tf[:,0:1]\n",
    "    t_f = X_f_train_tf[:,1:2]\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch([W,b])\n",
    "        u_nn = net_u(x_u, t_u, W, b) \n",
    "        f_nn = net_f(x_f,t_f, W, b, nu)\n",
    "        loss =  tf.reduce_mean(tf.square(u_nn - u_train_tf)) + tf.reduce_mean(tf.square(f_nn)) \n",
    "    grads = tape.gradient(loss, train_vars(W,b))\n",
    "    opt.apply_gradients(zip(grads, train_vars(W,b)))\n",
    "    return loss\n",
    "\n",
    "\n",
    "    \n",
    "nu = 0.01/np.pi\n",
    "noise = 0.0        \n",
    "N_u = 100\n",
    "N_f = 10000\n",
    "Nmax=20000\n",
    "\n",
    "layers = [2, 20, 20, 20, 20, 20, 20, 20, 20, 1]\n",
    "L = len(layers)\n",
    "W = [hyper_initial([layers[l-1], layers[l]]) for l in range(1, L)] \n",
    "b = [tf.Variable(tf.zeros([1, layers[l]])) for l in range(1, L)] \n",
    "\n",
    "data = scipy.io.loadmat('./Data/burgers_shock.mat')\n",
    "\n",
    "t = data['t'].flatten()[:,None]\n",
    "x = data['x'].flatten()[:,None]\n",
    "Exact = np.real(data['usol']).T\n",
    "X, T = np.meshgrid(x,t)\n",
    "X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))\n",
    "u_star = Exact.flatten()[:,None]              \n",
    "# Doman bounds\n",
    "lb = X_star.min(0)\n",
    "ub = X_star.max(0)    \n",
    "xx1 = np.hstack((X[0:1,:].T, T[0:1,:].T))\n",
    "uu1 = Exact[0:1,:].T\n",
    "xx2 = np.hstack((X[:,0:1], T[:,0:1]))\n",
    "uu2 = Exact[:,0:1]\n",
    "xx3 = np.hstack((X[:,-1:], T[:,-1:]))\n",
    "uu3 = Exact[:,-1:]\n",
    "\n",
    "X_u_train = np.vstack([xx1, xx2, xx3])\n",
    "X_f_train = lb + (ub-lb)*lhs(2, N_f)\n",
    "X_f_train = np.vstack((X_f_train, X_u_train))\n",
    "u_train = np.vstack([uu1, uu2, uu3])\n",
    "\n",
    "idx = np.random.choice(X_u_train.shape[0], N_u, replace=False)\n",
    "\n",
    "X_u_train = X_u_train[idx, :]\n",
    "u_train = u_train[idx,:]\n",
    "\n",
    "X_u_train_tf = tf.convert_to_tensor(X_u_train, dtype=tf.float32)\n",
    "u_train_tf =   tf.convert_to_tensor(u_train, dtype=tf.float32)\n",
    "X_f_train_tf = tf.convert_to_tensor(X_f_train, dtype=tf.float32)\n",
    "\n",
    "lr = 1e-3\n",
    "optimizer = tf.optimizers.Adam(learning_rate=lr)\n",
    "\n",
    "start_time = time.time()\n",
    "n=0\n",
    "loss = []\n",
    "while n <= Nmax:\n",
    "    loss_= train_step(W, b, X_u_train_tf, u_train_tf, X_f_train_tf, optimizer, nu)\n",
    "    loss.append(loss_)    \n",
    "    print(f\"Iteration is: {n} and loss is: {loss_}\")\n",
    "    n+=1\n",
    "\n",
    "elapsed = time.time() - start_time                \n",
    "print('Training time: %.4f' % (elapsed))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fe12351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error u: 6.463021e-01\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, '$t = 0.75$')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "findfont: Font family ['serif'] not found. Falling back to DejaVu Sans.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 388.543x264.146 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def predict(X_star_tf, w, b):\n",
    "    x_star = X_star_tf[:,0:1]\n",
    "    t_star = X_star_tf[:,1:2]\n",
    "    u_pred = net_u(x_star, t_star, w, b)\n",
    "    return u_pred\n",
    "    \n",
    "X_star_tf = tf.convert_to_tensor(X_star, dtype=tf.float32)    \n",
    "u_pred = predict(X_star_tf, W, b)\n",
    "error_u = np.linalg.norm(u_star-u_pred,2)/np.linalg.norm(u_star,2)\n",
    "print('Error u: %e' % (error_u))                     \n",
    "U_pred = griddata(X_star, u_pred.numpy().flatten(), (X, T), method='cubic')\n",
    "Error = np.abs(Exact - U_pred)\n",
    "\n",
    "\n",
    "fig, ax = newfig(1.0, 1.1)\n",
    "ax.axis('off')\n",
    "\n",
    "####### Row 0: u(t,x) ##################    \n",
    "gs0 = gridspec.GridSpec(1, 2)\n",
    "gs0.update(top=1-0.06, bottom=1-1/3, left=0.15, right=0.85, wspace=0)\n",
    "ax = plt.subplot(gs0[:, :])\n",
    "\n",
    "h = ax.imshow(U_pred.T, interpolation='nearest', cmap='rainbow', \n",
    "              extent=[t.min(), t.max(), x.min(), x.max()], \n",
    "              origin='lower', aspect='auto')\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "fig.colorbar(h, cax=cax)\n",
    "\n",
    "ax.plot(X_u_train[:,1], X_u_train[:,0], 'kx', label = 'Data (%d points)' % (u_train.shape[0]), markersize = 4, clip_on = False)\n",
    "#ax.plot(X_f_train[0:100,1], X_f_train[0:100,0], 'bx', label = 'Data (%d points)' % (u_train.shape[0]), markersize = 4, clip_on = False)\n",
    "\n",
    "\n",
    "line = np.linspace(x.min(), x.max(), 2)[:,None]\n",
    "ax.plot(t[25]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
    "ax.plot(t[50]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
    "ax.plot(t[75]*np.ones((2,1)), line, 'w-', linewidth = 1)    \n",
    "\n",
    "ax.set_xlabel('$t$')\n",
    "ax.set_ylabel('$x$')\n",
    "ax.legend(frameon=False, loc = 'best')\n",
    "ax.set_title('$u(t,x)$', fontsize = 10)\n",
    "\n",
    "####### Row 1: u(t,x) slices ##################    \n",
    "gs1 = gridspec.GridSpec(1, 3)\n",
    "gs1.update(top=1-1/3, bottom=0, left=0.1, right=0.9, wspace=0.5)\n",
    "\n",
    "ax = plt.subplot(gs1[0, 0])\n",
    "ax.plot(x,Exact[25,:], 'b-', linewidth = 2, label = 'Exact')       \n",
    "ax.plot(x,U_pred[25,:], 'r--', linewidth = 2, label = 'Prediction')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$u(t,x)$')    \n",
    "ax.set_title('$t = 0.25$', fontsize = 10)\n",
    "ax.axis('square')\n",
    "ax.set_xlim([-1.1,1.1])\n",
    "ax.set_ylim([-1.1,1.1])\n",
    "\n",
    "ax = plt.subplot(gs1[0, 1])\n",
    "ax.plot(x,Exact[50,:], 'b-', linewidth = 2, label = 'Exact')       \n",
    "ax.plot(x,U_pred[50,:], 'r--', linewidth = 2, label = 'Prediction')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$u(t,x)$')\n",
    "ax.axis('square')\n",
    "ax.set_xlim([-1.1,1.1])\n",
    "ax.set_ylim([-1.1,1.1])\n",
    "ax.set_title('$t = 0.50$', fontsize = 10)\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.35), ncol=5, frameon=False)\n",
    "\n",
    "ax = plt.subplot(gs1[0, 2])\n",
    "ax.plot(x,Exact[75,:], 'b-', linewidth = 2, label = 'Exact')       \n",
    "ax.plot(x,U_pred[75,:], 'r--', linewidth = 2, label = 'Prediction')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$u(t,x)$')\n",
    "ax.axis('square')\n",
    "ax.set_xlim([-1.1,1.1])\n",
    "ax.set_ylim([-1.1,1.1])    \n",
    "ax.set_title('$t = 0.75$', fontsize = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845d395c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffb8cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "it = np.arange(0,40001, 1)\n",
    "loss_list = [loss[i].numpy() for i in range(0,40001)]\n",
    "plt.semilogy(it, np.asarray(loss_list), 'b-', linewidth = 2, label = 'Exact')       \n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('$\\mathcal{L}$')    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ab4ee8",
   "metadata": {},
   "source": [
    "## PINN-for-Burger's-Equation-in-JAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b89ab85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap\n",
    "from jax import random\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "from pyDOE import lhs\n",
    "from jax.nn import tanh, relu\n",
    "import sys\n",
    "from jax.example_libraries import optimizers\n",
    "from tqdm import trange\n",
    "sys.path.insert(0, 'Utilities/')\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import griddata\n",
    "from plotting import newfig, savefig\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb8c083",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_layer_params(m, n, key, scale):\n",
    "    w_key, b_key = random.split(key)\n",
    "    return scale*random.normal(w_key, (m, n)), jnp.zeros(n)\n",
    "\n",
    "def init_network_params(sizes, key):\n",
    "    keys = random.split(key, len(sizes))\n",
    "    return [random_layer_params(m, n, k, 2.0/(jnp.sqrt(m+n))) \\\n",
    "            for m, n, k in zip(sizes[:-1], sizes[1:], keys)]\n",
    "\n",
    "\n",
    "@jit\n",
    "def predict(params, X, lb, ub):\n",
    "    H =  2.0*(X - lb)/(ub - lb) - 1.0\n",
    "    for w, b in params[:-1]:\n",
    "        H = tanh(jnp.dot(H, w) + b)\n",
    "        final_w, final_b = params[-1]\n",
    "    H = jnp.dot(H, final_w) + final_b\n",
    "    return H\n",
    "\n",
    "\n",
    "@jit\n",
    "def net_u(params, x, t, lb, ub):\n",
    "    x_con =jnp.array([x, t])\n",
    "    y_pred = predict(params, x_con, lb, ub)\n",
    "    return y_pred\n",
    "\n",
    "@jit\n",
    "def net_u_grad(params, x, t, lb, ub):\n",
    "    x_con =jnp.array([x, t])\n",
    "    y_pred = predict(params, x_con, lb, ub)\n",
    "    return y_pred[0]\n",
    "\n",
    "def net_f(params, lb, ub):\n",
    "    def u_t(x, t):\n",
    "        ut = grad(net_u_grad, argnums=2)(params, x, t, lb, ub) \n",
    "        return ut\n",
    "\n",
    "    def u_x(x, t):\n",
    "        ux = grad(net_u_grad, argnums=1)(params, x, t, lb, ub) \n",
    "        return ux   \n",
    "    return jit(u_t), jit(u_x)\n",
    "\n",
    "\n",
    "def net_fxx(params, lb, ub):\n",
    "    def u_xx(x, t):\n",
    "        _, u_x = net_f(params, lb, ub) \n",
    "        ux = grad(u_x, argnums=0)(x, t) \n",
    "        return ux   \n",
    "    return jit(u_xx)\n",
    "\n",
    "\n",
    "@jit\n",
    "def loss_data(params,x,t, lb, ub, u_train):\n",
    "    u_pred = vmap(net_u, (None, 0, 0, None, None))(params, x, t, lb, ub)\n",
    "    loss = jnp.mean((u_pred - u_train)**2 )\n",
    "    return loss\n",
    "\n",
    "@jit\n",
    "def loss_f(params, x, t, lb, ub, nu):\n",
    "    u = vmap(net_u, (None, 0, 0, None, None))(params, x, t, lb, ub)\n",
    "    u_tf, u_xf = net_f(params, lb, ub)\n",
    "    u_xxf = net_fxx(params, lb, ub)\n",
    "    u_t = vmap(u_tf, (0, 0))(x, t)\n",
    "    u_x = vmap(u_xf, (0, 0))(x, t)\n",
    "    u_xx = vmap(u_xxf, (0, 0))(x, t)\n",
    "    res = u_t + u.flatten() * u_x - nu * u_xx \n",
    "    loss_f = jnp.mean((res.flatten())**2)\n",
    "    return loss_f\n",
    "\n",
    "@jit\n",
    "def predict_u(params, x_star, t_star, lb, ub):\n",
    "    u_pred = vmap(net_u, (None, 0, 0, None, None))\\\n",
    "    (params, x_star, t_star, lb, ub)\n",
    "    return u_pred\n",
    "\n",
    "def loss_fn(params, x_f, t_f,x_d, t_d, lb, ub, nu, y_d):\n",
    "    loss_res = loss_f(params, x_f, t_f, lb, ub, nu)\n",
    "    data_loss = loss_data(params, x_d, t_d, lb, ub, y_d) \n",
    "    return loss_res + data_loss\n",
    "\n",
    "@jit\n",
    "def step(istep, opt_state, t_d, x_d, y_d, t_f, x_f, lb, ub):\n",
    "    param = get_params(opt_state) \n",
    "    g = grad(loss_fn, argnums=0)(param, x_f, t_f,x_d, t_d, lb, ub, nu, y_d)\n",
    "    return opt_update(istep, g, opt_state)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    nu = 0.01/np.pi\n",
    "    N_u = 100\n",
    "    N_f = 10000\n",
    "    layers = [2, 20, 20, 20, 20, 20, 20, 20,20, 20, 1]\n",
    "    data = scipy.io.loadmat('Data/burgers_shock.mat')\n",
    "    t = data['t'].flatten()[:,None]\n",
    "    x = data['x'].flatten()[:,None]\n",
    "    Exact = np.real(data['usol']).T    \n",
    "    X, T = np.meshgrid(x,t)\n",
    "    X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))\n",
    "    u_star = Exact.flatten()[:,None]              \n",
    "    lb = X_star.min(0)\n",
    "    ub = X_star.max(0)\n",
    "    xx1 = np.hstack((X[0:1,:].T, T[0:1,:].T))\n",
    "    uu1 = Exact[0:1,:].T\n",
    "    xx2 = np.hstack((X[:,0:1], T[:,0:1]))\n",
    "    uu2 = Exact[:,0:1]\n",
    "    xx3 = np.hstack((X[:,-1:], T[:,-1:]))\n",
    "    uu3 = Exact[:,-1:]\n",
    "    X_u_train = np.vstack([xx1, xx2, xx3])\n",
    "    X_f_train = lb + (ub-lb)*lhs(2, N_f)\n",
    "    X_f_train = np.vstack((X_f_train, X_u_train))\n",
    "    u_train = np.vstack([uu1, uu2, uu3])\n",
    "    idx = np.random.choice(X_u_train.shape[0], N_u, replace=False)\n",
    "    X_u_train = X_u_train[idx, :]\n",
    "    u_train = u_train[idx,:]\n",
    "    x_d = X_u_train[:, 0]\n",
    "    t_d = X_u_train[:, 1]\n",
    "    x_f = X_f_train[:, 0]\n",
    "    t_f = X_f_train[:, 1]\n",
    "    x_star = X_star[:, 0]\n",
    "    t_star = X_star[:, 1]\n",
    "    params = init_network_params(layers, random.PRNGKey(1234))\n",
    "    opt_init, opt_update, get_params = optimizers.adam(5e-4)\n",
    "    opt_state = opt_init(params)\n",
    "    nIter = 20000 + 1\n",
    "    ld_list = []\n",
    "    lf_list = []\n",
    "    pbar = trange(nIter)\n",
    "    \n",
    "    for it in pbar:\n",
    "        opt_state = step(it, opt_state, t_d, x_d, u_train, t_f, x_f, lb, ub)\n",
    "        if it % 1 == 0:\n",
    "            params = get_params(opt_state)\n",
    "            l_d = loss_data(params, x_d, t_d, lb, ub, u_train)\n",
    "            l_f = loss_f(params, x_f, t_f, lb, ub, nu)\n",
    "            pbar.set_postfix({'Loss': l_d, 'loss_physics': l_f})\n",
    "            ld_list.append(l_d)\n",
    "            lf_list.append(l_f)\n",
    "\n",
    "\n",
    "    u_pred = predict_u(params, x_star, t_star, lb, ub)\n",
    "            \n",
    "    error_u = jnp.linalg.norm(u_star-u_pred,2)/np.linalg.norm(u_star,2)\n",
    "    print('Error u: %e' % (error_u))\n",
    "    np.save(\"ld_list.npy\", np.array(ld_list), allow_pickle=True) \n",
    "    np.save(\"lf_list.npy\", np.array(lf_list), allow_pickle=True)  \n",
    " \n",
    "    \n",
    "    U_pred = griddata(X_star, u_pred.flatten(), (X, T), method='cubic')\n",
    "    Error = np.abs(Exact - U_pred)\n",
    "    \n",
    "    fig, ax = newfig(1.0, 1.1)\n",
    "    ax.axis('off')\n",
    "    \n",
    "    ####### Row 0: u(t,x) ##################    \n",
    "    gs0 = gridspec.GridSpec(1, 2)\n",
    "    gs0.update(top=1-0.06, bottom=1-1/3, left=0.15, right=0.85, wspace=0)\n",
    "    ax = plt.subplot(gs0[:, :])\n",
    "    \n",
    "    h = ax.imshow(U_pred.T, interpolation='nearest', cmap='rainbow', \n",
    "                  extent=[t.min(), t.max(), x.min(), x.max()], \n",
    "                  origin='lower', aspect='auto')\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    fig.colorbar(h, cax=cax)\n",
    "    \n",
    "    ax.plot(X_u_train[:,1], X_u_train[:,0], 'kx', label = 'Data (%d points)' % (u_train.shape[0]), markersize = 4, clip_on = False)\n",
    "    \n",
    "    line = np.linspace(x.min(), x.max(), 2)[:,None]\n",
    "    ax.plot(t[25]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
    "    ax.plot(t[50]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
    "    ax.plot(t[75]*np.ones((2,1)), line, 'w-', linewidth = 1)    \n",
    "    \n",
    "    ax.set_xlabel('$t$')\n",
    "    ax.set_ylabel('$x$')\n",
    "    ax.legend(frameon=False, loc = 'best')\n",
    "    ax.set_title('$u(t,x)$', fontsize = 10)\n",
    "    \n",
    "    ####### Row 1: u(t,x) slices ##################    \n",
    "    gs1 = gridspec.GridSpec(1, 3)\n",
    "    gs1.update(top=1-1/3, bottom=0, left=0.1, right=0.9, wspace=0.5)\n",
    "    \n",
    "    ax = plt.subplot(gs1[0, 0])\n",
    "    ax.plot(x,Exact[25,:], 'b-', linewidth = 2, label = 'Exact')       \n",
    "    ax.plot(x,U_pred[25,:], 'r--', linewidth = 2, label = 'Prediction')\n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.set_ylabel('$u(t,x)$')    \n",
    "    ax.set_title('$t = 0.25$', fontsize = 10)\n",
    "    ax.axis('square')\n",
    "    ax.set_xlim([-1.1,1.1])\n",
    "    ax.set_ylim([-1.1,1.1])\n",
    "    \n",
    "    ax = plt.subplot(gs1[0, 1])\n",
    "    ax.plot(x,Exact[50,:], 'b-', linewidth = 2, label = 'Exact')       \n",
    "    ax.plot(x,U_pred[50,:], 'r--', linewidth = 2, label = 'Prediction')\n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.set_ylabel('$u(t,x)$')\n",
    "    ax.axis('square')\n",
    "    ax.set_xlim([-1.1,1.1])\n",
    "    ax.set_ylim([-1.1,1.1])\n",
    "    ax.set_title('$t = 0.50$', fontsize = 10)\n",
    "    ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.35), ncol=5, frameon=False)\n",
    "    \n",
    "    ax = plt.subplot(gs1[0, 2])\n",
    "    ax.plot(x,Exact[75,:], 'b-', linewidth = 2, label = 'Exact')       \n",
    "    ax.plot(x,U_pred[75,:], 'r--', linewidth = 2, label = 'Prediction')\n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.set_ylabel('$u(t,x)$')\n",
    "    ax.axis('square')\n",
    "    ax.set_xlim([-1.1,1.1])\n",
    "    ax.set_ylim([-1.1,1.1])    \n",
    "    ax.set_title('$t = 0.75$', fontsize = 10)\n",
    "\n",
    "    savefig(\"Burgers\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6b99fc",
   "metadata": {},
   "source": [
    "## PINN-for-a-Boundary-Layer-Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66ea9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Solution of Equation using Central Finite Difference Equation\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-poster')\n",
    "%matplotlib inline\n",
    "matplotlib.rc('font', family='serif', serif='cm10')\n",
    "\n",
    "matplotlib.rc('text', usetex=True)\n",
    "### Number of Gridpoints\n",
    "nu =1.0/10**-3\n",
    "n = 100\n",
    "h = (1+1) / n\n",
    "x = np.linspace(-1,1,n+1)\n",
    "# Difference Operator\n",
    "A = np.zeros((n+1, n+1))\n",
    "\n",
    "## Coefficient For Boundary Condition\n",
    "A[0, 0] = 1\n",
    "A[n, n] = 1\n",
    "\n",
    "### Maric for Interior Point\n",
    "for i in range(1, n):\n",
    "    A[i, i-1] = 1\n",
    "    A[i, i] = -(2 + (nu)*h**2)\n",
    "    A[i, i+1] = 1\n",
    "\n",
    "# Get b\n",
    "b = np.zeros(n+1)\n",
    "b = (np.exp(x))*h*h*(nu)\n",
    "\n",
    "#### Imposition Boundary Condition \n",
    "b[0] = 1\n",
    "b[-1] = 0\n",
    "# solve the linear equations\n",
    "y_act = np.linalg.solve(A, b)\n",
    "### Plot of Equation\n",
    "gs = gridspec.GridSpec(1, 1)\n",
    "ax = plt.subplot(gs[0, 0])\n",
    "ax.plot(x, y_act, \"-\", lw=2.0, color=\"b\")\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel(\"$u$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7065149",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, 'Utilities/')\n",
    "import os\n",
    "\n",
    "from scipy.interpolate import griddata\n",
    "from pyDOE import lhs\n",
    "from plotting import newfig, savefig\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import scipy.io\n",
    "\n",
    "np.random.seed(seed=1234)\n",
    "tf.random.set_seed(1234)\n",
    "tf.config.experimental.enable_tensor_float_32_execution(False)\n",
    "lb = -1\n",
    "ub = 1   \n",
    "\n",
    "# Initalization of Network\n",
    "def hyper_initial(size):\n",
    "    in_dim = size[0]\n",
    "    out_dim = size[1]\n",
    "    std = np.sqrt(2.0/(in_dim + out_dim))\n",
    "    return tf.Variable(tf.random.truncated_normal(shape=size, stddev = std))\n",
    "\n",
    "# Neural Network \n",
    "def DNN(X, W, b):\n",
    "    A = 2.0*(X - lb)/(ub - lb) - 1.0\n",
    "    L = len(W)\n",
    "    for i in range(L-1):\n",
    "        A = tf.tanh(tf.add(tf.matmul(A, W[i]), b[i]))\n",
    "    Y = tf.add(tf.matmul(A, W[-1]), b[-1])\n",
    "    return Y\n",
    "\n",
    "def train_vars(W, b):\n",
    "    return W + b\n",
    "\n",
    "def net_u(x,w, b):\n",
    "    u = DNN(x, w, b)\n",
    "    return u\n",
    "\n",
    "\n",
    "#@tf.function(jit_compile=True)\n",
    "@tf.function\n",
    "def net_f(x,W, b, nu):\n",
    "    with tf.GradientTape(persistent=True) as tape1:\n",
    "        tape1.watch([x])\n",
    "        with tf.GradientTape(persistent=True) as tape2:\n",
    "            tape2.watch([x])\n",
    "            u=net_u(x, W, b)\n",
    "        u_x = tape2.gradient(u, x)\n",
    "        del tape2\n",
    "    u_xx = tape1.gradient(u_x, x)  \n",
    "    del tape1\n",
    "    f = u_xx - (1/nu)*u-(1/nu)*tf.exp(x)\n",
    "    return f\n",
    "\n",
    "\n",
    "\n",
    "#@tf.function(jit_compile=True)\n",
    "@tf.function\n",
    "def train_step(W, b, X_u_train_tf, u_train_tf, X_f_train_tf, opt, nu):\n",
    "    x_u = X_u_train_tf\n",
    "    x_f = X_f_train_tf\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch([W,b])\n",
    "        u_nn = net_u(x_u, W, b) \n",
    "        f_nn = net_f(x_f,W, b, nu)\n",
    "        loss =  100.0*tf.reduce_mean(tf.square(u_nn - u_train_tf)) + tf.reduce_mean(tf.square(f_nn)) \n",
    "    grads = tape.gradient(loss, train_vars(W,b))\n",
    "    opt.apply_gradients(zip(grads, train_vars(W,b)))\n",
    "    return loss\n",
    "\n",
    "def predict(X_star_tf, w, b):\n",
    "    u_pred = net_u(X_star_tf, w, b)\n",
    "    return u_pred\n",
    "    \n",
    "nu = 10**3\n",
    "noise = 0.0        \n",
    "N_f = 300\n",
    "Nmax=3000\n",
    "\n",
    "layers = [1, 4,4,4,4,4,4, 1]\n",
    "L = len(layers)\n",
    "W = [hyper_initial([layers[l-1], layers[l]]) for l in range(1, L)] \n",
    "b = [tf.Variable(tf.zeros([1, layers[l]])) for l in range(1, L)] \n",
    "\n",
    "x_0 = -1\n",
    "x_1 = 1\n",
    "u_0 = 1\n",
    "u_1 = 0\n",
    "\n",
    "X_u_train = np.vstack([x_0, x_1])\n",
    "u_train = np.vstack([u_0, u_1])\n",
    "\n",
    "X_f_train = lb + (ub-lb)*lhs(1, N_f)\n",
    "X_f_star =  np.linspace(-1,1,200)\n",
    "X_f_star = X_f_star.reshape((-1,1))\n",
    "\n",
    "\n",
    "X_u_train_tf = tf.convert_to_tensor(X_u_train, dtype=tf.float32)\n",
    "u_train_tf =   tf.convert_to_tensor(u_train, dtype=tf.float32)\n",
    "X_f_train_tf = tf.convert_to_tensor(X_f_train, dtype=tf.float32)\n",
    "X_star_tf = tf.convert_to_tensor(X_f_star, dtype=tf.float32)\n",
    "\n",
    "\n",
    "lr = 5e-3\n",
    "optimizer = tf.optimizers.Adam(learning_rate=lr)\n",
    "\n",
    "start_time = time.time()\n",
    "n=0\n",
    "loss = []\n",
    "while n <= Nmax:\n",
    "    loss_= train_step(W, b, X_u_train_tf, u_train_tf, X_f_train_tf, optimizer, nu)\n",
    "    loss.append(loss_)    \n",
    "    print(f\"Iteration is: {n} and loss is: {loss_}\")\n",
    "    n+=1\n",
    "\n",
    "elapsed = time.time() - start_time                \n",
    "print('Training time: %.4f' % (elapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a128727",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X_star_tf, w, b):\n",
    "    x_star = X_star_tf\n",
    "    u_pred = net_u(x_star, w, b)\n",
    "    return u_pred\n",
    "\n",
    "u_star = y_act\n",
    "    \n",
    "u_pred = predict(X_star_tf, W, b)\n",
    "error_u = np.linalg.norm(u_star-u_pred,2)/np.linalg.norm(u_star,2)\n",
    "print('Error u: %e' % (error_u))                     \n",
    "\n",
    "\n",
    "\n",
    "####### Row 1: u(t,x) slices ##################    \n",
    "gs1 = gridspec.GridSpec(1, 1)\n",
    "#gs1.update(top=1-1/3, bottom=0, left=0.1, right=0.9, wspace=0.5)\n",
    "\n",
    "ax = plt.subplot(gs1[0, 0])\n",
    "ax.plot(x,y_act, 'b-', linewidth = 2, label = 'Exact')       \n",
    "ax.plot(X_f_star, u_pred, '--r', linewidth = 2, label = 'Prediction')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$u(x)$') \n",
    "plt.legend()\n",
    "#ax.axis('square')\n",
    "ax.set_xlim([-1.1,1.1])\n",
    "ax.set_ylim([-3.1,1.1])\n",
    "plt.show()\n",
    "\n",
    "###############################################\n",
    "loss_list = [loss[i].numpy() for i in range(len(loss))]\n",
    "\n",
    "\n",
    "gs2 = gridspec.GridSpec(1, 1)\n",
    "ax = plt.subplot(gs2[0, 0])\n",
    "\n",
    "ep = np.arange(0,Nmax+1,1)\n",
    "ax.semilogy(ep,loss_list, 'g-', linewidth = 2)       \n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('Loss') \n",
    "plt.legend()\n",
    "#ax.axis('square')\n",
    "##ax.set_xlim([-1.1,1.1])\n",
    "##x.set_ylim([-3.1,1.1])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3b22f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Self Adaptive for residual minimzation\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, 'Utilities/')\n",
    "import os\n",
    "\n",
    "from scipy.interpolate import griddata\n",
    "from pyDOE import lhs\n",
    "from plotting import newfig, savefig\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import scipy.io\n",
    "\n",
    "np.random.seed(seed=1234)\n",
    "tf.random.set_seed(1234)\n",
    "tf.config.experimental.enable_tensor_float_32_execution(False)\n",
    "lb = -1\n",
    "ub = 1   \n",
    "\n",
    "# Initalization of Network\n",
    "def hyper_initial(size):\n",
    "    in_dim = size[0]\n",
    "    out_dim = size[1]\n",
    "    std = np.sqrt(2.0/(in_dim + out_dim))\n",
    "    return tf.Variable(tf.random.truncated_normal(shape=size, stddev = std))\n",
    "\n",
    "# Neural Network \n",
    "def DNN(X, W, b):\n",
    "    A = 2.0*(X - lb)/(ub - lb) - 1.0\n",
    "    L = len(W)\n",
    "    for i in range(L-1):\n",
    "        A = tf.tanh(tf.add(tf.matmul(A, W[i]), b[i]))\n",
    "    Y = tf.add(tf.matmul(A, W[-1]), b[-1])\n",
    "    return Y\n",
    "\n",
    "def train_vars_nn(W, b):\n",
    "    return W + b\n",
    "\n",
    "def train_vars_total(W, b, lambda_r, lambda_b):\n",
    "    return W + b + lambda_r + lambda_b\n",
    "\n",
    "def train_vars_sa(lambda_r, lambda_b):\n",
    "    return lambda_r + lambda_b\n",
    "\n",
    "def net_u(x,w, b):\n",
    "    u = DNN(x, w, b)\n",
    "    return u\n",
    "\n",
    "def loss_weight(N_r, N_b):\n",
    "    alpha_b = tf.Variable(tf.reshape(tf.repeat(1000.0, N_b), (N_b, -1))) \n",
    "    alpha_r = tf.Variable(tf.ones(shape=[N_r, 1]), dtype=tf.float32)\n",
    "    return alpha_r, alpha_b\n",
    "\n",
    "\n",
    "#@tf.function(jit_compile=True)\n",
    "@tf.function\n",
    "def net_f(x, W, b, nu):\n",
    "    with tf.GradientTape(persistent=True) as tape1:\n",
    "        tape1.watch([x])\n",
    "        with tf.GradientTape(persistent=True) as tape2:\n",
    "            tape2.watch([x])\n",
    "            u=net_u(x, W, b)\n",
    "        u_x = tape2.gradient(u, x)\n",
    "        del tape2\n",
    "    u_xx = tape1.gradient(u_x, x)  \n",
    "    del tape1\n",
    "    f = u_xx - (1/nu)*u-(1/nu)*tf.exp(x)\n",
    "    return f\n",
    "\n",
    "@tf.function\n",
    "def net_fd(x, W, b, nu):\n",
    "    h=0.1\n",
    "    u_xx= (1/(h**2))*(net_u(x + h , W, b) - net_u(x, W, b) + net_u(x-h, W, b))\n",
    "    f = u_xx - (1/nu)*u-(1/nu)*tf.exp(x)\n",
    "    return f\n",
    "\n",
    "\n",
    "\n",
    "#@tf.function(jit_compile=True)\n",
    "@tf.function\n",
    "def train_step(W, b, X_u_train_tf, u_train_tf, X_f_train_tf, opt, nu, lambda_r, lambda_b):\n",
    "    x_u = X_u_train_tf\n",
    "    x_f = X_f_train_tf\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        tape.watch([W,b,lambda_r,lambda_b])\n",
    "        u_nn = net_u(x_u, W, b) \n",
    "        f_nn = net_f(x_f,W, b, nu)\n",
    "        loss_r = tf.square(lambda_r*f_nn)\n",
    "        loss_b = tf.square(lambda_b*(u_nn-u_train_tf))\n",
    "        loss =    tf.reduce_mean(loss_b) + tf.reduce_mean(loss_r)  \n",
    "    grads = tape.gradient(loss, train_vars_nn(W, b))\n",
    "    grads_u = tape.gradient(loss, lambda_r)\n",
    "    grads_b = tape.gradient(loss, lambda_b)\n",
    "    opt.apply_gradients(zip(grads, train_vars_nn(W,b)))\n",
    "    opt.apply_gradients(zip([-grads_u], [lambda_r]))\n",
    "    opt.apply_gradients(zip([-grads_b], [lambda_b]))\n",
    "    return loss\n",
    "\n",
    "def predict(X_star_tf, w, b):\n",
    "    u_pred = net_u(X_star_tf, w, b)\n",
    "    return u_pred\n",
    "    \n",
    "nu = 10**-3\n",
    "Nmax= 3000\n",
    "N_f = 500\n",
    "N_b = 2\n",
    "\n",
    "layers = [1, 8, 8, 8,8,8,8, 1]\n",
    "L = len(layers)\n",
    "W = [hyper_initial([layers[l-1], layers[l]]) for l in range(1, L)] \n",
    "b = [tf.Variable(tf.zeros([1, layers[l]])) for l in range(1, L)] \n",
    "\n",
    "alpha_r, alpha_b = loss_weight(N_f, N_b)\n",
    "\n",
    "x_0 = -1\n",
    "x_1 = 1\n",
    "u_0 = 1\n",
    "u_1 = 0\n",
    "\n",
    "X_u_train = np.vstack([x_0, x_1])\n",
    "u_train = np.vstack([u_0, u_1])\n",
    "\n",
    "X_f_train = lb + (ub-lb)*lhs(1, N_f)\n",
    "X_f_star =  np.linspace(-1,1,200)\n",
    "X_f_star = X_f_star.reshape((-1,1))\n",
    "\n",
    "\n",
    "X_u_train_tf = tf.convert_to_tensor(X_u_train, dtype=tf.float32)\n",
    "u_train_tf =   tf.convert_to_tensor(u_train, dtype=tf.float32)\n",
    "X_f_train_tf = tf.convert_to_tensor(X_f_train, dtype=tf.float32)\n",
    "X_star_tf = tf.convert_to_tensor(X_f_star, dtype=tf.float32)\n",
    "\n",
    "\n",
    "lr = 1e-3\n",
    "optimizer = tf.optimizers.Adam(learning_rate=lr)\n",
    "\n",
    "start_time = time.time()\n",
    "n=0\n",
    "loss = []\n",
    "while n <= Nmax:\n",
    "    loss_= train_step(W, b, X_u_train_tf, u_train_tf, X_f_train_tf, optimizer, nu, alpha_r, alpha_b)\n",
    "    loss.append(loss_)    \n",
    "    print(f\"Iteration is: {n} and loss is: {loss_}\")\n",
    "    n+=1\n",
    "\n",
    "elapsed = time.time() - start_time                \n",
    "print('Training time: %.4f' % (elapsed))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189805db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X_star_tf, w, b):\n",
    "    x_star = X_star_tf\n",
    "    u_pred = net_u(x_star, w, b)\n",
    "    return u_pred\n",
    "\n",
    "u_star = y_act\n",
    "    \n",
    "u_pred = predict(X_star_tf, W, b)\n",
    "error_u = np.linalg.norm(u_star-u_pred,2)/np.linalg.norm(u_star,2)\n",
    "print('Error u: %e' % (error_u))                     \n",
    "\n",
    "\n",
    "\n",
    "####### Row 1: u(t,x) slices ##################    \n",
    "gs1 = gridspec.GridSpec(1, 1)\n",
    "#gs1.update(top=1-1/3, bottom=0, left=0.1, right=0.9, wspace=0.5)\n",
    "\n",
    "ax = plt.subplot(gs1[0, 0])\n",
    "ax.plot(x,y_act, 'b-', linewidth = 2, label = 'Exact')       \n",
    "ax.plot(X_f_star, u_pred, '--r', linewidth = 2, label = 'Prediction')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$u(x)$') \n",
    "plt.legend()\n",
    "#ax.axis('square')\n",
    "ax.set_xlim([-1.1,1.1])\n",
    "ax.set_ylim([-3.1,1.1])\n",
    "plt.show()\n",
    "\n",
    "###############################################\n",
    "loss_list = [loss[i].numpy() for i in range(len(loss))]\n",
    "\n",
    "\n",
    "gs2 = gridspec.GridSpec(1, 1)\n",
    "ax = plt.subplot(gs2[0, 0])\n",
    "\n",
    "ep = np.arange(0,Nmax+1,1)\n",
    "ax.semilogy(ep,loss_list/(np.max(loss_list)), 'g-', linewidth = 2)       \n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('Loss') \n",
    "plt.legend()\n",
    "#ax.axis('square')\n",
    "##ax.set_xlim([-1.1,1.1])\n",
    "##x.set_ylim([-3.1,1.1])\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13e8349",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, 'Utilities/')\n",
    "import os\n",
    "\n",
    "from scipy.interpolate import griddata\n",
    "from pyDOE import lhs\n",
    "from plotting import newfig, savefig\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import scipy.io\n",
    "\n",
    "np.random.seed(seed=1234)\n",
    "tf.random.set_seed(1234)\n",
    "tf.config.experimental.enable_tensor_float_32_execution(False)\n",
    "lb = -1\n",
    "ub = 1   \n",
    "\n",
    "# Initalization of Network\n",
    "def hyper_initial(size):\n",
    "    in_dim = size[0]\n",
    "    out_dim = size[1]\n",
    "    std = np.sqrt(2.0/(in_dim + out_dim))\n",
    "    return tf.Variable(tf.random.truncated_normal(shape=size, stddev = std))\n",
    "\n",
    "# Neural Network \n",
    "def DNN(X, W, b):\n",
    "    A = 2.0*(X - lb)/(ub - lb) - 1.0\n",
    "    L = len(W)\n",
    "    for i in range(L-1):\n",
    "        A = tf.tanh(tf.add(tf.matmul(A, W[i]), b[i]))\n",
    "    Y = tf.add(tf.matmul(A, W[-1]), b[-1])\n",
    "    return Y\n",
    "\n",
    "def train_vars(W, b):\n",
    "    return W + b\n",
    "\n",
    "def net_u(x,w, b):\n",
    "    u = DNN(x, w, b)\n",
    "    return u\n",
    "\n",
    "\n",
    "#@tf.function(jit_compile=True)\n",
    "@tf.function\n",
    "def net_f(x,W, b, nu):\n",
    "    with tf.GradientTape(persistent=True) as tape1:\n",
    "        tape1.watch([x])\n",
    "        with tf.GradientTape(persistent=True) as tape2:\n",
    "            tape2.watch([x])\n",
    "            u=net_u(x, W, b)\n",
    "        u_x = tape2.gradient(u, x)\n",
    "        del tape2\n",
    "    u_xx = tape1.gradient(u_x, x)  \n",
    "    del tape1\n",
    "    f = u_xx - (1/nu)*u-(1/nu)*tf.exp(x)\n",
    "    return f\n",
    "\n",
    "@tf.function\n",
    "def net_f(x,W, b, nu):\n",
    "    with tf.GradientTape(persistent=True) as tape1:\n",
    "        tape1.watch([x])\n",
    "        with tf.GradientTape(persistent=True) as tape2:\n",
    "            tape2.watch([x])\n",
    "            u=net_u(x, W, b)\n",
    "        u_x = tape2.gradient(u, x)\n",
    "        del tape2\n",
    "    u_xx = tape1.gradient(u_x, x)  \n",
    "    del tape1\n",
    "    f = u_xx - (1/nu)*u-(1/nu)*tf.exp(x)\n",
    "    return f\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#@tf.function(jit_compile=True)\n",
    "@tf.function\n",
    "def train_step(layers, W, b, X_u_train_tf, u_train_tf, X_f_train_tf, opt, nu, lambda_b, beta):\n",
    "    x_u = X_u_train_tf[:,0:1]\n",
    "    x_f = X_f_train_tf[:,0:1]\n",
    "    adpative_constant_bcs_list = []\n",
    "    lambda_b_list = []\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        tape.watch([W,b])\n",
    "        u_nn = net_u(x_u,W, b) \n",
    "        f_nn = net_f(x_f,W, b, nu)\n",
    "        bc_loss =  tf.reduce_mean(tf.square(u_nn - u_train_tf))\n",
    "        phys_loss = tf.reduce_mean(tf.square(f_nn)) \n",
    "        loss = lambda_b[-1]*bc_loss  + phys_loss\n",
    "        loss = loss / (1 + lambda_b[-1])\n",
    "        p_loss = phys_loss/(1 + lambda_b[-1])\n",
    "        b_loss = lambda_b[-1]*bc_loss/(1 + lambda_b[-1])\n",
    "    grad_loss = tape.gradient(loss, train_vars(W,b))\n",
    "    opt.apply_gradients(zip(grad_loss, train_vars(W,b)))\n",
    "    grads_bc = tape.gradient(bc_loss, train_vars(W,b))\n",
    "    grads_phys = tape.gradient(phys_loss, train_vars(W,b))\n",
    "    for i in range(len(layers) - 1):\n",
    "        adpative_constant_bcs_list.append(\n",
    "                tf.reduce_mean(tf.abs(grads_phys[i])) / tf.reduce_mean(tf.abs(grads_bc[i])))\n",
    "        \n",
    "    lambda_b_new = tf.reduce_mean(tf.stack(adpative_constant_bcs_list))\n",
    "    lambda_b_new = (1-beta)*lambda_b[-1] + beta*lambda_b_new\n",
    "    lambda_b_list.append(lambda_b_new)\n",
    "                                      \n",
    "    return loss, lambda_b_list, p_loss, b_loss\n",
    "\n",
    "\n",
    "def predict(X_star_tf, w, b):\n",
    "    u_pred = net_u(X_star_tf, w, b)\n",
    "    return u_pred\n",
    "    \n",
    "nu = 10**-3\n",
    "noise = 0.0        \n",
    "N_f = 500\n",
    "Nmax=50000\n",
    "\n",
    "layers = [1, 8,8,8,8,8,8, 1]\n",
    "L = len(layers)\n",
    "W = [hyper_initial([layers[l-1], layers[l]]) for l in range(1, L)] \n",
    "b = [tf.Variable(tf.zeros([1, layers[l]])) for l in range(1, L)] \n",
    "\n",
    "x_0 = -1\n",
    "x_1 = 1\n",
    "u_0 = 1\n",
    "u_1 = 0\n",
    "\n",
    "X_u_train = np.vstack([x_0, x_1])\n",
    "u_train = np.vstack([u_0, u_1])\n",
    "X_f_train = lb + (ub-lb)*lhs(1, N_f)\n",
    "X_f_star =  np.linspace(-1,1,200)\n",
    "X_f_star = X_f_star.reshape((-1,1))\n",
    "X_u_train_tf = tf.convert_to_tensor(X_u_train, dtype=tf.float32)\n",
    "u_train_tf =   tf.convert_to_tensor(u_train, dtype=tf.float32)\n",
    "X_f_train_tf = tf.convert_to_tensor(X_f_train, dtype=tf.float32)\n",
    "X_star_tf = tf.convert_to_tensor(X_f_star, dtype=tf.float32)\n",
    "lam_b = np.array([1.0])\n",
    "lam_b_tf= tf.convert_to_tensor(lam_b, dtype=tf.float32)                                \n",
    "lambda_b_list = [lam_b_tf]                               \n",
    "lr = 1e-3\n",
    "optimizer = tf.optimizers.Adam(learning_rate=lr)\n",
    "start_time = time.time()\n",
    "n=0\n",
    "loss = []\n",
    "bc_loss = []\n",
    "phys_loss = []\n",
    "beta = 0.1\n",
    "\n",
    "while n <= Nmax:\n",
    "    loss_, lambda_b_out, phys_loss_, bc_loss_ = train_step(layers, W, b, X_u_train_tf,\\\n",
    "                                                u_train_tf, X_f_train_tf, optimizer, nu,lambda_b_list, beta)\n",
    "    lambda_b_list = lambda_b_out\n",
    "    loss.append(loss_) \n",
    "    bc_loss.append(bc_loss_)\n",
    "    phys_loss.append(phys_loss_)\n",
    "    \n",
    "    print(f\"Iteration is: {n} and loss is: {loss_}\")\n",
    "    n+=1\n",
    "\n",
    "elapsed = time.time() - start_time                \n",
    "print('Training time: %.4f' % (elapsed))               \n",
    "print('Training time: %.4f' % (elapsed))\n",
    "\n",
    "\n",
    "def predict(X_star_tf, w, b):\n",
    "    x_star = X_star_tf\n",
    "    u_pred = net_u(x_star, w, b)\n",
    "    return u_pred\n",
    "\n",
    "u_star = y_act\n",
    "    \n",
    "u_pred = predict(X_star_tf, W, b)\n",
    "error_u = np.linalg.norm(u_star-u_pred,2)/np.linalg.norm(u_star,2)\n",
    "print('Error u: %e' % (error_u))                     \n",
    "\n",
    "\n",
    "\n",
    "####### Row 1: u(t,x) slices ##################    \n",
    "gs1 = gridspec.GridSpec(1, 1)\n",
    "#gs1.update(top=1-1/3, bottom=0, left=0.1, right=0.9, wspace=0.5)\n",
    "\n",
    "ax = plt.subplot(gs1[0, 0])\n",
    "ax.plot(x,y_act, 'b-', linewidth = 2, label = 'Exact')       \n",
    "ax.plot(X_f_star, u_pred, '--r', linewidth = 2, label = 'Prediction')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$u(x)$') \n",
    "plt.legend()\n",
    "#ax.axis('square')\n",
    "ax.set_xlim([-1.1,1.1])\n",
    "ax.set_ylim([-3.1,1.1])\n",
    "plt.show()\n",
    "\n",
    "###############################################\n",
    "loss_list = [loss[i].numpy() for i in range(len(loss))]\n",
    "bc_loss_list = [bc_loss[i].numpy() for i in range(len(bc_loss))]\n",
    "phys_loss_list = [phys_loss[i].numpy() for i in range(len(phys_loss))]\n",
    "\n",
    "\n",
    "gs2 = gridspec.GridSpec(1, 1)\n",
    "ax = plt.subplot(gs2[0, 0])\n",
    "\n",
    "ep = np.arange(0,Nmax+1,1)\n",
    "ax.semilogy(ep,bc_loss_list, 'g-', linewidth = 2, label=\"Boundary Loss\")   \n",
    "ax.semilogy(ep,phys_loss_list, 'r-', linewidth = 2, label=\"Residual Loss\")\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('Loss') \n",
    "plt.legend()\n",
    "#ax.axis('square')\n",
    "##ax.set_xlim([-1.1,1.1])\n",
    "##x.set_ylim([-3.1,1.1])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907589ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X_star_tf, w, b):\n",
    "    x_star = X_star_tf\n",
    "    u_pred = net_u(x_star, w, b)\n",
    "    return u_pred\n",
    "\n",
    "u_star = y_act\n",
    "    \n",
    "u_pred = predict(X_star_tf, W, b)\n",
    "error_u = np.linalg.norm(u_star-u_pred,2)/np.linalg.norm(u_star,2)\n",
    "print('Error u: %e' % (error_u))                     \n",
    "\n",
    "\n",
    "\n",
    "####### Row 1: u(t,x) slices ##################    \n",
    "gs1 = gridspec.GridSpec(1, 1)\n",
    "#gs1.update(top=1-1/3, bottom=0, left=0.1, right=0.9, wspace=0.5)\n",
    "\n",
    "ax = plt.subplot(gs1[0, 0])\n",
    "ax.plot(x,y_act, 'b-', linewidth = 2, label = 'Exact')       \n",
    "ax.plot(X_f_star, u_pred, '--r', linewidth = 2, label = 'Prediction')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$u(x)$') \n",
    "plt.legend()\n",
    "#ax.axis('square')\n",
    "ax.set_xlim([-1.1,1.1])\n",
    "ax.set_ylim([-3.1,1.1])\n",
    "plt.show()\n",
    "\n",
    "###############################################\n",
    "loss_list = [loss[i].numpy() for i in range(len(loss))]\n",
    "bc_loss_list = [bc_loss[i].numpy() for i in range(len(bc_loss))]\n",
    "phys_loss_list = [phys_loss[i].numpy() for i in range(len(phys_loss))]\n",
    "\n",
    "\n",
    "gs2 = gridspec.GridSpec(1, 1)\n",
    "ax = plt.subplot(gs2[0, 0])\n",
    "\n",
    "ep = np.arange(0,Nmax+1,1)\n",
    "ax.semilogy(ep, bc_loss_list, 'g-', linewidth = 2, label=\"Boundary Loss\")   \n",
    "ax.semilogy(ep,phys_loss_list, 'r-', linewidth = 2, label=\"Residual Loss\")\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('Loss') \n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b2f706",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/jnh277/Linearly-Constrained-NN/blob/release/simulted_divergence_free.py\n",
    "\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils import data\n",
    "import numpy as np\n",
    "import torch.autograd as ag\n",
    "import torch.nn as nn\n",
    "torch.manual_seed(1234)\n",
    "epochs = 400\n",
    "n_data = 200\n",
    "### Input Data\n",
    "def vector_field(x, y, a=0.01):\n",
    "    v1 = torch.exp(-a*x*y)*(a*x*torch.sin(x*y) - x*torch.cos(x*y))\n",
    "    v2 = torch.exp(-a*x*y)*(y*torch.cos(x*y) - a*y*torch.sin(x*y))\n",
    "    return (v1, v2)\n",
    "\n",
    "\n",
    "## ------------------ set up models-------------------------- ##\n",
    "# set network size\n",
    "n_in = 2\n",
    "n_h1 = 100\n",
    "n_h2 = 50\n",
    "n_o = 1\n",
    "\n",
    "# two outputs for the unconstrained network\n",
    "n_o_uc = 2\n",
    "\n",
    "# define model class\n",
    "class DivFree2D(torch.nn.Module):\n",
    "    def __init__(self, base_net):\n",
    "        super(DivFree2D, self).__init__()\n",
    "        self.base_net = base_net\n",
    "\n",
    "    def forward(self, x):\n",
    "        x.requires_grad = True\n",
    "        y = self.base_net(x)\n",
    "        dydx = ag.grad(outputs=y, inputs=x, create_graph=True, grad_outputs=torch.ones(y.size()),\n",
    "                       retain_graph=True, only_inputs=True)[0]\n",
    "        return y, dydx[:,1].unsqueeze(1), -dydx[:,0].unsqueeze(1)\n",
    "\n",
    "model = DivFree2D(nn.Sequential(nn.Linear(n_in,n_h1),nn.Tanh(),nn.Linear(n_h1,n_h2),\n",
    "                                         nn.Tanh(),nn.Linear(n_h2,n_o)))\n",
    "\n",
    "\n",
    "model_uc = torch.nn.Sequential(\n",
    "    torch.nn.Linear(n_in, n_h1),\n",
    "    torch.nn.Tanh(),\n",
    "    torch.nn.Linear(n_h1, n_h2),\n",
    "    torch.nn.Tanh(),\n",
    "    torch.nn.Linear(n_h2, n_o_uc),\n",
    ")\n",
    "\n",
    "\n",
    "# pregenerate validation data\n",
    "x_val = 4.0 * torch.rand(2000, 2)\n",
    "x1_val = x_val[:, 0].unsqueeze(1)\n",
    "x2_val = x_val[:, 1].unsqueeze(1)\n",
    "\n",
    "(v1, v2) = vector_field(x1_val, x2_val)\n",
    "y1_val = v1 + 0.1 * torch.randn(x1_val.size())\n",
    "y2_val = v2 + 0.1 * torch.randn(x1_val.size())\n",
    "y_val = torch.cat((y1_val, y2_val), 1)\n",
    "\n",
    "\n",
    "# Get the true function values on a grid\n",
    "xv, yv = torch.meshgrid([torch.arange(0.0, 20.0) * 4.0 / 20.0, torch.arange(0.0, 20.0) * 4.0 / 20.0])\n",
    "(v1, v2) = vector_field(xv, yv)\n",
    "\n",
    "# generate training data\n",
    "x_train = 4.0 * torch.rand(n_data, 2)\n",
    "x1_train = x_train[:, 0].unsqueeze(1)\n",
    "x2_train = x_train[:, 1].unsqueeze(1)\n",
    "\n",
    "(v1_t, v2_t) = vector_field(x1_train, x2_train)\n",
    "y1_train = v1_t + 0.1 * torch.randn(x1_train.size())\n",
    "y2_train = v2_t + 0.1 * torch.randn(x1_train.size())\n",
    "\n",
    "\n",
    "# define Dataset class\n",
    "class Dataset(data.Dataset):\n",
    "    'Characterizes a dataset for PyTorch'\n",
    "\n",
    "    def __init__(self, x1, x2, y1, y2):\n",
    "        'Initialization'\n",
    "        self.x1 = x1\n",
    "        self.x2 = x2\n",
    "        self.y1 = y1\n",
    "        self.y2 = y2\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.x1)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        x1 = self.x1[index]\n",
    "        x2 = self.x2[index]\n",
    "        y1 = self.y1[index]\n",
    "        y2 = self.y2[index]\n",
    "\n",
    "        return x1, x2, y1, y2\n",
    "\n",
    "training_set = Dataset(x1_train, x2_train, y1_train, y2_train)\n",
    "\n",
    "# data loader Parameters\n",
    "DL_params = {'batch_size': 100,\n",
    "             'shuffle': True,\n",
    "             'num_workers': 0,\n",
    "             'pin_memory': False}\n",
    "training_generator = data.DataLoader(training_set, **DL_params)\n",
    "\n",
    "\n",
    "# ---------------  Set up and train the constrained model -------------------------------\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)  \n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau\\\n",
    "                                    (optimizer, patience=10,\n",
    "                                     min_lr=1e-10,\n",
    "                                     factor=0.5, cooldown=15)\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    n_batches = 0\n",
    "    for x1_train, x2_train, y1_train, y2_train in training_generator:\n",
    "        optimizer.zero_grad()\n",
    "        x_train = torch.cat((x1_train, x2_train), 1)\n",
    "        (yhat, v1hat, v2hat) = model(x_train)\n",
    "        loss = (criterion(y1_train, v1hat) + criterion(y2_train, v2hat)) / 2  # divide by 2 as it is a mean\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss\n",
    "        n_batches += 1\n",
    "    return total_loss / n_batches\n",
    "\n",
    "def eval(epoch):\n",
    "    model.eval()\n",
    "    # with torch.no_grad():\n",
    "    (yhat, v1hat, v2hat) = model(x_val)\n",
    "    loss = (criterion(y1_val, v1hat) + criterion(y2_val, v2hat)) / 2\n",
    "    return loss.cpu()\n",
    "\n",
    "\n",
    "train_loss = np.empty([epochs, 1])\n",
    "val_loss = np.empty([epochs, 1])\n",
    "\n",
    "print('Training Constrained NN')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss[epoch] = train(epoch).detach().numpy()\n",
    "    v_loss = eval(epoch)\n",
    "    scheduler.step(v_loss)\n",
    "    val_loss[epoch] = v_loss.detach().numpy()\n",
    "    print('Constrained NN: epoch: ', epoch, 'training loss ', train_loss[epoch], \\\n",
    "          'validation loss', val_loss[epoch])\n",
    "\n",
    "\n",
    "# work out the rms error for this one\n",
    "x_pred = torch.cat((xv.reshape(20 * 20, 1), yv.reshape(20 * 20, 1)), 1)\n",
    "(f_pred, v1_pred, v2_pred) = model(x_pred)\n",
    "error_new = torch.cat((v1.reshape(400, 1) - v1_pred.detach(), v2.reshape(400, 1) - v2_pred.detach()), 0)\n",
    "rms_error = torch.sqrt(sum(error_new * error_new) / 800)\n",
    "\n",
    "# ---------------  Set up and train the uncconstrained model -------------------------------\n",
    "optimizer_uc = torch.optim.Adam(model_uc.parameters(), lr=0.01)\n",
    "scheduler_uc = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_uc, patience=10,\n",
    "                                                     min_lr=1e-10,\n",
    "                                                    factor=0.5,\n",
    "                                                    cooldown=15)\n",
    "\n",
    "def train_uc(epoch):\n",
    "    model_uc.train()\n",
    "    total_loss = 0\n",
    "    n_batches = 0\n",
    "    for x1_train, x2_train, y1_train, y2_train in training_generator:\n",
    "        optimizer_uc.zero_grad()\n",
    "        x_train = torch.cat((x1_train, x2_train), 1)\n",
    "        vhat = model_uc(x_train)\n",
    "        y_train = torch.cat((y1_train, y2_train), 1)\n",
    "        loss = criterion(y_train, vhat)\n",
    "        loss.backward()\n",
    "        optimizer_uc.step()\n",
    "        total_loss += loss.cpu()\n",
    "        n_batches += 1\n",
    "    return total_loss / n_batches\n",
    "\n",
    "def eval_uc(epoch):\n",
    "    model_uc.eval()\n",
    "    with torch.no_grad():\n",
    "        (vhat) = model_uc(x_val)\n",
    "        loss = criterion(y_val, vhat)\n",
    "    return loss.cpu()\n",
    "\n",
    "\n",
    "train_loss_uc = np.empty([epochs, 1])\n",
    "val_loss_uc = np.empty([epochs, 1])\n",
    "\n",
    "\n",
    "print('Training standard NN')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss_uc[epoch] = train_uc(epoch).detach().numpy()\n",
    "    v_loss = eval_uc(epoch)\n",
    "    scheduler_uc.step(v_loss)\n",
    "    val_loss_uc[epoch] = v_loss.detach().numpy()\n",
    "    print('Standard NN: epoch: ', epoch, 'training loss ', \\\n",
    "          train_loss_uc[epoch], 'validation loss', val_loss_uc[epoch])\n",
    "\n",
    "\n",
    "# work out final rms error for unconstrainted net\n",
    "(v_pred_uc) = model_uc(x_pred)\n",
    "v1_pred_uc = v_pred_uc[:, 0]\n",
    "v2_pred_uc = v_pred_uc[:, 1]\n",
    "\n",
    "error_uc = torch.cat((v1.reshape(400) - v1_pred_uc.detach(), v2.reshape(400) - v2_pred_uc.detach()), 0)\n",
    "rms_uc = torch.sqrt(sum(error_uc * error_uc) / 800)\n",
    "\n",
    "\n",
    "\n",
    "print('Finished')\n",
    "print(\"Final RMSE for constrained neural network: \",rms_error.item())\n",
    "print(\"Final RMSE for standard neural network: \",rms_uc.item())\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Initialize plot\n",
    "    f, ax = plt.subplots(2, 2, figsize=(20, 16))\n",
    "    # ax.pcolor(xv,yv,f_scalar)\n",
    "    ax[0, 0].quiver(xv, yv, v1, v2)\n",
    "    ax[0, 0].quiver(xv, yv, v1_pred.reshape(20, 20).detach(), v2_pred.reshape(20, 20).detach(), color='r')\n",
    "    ax[0, 0].legend(['true', 'predicted'], fontsize=22)\n",
    "    ax[0, 0].set_title('Constrained NN ', fontsize=22)\n",
    "    ax[0, 0].tick_params(axis='x', labelsize=14 )\n",
    "    ax[0, 0].tick_params(axis='y', labelsize=14 )\n",
    "\n",
    "\n",
    "    ax[1, 0].plot(np.log(train_loss),lw=3.0)\n",
    "    ax[1, 0].plot(np.log(val_loss), lw=3.0)\n",
    "    # ax[1].plot(loss_save[1:epoch].log().detach().numpy())\n",
    "    ax[1, 0].set_xlabel('training epoch',  fontsize=22)\n",
    "    ax[1, 0].set_ylabel('Loss',  fontsize=22)\n",
    "    ax[1, 0].legend(['training loss', 'val loss'], fontsize=22)\n",
    "    ax[1, 0].tick_params(axis='x', labelsize=14 )\n",
    "    ax[1, 0].tick_params(axis='y', labelsize=14 )\n",
    "\n",
    "    ax[0, 1].quiver(xv, yv, v1, v2)\n",
    "    ax[0, 1].quiver(xv, yv, v1_pred_uc.reshape(20, 20).detach(), v2_pred_uc.reshape(20, 20).detach(), color='r')\n",
    "    ax[0, 1].legend(['true', 'predicted'], fontsize=22)\n",
    "    ax[0, 1].set_title('Standard NN ',fontsize=22)\n",
    "    ax[0, 1].tick_params(axis='x', labelsize=14 )\n",
    "    ax[0,1].tick_params(axis='y', labelsize=14 )\n",
    "\n",
    "    ax[1, 1].plot(np.log(train_loss_uc),lw=3.0)\n",
    "    ax[1, 1].plot(np.log(val_loss_uc),lw=3.0)\n",
    "    ax[1, 1].set_ylabel('Loss', fontsize=22)\n",
    "    ax[1, 1].set_xlabel('training epoch', fontsize=22)\n",
    "    ax[1, 1].legend(['training loss','val loss'], fontsize=22)\n",
    "    ax[1, 1].tick_params(axis='x', labelsize=14)\n",
    "    ax[1,1].tick_params(axis='y', labelsize=14 )\n",
    "    plt.savefig(\"Figure_1.png\", dpi=300)\n",
    "    plt.show()\n",
    "   \n",
    "    \n",
    "\n",
    "    # Initialize second plot\n",
    "    f2, ax2 = plt.subplots(1, 3, figsize=(48, 10))\n",
    "    Q = ax2[0].quiver(xv, yv, v1, v2, scale=None, scale_units='inches')\n",
    "    Q._init()\n",
    "    assert isinstance(Q.scale, float)\n",
    "    ax2[0].quiver(x1_train, x2_train, y1_train, y2_train, scale=Q.scale, scale_units='inches', color='r')\n",
    "    ax2[0].set_xlabel('$x_1$')\n",
    "    ax2[0].set_ylabel('$x_2$')\n",
    "    ax2[0].tick_params(axis='x', labelsize=36)\n",
    "    ax2[0].tick_params(axis='y', labelsize=36)\n",
    "    \n",
    "\n",
    "\n",
    "    error_new = torch.cat((v1.reshape(400, 1) - v1_pred.detach(), v2.reshape(400, 1) - v2_pred.detach()), 0)\n",
    "    rms_new = torch.sqrt(sum(error_new * error_new) / 800)\n",
    "\n",
    "    ax2[1].quiver(xv, yv, v1 - v1_pred.reshape(20, 20).detach(), v2 - v2_pred.reshape(20, 20).detach(),\n",
    "                  scale=Q.scale, scale_units='inches')\n",
    "    ax2[1].set_xlabel('$x_1$')\n",
    "    ax2[1].set_ylabel('$x_2$')\n",
    "    ax2[1].set_title('Constrained Approach RMSE={0:.2f}'.format(rms_new.item()), fontsize=36)\n",
    "    ax2[1].tick_params(axis='x', labelsize=36 )\n",
    "    ax2[1].tick_params(axis='y', labelsize=36 )\n",
    "    \n",
    "\n",
    "\n",
    "    error_uc = torch.cat((v1.reshape(400) - v1_pred_uc.detach(), v2.reshape(400) - v2_pred_uc.detach()), 0)\n",
    "    rms_uc = torch.sqrt(sum(error_uc * error_uc) / 800)\n",
    "\n",
    "    ax2[2].quiver(xv, yv, v1 - v1_pred_uc.reshape(20, 20).detach(), v2 - v2_pred_uc.reshape(20, 20).detach(),\n",
    "                  scale=Q.scale, scale_units='inches')\n",
    "    ax2[2].set_xlabel('$x_1$')\n",
    "    ax2[2].set_ylabel('$x_2$')\n",
    "    ax2[2].set_title('Unconstrained NN RMSE={0:.2f}'.format(rms_uc.item()), fontsize=36)\n",
    "    ax2[2].tick_params(axis='x', labelsize=36 )\n",
    "    ax2[2].tick_params(axis='y', labelsize=36 )\n",
    "    plt.savefig(\"Figure_2.png\", dpi=300)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97dbec4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
